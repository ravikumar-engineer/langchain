{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb444bb3",
   "metadata": {},
   "source": [
    "## **Data Transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40bf733",
   "metadata": {},
   "source": [
    "### **1. Using Recursive Character Text Splitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24c73b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 0, 'page_label': '1'}, page_content='Deep Learning\\nwith Python\\nFRANÇOIS CHOLLET\\nMANNING\\nSHELTER ISLAND'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 1, 'page_label': '2'}, page_content='ISBN 9781617294433\\nPrinted in the United States of America\\n©2018 by Manning Publications Co.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 2, 'page_label': '3'}, page_content='brief contents\\nPART 1F UNDAMENTALS OF DEEP LEARNING .................................. 1\\n1 ■ What is deep learning? 3\\n2 ■ Before we begin: the mathematical building blocks of neural \\nnetworks 25\\n3 ■ Getting started with neural networks 56\\n4 ■ Fundamentals of machine learning 93\\nPART 2D EEP LEARNING IN PRACTICE ........................................ 117\\n5 ■ Deep learning for computer vision 119\\n6 ■ Deep learning for text and sequences 178\\n7 ■ Advanced deep-learning best practices 233\\n8 ■ Generative deep learning 269\\n9 ■ Conclusions 314'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 3, 'page_label': '4'}, page_content='contents\\npreface xiii\\nacknowledgments xv\\nabout this book xvi\\nabout the author xx\\nabout the cover xxi\\nPART 1F UNDAMENTALS OF DEEP LEARNING ...................1\\n1 What is deep learning? 3\\n1.1 Artificial intelligence, machine learning, \\nand deep learning 4\\nArtificial intelligence 4 ■ Machine learning 4 ■ Learning \\nrepresentations from data 6 ■ The “deep” in deep learning 8\\nUnderstanding how deep learning works, in three figures 9\\nWhat deep learning has achieved so far 11 ■ Don’t believe \\nthe short-term hype 12 ■ The promise of AI 13\\n1.2 Before deep learning: a brief history of machine \\nlearning 14\\nProbabilistic modeling 14 ■ Early neural networks 14\\nKernel methods 15 ■ Decision trees, random forests, \\nand gradient boosting machines 16 ■ Back to neural \\nnetworks 17 ■ What makes deep learning different 17\\nThe modern machine-learning landscape 18'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 4, 'page_label': '5'}, page_content='1.3 Why deep learning? Why now? 20\\nHardware 20 ■ Data 21 ■ Algorithms 21 ■ A new \\nwave of investment 22 ■ The democratization of deep \\nlearning 23 ■ Will it last? 23\\n2 Before we begin: the mathematical building blocks of \\nneural networks 25\\n2.1 A first look at a neural network 27\\n2.2 Data representations for neural networks 31\\nScalars (0D tensors) 31 ■ Vectors (1D tensors) 31\\nMatrices (2D tensors) 31 ■ 3D tensors and higher-\\ndimensional tensors 32 ■ Key attributes 32\\nManipulating tensors in Numpy 34 ■ The notion \\nof data batches 34 ■ Real-world examples of data \\ntensors 35 ■ Vector data 35 ■ Timeseries data or \\nsequence data 35 ■ Image data 36 ■ Video data 37\\n2.3 The gears of neural ne tworks: tensor operations 38\\nElement-wise operations 38 ■ Broadcasting 39 ■ Tensor \\ndot 40 ■ Tensor reshaping 42 ■ Geometric interpretation \\nof tensor operations 43 ■ A geometric interpretation of deep \\nlearning 44\\n2.4 The engine of neural networks: gradient-based \\noptimization 46\\nWhat’s a derivative? 47 ■ Derivative of a tensor operation: \\nthe gradient 48 ■ Stochastic gradient descent 48\\nChaining derivatives: the Backpropagation algorithm 51\\n2.5 Looking back at our first example 53\\n2.6 Chapter summary 55\\n3 Getting started with neural networks 56\\n3.1 Anatomy of a neural network 58\\nLayers: the building blocks of deep learning 58 ■ Models: \\nnetworks of layers 59 ■ Loss functions and optimizers: keys \\nto configuring the learning process 60\\n3.2 Introduction to Keras 61\\nKeras, TensorFlow, Theano, and CNTK 62 ■ Developing \\nwith Keras: a quick overview 62\\n3.3 Setting up a deep-learning workstation 65\\nJupyter notebooks: the preferred way to run deep-learning \\nexperiments 65 ■ Getting Keras running: two options 66'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 5, 'page_label': '6'}, page_content='Running deep-learning jobs in the cloud: pros and cons 66\\nWhat is the best GPU for deep learning? 66\\n3.4 Classifying movie reviews:  a binary classification \\nexample 68\\nThe IMDB dataset 68 ■ Preparing the data 69\\nBuilding your network 70 ■ Validating your approach 73\\nUsing a trained network to generate predictions on new \\ndata 76 ■ Further experiments 77 ■ Wrapping up 77\\n3.5 Classifying newswires: a multiclass classification \\nexample 78\\nThe Reuters dataset 78 ■ Preparing the data 79\\nBuilding your network 79 ■ Validating your approach 80\\nGenerating predictions on new data 83 ■ A different way to \\nhandle the labels and the loss 83 ■ The importance of \\nhaving sufficiently large intermediate layers 83 ■ Further \\nexperiments 84 ■ Wrapping up 84\\n3.6 Predicting house prices : a regression example 85\\nThe Boston Housing Price dataset 85 ■ Preparing the \\ndata 86 ■ Building your network 86 ■ Validating \\nyour approach using K-fold validation 87 ■ Wrapping up 91\\n3.7 Chapter summary 92\\n4 Fundamentals of machine learning 93\\n4.1 Four branches of machine learning 94\\nSupervised learning 94 ■ Unsupervised learning 94\\nSelf-supervised learning 94 ■ Reinforcement learning 95\\n4.2 Evaluating machine-learning models 97\\nTraining, validation, and test sets 97 ■ Things to \\nkeep in mind 100\\n4.3 Data preprocessing, feature engineering, \\nand feature learning 101\\nData preprocessing for neural networks 101 ■ Feature \\nengineering 102\\n4.4 Overfitting and underfitting 104\\nReducing the network’s size 104 ■ Adding weight \\nregularization 107 ■ Adding dropout 109\\n4.5 The universal workflow of machine learning 111\\nDefining the problem and assembling a dataset 111\\nChoosing a measure of success 112 ■ Deciding on an'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 6, 'page_label': '7'}, page_content='evaluation protocol 112 ■ Preparing your data 112\\nDeveloping a model that does better than a baseline 113\\nScaling up: developing a model that overfits 114\\nRegularizing your model and tuning your hyperparameters 114\\n4.6 Chapter summary 116\\nPART 2D EEP LEARNING IN PRACTICE .........................117\\n5 Deep learning for computer vision 119\\n5.1 Introduction to convnets 120\\nThe convolution operation 122 ■ The max-pooling \\noperation 127\\n5.2 Training a convnet from scratch on a small dataset 130\\nThe relevance of deep learning for small-data problems 130\\nDownloading the data 131 ■ Building your network 133\\nData preprocessing 135 ■ Using data augmentation 138\\n5.3 Using a pretrained convnet 143\\nFeature extraction 143 ■ Fine-tuning 152 ■ Wrapping \\nup 159\\n5.4 Visualizing what convnets learn 160\\nVisualizing intermediate activations 160 ■ Visualizing \\nconvnet filters 167 ■ Visualizing heatmaps of class \\nactivation 172\\n5.5 Chapter summary 177\\n6 Deep learning for text and sequences 178\\n6.1 Working with text data 180\\nOne-hot encoding of words and characters 181 ■ Using \\nword embeddings 184 ■ Putting it all together: from raw \\ntext to word embeddings 188 ■ Wrapping up 195\\n6.2 Understanding recurrent neural networks 196\\nA recurrent layer in Keras 198 ■ Understanding the \\nLSTM and GRU layers 202 ■ A concrete LSTM example \\nin Keras 204 ■ Wrapping up 206\\n6.3 Advanced use of recurrent neural networks 207\\nA temperature-forecasting problem 207 ■ Preparing the \\ndata 210 ■ A common-sense, non-machine-learning \\nbaseline 212 ■ A basic machine-learning approach 213\\nA first recurrent baseline 215 ■ Using recurrent dropout'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 7, 'page_label': '8'}, page_content='to fight overfitting 216 ■ Stacking recurrent layers 217\\nUsing bidirectional RNNs 219 ■ Going even further 222\\nWrapping up 223\\n6.4 Sequence processing with convnets 225\\nUnderstanding 1D convolution for sequence data 225\\n1D pooling for sequence data 226 ■ Implementing a 1D \\nconvnet 226 ■ Combining CNNs and RNNs to process long \\nsequences 228 ■ Wrapping up 231\\n6.5 Chapter summary 232\\n7 Advanced deep-learning best practices 233\\n7.1 Going beyond the Sequ ential model: the Keras\\n functional API 234\\nIntroduction to the functional API 236 ■ Multi-input \\nmodels 238 ■ Multi-output models 240 ■ Directed acyclic \\ngraphs of layers 242 ■ Layer weight sharing 246 ■ Models \\nas layers 247 ■ Wrapping up 248\\n7.2 Inspecting and monitoring deep-learning models using \\nKeras callbacks and TensorBoard 249\\nUsing callbacks to act on a model during training 249\\nIntroduction to TensorBoard: the TensorFlow visualization \\nframework 252 ■ Wrapping up 259\\n7.3 Getting the most out of your models 260\\nAdvanced architecture patterns 260 ■ Hyperparameter \\noptimization 263 ■ Model ensembling 264 ■ Wrapping \\nup 266\\n7.4 Chapter summary 268\\n8 Generative deep learning 269\\n8.1 Text generation with LSTM 271\\nA brief history of generative recurrent networks 271 ■ How \\ndo you generate sequence data? 272 ■ The importance of \\nthe sampling strategy 272 ■ Implementing character-level \\nLSTM text generation 274 ■ Wrapping up 279\\n8.2 DeepDream 280\\nImplementing DeepDream in Keras 281 ■ Wrapping up 286\\n8.3 Neural style transfer 287\\nThe content loss 288 ■ The style loss 288 ■ Neural style \\ntransfer in Keras 289 ■ Wrapping up 295'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 8, 'page_label': '9'}, page_content='8.4 Generating images with variational autoencoders 296\\nSampling from latent spaces of images 296 ■ Concept vectors \\nfor image editing 297 ■ Variational autoencoders 298\\nWrapping up 304\\n8.5 Introduction to generati ve adversarial networks 305\\nA schematic GAN implementation 307 ■ A bag of tricks 307\\nThe generator 308 ■ The discriminator 309 ■ The adversarial \\nnetwork 310 ■ How to train your DCGAN 310 ■ Wrapping \\nup 312\\n8.6 Chapter summary 313\\n9 Conclusions 314\\n9.1 Key concepts in review 315\\nVarious approaches to AI 315 ■ What makes deep learning \\nspecial within the field of machine learning 315 ■ How to \\nthink about deep learning 316 ■ Key enabling technologies 317\\nThe universal machine-learning workflow 318 ■ Key network \\narchitectures 319 ■ The space of possibilities 322\\n9.2 The limitations of deep learning 325\\nThe risk of anthropomorphizing machine-learning models 325\\nLocal generalization vs. extreme generalization 327\\nWrapping up 329\\n9.3 The future of deep learning 330\\nModels as programs 330 ■ Beyond backpropagation and \\ndifferentiable layers 332 ■ Automated machine learning 332\\nLifelong learning and modular subroutine reuse 333\\nThe long-term vision 335\\n9.4 Staying up to date in a fast-moving field 337\\nPractice on real-world problems using Kaggle 337\\nRead about the latest developments on arXiv 337\\nExplore the Keras ecosystem 338\\n9.5 Final words 339\\nappendix A Installing Keras and its dependencies on Ubuntu 340\\nappendix B Running Jupyter note books on an EC2 GPU instance 345\\nindex 353'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 9, 'page_label': '10'}, page_content='preface\\nIf you’ve picked up this book, you’re pr obably aware of the extraordinary progress\\nthat deep learning has represented for the fi eld of artificial intelligence in the recent\\npast. In a mere five years,  we’ve gone from near-unusable image recognition and\\nspeech transcription, to superhuman performance on these tasks.\\n The consequences of this sudden progre ss extend to almost every industry. But in\\norder to begin deploying deep-learning technology to every problem that it could\\nsolve, we need to make it accessible to as many people as possible, including non-\\nexperts—people who aren’t re searchers or graduate stud ents. For deep learning to\\nreach its full potential, we need to radically democratize it.\\n When I released the first version of th e Keras deep-learning framework in March\\n2015, the democratization of AI wasn’t what I had in mind. I had been doing research\\nin machine learning for several years, and had built Keras to help me with my own\\nexperiments. But throughout 2015 and 2016, tens of thousands of new people\\nentered the field of deep learning; many of them picked up Keras because it was—and\\nstill is—the easiest framework to get started with. As I watched scores of newcomers\\nuse Keras in unexpected, powerful ways, I ca me to care deeply about the accessibility\\nand democratization of AI. I realized that the further we spread these technologies,\\nthe more useful and valuable they become . Accessibility quickly became an explicit\\ngoal in the development of Keras, and over a few short years, the Keras developer\\ncommunity has made fantastic achievements on this front. We’ve put deep learning\\ninto the hands of tens of thousands of people, who in turn are using it to solve import-\\nant problems we didn’t even know existed until recently.\\n The book you’re holding is another step on the way to making deep learning avail-\\nable to as many people as  possible. Keras had always needed a companion course to'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 10, 'page_label': '11'}, page_content='simultaneously cover fundamentals of deep learning, Keras usage patterns, and deep-\\nlearning best practices. This book is my best effort to produce such a course. I wrote it\\nwith a focus on making the concepts behi nd deep learning, and their implementa-\\nt i o n ,  a s  a p p r o a c h a b l e  a s  p o s s i b l e .  D o i n g  s o  d i d n ’ t  r e q u i r e  m e  t o  d u m b  d o w n  a n y -\\nthing—I strongly believe that there are no  difficult ideas in deep learning. I hope\\nyou’ll find this book valuable and that it w ill enable you to begin building intelligent\\napplications and solve the problems that matter to you.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 11, 'page_label': '12'}, page_content='about this book\\nThis book was written for anyone who wishes to explore deep learning from scratch or\\nbroaden their understanding of deep learning. Whether you’re a practicing machine-learn-\\ning engineer, a software developer, or a college student, you’ll find value in these pages.\\n This book offers a practical, hands-on exploration of deep learning. It avoids math-\\nematical notation, preferring instead to ex plain quantitative concepts via code snip-\\npets and to build practical intuition about the core ideas of machine learning and\\ndeep learning. \\n You’ll learn from more than 30 code examples that include detailed commentary,\\npractical recommendations, and simple high -level explanations of everything you\\nneed to know to start using deep learning to solve concrete problems.\\n The code examples use the Python deep-learning framework Keras, with Tensor-\\nFlow as a backend engine. Keras, one of the most popular and fastest-growing deep-\\nlearning frameworks, is widely recommended as the best tool to get started with deep\\nlearning. \\n After reading this book, you’ll have a soli d understand of what deep learning is,\\nwhen it’s applicable, and what its limitations  are. You’ll be familiar with the standard\\nworkflow for approaching and solving machine-learning problems, and you’ll know\\nhow to address commonly enco untered issues. You’ll be ab le to use Keras to tackle\\nreal-world problems ranging fr om computer vision to na tural-language processing:\\nimage classification, timeseries forecasting, sentiment analysis, image and text genera-\\ntion, and more.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 12, 'page_label': '13'}, page_content='Who should read this book\\nThis book is written for people with Python programming experience who want to get\\nstarted with machine learning and deep lear ning. But this book can also be valuable\\nto many different types of readers:\\n\\uf0a1 If you’re a data scientist familiar with machine learning, this book will provide\\nyou with a solid, practical introduction to deep learning, the fastest-growing\\nand most significant subfield of machine learning.\\n\\uf0a1 If you’re a deep-learning expert looking to get started with the Keras frame-\\nwork, you’ll find this book to be the best Keras crash course available.\\n\\uf0a1 If you’re a graduate studen t studying deep learning in  a formal setting, you’ll\\nfind this book to be a practical comp lement to your education, helping you\\nbuild intuition around the behavior of deep neural networks and familiarizing\\nyou with key best practices.\\nEven technically minded people who don’t code regularly will find this book useful as\\nan introduction to both basic and advanced deep-learning concepts.\\n In order to use Keras, you’ll need reasonable Python proficiency. Additionally, famil-\\niarity with the Numpy library will be helpful, although it isn’t required. You don’t need\\nprevious experience with machine learning  or deep learning: this book covers from\\nscratch all the necessary basics. You don’t need an advanced mathematics background,\\neither—high school–level mathematics should suffice in order to follow along.\\nRoadmap\\nThis book is structured in two parts. If you have no prior experience with machine\\nlearning, I strongly recommend that you co mplete part 1 before approaching part 2.\\nWe’ll start with simple examples, and as th e book goes on, we’ll get increasingly close\\nto state-of-the-art techniques.\\n Part 1 is a high-level introduction to deep learning, providing context and defini-\\ntions, and explaining all the notions requ ired to get started with machine learning\\nand neural networks:\\n\\uf0a1 Chapter 1 presents essential contex t and background knowledge around AI,\\nmachine learning, and deep learning.\\n\\uf0a1 Chapter 2 introduces fundamental conc epts necessary in order to approach\\ndeep learning: tensors, te nsor operations, gradient descent, and backpropaga-\\ntion. This chapter also features the bo ok’s first example of a working neural\\nnetwork.\\n\\uf0a1 Chapter 3 includes everything you need to get started with neural networks: an\\nintroduction to Keras, our deep-learning framework of choice; a guide for set-\\nting up your workstation; and three foundational code examples with detailed\\nexplanations. By the end of this chapter,  you’ll be able to  train simple neural'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 13, 'page_label': '14'}, page_content='networks to handle classification and re gression tasks, and you’ll have a solid\\nidea of what’s happening in the background as you train them.\\n\\uf0a1 Chapter 4 explores the canonical machine- learning workflow. You’ll also learn\\nabout common pitfalls and their solutions.\\nPart 2 takes an in-depth dive into practica l applications of deep learning in computer\\nvision and natural-language processing. Many of the examples introduced in this part\\ncan be used as templates to solve problems you’ll encounter in the real-world practice\\nof deep learning:\\n\\uf0a1 Chapter 5 examines a range of practical computer-vision examples, with a focus\\non image classification.\\n\\uf0a1 Chapter 6 gives you practice with techniques for processing sequence data, such\\nas text and timeseries.\\n\\uf0a1 Chapter 7 introduces advanc ed techniques for building state-of-the-art deep-\\nlearning models.\\n\\uf0a1 Chapter 8 explains generative models: deep-learning models capable of creat-\\ning images and text, with sometimes surprisingly artistic results.\\n\\uf0a1 Chapter 9 is dedicated to consolidating what you’ve learned throughout the\\nbook, as well as opening perspectives on  the limitations of deep learning and\\nexploring its probable future.\\nSoftware/hardware requirements\\nAll of this book’s code examples use the Keras deep-learning framework ( https:/ /\\nkeras.io), which is open source and free to  download. You’ll need access to a UNIX\\nmachine; it’s possible to us e Windows, too, but I don’t recommend it. Appendix A\\nwalks you through the complete setup. \\n I also recommend that you have a recent NVIDIA GPU on your machine, such as a\\nTITAN X. This isn’t required, but it will make your experience better by allowing you\\nto run the code examples several times faster. See section 3.3 for more information\\nabout setting up a deep-learning workstation.\\n If you don’t have access to a local workstation with a recent NVIDIA GPU, you can\\nuse a cloud environment, instead. In particular, you can use Google Cloud instances\\n(such as an n1-standard-8 instance with an NVIDIA Tesla K80 add-on) or Amazon Web\\nServices (AWS) GPU instances (such as a p2.xlarge in stance). Appendix B presents in\\ndetail one possible cloud workflow that runs an AWS instance via Jupyter notebooks,\\naccessible in your browser.\\nSource code\\nAll code examples in this book are availa ble for download as Jupyter notebooks from\\nthe book’s website, www.manning.com/books/deep-learning-with-python, and on\\nGitHub at https:/ /github.com/fchollet/deep-learning-with-python-notebooks.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 14, 'page_label': '15'}, page_content='Book forum\\nPurchase of Deep Learning with Python includes free access to a private web forum run by\\nManning Publications where you can make  comments about the book, ask technical\\nquestions, and receive help from the author and from other users. To access the forum,\\ngo to https:/ /forums.manning.com/forums/deep-learning-with-python. You can also\\nlearn more about Manning’s forums and the rules of conduct at https:/ /forums\\n.manning.com/forums/about.\\n Manning’s commitment to our readers is  to provide a venue where a meaningful\\ndialogue between individual readers and between readers and the author can take\\nplace. It isn’t a commitment to any specific amount of participation on the part of the\\nauthor, whose contribution to the forum remains voluntary (and unpaid). We suggest\\nyou try asking him some chal lenging questions lest his in terest stray! The forum and\\nthe archives of previous discussions will be accessible from the publisher’s website as\\nlong as the book is in print.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 15, 'page_label': '16'}, page_content='Part 1\\nFundamentals\\nof deep learning\\nChapters 1–4 of this book will give you a foundational understanding of\\nwhat deep learning is, what it can achieve, and how it works. It will also make you\\nfamiliar with the canonical workflow for solving data problems using deep learn-\\ning. If you aren’t already highly knowledgeable about deep learning, you should\\ndefinitely begin by reading part 1 in full before moving on to the practical appli-\\ncations in part 2.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 16, 'page_label': '17'}, page_content='What is deep learning?\\nIn the past few years, artificial intelligence (AI) has been a subject of intense media\\nhype. Machine learning, deep learning, and AI come up in countless articles, often\\noutside of technology-minded publications. We’re promised a futu re of intelligent\\nchatbots, self-driving cars, and virtual a ssistants—a future sometimes painted in a\\ngrim light and other times as utopian, wh ere human jobs will be scarce and most\\neconomic activity will be handled by robots or AI agents. For a future or current\\npractitioner of machine learning, it’s impo rtant to be able to recognize the signal\\nin the noise so that you can tell world-changing developments from overhyped\\npress releases. Our future is at stake, and it’s a future in which you have an active\\nrole to play: after reading this book, you’ll be one of those who develop the AI\\nagents. So let’s tackle these questions: What has deep learning achieved so far?\\nHow significant is it? Where are we headed next? Should you believe the hype?\\n This chapter provides essential context around artificial in telligence, machine\\nlearning, and deep learning.\\nThis chapter covers\\n\\uf0a1 High-level definitions of fundamental concepts\\n\\uf0a1 Timeline of the development of machine learning\\n\\uf0a1 Key factors behind deep learning’s rising \\npopularity and future potential'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 17, 'page_label': '18'}, page_content='4 CHAPTER 1 What is deep learning?\\n1.1 Artificial intelligen ce, machine learning, \\nand deep learning\\nFirst, we need to define clearly what  we’re talking about when we mention AI. What\\nare artificial intelligence, machine learning, and deep learning (see figure 1.1)? How\\ndo they relate to each other?\\n1.1.1 Artificial intelligence\\nArtificial intelligence was born in the 195 0s, when a handful of pioneers from the\\nnascent field of computer science started asking whether computers could be made to\\n“think”—a question whose ramifications we’r e still exploring today. A concise defini-\\ntion of the field would be as follows: the effort to automate inte llectual tasks normally per-\\nformed by humans. As such, AI is a general field that encompasses machine learning and\\ndeep learning, but that also includes many  more approaches that don’t involve any\\nlearning. Early chess programs, for instance, only involved hardcoded rules crafted by\\nprogrammers, and didn’t qualify as machin e learning. For a fairly long time, many\\nexperts believed that human-le vel artificial intelligence could be achieved by having\\nprogrammers handcraft a suffi ciently large set of explicit rules for manipulating\\nknowledge. This approach is known as symbolic AI, and it was the dominant paradigm\\nin AI from the 1950s to the late 1980s. It reached its peak popularity during the expert\\nsystems boom of the 1980s.\\n Although symbolic AI proved suitable to solve well-defined, logical problems, such as\\nplaying chess, it turned out to be intractable to figure out explicit rules for solving more\\ncomplex, fuzzy problems, such as image cla ssification, speech recognition, and lan-\\nguage translation. A new approach arose to take symbolic AI’s place: machine learning.\\n1.1.2 Machine learning\\nIn Victorian England, Lady Ada Lovelace was a friend and coll aborator of Charles\\nBabbage, the inventor of the Analytical Engine : the first-known general-purpose,\\nmechanical computer. Although visionary and far ahead of its time, the Analytical\\nArtificial\\nintelligence\\nMachine\\nlearning\\nDeep\\nlearning\\nFigure 1.1 Artificial intelligence, \\nmachine learning, and deep learning\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 18, 'page_label': '19'}, page_content='5Artificial intelligence, machine learning, and deep learning\\nEngine wasn’t meant as a general-purpose computer when it was designed in the\\n1830s and 1840s, because the concept of general-purpose computation was yet to be\\ninvented. It was merely meant as a way to use mechanical operations to automate cer-\\ntain computations from the field of math ematical analysis—hence, the name Analyti-\\ncal Engine. In 1843, Ada Lovelace remarked on the invention, “The Analytical Engine\\nhas no pretensions whatever to originate anything. It can do whatever we know how to\\norder it to perform.… Its prov ince is to assist us in making available what we’re\\nalready acquainted with.”\\n This remark was later quoted by AI pioneer Alan Turing as “Lady Lovelace’s objec-\\ntion” in his landmark 1950 paper “Computing Machinery and Intelligence,” 1 which\\nintroduced the Turing test as well as key concepts that would come to shape AI. Turing\\nwas quoting Ada Lovelace while pondering whether general-purpose computers could\\nbe capable of learning and originality, and he came to the conclusion that they could.\\n Machine learning arises from this question: could a computer go beyond “what we\\nknow how to order it to perform” and learn on its own how to perform a specified task?\\nCould a computer surprise us? Rather th an programmers crafting data-processing\\nrules by hand, could a computer automatically learn these rules by looking at data?\\n This question opens the door to a ne w programming paradigm. In classical pro-\\ngramming, the paradigm of symbolic AI, humans input rules (a program) and data to\\nbe processed according to th ese rules, and out come answ ers (see figure 1.2). With\\nmachine learning, humans input data as well  as the answers expected from the data,\\nand out come the rules. These rules can then be applied to new data to produce orig-\\ninal answers.\\nA machine-learning system is trained rather than explicitly programmed. It’s presented\\nwith many examples relevant to a task, and it finds statistical structure in these exam-\\nples that eventually allows the system to come up with rules for automating the task.\\nFor instance, if you wished to automate the task of tagging your vacation pictures, you\\ncould present a machine-learning system wi th many examples of pictures already\\ntagged by humans, and the system would lear n statistical rules for associating specific\\npictures to specific tags.\\n1 A. M. Turing, “Computing Machinery and Intelligence,” Mind 59, no. 236 (1950): 433-460.\\nAnswers\\nRules\\nData\\nClassical\\nprogramming\\nRules\\nData\\nAnswers\\nMachine\\nlearning\\nFigure 1.2 Machine learning: \\na new programming paradigm\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 19, 'page_label': '20'}, page_content='6 CHAPTER 1 What is deep learning?\\n Although machine learning only started to  flourish in the 1990s, it has quickly\\nbecome the most popular and mo st successful subfield of AI, a trend driven by the\\navailability of faster hardware and larger datasets. Machine learning is tightly related\\nto mathematical statistics, but it differs from statistics in several important ways.\\nUnlike statistics, machine learning tends to deal with large, complex datasets (such as\\na dataset of millions of images, each consis ting of tens of thousands of pixels) for\\nwhich classical statistical analys is such as Bayesian analysis  would be impractical. As a\\nresult, machine learning, and especially de ep learning, exhibits  comparatively little\\nmathematical theory—maybe too little—and is engineering oriented. It’s a hands-on\\ndiscipline in which ideas are proven empirically more often than theoretically.\\n1.1.3 Learning representations from data\\nTo define deep learning  and understand the difference between deep learning\\nand other machine-learning ap proaches, first we need some idea of what machine-\\nlearning algorithms do. I just stated that machine lear ning discovers rules to execute\\na data-processing task, given examples of what’s expected. So, to do machine learn-\\ning, we need three things:\\n\\uf0a1 Input data points —For instance, if the task is speech recognition, these data\\npoints could be sound files of people speaking. If the task  is image tagging,\\nthey could be pictures.\\n\\uf0a1 Examples of the expected output —In a speech-recognition task, these could be\\nhuman-generated transcripts of sound files. In an image task, expected outputs\\ncould be tags such as “dog,” “cat,” and so on.\\n\\uf0a1 A way to measure whether the algorithm is doing a good job —This is necessary in\\norder to determine the distance between the algorithm’s current output and\\nits expected output. The measurement is used as a feedback signal to adjust\\nthe way the algorithm works. This adjustment step is what we call learning.\\nA machine-learning model tran sforms its input data into meaningful outputs, a pro-\\ncess that is “learned” from exposure to known examples of inputs and outputs. There-\\nfore, the central problem in machine learning and deep learning is to meaningfully\\ntransform data : in other words, to learn useful representations of the input data at\\nhand—representations that get us closer to  the expected output. Before we go any\\nfurther: what’s a representation? At its core, it’s a different way to look at data—to rep-\\nresent or encode data. For instance, a color image can be encoded in the \\nRGB format\\n(red-green-blue) or in the HSV format (hue-saturation-value): these are two different\\nrepresentations of the same data. Some task s that may be difficult with one represen-\\ntation can become easy with another. For example, the task “select all red pixels in the\\nimage” is simpler in the RG format, whereas “make the image less saturated” is simpler\\nin the HSV format. Machine-learning models ar e all about finding appropriate repre-\\nsentations for their input data—transformations of the data that make it more amena-\\nble to the task at hand, such as a classification task.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 20, 'page_label': '21'}, page_content='7Artificial intelligence, machine learning, and deep learning\\n Let’s make this concrete. Cons ider an x-axis, a y-axis, and\\nsome points represented by their coordinates in the (x, y) sys-\\ntem, as shown in figure 1.3.\\n As you can see, we have a few white points and a few black\\npoints. Let’s say we want to develop an algorithm that can take\\nthe coordinates (x, y) of a point and output whether that\\npoint is likely to be black or to be white. In this case,\\n\\uf0a1 The inputs are the coordinates of our points.\\n\\uf0a1 The expected outputs are the colors of our points.\\n\\uf0a1 A way to measure whether our algorithm is doing a\\ngood job could be, for instance, the percentage of\\npoints that are being correctly classified.\\nWhat we need here is a new representation of our data that cleanly separates the white\\npoints from the black points. One transfor mation we could use, among many other\\npossibilities, would be a coordinate change, illustrated in figure 1.4.\\nIn this new coordinate system, the coordinate s of our points can be said to be a new\\nrepresentation of our data. And it’s a go od one! With this representation, the\\nblack/white classification problem can be expressed as a simple rule: “Black points\\nare such that x > 0,” or “White points are such that x < 0.” This new representation\\nbasically solves the classification problem.\\n In this case, we defined the coordinate change by hand. But if instead we tried sys-\\ntematically searching for different possible coordinate changes, and used as feedback\\nthe percentage of points being correctly cl assified, then we would be doing machine\\nlearning. Learning, in the context of machine learni ng, describes an automatic search\\nprocess for better representations.\\n All machine-learning algorithms consist of  automatically finding such transforma-\\ntions that turn data into more-useful re presentations for a given task. These opera-\\ntions can be coordinate changes, as you ju st saw, or linear pr ojections (which may\\ndestroy information), translations, nonlinea r operations (such as “select all points\\nsuch that x > 0”), and so on. Machine-lear ning algorithms aren’t usually creative in\\ny\\n2: Coordinate change\\nx\\ny\\n1: Raw data\\nx\\ny\\n3: Better representation\\nx\\nFigure 1.4 Coordinate change\\ny\\nx\\nFigure 1.3\\nSome sample data\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 21, 'page_label': '22'}, page_content='8 CHAPTER 1 What is deep learning?\\nfinding these transformations; they’re mere ly searching through a predefined set of\\noperations, called a hypothesis space.\\n So that’s what machine learning is, te chnically: searching for useful representa-\\ntions of some input data, within a predef ined space of possibi lities, using guidance\\nfrom a feedback signal. This simple idea allows for solv ing a remarkably broad range\\nof intellectual tasks, from speech recognition to autonomous car driving.\\n Now that you understand what we mean by learning, let’s take a look at what makes\\ndeep learning special. \\n1.1.4 The “deep” in deep learning\\nDeep learning is a specific subfield of machine learning: a new take on learning repre-\\nsentations from data that puts an emphasis on learning successive layers of increasingly\\nmeaningful representations. The deep in deep learning isn’t a reference to any kind of\\ndeeper understanding achieved by the approach ; rather, it stands for this idea of suc-\\ncessive layers of representations. How many layers contribute to a model of the data is\\ncalled the depth of the model. Other appropriate names for the field could have been\\nlayered representations learning  and hierarchical representations learning . Modern deep\\nlearning often involves tens or even hundreds of successive layers of representations—\\nand they’re all learned automatically from  exposure to training data. Meanwhile,\\nother approaches to machine learning tend to focus on learning only one or two lay-\\ners of representations of the data; hence, they’re sometimes called shallow learning.\\n In deep learning, these layered representations are (almost always) learned via\\nmodels called neural networks, structured in literal layers stacked on top of each other.\\nThe term neural network is a reference to neurobiology, but although some of the cen-\\ntral concepts in deep learning were developed in part by drawing inspiration from our\\nunderstanding of the brain, deep-learning models are not m o d e l s  o f  t h e  b r a i n .\\nThere’s no evidence that the brain implem ents anything like the learning mecha-\\nnisms used in modern deep-learning mode ls. You may come across pop-science arti-\\ncles proclaiming that deep learning work s like the brain or was modeled after the\\nbrain, but that isn’t the case. It would be confusing and counterproductive for new-\\ncomers to the field to think of deep learning as being in any way related to neurobiol-\\nogy; you don’t need that shroud of “just li ke our minds” mystique and mystery, and\\nyou may as well forget anyt hing you may have read abou t hypothetical links between\\ndeep learning and biology. For our purposes , deep learning is a mathematical frame-\\nwork for learning representations from data.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 22, 'page_label': '23'}, page_content='9Artificial intelligence, machine learning, and deep learning\\n What do the representations learned by a deep-learning algorithm look like? Let’s\\nexamine how a network several layers deep (see figure 1.5) transforms an image of a\\ndigit in order to recognize what digit it is.\\nAs you can see in figure 1.6, the network transforms the digit image into representa-\\ntions that are increasingly different from the original image and increasingly informa-\\ntive about the final result. You can th ink of a deep network as a multistage\\ninformation-distillation operation, where in formation goes through successive filters\\nand comes out increasingly purified (that is, useful with regard to some task).\\nSo that’s what deep learning is, technically: a multistage way to learn data representa-\\ntions. It’s a simple idea—but, as it turn s out, very simple me chanisms, sufficiently\\nscaled, can end up looking like magic. \\n1.1.5 Understanding how deep lear ning works, in three figures\\nAt this point, you know that machine learning is about mapping inputs (such as\\nimages) to targets (such as the label “cat”), which is done by observing many examples\\nof input and targets. You also know that de ep neural networks do this input-to-target\\nLayer 1\\nOriginal\\ninput\\nFinal\\noutput\\nLayer 2 Layer 3 Layer 4\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nFigure 1.5 A deep neural \\nnetwork for digit classification\\nLayer 1\\nrepresentations\\nOriginal\\ninput\\nLayer 2\\nrepresentations\\nLayer 3\\nrepresentations\\nLayer 4\\nrepresentations\\n(final output)\\nLayer 1 Layer 2 Layer 3 Layer 4\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nFigure 1.6 Deep representations learned by a digit-classification model\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 23, 'page_label': '24'}, page_content=\"10 CHAPTER 1 What is deep learning?\\nmapping via a deep sequence of simple data  transformations (layers) and that these\\ndata transformations are learned by exposure to examples. Now let’s look at how this\\nlearning happens, concretely.\\n The specification of what a layer does to  its input data is stored in the layer’s\\nweights, which in essence are a bunch of numbers. In technical terms, we’d say that the\\ntransformation implemented by a layer is parameterized by its weights (see figure 1.7).\\n(Weights are also sometimes called the parameters of a layer.) In this context, learning\\nmeans finding a set of values for the weights of all layers in a network, such that the\\nnetwork will correctly map example inputs to  their associated targets. But here’s the\\nthing: a deep neural networ k can contain tens of millions of parameters. Finding the\\ncorrect value for all of them may seem like a daunting task, especially given that mod-\\nifying the value of one parameter will affect the behavior of all the others!\\nTo control something, first you need to be able to observe it. To control the output of\\na neural network, you need to be able to measure how far this output is from what you\\nexpected. This is the job of the loss function  of the network, also called the objective\\nfunction. The loss function takes the prediction s of the network and the true target\\n(what you wanted the network to output) and computes a distance score, capturing\\nhow well the network has done on this specific example (see figure 1.8).\\nGoal: finding the\\nright values for\\nthese weights\\nLayer\\n(data transformation)\\nInput X\\nWeights\\nLayer\\n(data transformation)\\nPredictions\\nY'\\nWeights\\nFigure 1.7 A neural network is \\nparameterized by its weights.\\nLayer\\n(data transformation)\\nInput X\\nWeights\\nLayer\\n(data transformation)\\nPredictions\\nY'\\nTrue targets\\nY\\nWeights\\nLoss function\\nLoss score\\nFigure 1.8 A loss function measures \\nthe quality of the network’s output.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 24, 'page_label': '25'}, page_content=\"11Artificial intelligence, machine learning, and deep learning\\nThe fundamental trick in deep learning is to use this score as a feedback signal to\\nadjust the value of the weight s a little, in a direction that will lower the loss score for\\nthe current example (see figure 1.9). This adjustment is the job of the optimizer, which\\nimplements what’s called the Backpropagation algorithm: the central algorithm in deep\\nlearning. The next chapter explains in more detail how backpropagation works.\\nInitially, the weights of the network are assigned random values, so the network\\nmerely implements a series of random tran sformations. Naturally, its output is far\\nfrom what it should ideally be, and the lo ss score is accordingly very high. But with\\nevery example the network processes, the weig hts are adjusted a little in the correct\\ndirection, and the loss score decreases. This is the training loop, which, repeated a suffi-\\ncient number of times (typically tens of it erations over thousands of examples), yields\\nweight values that minimize the loss function. A network with a minimal loss is one for\\nwhich the outputs are as close as they can be to the targets: a trained network. Once\\nagain, it’s a simple mechanism that, once scaled, ends up looking like magic. \\n1.1.6 What deep learning has achieved so far\\nAlthough deep learning is a fairly old subfield of machine learning, it only rose to\\nprominence in the early 2010s. In the few years since, it has achieved nothing short of\\na revolution in the field, with remarkable results on perceptual problems such as see-\\ning and hearing—problems involving skills that seem natural and intuitive to humans\\nbut have long been elusive for machines.\\n In particular, deep learning has achieved the following breakthroughs, all in his-\\ntorically difficult areas of machine learning:\\n\\uf0a1 Near-human-level image classification\\n\\uf0a1 Near-human-level speech recognition\\n\\uf0a1 Near-human-level handwriting transcription\\n\\uf0a1 Improved machine translation\\nLayer\\n(data transformation)\\nInput X\\nWeights\\nLayer\\n(data transformation)\\nPredictions\\nY'\\nWeight\\nupdate\\nTrue targets\\nY\\nWeights\\nLoss functionOptimizer\\nLoss score\\nFigure 1.9 The loss score is used as a \\nfeedback signal to adjust the weights.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 25, 'page_label': '26'}, page_content='12 CHAPTER 1 What is deep learning?\\n\\uf0a1 Improved text-to-speech conversion\\n\\uf0a1 Digital assistants such as Google Now and Amazon Alexa\\n\\uf0a1 Near-human-level autonomous driving\\n\\uf0a1 Improved ad targeting, as used by Google, Baidu, and Bing\\n\\uf0a1 Improved search results on the web\\n\\uf0a1 Ability to answer natural-language questions\\n\\uf0a1 Superhuman Go playing\\nWe’re still exploring the full extent of what deep learning can do. We’ve started apply-\\ning it to a wide variety of problems outside of machine perception and natural-language\\nunderstanding, such as formal reasoning. If successful, this may herald an age where\\ndeep learning assists humans in science, software development, and more. \\n1.1.7 Don’t believe the short-term hype\\nAlthough deep learning has led to remarkab le achievements in recent years, expecta-\\ntions for what the field will be able to ac hieve in the next decade tend to run much\\nhigher than what will likely be possible. Although some world- changing applications\\nlike autonomous cars are already within reach, many more are likely to remain elusive\\nfor a long time, such as beli evable dialogue systems, human-level machine translation\\nacross arbitrary langua ges, and human-level natural-la nguage understanding. In par-\\nticular, talk of human-level general intelligence shouldn’t be taken too seriously. The risk\\nwith high expectations for the short term is that, as technology fails to deliver,\\nresearch investment will dry up, slowing progress for a long time.\\n This has happened before. Twice in the pa st, AI went through a cycle of intense\\noptimism followed by disappointment and skepticism, wi th a dearth of funding as a\\nresult. It started with symbolic \\nAI in the 1960s. In those early days, projections about AI\\nwere flying high. One of the best-known pi oneers and proponents of the symbolic AI\\napproach was Marvin Minsky, who claimed in 1967, “Within a generation … the prob-\\nlem of creating ‘artificial intelligence’ will substantially be solved.” Three years later, in\\n1970, he made a more precisely quantified prediction: “In from three to eight years we\\nwill have a machine with the general intelligence of an average human being.” In 2016,\\nsuch an achievement still appears to be far in the future—so far that we have no way to\\npredict how long it will take—but in the 1960s and early 1970s, several experts believed\\nit to be right around the corner (as do many  people today). A few years later, as these\\nhigh expectations failed to materialize,  researchers and gove rnment funds turned\\naway from the field, marking the start of the first \\nAI winter (a reference to a nuclear win-\\nter, because this was shortly after the height of the Cold War).\\n It wouldn’t be the last one. In the 1980s, a new take on symbolic AI, expert systems,\\nstarted gathering steam among large companies. A few initial success stories triggered\\na wave of investment, with corporations around the world starting their own in-house\\nAI departments to develop ex pert systems. Around 1985,  companies were spending\\nover $1 billion each year on the technolo gy; but by the early 1990s, these systems had\\nproven expensive to maintain, difficult to scale, and limited in scope, and interest\\ndied down. Thus began the second AI winter.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 26, 'page_label': '27'}, page_content='13Artificial intelligence, machine learning, and deep learning\\n We may be currently witnessing the third cycle of AI hype and disappointment—\\nand we’re still in the phase of intense optimism. It’s best to moderate our expectations\\nfor the short term and make sure people le ss familiar with the technical side of the\\nfield have a clear idea of what deep learning can and can’t deliver. \\n1.1.8 The promise of AI\\nAlthough we may have unrealistic short-term expectations for AI, the long-term pic-\\nture is looking bright. We’re only getting started in applying deep learning to many\\nimportant problems for which it could prov e transformative, from medical diagnoses\\nto digital assistants. AI research has been moving forward amazingly quickly in the past\\nfive years, in large part due to a level of funding never before seen in the short history\\nof AI, but so far relatively little of this progress has made its way into the products and\\nprocesses that form our world. Most of the research findings of deep learning aren’t\\nyet applied, or at least not applied to th e full range of problems they can solve across\\nall industries. Your doctor doesn’t yet use \\nAI, and neither does your accountant. You\\nprobably don’t use AI technologies in your day-to-day life. Of course, you can ask your\\nsmartphone simple questions and get reason able answers, you can get fairly useful\\nproduct recommendations on Amazon.com, and you can search for “birthday” on\\nGoogle Photos and instantly find those pict ures of your daughter’s birthday party\\nfrom last month. That’s a far cry from wh ere such technologies used to stand. But\\nsuch tools are still only acce ssories to our daily lives. AI has yet to transition to being\\ncentral to the way we work, think, and live.\\n Right now, it may seem hard to believe that AI could have a large impact on our\\nworld, because it isn’t yet widely deployed—much as, back in 1995, it would have been\\ndifficult to believe in the future impact of the internet. Back then, most people didn’t\\nsee how the internet was relevant to them and how it was going to change their lives. The\\nsame is true for deep learning and AI today. But make no mistake: AI is coming. In a not-\\nso-distant future, AI will be your assistant, even your friend; it will answer your questions,\\nhelp educate your kids, and watch over your health. It will deliver your groceries to your\\ndoor and drive you from point A to point B. It will be your interface to an increasingly\\ncomplex and information-intensive world. And, even more important, AI will help\\nhumanity as a whole move forward, by assisting human scientists in new breakthrough\\ndiscoveries across all scientific fields, from genomics to mathematics.\\n On the way, we may face a few setbacks and maybe a new AI winter—in much the\\nsame way the internet industry was overhyped in 1998–1999 and suffered from a crash\\nthat dried up investment throughout the early 2000s. But we’ll get there eventually. \\nAI\\nwill end up being applied to nearly every process that makes up our society and our\\ndaily lives, much like the internet is today.\\n Don’t believe the short-term hype, but do  believe in the long-term vision. It may\\ntake a while for AI to be deployed to its true pote ntial—a potential the full extent of\\nwhich no one has yet dared to dream—but AI is coming, and it will transform our\\nworld in a fantastic way. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 27, 'page_label': '28'}, page_content='14 CHAPTER 1 What is deep learning?\\n1.2 Before deep learning: \\na brief history of machine learning\\nDeep learning has reached a level of pub lic attention and industry investment never\\nbefore seen in the history of AI, but it isn’t the first succe ssful form of machine learn-\\ning. It’s safe to say that most of the mach ine-learning algorithms used in the industry\\ntoday aren’t deep-learning algorithms. Deep learning isn’t always the right tool for the\\njob—sometimes there isn’t enough data for deep learning to be applicable, and some-\\ntimes the problem is better solved by a diff erent algorithm. If d eep learning is your\\nfirst contact with machine learning, then you may find yourself in a situation where all\\nyou have is the deep-learning hammer, and every machine-learning problem starts to\\nlook like a nail. The only way not to fall into this trap is to be familiar with other\\napproaches and practice them when appropriate.\\n A detailed discussion of classical machin e-learning approaches is outside of the\\nscope of this book, but we’ll briefly go over  them and describe the historical context\\nin which they were developed. This will allow us to place deep learning in the broader\\ncontext of machine learning and better un derstand where deep learning comes from\\nand why it matters.\\n1.2.1 Probabilistic modeling\\nProbabilistic modeling is the application of the principles  of statistics to data analysis. It\\nwas one of the earliest forms of machine learning, and it’s still widely used to this day.\\nOne of the best-known algorithms in this category is the Naive Bayes algorithm.\\n Naive Bayes is a type of machine-learning classifier based on applying Bayes’ theo-\\nrem while assuming that the fe atures in the input data are all independent (a strong,\\nor “naive” assumption, which is where the name comes from). This form of data analy-\\nsis predates computers and was applied by hand decade s before its first computer\\nimplementation (most likely dating back to the 1950s). Bayes’ theorem and the foun-\\ndations of statistics date back to the eighteenth century, and these are all you need to\\nstart using Naive Bayes classifiers.\\n A closely related model is the logistic regression (logreg for short) , which is some-\\ntimes considered to be the “hello world” of modern machine learning. Don’t be mis-\\nled by its name—logreg is a classification algorithm rather than a regression\\nalgorithm. Much like Naive Bayes, logreg pr edates computing by a long time, yet it’s\\nstill useful to this day, thanks to its simple and versatile nature. It’s often the first thing\\na data scientist will try on a dataset to get a feel for the classification task at hand. \\n1.2.2 Early neural networks\\nEarly iterations of neural networks have  been completely supplanted by the modern\\nvariants covered in these pages, but it’s helpful to be aware of how deep learning origi-\\nnated. Although the core ideas of neural networks were investigated in toy forms as early\\nas the 1950s, the approach took decades to get started. For a long time, the missing piece\\nwas an efficient way to trai n large neural networks. This changed in the mid-1980s,\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 28, 'page_label': '29'}, page_content='15Before deep learning: a brief history of machine learning\\nwhen  multiple people independently rediscovered the Backpropagation algorithm—\\na way to train chains of parametric oper ations using gradient-descent optimization\\n(later in the book, we’ll precisely define these concepts)—and st arted applying it to\\nneural networks.\\n T h e  f i r s t  s u c c e s s f u l  p r a c t i c a l  a p p l i c a t i o n  o f  n e u r a l  n e t s  c a m e  i n  1 9 8 9  f r o m  B e l l\\nLabs, when Yann LeCun combined the earlier ideas of convolutional neural networks\\nand backpropagation, and a pplied them to the problem of classifying handwritten\\ndigits. The resulting network, dubbed LeNet, was used by the United States Postal Ser-\\nvice in the 1990s to automate the reading of ZIP codes on mail envelopes. \\n1.2.3 Kernel methods\\nAs neural networks started to gain some  respect among researchers in the 1990s,\\nthanks to this first success, a new approa ch to machine learning rose to fame and\\nquickly sent neural nets back to oblivion: kernel methods. Kernel methods are a group of\\nclassification algorithms, the best known of which is the support vector machine  (SVM).\\nThe modern formulation of an SVM was developed by Vladimir\\nVapnik and Corinna Cortes in the early 1990s at Bell Labs and\\npublished in 1995, 2 although an older li near formulation was\\npublished by Vapnik and Alexey Chervonenkis as early as 1963.3\\n SVMs aim at solving classificati on problems by finding good\\ndecision boundaries (see figure 1.10) between two sets of points\\nbelonging to two different categories. A decision boundary can\\nbe thought of as a line or surf ace separating your training data\\ninto two spaces corresponding to two categories. To classify new\\ndata points, you just need to check which side of the decision\\nboundary they fall on.\\n \\nSVMs proceed to find these boundaries in two steps:\\n1 The data is mapped to a new high-dimensional representation where the\\ndecision boundary can be expressed as a hyperplane (if the data was two-\\ndimensional, as in figure 1.10, a hyperplane would be a straight line).\\n2 A good decision boundary (a separation hyperplane) is com puted by trying to\\nmaximize the distance between the hyperplane and the closest data points from\\neach class, a step called maximizing the margin. This allows the boundary to gen-\\neralize well to new samples outside of the training dataset.\\nThe technique of mapping data to a high-dimensional representation where a classifi-\\ncation problem becomes simpler may look good on paper, but in practice it’s\\noften computationally intractable. That’s where the kernel trick comes in (the key idea\\nthat kernel methods are named after). Here’s  t h e  g i s t  o f  i t :  t o  f i n d  g o o d  d e c i s i o n\\n2 Vladimir Vapnik and Corinna Cortes, “Support-Vector Networks,” Machine Learning 20, no. 3 (1995): 273–297.\\n3 Vladimir Vapnik and Alexey Chervonenkis, “A Note on One Class of Perceptrons,” Automation and Remote Con-\\ntrol 25 (1964).\\nFigure 1.10\\nA decision boundary\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 29, 'page_label': '30'}, page_content='16 CHAPTER 1 What is deep learning?\\nhyperplanes in the ne w representation space, you don’ t have to explicitly compute\\nthe coordinates of your points in the new space; you just need to compute the dis-\\ntance between pairs of points in that spac e, which can be done efficiently using a ker-\\nnel function. A kernel function is a computationa lly tractable operation that maps any\\ntwo points in your initial space to the di stance between these points in your target\\nrepresentation space, completely bypassing  the explicit computation of the new rep-\\nresentation. Kernel functions are typically crafted by hand rather than learned from\\ndata—in the case of an SVM, only the separation hyperplane is learned.\\n At the time they were developed, SVMs exhibited state-of-the-art performance on\\nsimple classification problems and were one of the few machine-learning methods\\nbacked by extensive theory and amenable to serious mathematical analysis, making\\nthem well understood and easily interpreta ble. Because of these useful properties,\\nSVMs became extremely popular in the field for a long time.\\n But SVMs proved hard to scale to large datasets and didn’t provide good results for\\nperceptual problems such as im age classification. Because an SVM is a shallow\\nmethod, applying an SVM to perceptual problems requires first extracting useful rep-\\nresentations manually (a step called feature engineering), which is difficult and brittle. \\n1.2.4 Decision trees, random forests, and gradient boosting machines\\nDecision trees are flowchart-like structur es that let you classify input data points or pre-\\ndict output values given inputs (see figure 1.11). They’re easy to visualize and inter-\\npret. Decisions trees learned from data began to receive significant research interest\\nin the 2000s, and by 2010 they were often preferred to kernel methods.\\nIn particular, the Random Forest  algorithm introduced a robust, practical take on\\ndecision-tree learning that involves buildi ng a large number of specialized decision\\ntrees and then ensembling their outputs. Random forests are a pplicable to a wide\\nrange of problems—you could say that they’re almost always the second-best algorithm\\nfor any shallow machine-learning task. When the popular machine-learning competi-\\ntion website Kaggle ( http:/ /kaggle.com) got started in 2010, random forests quickly\\nbecame a favorite on the platform—until 2014, when gradient boosting machines  took\\nover. A gradient boosting machine, much li ke a random forest, is a machine-learning\\ntechnique based on ensembling weak predic tion models, generally decision trees. It\\nQuestion\\nCategory Category\\nQuestion\\nInput data\\nQuestion\\nCategory Category\\nFigure 1.11 A decision tree: the parameters \\nthat are learned are the questions about the \\ndata. A question could be, for instance, “Is \\ncoefficient 2 in the data greater than 3.5?”\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 30, 'page_label': '31'}, page_content='17Before deep learning: a brief history of machine learning\\nuses gradient boosting, a way to improve any machine-learning model by iteratively train-\\ning new models that specialize  in addressing the weak points of the previous models.\\nApplied to decision trees, the use of the gradient boosting technique results in models\\nthat strictly outperform random forests most of the time, while having similar proper-\\nties. It may be one of the best, if not the best, algorithm for dealing with nonperceptual\\ndata today. Alongside deep learning, it’s one of the most commonly used techniques in\\nKaggle competitions. \\n1.2.5 Back to neural networks\\nAround 2010, although neural networks we re almost completely shunned by the sci-\\ne n t i f i c  c o m m u n i t y  a t  l a r g e ,  a  n u m b e r  o f  p e o p l e  s t i l l  w o r k i n g  o n  n e u r a l  n e t w o r k s\\nstarted to make important breakthroughs: th e groups of Geoffrey Hinton at the Uni-\\nversity of Toronto, Yoshua Bengio at the University of Montreal, Yann LeCun at New\\nYork University, and IDSIA in Switzerland.\\n In 2011, Dan Ciresan from IDSIA began to win academic image-classification com-\\npetitions with GPU-trained deep neural networks—the first practical success of mod-\\nern deep learning. But the watershed mo ment came in 2012, with the entry of\\nHinton’s group in the yearly large-scale im age-classification challenge ImageNet. The\\nImageNet challenge was notoriously difficult at the time, consisting of classifying high-\\nresolution color images into 1,000 different categories after training on 1.4 million\\nimages. In 2011, the top-five accuracy of  the winning model, based on classical\\napproaches to computer vision, was only 74.3%. Then, in 2012, a team led by Alex\\nKrizhevsky and advised by Geo ffrey Hinton was able to ac hieve a top-five accuracy of\\n83.6%—a significant breakthrough. The co mpetition has been dominated by deep\\nconvolutional neural networks every year since. By 2015, the winner reached an accu-\\nracy of 96.4%, and the classification task  on ImageNet was cons idered to be a com-\\npletely solved problem.\\n Since 2012, deep convolut ional neural networks (convnets) have become the go-to\\nalgorithm for all computer vision tasks; mo re generally, they work on all perceptual\\ntasks. At major computer vi sion conferences in 2015 and 2016, it was nearly impossi-\\nble to find presentations that didn’t involve convnets in some form. At the same time,\\ndeep learning has also found applications in many other types of problems, such as\\nnatural-language processing. It has completely replaced SVMs and decision trees in a\\nwide range of applications. For instance, for several years, the European Organization\\nfor Nuclear Research, CERN, used decision tree–based methods for analysis of particle\\ndata from the ATLAS detector at the Large Hadron Collider ( LHC); but CERN eventu-\\nally switched to Keras-base d deep neural networks due to their higher performance\\nand ease of training on large datasets. \\n1.2.6 What makes deep learning different\\nThe primary reason deep learning took off so  quickly is that it offered better perfor-\\nmance on many problems. But that’s not th e only reason. Deep learning also makes\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 31, 'page_label': '32'}, page_content='18 CHAPTER 1 What is deep learning?\\nproblem-solving much easier, because it completely automates what used to be the\\nmost crucial step in a machine-learning workflow: feature engineering.\\n Previous machine-learning  techniques—shallow lear ning—only involved trans-\\nforming the input data into one or two succ essive representation spaces, usually via\\nsimple transformations such as high -dimensional non-linear projections ( SVMs) or\\ndecision trees. But the refined representa tions required by co mplex problems gener-\\nally can’t be attained by such techniques. As such, humans had to go to great lengths\\nto make the initial input data more amenab le to processing by these methods: they\\nhad to manually engineer good layers of representations for their data. This is called\\nfeature engineering. Deep learning, on the other hand , completely automates this step:\\nwith deep learning, you learn all features in one pass rather than having to engineer\\nthem yourself. This has greatly simplified machine-learning workflows, often replac-\\ning sophisticated multistage pipelines with a single, simple, end-to-end deep-learning\\nmodel.\\n You may ask, if the crux of the issue is to have multiple successive layers of repre-\\nsentations, could shallow methods be appl ied repeatedly to emulate the effects of\\ndeep learning? In practice, there are fast-diminishing re turns to successive applica-\\ntions of shallow-learning methods, because the optimal first representation layer in a three-\\nlayer model isn’t the optimal first lay er in a one-layer or two-layer model . What is transforma-\\ntive about deep learning is th at it allows a model to learn all layers of representation\\njointly, at the same time, rather than in succession ( greedily, as it’s called). With joint\\nfeature learning, whenever the model adjusts one of its internal features, all other fea-\\ntures that depend on it automatically ad apt to the change, without requiring human\\nintervention. Everything is supervised by a single feedback signal: every change in the\\nmodel serves the end goal. This is much more powerful than greedily stacking shallow\\nmodels, because it allows for complex, ab stract representations to be learned by\\nb r e a k i n g  t h e m  d o w n  i n t o  l o n g  s e r i e s  o f  intermediate spaces (layers); each space is\\nonly a simple transformation away from the previous one.\\n These are the two essential characteristics of how deep learning learns from data:\\nthe incremental, layer-by-layer way in which incr easingly complex representations are developed ,\\nand the fact that these intermediate incremental representations are learned jointly, each layer\\nbeing updated to follow both the representational needs of the layer above and the\\nneeds of the layer below. Together, these two properties have made deep learning\\nvastly more successful than previous approaches to machine learning. \\n1.2.7 The modern machine-learning landscape\\nA great way to get a sense of the current landscape of machine-learning algorithms\\nand tools is to look at machine-learning competitions on Kaggle. Due to its highly\\ncompetitive environment (some contests ha ve thousands of entrants and million-\\ndollar prizes) and to the wide variety of  machine-learning problems covered, Kaggle\\noffers a realistic way to assess what works and what doesn’t. So, what kind of algorithm\\nis reliably winning competitions? What tools do top entrants use?\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 32, 'page_label': '33'}, page_content='19Before deep learning: a brief history of machine learning\\n In 2016 and 2017, Kaggle was dominate d by two approaches: gradient boosting\\nmachines and deep learning . Specifically, gradient boosting is used for problems\\nwhere structured data is available, whereas deep learning is used for perceptual prob-\\nlems such as image classifica tion. Practitioners of the fo rmer almost always use the\\nexcellent XGBoost library, which offers support for the two most popular languages of\\ndata science: Python and R. Meanwhile, most of the Kaggle entrants using deep learn-\\ning use the Keras library, due to its ease of use, flexibility, and support of Python.\\n These are the two techniques you should be  the most familiar with in order to be\\nsuccessful in applied machine learning today: gradient boosting machines, for shallow-\\nlearning problems; and deep learning, for perceptual pr oblems. In technical terms,\\nthis means you’ll need to be familiar with XGBoost and Keras—the two libraries that\\ncurrently dominate Kaggle competitions. With  this book in hand, you’re already one\\nbig step closer. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 33, 'page_label': '34'}, page_content='20 CHAPTER 1 What is deep learning?\\n1.3 Why deep learning? Why now?\\nThe two key ideas of deep learning for co mputer vision—convolutional neural net-\\nworks and backpropagation—were already well understood in 1989. The Long Short-\\nTerm Memory ( LSTM) algorithm, which is fundamental to deep learning for\\ntimeseries, was developed in 1997 and ha s barely changed since. So why did deep\\nlearning only take off after 2012? What changed in these two decades?\\n In general, three technical forces are driving advances in machine learning:\\n\\uf0a1 Hardware\\n\\uf0a1 Datasets and benchmarks\\n\\uf0a1 Algorithmic advances\\nBecause the field is guided by experimental findings rather than by theory, algorith-\\nmic advances only become possible when appropriate data and hardware are available\\nto try new ideas (or scale up old ideas, as is often the case). Machine learning isn’t\\nmathematics or physics, where major advances can be done with a pen and a piece of\\npaper. It’s an engineering science.\\n The real bottlenecks throughout the 1990s and 2000s were data and hardware. But\\nhere’s what happened during  that time: the internet to ok off, and high-performance\\ngraphics chips were developed for the needs of the gaming market.\\n1.3.1 Hardware\\nBetween 1990 and 2010, off-the-shelf CPUs became faster by a factor of approximately\\n5,000. As a result, nowadays it’s possible to run small deep-learning models on your\\nlaptop, whereas this would have been intractable 25 years ago.\\n But typical deep-learning models used in  computer vision or speech recognition\\nrequire orders of magnitude more computational power th an what your laptop can\\ndeliver. Throughout the 2000s, companies like NVIDIA and AMD have been investing\\nbillions of dollars in developing fast, massively parallel chips (graphical processing\\nunits [\\nGPUs]) to power the graphics of incr easingly photorealistic video games—\\ncheap, single-purpose supercomputers designed to render complex 3D scenes on your\\nscreen in real time. This investment came  to benefit the scientific community when,\\nin 2007, NVIDIA launched CUDA (https:/ /developer.nvidia.com/about-cuda), a pro-\\ngramming interface for its line of GPUs. A small number of GPUs started replacing\\nmassive clusters of CPUs in various highly parallelizable applications, beginning with\\nphysics modeling. Deep neural networks, consisting mostly of many small matrix mul-\\ntiplications, are also highly parallelizable;  and around 2011, so me researchers began\\nto write CUDA implementations of neural nets—Dan Ciresan 4 and Alex Krizhevsky 5\\nwere among the first.\\n4 See “Flexible, High Performance Convolutional Neural Networks for Image Classification,” Proceedings of the\\n22nd International Joint Conference on Artificial Intelligence  (2011), www.ijcai.org/Proceedings/11/Papers/\\n210.pdf.\\n5 See “ImageNet Classification with Deep Convolutional Neural Networks,” Advances in Neural Information Pro-\\ncessing Systems 25 (2012), http://mng.bz/2286.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 34, 'page_label': '35'}, page_content='21Why deep learning? Why now?\\n What happened is that the gaming market subsidized supercomputing for the next\\ngeneration of artificial intelligence ap plications. Sometimes, big things begin as\\ngames. Today, the NVIDIA TITAN X, a gaming GPU that cost $1,000 at the end of 2015,\\ncan deliver a peak of 6.6 TFLOPS in single precision: 6.6 trillion float32 operations\\nper second. That’s about 350 times more th an what you can get out of a modern lap-\\ntop. On a TITAN X, it takes only a couple of days to train an ImageNet model of the\\nsort that would have won the ILSVRC competition a few years ago. Meanwhile, large\\ncompanies train deep-learning models on clusters of hundreds of GPUs of a type\\ndeveloped specifically for the need s of deep learning, such as the NVIDIA Tesla K80 .\\nThe sheer computational power of such clus ters is something that would never have\\nbeen possible without modern GPUs.\\n What’s more, the deep-learning industry is starting to go beyond GPUs and is\\ninvesting in increasingly specialized, effici ent chips for deep learning. In 2016, at its\\nannual I/O convention, Google revealed its tensor processing unit ( TPU) project: a\\nnew chip design developed from the ground up to run deep neural networks, which is\\nreportedly 10 times faster and far more energy efficient than top-of-the-line GPUs. \\n1.3.2 Data\\nAI is sometimes heralded as the new industrial revolution. If deep learning is the steam\\nengine of this revolution, then data is its coal: the raw material that powers our intelli-\\ngent machines, without which nothing would be possible. When it comes to data, in\\naddition to the exponential progress in stor age hardware over the past 20 years (fol-\\nlowing Moore’s law), the game changer has been the rise of the internet, making it fea-\\nsible to collect and distribute  very large datasets for machine learning. Today, large\\ncompanies work with image datasets, video datasets, and natural-language datasets that\\ncouldn’t have been collected without th e internet. User-generated image tags on\\nFlickr, for instance, have been a treasure trove of data for computer vision. So are You-\\nTube videos. And Wikipedia is a key dataset for natural-language processing.\\n If there’s one dataset that has been a cataly st for the rise of deep learning, it’s the\\nImageNet dataset, consisting of 1.4 millio n images that have been hand annotated\\nwith 1,000 image categories (1 category per image). But what makes ImageNet special\\nisn’t just its large size, but also the yearly competition associated with it.6\\n As Kaggle has been demonstrating sinc e 2010, public competitions are an excel-\\nlent way to motivate researchers and engineers to push the envelope. Having common\\nbenchmarks that researchers compete to be at has greatly helped the recent rise of\\ndeep learning. \\n1.3.3 Algorithms\\nIn addition to hardware and data, until the late 2000s, we were missing a reliable way to\\ntrain very deep neural networks. As a result , neural networks were still fairly shallow,\\n6 The ImageNet Large Scale Visual Recognition Challenge (ILSVRC), www.image-net.org/challenges/LSVRC.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 35, 'page_label': '36'}, page_content='22 CHAPTER 1 What is deep learning?\\nusing only one or two layers of representations; thus, they weren’t able to shine against\\nmore-refined shallow methods such as SVMs and random forests. The key issue was that\\nof gradient propagation through deep stacks of layers. The feedback signal used to train\\nneural networks would fade away as the number of layers increased.\\n This changed around 2009–2010 with the advent of several simple but important\\nalgorithmic improvements that allowed for better gradient propagation:\\n\\uf0a1 Better activation functions for neural layers\\n\\uf0a1 Better weight-initialization schemes, starting with layer-wise pretraining, which was\\nquickly abandoned\\n\\uf0a1 Better optimization schemes, such as RMSProp and Adam\\nOnly when these improvements began to al low for training models with 10 or more\\nlayers did deep learning start to shine.\\n Finally, in 2014, 2015, and 2016, even more advanced ways to help gradient propa-\\ngation were discovered, such as batch normalization, residual connections, and depth-\\nwise separable convolutions . Today we can train from scratch models that are\\nthousands of layers deep. \\n1.3.4 A new wave of investment\\nAs deep learning became the new state of  the art for computer vision in 2012–2013,\\nand eventually for all perceptual tasks, in dustry leaders took note. What followed was\\na gradual wave of industry investment far be yond anything previously seen in the his-\\ntory of AI.\\n In 2011, right before deep learning took  the spotlight, the total venture capital\\ninvestment in AI was around $19 million, which went almost entirely to practical appli-\\ncations of shallow machine-le arning approaches. By 2014, it had risen to a staggering\\n$394 million. Dozens of startups launched in these three years, trying to capitalize on\\nthe deep-learning hype. Mean while, large tech companies such as Google, Facebook,\\nBaidu, and Microsoft have invested in inte rnal research departments in amounts that\\nwould most likely dwarf the flow of vent ure-capital money. Only a few numbers have\\nsurfaced: In 2013, Google acquired th e deep-learning startup DeepMind for a\\nreported $500 million—the largest acquisition of an AI company in history. In 2014,\\nBaidu started a deep-learning research center in Silicon Valley, investing $300 million\\nin the project. The deep-learning hardware  startup Nervana Syst ems was acquired by\\nIntel in 2016 for over $400 million.\\n Machine learning—in particular, deep learning—has become central to the prod-\\nuct strategy of these tech gi ants. In late 2015, Google \\nCEO Sundar Pichai stated,\\n“Machine learning is a core, transformative  way by which we’re rethinking how we’re\\ndoing everything. We’re thoughtfully applying it across all our products, be it search,\\nads, YouTube, or Play. And we’re in early days, but you’ll see us—in a systematic way—\\napply machine learning in all these areas.”7\\n7 Sundar Pichai, Alphabet earnings call, Oct. 22, 2015.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 36, 'page_label': '37'}, page_content='23Why deep learning? Why now?\\n A s  a  r e s u l t  o f  t h i s  w a v e  o f  i n v e s t m e n t ,  t h e  n u m b e r  o f  p e o p l e  w o r k i n g  o n  d e e p\\nlearning went in just five years from a few hundred to tens of thousands, and research\\nprogress has reached a frenetic pace. There ar e currently no signs that this trend will\\nslow any time soon. \\n1.3.5 The democratization of deep learning\\nOne of the key factors driving this inflow of  new faces in deep learning has been the\\ndemocratization of the toolsets used in the field. In the early days, doing deep learning\\nrequired significant C++ and CUDA expertise, which few peop le possessed. Nowadays,\\nbasic Python scripting skills suffice to do advanced deep-learning research. This has been\\ndriven most notably by the development of Theano and then TensorFlow—two symbolic\\ntensor-manipulation frameworks for Python that support autodifferentiation, greatly sim-\\nplifying the implementation of new models—a nd by the rise of user-friendly libraries\\nsuch as Keras, which makes deep learning as easy as manipulating LEGO bricks. After its\\nrelease in early 2015, Keras quickly became the go-to deep-learning solution for large\\nnumbers of new startups, graduate students, and researchers pivoting into the field. \\n1.3.6 Will it last?\\nIs there anything special abou t deep neural networks that makes them the “right”\\napproach for companies to be investing in and for researchers to flock to? Or is deep\\nlearning just a fad that may not last? Will we still be using deep neural networks in\\n20 years?\\n Deep learning has several pr operties that justify its status as an AI revolution, and\\nit’s here to stay. We may not be using neural networks two decades from now, but what-\\never we use will directly inherit from mo dern deep learning and its core concepts.\\nThese important properties can be broadly sorted into three categories:\\n\\uf0a1 Simplicity—Deep learning removes the need fo r feature engineering, replacing\\ncomplex, brittle, engineering-heavy pipe lines with simple, end-to-end trainable\\nmodels that are typically built using only five or six different tensor operations.\\n\\uf0a1 Scalability—Deep learning is highly amenable to parallelization on GPUs or\\nTPUs, so it can take full advantage of Moore’s law. In addition, deep-learning\\nmodels are trained by iterating over small  batches of data, allowing them to be\\ntrained on datasets of arbitrary size. (The only bottleneck is the amount of\\nparallel computational power available, which, thanks to  Moore’s law, is a fast-\\nmoving barrier.)\\n\\uf0a1 Versatility and reusability —Unlike many prior machine-learning approaches,\\ndeep-learning models can be trained on additional data without restarting from\\nscratch, making them viable for cont inuous online learning—an important\\nproperty for very large production models. Furthermore, trained deep-learning\\nmodels are repurposable and thus reusable : for instance, it’s possible to take a\\ndeep-learning model trained for image cla ssification and drop it into a video-\\nprocessing pipeline. This allows us to reinvest previous work into increasingly\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 37, 'page_label': '38'}, page_content='24 CHAPTER 1 What is deep learning?\\ncomplex and powerful mode ls. This also makes deep learning applicable to\\nfairly small datasets.\\nDeep learning has only been in the spotlight for a few years, and we haven’t yet estab-\\nlished the full scope of what it can do. Wi th every passing month, we learn about new\\nuse cases and engineering improvements that lift previous limitations. Following a sci-\\nentific revolution, progress generally follows a sigmoid curve: it starts with a period of\\nfast progress, which graduall y stabilizes as researchers hi t hard limitations, and then\\nfurther improvements become incremental. Deep learning in 2017 seems to be in the\\nfirst half of that sigmoid, with much more progress to come in the next few years. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 38, 'page_label': '39'}, page_content='Before we begin: the\\nmathematical building\\nblocks of neural networks\\nUnderstanding deep learning requires familiarity with many simple mathematical\\nconcepts: tensors, tensor operations, differ entiation, gradient descent, and so on.\\nOur goal in this chapter will be to build your intuition about these notions without\\ngetting overly technical. In particular, we’ll steer away from mathematical notation,\\nwhich can be off-putting for those without any mathematics background and isn’t\\nstrictly necessary to explain things well.\\n To add some context for tensors and gr adient descent, we’ll begin the chapter\\nwith a practical example of a neural network. Then we’ll go over every new concept\\nThis chapter covers\\n\\uf0a1 A first example of a neural network\\n\\uf0a1 Tensors and tensor operations\\n\\uf0a1 How neural networks learn via backpropagation \\nand gradient descent'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 39, 'page_label': '40'}, page_content='26 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nthat’s been introduced, point by point. Keep in mind that these concepts will be essen-\\ntial for you to understand the practical ex amples that will come in the following\\nchapters!\\n After reading this chapter, you’ll have  an intuitive understanding of how neural\\nnetworks work, and you’ll be able to move  on to practical ap plications—which will\\nstart with chapter 3.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 40, 'page_label': '41'}, page_content='27A first look at a neural network\\n2.1 A first look at a neural network\\nLet’s look at a concrete example of a neural network that uses the Python library Keras\\nto learn to classify handwritten digits. Un less you already have ex perience with Keras\\nor similar libraries, you won’t understand everything about this first example right\\naway. You probably haven’t even installed Ke ras yet; that’s fine. In the next chapter,\\nwe’ll review each element in the example and explain them in detail. So don’t worry if\\nsome steps seem arbitrary or look like magic to you! We’ve got to start somewhere.\\n The problem we’re trying to solve here is to classify grayscale images of handwrit-\\nten digits (28 × 28 pixels) into their 10 categories (0 through 9). We’ll use the MNIST\\ndataset, a classic in the machine-learning  community, which has been around almost\\nas long as the field itself and has been inte nsively studied. It’s a set of 60,000 training\\nimages, plus 10,000 test images, assembled by  the National Institute of Standards and\\nTechnology (the NIST in MNIST) in the 1980s. You can think of “solving” MNIST as the\\n“Hello World” of deep learning—it’s what you do to verify that your algorithms are\\nworking as expected. As you become a ma chine-learning practitioner, you’ll see\\nMNIST come up over and over again, in scientific papers, blog posts, and so on. You\\ncan see some MNIST samples in figure 2.1.\\nYou don’t need to try to reproduce this example on your machine just now. If you wish\\nto, you’ll first need to set up Keras, which is covered in section 3.3.\\n The MNIST dataset comes preloaded in Keras, in  the form of a set of four Numpy\\narrays.\\nfrom keras.datasets import mnist\\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\\ntrain_images and train_labels form the training set , the data that the model will\\nlearn from. The model will then be tested on the test set, test_images and test_labels.\\nListing 2.1 Loading the MNIST dataset in Keras\\nNote on classes and labels\\nIn machine learning, a category in a classification problem is called a class. Data\\npoints are called samples. The class associated with a specific sample is called a\\nlabel.\\nFigure 2.1 MNIST sample digits\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 41, 'page_label': '42'}, page_content=\"28 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nThe images are encoded as Numpy arrays, and the labels are an array of digits, ranging\\nfrom 0 to 9. The images and labels have a one-to-one correspondence.\\n Let’s look at the training data:\\n>>> train_images.shape\\n(60000, 28, 28)\\n>>> len(train_labels)\\n60000\\n>>> train_labels\\narray([5, 0, 4, ..., 5, 6, 8], dtype=uint8)\\nAnd here’s the test data:\\n>>> test_images.shape\\n(10000, 28, 28)\\n>>> len(test_labels)\\n10000\\n>>> test_labels\\narray([7, 2, 1, ..., 4, 5, 6], dtype=uint8)\\nThe workflow will be as follow s: First, we’ll feed the neur al network the training data,\\ntrain_images and train_labels. The network will then learn to associate images and\\nlabels. Finally, we’ll ask the network to produce predictions for test_images, and we’ll\\nverify whether these predictions match the labels from test_labels.\\n Let’s build the network—again, remember that you aren’t expected to understand\\neverything about this example yet.\\nfrom keras import models\\nfrom keras import layers\\nnetwork = models.Sequential()\\nnetwork.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\\nnetwork.add(layers.Dense(10, activation='softmax'))\\nThe core building block of neural networks is the layer, a data-processing module that\\nyou can think of as a filter for data. Some data goes in, and it comes out in a more use-\\nful form. Specifically, layers extract representations out of the data fed into them—hope-\\nfully, representations that are more mean ingful for the problem at hand. Most of\\ndeep learning consists of chaining together  simple layers that will implement a form\\nof progressive data distillation. A deep-learning model is like a sieve for data process-\\ning, made of a succession of increasingly refined data filters—the layers.\\n Here, our network consists of a sequence of two Dense layers, which are densely\\nconnected (also called fully connected) neural layers. The second (and last) layer is a\\n10-way softmax layer, which means it will return an array of 10 probability scores (sum-\\nming to 1). Each score will be the probabi lity that the current digit image belongs to\\none of our 10 digit classes.\\nListing 2.2 The network architecture\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 42, 'page_label': '43'}, page_content=\"29A first look at a neural network\\n To make the network ready for training, we need to pick three more things, as part\\nof the compilation step:\\n\\uf0a1 A loss function —How the network will be able to measure its performance on\\nthe training data, and thus how it will be  able to steer itself in the right direc-\\ntion.\\n\\uf0a1 An optimizer —The mechanism through which the network will update itself\\nbased on the data it sees and its loss function.\\n\\uf0a1 Metrics to monitor during training and testing —Here, we’ll only care about accu-\\nracy (the fraction of the images that were correctly classified).\\nThe exact purpose of the loss function and the optimizer will be made clear through-\\nout the next two chapters.\\nnetwork.compile(optimizer='rmsprop',\\nloss='categorical_crossentropy',\\nmetrics=['accuracy'])\\nBefore training, we’ll preprocess the data by  reshaping it into the shape the network\\nexpects and scaling it so that all values are in the [0, 1] interval. Previously, our train-\\ning images, for instance, were stored in an array of shape (60000, 28, 28) of type\\nuint8 with values in the [0, 255] interval. We transform it into a float32 array of\\nshape (60000, 28 * 28) with values between 0 and 1.\\ntrain_images = train_images.reshape((60000, 28 * 28))\\ntrain_images = train_images.astype('float32') / 255\\ntest_images = test_images.reshape((10000, 28 * 28))\\ntest_images = test_images.astype('float32') / 255\\nWe also need to categorically encode the labels, a step that’s explained in chapter 3.\\nfrom keras.utils import to_categorical\\ntrain_labels = to_categorical(train_labels)\\ntest_labels = to_categorical(test_labels)\\nWe’re now ready to train the network, which in Keras is done via a call to the net-\\nwork’s fit method—we fit the model to its training data:\\n>>> network.fit(train_images, train_labels, epochs=5, batch_size=128)\\nEpoch 1/5\\n60000/60000 [==============================] - 9s - loss: 0.2524 - acc: 0.9273\\nEpoch 2/5\\n51328/60000 [========================>.....] - ETA: 1s - loss: 0.1035 - acc: 0.9692\\nListing 2.3 The compilation step\\nListing 2.4 Preparing the image data\\nListing 2.5 Preparing the labels\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 43, 'page_label': '44'}, page_content=\"30 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nTwo quantities are displayed during training: the loss of the network over the training\\ndata, and the accuracy of the network over the training data.\\n We quickly reach an accuracy of 0.989 (98.9%) on the training data. Now let’s\\ncheck that the model performs well on the test set, too:\\n>>> test_loss, test_acc = network.evaluate(test_images, test_labels)\\n>>> print('test_acc:', test_acc)\\ntest_acc: 0.9785\\nThe test-set accuracy turns out to be 97.8% —that’s quite a bit lower than the training\\nset accuracy. This gap between  training accuracy and test accuracy is an example of\\noverfitting: the fact that machine-learning mode ls tend to perform worse on new data\\nthan on their training data. Overfitting is a central topic in chapter 3.\\n This concludes our first example—you just saw how you can build and train a neu-\\nral network to classify handwritten digits in  less than 20 lines of Python code. In the\\nnext chapter, I’ll go into detail about every moving piece we just previewed and clarify\\nwhat’s going on behind the scenes. You’ll learn about tensors, the data-storing objects\\ngoing into the network; tensor operations , which layers are made of; and gradient\\ndescent, which allows your network to learn from its training examples. \\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 44, 'page_label': '45'}, page_content='31Data representations for neural networks\\n2.2 Data representations for neural networks\\nIn the previous example, we started from  data stored in multidimensional Numpy\\narrays, also called tensors. In general, all current machin e-learning systems use tensors\\nas their basic data structur e. Tensors are fundamental to  the field—so fundamental\\nthat Google’s TensorFlow was named after them. So what’s a tensor?\\n At its core, a tensor is a container for data—almost always numerical data. So, it’s a\\ncontainer for numbers. You may be already familiar with matrices, which are 2D ten-\\nsors: tensors are a generalization of matric es to an arbitrary number of dimensions\\n(note that in the context of tensors, a dimension is often called an axis).\\n2.2.1 Scalars (0D tensors)\\nA tensor that contains only one number is called a scalar (or scalar tensor, or 0-dimensional\\ntensor, or 0D tensor). In Numpy, a float32 or float64 number is a scalar tensor (or scalar\\narray). You can display the number of axes of a Numpy tensor via the ndim attribute; a sca-\\nlar tensor has 0 axes (ndim == 0). The number of axes of a tensor is also called its rank.\\nHere’s a Numpy scalar:\\n>>> import numpy as np\\n>>> x = np.array(12)\\n>>> x\\narray(12)\\n>>> x.ndim\\n0\\n2.2.2 Vectors (1D tensors)\\nAn array of numbers is called a vector, or 1D tensor. A 1D tensor is said to have exactly\\none axis. Following is a Numpy vector:\\n>>> x = np.array([12, 3, 6, 14])\\n>>> x\\narray([12, 3, 6, 14])\\n>>> x.ndim\\n1\\nThis vector has five entr ies and so is called a 5-dimensional vector. Don’t confuse a 5D\\nvector with a 5D tensor! A 5D vector has only one axis and has five dimensions along its\\naxis, whereas a 5D tensor has five axes (and ma y have any number of dimensions\\nalong each axis). Dimensionality can denote either the number of entries along a spe-\\ncific axis (as in the case of our 5D vector) or the number of axes in a tensor (such as a\\n5D tensor), which can be confusing at time s. In the latter case, it’s technically more\\ncorrect to talk about a tensor of rank 5 (the rank of a tensor being the number of axes),\\nbut the ambiguous notation 5D tensor is common regardless. \\n2.2.3 Matrices (2D tensors)\\nAn array of vectors is a matrix, or 2D tensor. A matrix has two axes (often referred to\\nrows and columns). You can visually interpret a matrix as a rectangular grid of numbers.\\nThis is a Numpy matrix:\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 45, 'page_label': '46'}, page_content='32 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\n>>> x = np.array([[5, 78, 2, 34, 0],\\n[6, 79, 3, 35, 1],\\n[7, 80, 4, 36, 2]])\\n>>> x.ndim\\n2\\nThe entries from the first axis are called the rows, and the entries from the second axis\\nare called the columns. In the previous example, [5, 78, 2, 34, 0] is the first row of x,\\nand [5, 6, 7] is the first column. \\n2.2.4 3D tensors and higher-dimensional tensors\\nIf you pack such matrices in a new array, you obtain a 3D tensor, which you can visually\\ninterpret as a cube of numbers. Following is a Numpy 3D tensor:\\n>>> x = np.array([[[5, 78, 2, 34, 0],\\n[6, 79, 3, 35, 1],\\n[7, 80, 4, 36, 2]],\\n[[5, 78, 2, 34, 0],\\n[6, 79, 3, 35, 1],\\n[7, 80, 4, 36, 2]],\\n[[5, 78, 2, 34, 0],\\n[6, 79, 3, 35, 1],\\n[7, 80, 4, 36, 2]]])\\n>>> x.ndim\\n3\\nBy packing 3D tensors in an array, you can create a 4D tensor, and so on. In deep learn-\\ning, you’ll generally manipulate tensors that are 0D to 4D, although you may go up to\\n5D if you process video data. \\n2.2.5 Key attributes\\nA tensor is defined by three key attributes:\\n\\uf0a1 Number of axes (rank)—For instance, a 3D tensor has three axes, and a matrix has\\ntwo axes. This is also called the tensor’s ndim in Python libraries such as Numpy.\\n\\uf0a1 Shape—This is a tuple of integers that de scribes how many dimensions the ten-\\nsor has along each axis. For instance, the previous matrix example has shape\\n(3, 5), and the 3D tensor example has shape (3, 3, 5). A vector has a shape\\nwith a single element, such as (5,), whereas a scalar has an empty shape, ().\\n\\uf0a1 Data type (usually called dtype in Python libraries)—This is the type of the data\\ncontained in the tensor; for inst ance, a tensor’s type could be float32, uint8,\\nfloat64, and so on. On rare o ccasions, you may see a char tensor. Note that\\nstring tensors don’t exist in Numpy (or in most other libraries), because tensors\\nlive in preallocated, contiguous memory  segments: and strings, being variable\\nlength, would preclude the use of this implementation.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 46, 'page_label': '47'}, page_content='33Data representations for neural networks\\nTo make this more concrete, let’s look back at the data we processed in the MNIST\\nexample. First, we load the MNIST dataset:\\nfrom keras.datasets import mnist\\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\\nNext, we display the number of axes of the tensor train_images, the ndim attribute:\\n>>> print(train_images.ndim)\\n3\\nHere’s its shape:\\n>>> print(train_images.shape)\\n(60000, 28, 28)\\nAnd this is its data type, the dtype attribute:\\n>>> print(train_images.dtype)\\nuint8\\nSo what we have here is a 3D tensor of 8-bit integers. More precisely, it’s an array of\\n60,000 matrices of 28 × 8 integers. Each such  matrix is a grayscale image, with coeffi-\\ncients between 0 and 255.\\n Let’s display the fourth digit in this 3D tensor, using the library Matplotlib (part of\\nthe standard scientific Python suite); see figure 2.2. \\ndigit = train_images[4]\\nimport matplotlib.pyplot as plt\\nplt.imshow(digit, cmap=plt.cm.binary)\\nplt.show()\\nListing 2.6 Displaying the fourth digit\\nFigure 2.2 The fourth sample in our dataset\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 47, 'page_label': '48'}, page_content='34 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\n2.2.6 Manipulating tensors in Numpy\\nIn the previous example, we selected a specific digit alongside the first axis using the\\nsyntax train_images[i]. Selecting specific elements in a tensor is called tensor slicing.\\nLet’s look at the tensor-slicing operations you can do on Numpy arrays.\\n The following example selects digits #10 to #100 (#100 isn’t included) and puts\\nthem in an array of shape (90, 28, 28):\\n>>> my_slice = train_images[10:100]\\n>>> print(my_slice.shape)\\n(90, 28, 28)\\nIt’s equivalent to this more detailed nota tion, which specifies a start index and stop\\nindex for the slice along each tensor axis. Note that : is equivalent to selecting the\\nentire axis:\\n>>> my_slice = train_images[10:100, :, :]\\n>>> my_slice.shape\\n(90, 28, 28)\\n>>> my_slice = train_images[10:100, 0:28, 0:28]\\n>>> my_slice.shape\\n(90, 28, 28)\\nIn general, you may select between any two indices along each tensor axis. For\\ninstance, in order to select 14 × 14 pixels in the bottom-right corner of all images, you\\ndo this:\\nmy_slice = train_images[:, 14:, 14:]\\nIt’s also possible to use negative indices. Much like negative indices in Python lists,\\nthey indicate a position relative to the end of the current axis. In order to crop the\\nimages to patches of 14 × 14 pixels centered in the middle, you do this:\\nmy_slice = train_images[:, 7:-7, 7:-7]\\n2.2.7 The notion of data batches\\nIn general, the first axis (axis 0, because inde xing starts at 0) in all data tensors you’ll\\ncome across in deep learning will be the samples axis  (sometimes called the samples\\ndimension). In the MNIST example, samples are images of digits.\\n In addition, deep-learning models don’t pr ocess an entire dataset at once; rather,\\nthey break the data into small batches. Concretely, here’s one batch of our MNIST dig-\\nits, with batch size of 128:\\nbatch = train_images[:128]\\nAnd here’s the next batch:\\nbatch = train_images[128:256]\\nAnd the nth batch:\\nbatch = train_images[128 * n:128 * (n + 1)]\\nEquivalent to the \\nprevious example\\nAlso equivalent to the \\nprevious example\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 48, 'page_label': '49'}, page_content='35Data representations for neural networks\\nWhen considering such a batch tensor, the first axis (axis 0) is called the batch axis or\\nbatch dimension. This is a term you’ll frequently encounter when using Keras and other\\ndeep-learning libraries. \\n2.2.8 Real-world examples of data tensors\\nLet’s make data tensors more concrete wi th a few examples similar to what you’ll\\nencounter later. The data you’ll manipulate wi ll almost always fall into one of the fol-\\nlowing categories:\\n\\uf0a1 Vector data—2D tensors of shape (samples, features)\\n\\uf0a1 Timeseries data or sequence data —3D tensors of shape (samples, timesteps,\\nfeatures)\\n\\uf0a1 Images—4D tensors of shape (samples, height, width, channels) or (samples,\\nchannels, height, width)\\n\\uf0a1 Video—5D tensors of shape (samples, frames, height, width, channels) or\\n(samples, frames, channels, height, width)\\n2.2.9 Vector data\\nThis is the most common case. In such a dataset, each single data point can be encoded\\nas a vector, and thus a batch of data will be encoded as a 2D tensor (that is, an array of\\nvectors), where the first axis is the samples axis and the second axis is the features axis.\\n Let’s take a look at two examples:\\n\\uf0a1 An actuarial dataset of people, wher e we consider each person’s age, ZIP code,\\nand income. Each person can be characterized as a vector of 3 values, and thus\\nan entire dataset of 100,000 people can be stored in a \\n2D tensor of shape\\n(100000, 3).\\n\\uf0a1 A dataset of text documents, where we represent each document by the counts\\nof how many times each word appears in it (out of a dictionary of 20,000 com-\\nmon words). Each document can be encode d as a vector of 20,000 values (one\\ncount per word in the dictionary), and thus an entire dataset of 500 documents\\ncan be stored in a tensor of shape (500, 20000). \\n2.2.10 Timeseries data or sequence data\\nWhenever time matters in your data (or the notion of sequence order), it makes sense\\nto store it in a 3D tensor with an explicit time axis. Each sample can be encoded as a\\nsequence of vectors (a 2D tensor), and thus a batch of data will be encoded as a 3D\\ntensor (see figure 2.3).\\nFeatures\\nTimesteps\\nSamples\\nFigure 2.3 A 3D timeseries data tensor\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 49, 'page_label': '50'}, page_content='36 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nThe time axis is always the second axis (axis of index 1), by convention. Let’s look at a\\nfew examples:\\n\\uf0a1 A dataset of stock prices. Every minute, we store the current price of the stock,\\nthe highest price in the past minute, an d the lowest price in the past minute.\\nThus every minute is encoded as a 3D vector, an entire day of trading is\\nencoded as a 2D tensor of shape (390, 3) (there are 390 minutes in a trading\\nday), and 250 days’ worth of data can be stored in a 3D tensor of shape (250,\\n390, 3). Here, each sample would be one day’s worth of data.\\n\\uf0a1 A dataset of tweets, where we encode each tweet as a sequence of 280 characters\\nout of an alphabet of 128 unique characters. In this setting, each character can\\nbe encoded as a binary vector of size 128 (an all-zeros vector except for a 1 entry\\nat the index corresponding to the character). Then each tweet can be encoded\\nas a 2D tensor of shape (280, 128) , and a dataset of 1 million tweets can be\\nstored in a tensor of shape (1000000, 280, 128). \\n2.2.11 Image data\\nImages typically have three dimensions: he ight, width, and color depth. Although\\ngrayscale images (like our MNIST digits) have only a single color channel and could\\nthus be stored in 2D tensors, by convention image tensors are always 3D, with a one-\\ndimensional color channel for grayscale im ages. A batch of 128 grayscale images of\\nsize 256 × 256 could thus be stored in a tensor of shape (128, 256, 256, 1), and a\\nbatch of 128 color images could be stored in a tensor of shape (128, 256, 256, 3)\\n(see figure 2.4).\\nThere are two conventions for shapes of images tensors: the channels-last convention\\n(used by TensorFlow) and the channels-first convention (used by Theano). The Tensor-\\nFlow machine-learning framework, from G oogle, places the color-depth axis at the\\nend: (samples, height, width, color_depth). Meanwhile, Theano places the color\\ndepth axis right after the batch axis: (samples, color_depth, height, width). With\\nColor channels\\nHeight\\nWidth\\nSamples\\nFigure 2.4 A 4D image data \\ntensor (channels-first convention)\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 50, 'page_label': '51'}, page_content='37Data representations for neural networks\\nthe Theano convention, the previous examples would become (128, 1, 256, 256)\\nand (128, 3, 256, 256). The Keras framework provides support for both formats. \\n2.2.12 Video data\\nVideo data is one of the few types of real-world data for which you’ll need 5D tensors.\\nA video can be understood as a sequence of frames, each frame being a color image.\\nBecause each frame can be stored in a 3D tensor (height, width, color_depth), a\\nsequence of frames can be stored in a 4D tensor (frames, height, width, color_\\ndepth), and thus a batch of differen t videos can be stored in a 5D tensor of shape\\n(samples, frames, height, width, color_depth).\\n For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames per\\nsecond would have 240 frames. A batch of fo ur such video clips would be stored in a\\ntensor of shape (4, 240, 144, 256, 3). That’s a total of 106,168,320 values! If the\\ndtype of the tensor was float32, then each value would be stored in 32 bits, so the\\ntensor would represent 405 MB. Heavy! Vi deos you encounter in real life are much\\nlighter, because they aren’t stored in float32, and they’re typically compressed by a\\nlarge factor (such as in the MPEG format). \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 51, 'page_label': '52'}, page_content=\"38 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\n2.3 The gears of neural ne tworks: tensor operations\\nMuch as any computer program can be ulti mately reduced to a small set of binary\\noperations on binary inputs ( AND, OR, NOR, and so on), all transformations learned\\nby deep neural networks can be reduced to a handful of tensor operations applied to\\ntensors of numeric data. For instance, it’s possible to add tensor s, multiply tensors,\\nand so on.\\n In our initial example, we were building our network by stacking Dense layers on\\ntop of each other. A Keras layer instance looks like this:\\nkeras.layers.Dense(512, activation='relu')\\nThis layer can be interpreted as a function, which takes as input a 2D tensor and\\nreturns another 2D tensor—a new representation for the input tensor. Specifically, the\\nfunction is as follows (where W is a 2D tensor and b is a vector, both attributes of the\\nlayer):\\noutput = relu(dot(W, input) + b)\\nLet’s unpack this. We have three tensor operations here: a dot product (dot) between\\nthe input tensor and a tensor named W; an addition (+) between the resulting 2D ten-\\nsor and a vector b; and, finally, a relu operation. relu(x) is max(x, 0).\\nNOTE Although this section deals entirely with linear algebra expressions,\\nyou won’t find any mathematical notation here. I’ve found that mathematical\\nconcepts can be more readily mastered  by programmers with no mathemati-\\ncal background if they’re expressed as short Python snippets instead of math-\\nematical equations. So we’ll use Numpy code throughout.\\n2.3.1 Element-wise operations\\nThe relu operation and addition are element-wise operations: operations that are\\napplied independently to each entry in the tensors being considered. This means\\nthese operations are highly amenable to massively parallel implementations (vectorized\\nimplementations, a term that comes from the vector processor  supercomputer archi-\\ntecture from the 1970–1990 period). If you want to write a naive Python imple-\\nmentation of an element-wise operation, you use a for loop, as in this naive\\nimplementation of an element-wise relu operation:\\ndef naive_relu(x):\\nassert len(x.shape) == 2\\nx = x.copy()\\nfor i in range(x.shape[0]):\\nfor j in range(x.shape[1]):\\nx[i, j] = max(x[i, j], 0)\\nreturn x\\nx is a 2D Numpy tensor.\\nAvoid overwriting the input tensor.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 52, 'page_label': '53'}, page_content='39The gears of neural networks: tensor operations\\nYou do the same for addition:\\ndef naive_add(x, y):\\nassert len(x.shape) == 2\\nassert x.shape == y.shape\\nx = x.copy()\\nfor i in range(x.shape[0]):\\nfor j in range(x.shape[1]):\\nx[i, j] += y[i, j]\\nreturn x\\nOn the same principle, you can do element-wise multiplication, subtraction, and so on.\\n In practice, when dealing with Numpy arrays, these operations are available as well-\\noptimized built-in Numpy func tions, which themselves delegate the heavy lifting to a\\nBasic Linear Algebra Subprograms ( BLAS) implementation if you have one installed\\n(which you should). BLAS are low-level, highly parallel, efficient tensor-manipulation\\nroutines that are typically implemented in Fortran or C.\\n So, in Numpy, you can do the following element-wise operation, and it will be blaz-\\ning fast:\\nimport numpy as np\\nz=x+y\\nz = np.maximum(z, 0.)\\n2.3.2 Broadcasting\\nOur earlier naive implementation of naive_add only supports the addition of 2D ten-\\nsors with identical shapes. But in the Dense layer introduced earlier, we added a 2D\\ntensor with a vector. What happens with ad dition when the shapes of the two tensors\\nbeing added differ?\\n When possible, and if there’s no ambiguity, the smaller tensor will be broadcasted to\\nmatch the shape of the larger tensor. Broadcasting consists of two steps:\\n1 Axes (called broadcast axes) are added to the smaller tensor to match the ndim of\\nthe larger tensor.\\n2 The smaller tensor is repeated alongside these new axes to match the full shape\\nof the larger tensor.\\nLet’s look at a concre te example. Consider X with shape (32, 10) and y with shape\\n(10,). First, we add an empty first axis to y, whose shape becomes (1, 10). Then, we\\nrepeat y 32 times alongside this new axis, so that we end up with a tensor Y with shape\\n(32, 10), where Y[i, :] == y for i in range(0, 32). At this point, we can proceed to\\nadd X and Y, because they have the same shape.\\n In terms of implementation, no new 2D tensor is created, because that would be\\nterribly inefficient. The repetition operation is  entirely virtual: it happens at the algo-\\nrithmic level rather than at the memory level. But thinking of the vector being\\nx and y are 2D \\nNumpy tensors.\\nAvoid overwriting \\nthe input tensor.\\nElement-wise addition\\nElement-wise relu \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 53, 'page_label': '54'}, page_content='40 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nrepeated 10 times alongside a new axis is a helpful mental model. Here’s what a naive\\nimplementation would look like:\\ndef naive_add_matrix_and_vector(x, y):\\nassert len(x.shape) == 2\\nassert len(y.shape) == 1\\nassert x.shape[1] == y.shape[0]\\nx = x.copy()\\nfor i in range(x.shape[0]):\\nfor j in range(x.shape[1]):\\nx[i, j] += y[j]\\nreturn x\\nWith broadcasting, you can generally apply tw o-tensor element-wise operations if one\\ntensor has shape (a, b, … n, n + 1, … m) and the other has shape (n, n + 1, … m). The\\nbroadcasting will then automatically happen for axes a through n - 1.\\n The following example applies the element-wise maximum operation to two tensors\\nof different shapes via broadcasting:\\nimport numpy as np\\nx = np.random.random((64, 3, 32, 10))\\ny = np.random.random((32, 10))\\nz = np.maximum(x, y)\\n2.3.3 Tensor dot\\nThe dot operation,  also called a tensor product (not to be confused with an element-\\nwise product) is the most common, most  useful tensor operation. Contrary to\\nelement-wise operations, it combines entries in the input tensors.\\n An element-wise product is done with the * operator in Numpy, Keras, Theano,\\nand TensorFlow. dot uses a different syntax in Te nsorFlow, but in both Numpy and\\nKeras it’s done using the standard dot operator:\\nimport numpy as np\\nz = np.dot(x, y)\\nIn mathematical notation, you’d note the operation with a dot (.):\\nz=x.y\\nMathematically, what does the dot operatio n do? Let’s start with the dot product of\\ntwo vectors x and y. It’s computed as follows:\\ndef naive_vector_dot(x, y):\\nassert len(x.shape) == 1\\nassert len(y.shape) == 1\\nassert x.shape[0] == y.shape[0]\\nx is a 2D Numpy tensor.\\ny is a Numpy vector.\\nAvoid overwriting \\nthe input tensor.\\nx is a random tensor with \\nshape (64, 3, 32, 10).\\ny is a random tensor \\nwith shape (32, 10).\\nThe output z has shape \\n(64, 3, 32, 10) like x.\\nx and y are Numpy vectors.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 54, 'page_label': '55'}, page_content='41The gears of neural networks: tensor operations\\nz=0 .\\nfor i in range(x.shape[0]):\\nz += x[i] * y[i]\\nreturn z\\nYou’ll have noticed that the dot product betw een two vectors is a scalar and that only\\nvectors with the same number of elements are compatible for a dot product.\\n You can also take the dot product between a matrix x and a vector y, which returns\\na vector where the coefficients are the dot products between y and the rows of x. You\\nimplement it as follows:\\nimport numpy as np\\ndef naive_matrix_vector_dot(x, y):\\nassert len(x.shape) == 2\\nassert len(y.shape) == 1\\nassert x.shape[1] == y.shape[0]\\nz = np.zeros(x.shape[0])\\nfor i in range(x.shape[0]):\\nfor j in range(x.shape[1]):\\nz[i] += x[i, j] * y[j]\\nreturn z\\nYou could also reuse the code we wrote pr eviously, which highlights the relationship\\nbetween a matrix-vector product and a vector product:\\ndef naive_matrix_vector_dot(x, y):\\nz = np.zeros(x.shape[0])\\nfor i in range(x.shape[0]):\\nz[i] = naive_vector_dot(x[i, :], y)\\nreturn z\\nNote that as soon as one of the two tensors has an ndim greater than 1, dot is no lon-\\nger symmetric, which is to say that dot(x, y) isn’t the same as dot(y, x).\\n Of course, a dot product generalizes to te nsors with an arbitrary number of axes.\\nThe most common applications may be th e dot product between two matrices. You\\ncan take the dot product of two matrices x and y ( dot(x, y)) if and only if\\nx.shape[1] == y.shape[0]. The result is a matrix with shape (x.shape[0],\\ny.shape[1]), where the coefficients are the vector products between the rows of x\\nand the columns of y. Here’s the naive implementation:\\ndef naive_matrix_dot(x, y):\\nassert len(x.shape) == 2\\nassert len(y.shape) == 2\\nassert x.shape[1] == y.shape[0]\\nz = np.zeros((x.shape[0], y.shape[1]))\\nfor i in range(x.shape[0]):\\nfor j in range(y.shape[1]):\\nrow_x = x[i, :]\\ncolumn_y = y[:, j]\\nz[i, j] = naive_vector_dot(row_x, column_y)\\nreturn z\\nx is a Numpy matrix.\\ny is a Numpy vector.\\nThe first dimension of x must be the \\nsame as the 0th dimension of y!\\nThis operation returns a vector of \\n0s with the same shape as y.\\nx and y\\nare\\nNumpy\\nmatrices.\\nThe first dimension of x must be the \\nsame as the 0th dimension of y!\\nThis operation returns a matrix \\nof 0s with a specific shape.\\nIterates over the rows of x …\\n… and over the columns of y.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 55, 'page_label': '56'}, page_content='42 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nTo understand dot-product shape compatibility, it helps to visualize the input and out-\\nput tensors by aligning them as shown in figure 2.5.\\nx, y, and z are pictured as rectangl es (literal boxes of coefficients). Because the rows\\nand x and the columns of y must have the same size, it follows that the width of x must\\nmatch the height of y. If you go on to develop ne w machine-learning algorithms,\\nyou’ll likely be drawing such diagrams often.\\n More generally, you can take the dot product between higher-dimensional tensors,\\nfollowing the same rules for shape compatibility as outlined earlier for the 2D case:\\n(a, b, c, d) . (d,) -> (a, b, c)\\n(a, b, c, d) . (d, e) -> (a, b, c, e)\\nAnd so on. \\n2.3.4 Tensor reshaping\\nA third type of tensor operation that’s essential to understand is tensor reshaping .\\nAlthough it wasn’t used in the Dense layers in our first neural network example, we\\nused it when we preprocessed the digits data before feeding it into our network:\\ntrain_images = train_images.reshape((60000, 28 * 28))\\nReshaping a tensor means rearranging its ro ws and columns to match a target shape.\\nNaturally, the reshaped tensor has the same total number of coefficients as the initial\\ntensor. Reshaping is best understood via simple examples:\\n>>> x = np.array([[0., 1.],\\n[2., 3.],\\n[4., 5.]])\\n>>> print(x.shape)\\n(3, 2)\\na\\nb\\nx . y = z\\nb\\nx.shape:\\n(a, b)\\ny.shape:\\n(b, c)\\nz.shape:\\n(a, c)\\nRow of x\\nColumn of y\\nz [ i,  j ]\\nc\\nFigure 2.5 Matrix dot-product \\nbox diagram\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 56, 'page_label': '57'}, page_content='43The gears of neural networks: tensor operations\\n>>> x = x.reshape((6, 1))\\n>>> x\\narray([[ 0.],\\n[ 1.],\\n[ 2.],\\n[ 3.],\\n[ 4.],\\n[ 5.]])\\n>>> x = x.reshape((2, 3))\\n>>> x\\narray([[ 0., 1., 2.],\\n[ 3., 4., 5.]])\\nA special case of reshaping that’s commonly encountered is transposition. Transposing a\\nmatrix means exchanging its rows and its columns, so that x[i, :] becomes x[:, i]:\\n>>> x = np.zeros((300, 20))\\n>>> x = np.transpose(x)\\n>>> print(x.shape)\\n(20, 300)\\n2.3.5 Geometric interpretation of tensor operations\\nBecause the contents of the tensors manipu lated by tensor operations can be inter-\\npreted as coordinates of points in some ge ometric space, all tensor operations have a\\ngeometric interpretation. For instance, let’s consider addition. We’ll start with the fol-\\nlowing vector:\\nA = [0.5, 1]\\nIt’s a point in a 2D space (see figure 2.6). It’s commo n to picture a vector as an arrow\\nlinking the origin to the point, as shown in figure 2.7.\\nCreates an all-zeros matrix \\nof shape (300, 20) \\n1\\n1\\nA [0.5, 1]\\nFigure 2.6 A point in a 2D space\\n1\\n1\\nA [0.5, 1]\\nFigure 2.7 A point in a 2D space \\npictured as an arrow\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 57, 'page_label': '58'}, page_content='44 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nLet’s consider a new point, B = [1, 0.25], which we’ll add to the previous one. This is\\ndone geometrically by chaining together the vector arrows, with the resulting location\\nbeing the vector representing the sum of the previous two vectors (see figure 2.8).\\nIn general, elementary geometric operations such as affine transformations, rotations,\\nscaling, and so on can be ex pressed as tensor operations. For instance, a rotation of a\\n2D vector by an angle theta can be achiev ed via a dot product with a 2 × 2 matrix\\nR=[ u , v], where u and v are both vectors of the plane: u = [cos(theta),\\nsin(theta)] and v = [-sin(theta), cos(theta)]. \\n2.3.6 A geometric interpre tation of deep learning\\nYou just learned that neural networks consist entirely of chains of tensor operations and\\nthat all of these tensor operations are just geometric transformations of the input data.\\nIt follows that you can interpret a neural network as a very complex geometric transfor-\\nmation in a high-dimensional space, implemented via a long series of simple steps.\\n In 3D, the following mental image may prove useful. Imagine two sheets of colored\\npaper: one red and one blue. Put one on  top of the other. Now crumple them\\ntogether into a small ball. That crumpled paper ball is your input data, and each sheet\\nof paper is a class of data in a classifica tion problem. What a neural network (or any\\nother machine-learning model) is meant to do is figure out a transformation of the\\npaper ball that would uncrumple it, so as to make the two classes cleanly separable\\nagain. With deep learning, this would be im plemented as a series of simple transfor-\\nmations of the 3D space, such as those you could apply on the paper ball with your fin-\\ngers, one movement at a time.\\n1\\n1\\nA\\nB\\nA + B\\nFigure 2.8 Geometric interpretation of \\nthe sum of two vectors\\nFigure 2.9 Uncrumpling a \\ncomplicated manifold of data\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 58, 'page_label': '59'}, page_content='45The gears of neural networks: tensor operations\\nUncrumpling paper balls is what machine lear ning is about: find ing neat representa-\\ntions for complex, highly folded data mani f o l d s .  A t  t h i s  p o i n t ,  y o u  s h o u l d  h a v e  a\\npretty good intuition as to why deep learni ng excels at this: it takes the approach of\\nincrementally decomposing a complicated geometric transformation into a long\\nchain of elementary ones, which is pretty much the strategy a human would follow to\\nuncrumple a paper ball. Each layer in a d eep network applies a transformation that\\ndisentangles the data a little—and a deep stack of layers makes tractable an extremely\\ncomplicated disentanglement process. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 59, 'page_label': '60'}, page_content='46 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\n2.4 The engine of neural networks: \\ngradient-based optimization\\nAs you saw in the previous section, each neural layer from our first network example\\ntransforms its input data as follows:\\noutput = relu(dot(W, input) + b)\\nIn this expression, W and b are tensors that are attributes  of the layer. They’re called\\nthe weights or trainable parameters of the layer (the kernel and bias attributes, respec-\\ntively). These weights contain the informat ion learned by the network from exposure\\nto training data.\\n Initially, these weight matrices are filled with small random values (a step called ran-\\ndom initialization). Of course, there’s no reason to expect that relu(dot(W, input) + b),\\nwhen W and b are random, will yield any useful representations. The resulting represen-\\ntations are meaningless—but they’re a starting point. What comes next is to gradually\\nadjust these weights, based on a feedback signal. This gradual adjustment, also called\\ntraining, is basically the learning that machine learning is all about.\\n This happens within what’s called a training loop, which works as follows. Repeat\\nthese steps in a loop, as long as necessary:\\n1 Draw a batch of training samples x and corresponding targets y.\\n2 Run the network on x (a step called the forward pass) to obtain predictions y_pred.\\n3 Compute the loss of the network on the batch, a measure of the mismatch\\nbetween y_pred and y.\\n4 Update all weights of the network in a wa y that slightly reduces the loss on this\\nbatch.\\nYou’ll eventually end up with a network that has a very low loss on its training data: a\\nlow mismatch between predictions y_pred and expected targets y. The network has\\n“learned” to map its inputs to correct target s. From afar, it may look like magic, but\\nwhen you reduce it to elementary steps, it turns out to be simple.\\n Step 1 sounds easy enough—just I/O code. Steps 2 and 3 are merely the applica-\\ntion of a handful of tensor operations, so you could implement these steps purely\\nfrom what you learned in the previous section. The difficult part is step 4: updating\\nthe network’s weights. Given an individual weight coefficient in the network, how can\\nyou compute whether the coefficient should be increased or decreased, and by how\\nmuch?\\n One naive solution would be to freeze al l weights in the network except the one\\nscalar coefficient being consid ered, and try different values for this coefficient. Let’s\\nsay the initial value of the coefficient is 0. 3. After the forward pass on a batch of data,\\nthe loss of the network on the batch is 0.5. If you change the coefficient’s value to 0.35\\nand rerun the forward pass, the loss increases to 0.6. But if you lower the coefficient to\\n0.25, the loss falls to 0.4. In this case, it seems that updating the coefficient by -0.05\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 60, 'page_label': '61'}, page_content=\"47The engine of neural networks: gradient-based optimization\\nwould contribute to minimizing the loss. This would have to be repeated for all coeffi-\\ncients in the network.\\n But such an approach would be horribly  inefficient, because you’d need to com-\\npute two forward passes (which are expensiv e) for every individual coefficient (of\\nwhich there are many, usually thousands an d sometimes up to millions). A much bet-\\nter approach is to take advantage of the fa ct that all operations used in the network\\nare differentiable, and compute the gradient of the loss with regard to the network’s\\ncoefficients. You can then move the coeffici ents in the opposite direction from the\\ngradient, thus decreasing the loss.\\n If you already know what differentiable means and what a gradient is, you can skip to\\nsection 2.4.3. Otherwise, the following two sections will help you understand these\\nconcepts.\\n2.4.1 What’s a derivative?\\nConsider a continuous, smooth function f(x) = y, mapping a real number x to a new\\nreal number y. Because the function is continuous, a small change in x can only result\\nin a small change in y—that’s the intuition behind continuity. Let’s say you increase x\\nby a small factor epsilon_x: this results in a small epsilon_y change to y:\\nf(x + epsilon_x) = y + epsilon_y\\nIn addition, because the function is smooth (its curve doesn’t have any abrupt angles),\\nwhen epsilon_x is small enough, around a certain point p, it’s possible to approxi-\\nmate f as a linear function of slope a, so that epsilon_y becomes a * epsilon_x:\\nf(x + epsilon_x) = y + a * epsilon_x\\nObviously, this linear approximation is valid only when x is close enough to p.\\n The slope a is called the derivative of f in p. If a is negative, it means a small change\\nof x around p will result in a decrease of f(x) (as shown in figure 2.10); and if a is pos-\\nitive, a small change in x will result in an increase of f(x). Further, the absolute value\\nof a (the magnitude of the derivative) tells you how quickly this increase or decrease\\nwill happen.\\nFor every differentiable function f(x) (differentiable means “can be derived”: for exam-\\nple, smooth, continuous functions can be de rived), there exists a derivative function\\nf'(x) that maps values of x to the slope of the local linear approximation of f in those\\nLocal linear\\napproximation of f,\\nwith slope a\\nf Figure 2.10 Derivative of f in p\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 61, 'page_label': '62'}, page_content=\"48 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\npoints. For instance, the derivative of cos(x) is -sin(x), the derivative of f(x) = a * x\\nis f'(x) = a, and so on.\\n If you’re trying to update x by a factor epsilon_x in order to minimize f(x), and\\nyou know the derivative of f, then your job is done: the derivative completely\\ndescribes how f(x) evolves as you change x. If you want to reduce the value of f(x),\\nyou just need to move x a little in the opposite direction from the derivative. \\n2.4.2 Derivative of a tensor  operation: the gradient\\nA gradient is the derivative of a tensor operation. It’s the generalization of the concept\\nof derivatives to functions of multidimension al inputs: that is, to functions that take\\ntensors as inputs.\\n Consider an input vector x, a matrix W, a target y, and a loss function loss. You can\\nuse W to compute a target candidate y_pred, and compute the loss, or mismatch,\\nbetween the target candidate y_pred and the target y:\\ny_pred = dot(W, x)\\nloss_value = loss(y_pred, y)\\nIf the data inputs x and y are frozen, then this can be interpreted as a function map-\\nping values of W to loss values:\\nloss_value = f(W)\\nLet’s say the current value of W is W0. Then the derivative of f in the point W0 is a tensor\\ngradient(f)(W0) with the same shape as W, where each coefficient gradient(f)\\n(W0)[i, j] indicates the direction and magnitude of the change in loss_value you\\nobserve when modifying W0[i, j]. That tensor gradient(f)(W0) is the gradient of\\nthe function f(W) = loss_value in W0.\\n You saw earlier that the derivative of a function f(x) of a single coefficient can be\\ninterpreted as the slope of the curve of f. Likewise, gradient(f)(W0) can be inter-\\npreted as the tensor describing the curvature of f(W) around W0.\\n For this reason, in much the same way that, for a function f(x), you can reduce\\nthe value of f(x) by moving x a little in the opposite direction from the derivative,\\nwith a function f(W) of a tensor, you can reduce f(W) by moving W in the opposite\\ndirection from the gradient: for example, W1 = W0 - step * gradient(f)(W0) (where\\nstep is a small scaling factor). That means going against the curvature, which intui-\\ntively should put you lower on the cu rve. Note that the scaling factor step is needed\\nbecause gradient(f)(W0) only approximates the curv ature when you’re close to W0,\\nso you don’t want to get too far from W0. \\n2.4.3 Stochastic gradient descent\\nGiven a differentiable function, it’s theoreti cally possible to find its minimum analyti-\\ncally: it’s known that a function’s minimum is  a point where the derivative is 0, so all\\nyou have to do is find all the points where the derivative goes to 0 and check for which\\nof these points the function has the lowest value.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 62, 'page_label': '63'}, page_content='49The engine of neural networks: gradient-based optimization\\n Applied to a neural network, that mean s finding analytically the combination of\\nweight values that yields the smallest possibl e loss function. This can be done by solv-\\ning the equation gradient(f)(W) = 0 for W. This is a polynomial equation of N vari-\\nables, where N is the number of coefficients in the network. Although it would be\\npossible to solve such  an equation for N = 2 or N = 3, doing so is intractable for real\\nneural networks, where the number of para meters is never less than a few thousand\\nand can often be several tens of millions.\\n Instead, you can use the four-step algorith m outlined at the beginning of this sec-\\ntion: modify the parameters little by littl e based on the current loss value on a ran-\\ndom batch of data. Because you’re dealing with a differentiable function, you can\\ncompute its gradient, which gives you an efficient way to implement step 4. If you\\nupdate the weights in the opposite direction from the gradient, the loss will be a little\\nless every time:\\n1 Draw a batch of training samples x and corresponding targets y.\\n2 Run the network on x to obtain predictions y_pred.\\n3 Compute the loss of the network on the batch, a measure of the mismatch\\nbetween \\ny_pred and y.\\n4 Compute the gradient of the loss with regard to the network’s parameters (a\\nbackward pass).\\n5 Move the parameters a little in the o pposite direction from the gradient—for\\nexample W -= step * gradient—thus reducing the loss on the batch a bit.\\nEasy enough! What I just described is called mini-batch stochastic gradient descent  (mini-\\nbatch SGD). The term stochastic refers to the fact that each batch of data is drawn at\\nrandom (stochastic is a scientific synonym of random). Figure 2.11 illustrates what hap-\\npens in 1D, when the network has only one parameter and you have only one training\\nsample.\\nLoss\\nvalue Starting\\npoint (t=0)\\nStep, also called learning rate\\nt=1\\nt=2\\nt=3\\nParameter\\nvalue\\nFigure 2.11 SGD down a 1D loss \\ncurve (one learnable parameter)\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 63, 'page_label': '64'}, page_content='50 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nAs you can see, intuitively it’s important to pick a reasonable value for the step factor.\\nIf it’s too small, the descent down the curv e will take many iterations, and it could get\\nstuck in a local minimum. If step is too large, your updates may end up taking you to\\ncompletely random locations on the curve.\\n Note that a variant of the mini-batch SGD algorithm would be to draw a single sam-\\nple and target at each iteration, rather than drawing a batch of data. This would be\\ntrue SGD (as opposed to mini-batch SGD). Alternatively, going to the opposite extreme,\\nyou could run every step on all data available, which is called batch SGD. Each update\\nwould then be more accurate, but far mo re expensive. The efficient compromise\\nbetween these two extremes is to use mini-batches of reasonable size.\\n Although figure 2.11 illustrates gradient descent in a 1D parameter space, in prac-\\ntice you’ll use gradient descent in highly dimensional spaces: every weight coefficient\\nin a neural network is a free dimension in the space, and there may be tens of thou-\\nsands or even millions of them. To help you build intuition about loss surfaces, you\\ncan also visualize gradient descent along a 2D loss surface, as shown in figure 2.12. But\\nyou can’t possibly visualize what the actual process of training a neural network looks\\nlike—you can’t represent a 1,000,000-dimensional space in a way that makes sense to\\nhumans. As such, it’s good to keep in mi nd that the intuitions you develop through\\nthese low-dimensional representations may no t always be accurate in practice. This\\nhas historically been a source of issues in the world of deep-learning research.\\nAdditionally, there exist multiple variants of SGD tha t  d if fe r b y  t a ki ng i nto  a c co unt\\nprevious weight updates when computing the next weight update, rather than just\\nlooking at the current value of the gradients. There is, for instance, SGD with momen-\\ntum, as well as Adagrad, RMSProp, and several others. Such variants are known as opti-\\nmization methods or optimizers. In particular, the concept of momentum, which is used in\\nmany of these variants, deserves your atte ntion. Momentum addresses two issues with\\nSGD: convergence speed and local minima. Co nsider figure 2.13, which shows the\\ncurve of a loss as a function of a network parameter.\\nStarting point\\nFinal point\\n45\\n40\\n35\\n30\\n25\\n20\\n15\\n10\\n5\\nFigure 2.12 Gradient descent \\ndown a 2D loss surface (two \\nlearnable parameters)\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 64, 'page_label': '65'}, page_content=\"51The engine of neural networks: gradient-based optimization\\nAs you can see, around a certai n parameter value, there is a local minimum : around\\nthat point, moving left would result in th e loss increasing, but so would moving right.\\nIf the parameter under consideration were being optimized via SGD with a small\\nlearning rate, then the optimization proc ess would get stuck at the local minimum\\ninstead of making its way to the global minimum.\\n You can avoid such issues by using momentum, which draws inspiration from phys-\\nics. A useful mental image he re is to think of the optimization process as a small ball\\nrolling down the loss curve. If it has enou gh momentum, the ball won’t get stuck in a\\nravine and will end up at the global minimum. Momentum is implemented by moving\\nthe ball at each step based not only on th e current slope value (current acceleration)\\nbut also on the current velocity (resulting from past acceleration). In practice, this\\nmeans updating the parameter w based not only on the current gradient value but also\\non the previous parameter update, such as in this naive implementation:\\npast_velocity = 0.\\nmomentum = 0.1\\nwhile loss > 0.01:\\nw, loss, gradient = get_current_parameters()\\nvelocity = past_velocity * momentum + learning_rate * gradient\\nw=w+ momentum * velocity - learning_rate * gradient\\npast_velocity = velocity\\nupdate_parameter(w)\\n2.4.4 Chaining derivatives: the Backpropagation algorithm\\nIn the previous algorithm, we casually assu med that because a function is differentia-\\nble, we can explicitly compute its derivative. In practice, a neural network function\\nconsists of many tensor operations chai ned together, each of which has a simple,\\nknown derivative. For instance, this is a network f composed of three tensor opera-\\ntions, a, b, and c, with weight matrices W1, W2, and W3:\\nf(W1, W2, W3) = a(W1, b(W2, c(W3)))\\nCalculus tells us that such a chain of functions can be derived using the following iden-\\ntity, called the chain rule: f(g(x)) = f'(g(x)) * g'(x). Applying the chain rule to the\\ncomputation of the gradient values of a ne ural network gives rise to an algorithm\\nLoss\\nvalue\\nParameter\\nvalue\\nLocal\\nminimum\\nGlobal\\nminimum\\nFigure 2.13 A local minimum \\nand a global minimum\\nConstant momentum factor\\nOptimization loop \\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 65, 'page_label': '66'}, page_content='52 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\ncalled Backpropagation (also sometimes called reverse-mode differentiation). Backpropaga-\\ntion starts with the final loss value and work s backward from the top layers to the bot-\\ntom layers, applying the chain rule to com pute the contribution that each parameter\\nhad in the loss value.\\n Nowadays, and for years to come, peop le will implement networks in modern\\nframeworks that are capable of symbolic differentiation, such as TensorFlow. This means\\nthat, given a chain of operations with a known derivative, they can compute a gradient\\nfunction for the chain (by applying the chain rule) that maps network parameter values\\nto gradient values. When you have access to such a function, the backward pass is\\nreduced to a call to this gradient function . Thanks to symbolic differentiation, you’ll\\nnever have to implement the Backpropagation algorithm by hand. For this reason, we\\nwon’t waste your time and your focus on de riving the exact formulation of the Back-\\npropagation algorithm in these pages. All you need is a good understanding of how\\ngradient-based optimization works. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 66, 'page_label': '67'}, page_content=\"53Looking back at our first example\\n2.5 Looking back at our first example\\nYou’ve reached the end of this chapter, and you should now have a general under-\\nstanding of what’s going on behind the scenes in a neural network. Let’s go back to\\nthe first example and review each piece of it in the light of what you’ve learned in the\\nprevious three sections.\\n This was the input data:\\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\\ntrain_images = train_images.reshape((60000, 28 * 28))\\ntrain_images = train_images.astype('float32') / 255\\ntest_images = test_images.reshape((10000, 28 * 28))\\ntest_images = test_images.astype('float32') / 255\\nNow you understand that the input images are stored in Numpy tensors, which are\\nhere formatted as float32 tensors of shape (60000, 784) (training data) and (10000,\\n784) (test data), respectively.\\n This was our network:\\nnetwork = models.Sequential()\\nnetwork.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\\nnetwork.add(layers.Dense(10, activation='softmax'))\\nNow you understand that this network consists of a chain of two Dense layers, that\\neach layer applies a few simp le tensor operations to the input data, and that these\\noperations involve weight tensors. Weight te nsors, which are attributes of the layers,\\nare where the knowledge of the network persists.\\n This was the network-compilation step:\\nnetwork.compile(optimizer='rmsprop',\\nloss='categorical_crossentropy',\\nmetrics=['accuracy'])\\nNow you understand that categorical_crossentropy is the loss function that’s used\\nas a feedback signal for learning the weight tensors, and which the training phase will\\nattempt to minimize. You also know that th is reduction of the loss happens via mini-\\nbatch stochastic gradient descent. The exact rules governing a specific use of gradient\\ndescent are defined by the rmsprop optimizer passed as the first argument.\\n Finally, this was the training loop:\\nnetwork.fit(train_images, train_labels, epochs=5, batch_size=128)\\nNow you understand what happens when you call fit: the network will start to iterate\\non the training data in mini-batches of 128  samples, 5 times over (each iteration over\\nall the training data is called an epoch). At each iteration, the network will compute the\\ngradients of the weights with regard to th e loss on the batch, and update the weights\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 67, 'page_label': '68'}, page_content='54 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\naccordingly. After these 5 epochs, the network will have performed 2,345 gradient\\nupdates (469 per epoch), and the loss of the network will be sufficiently low that the\\nnetwork will be capable of classifying handwritten digits with high accuracy.\\n At this point, you already know most of what there is to know about neural networks.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 68, 'page_label': '69'}, page_content='55Looking back at our first example\\nChapter summary\\n\\uf0a1 Learning means finding a combination of model parameters that mini-\\nmizes a loss function for a given set of training data samples and their cor-\\nresponding targets.\\n\\uf0a1 Learning happens by drawing random batches of data samples and their\\ntargets, and computing the gradient  of the network parameters with\\nrespect to the loss on the batch. The network parameters are then moved\\na bit (the magnitude of the move is defined by the learning rate) in the\\nopposite direction from the gradient.\\n\\uf0a1 The entire learning process is made possible by the fact that neural net-\\nworks are chains of differentiable tensor operations, and thus it’s possible\\nto apply the chain rule of derivation  to find the gradient function map-\\nping the current parameters and current batch of data to a gradient value.\\n\\uf0a1 Two key concepts you’ll see frequently in future chapters are loss and opti-\\nmizers. These are the two things you need to define before you begin feed-\\ning data into a network.\\n\\uf0a1 The loss is the quantity you’ll attempt to minimize during training, so it\\nshould represent a measure of success for the task you’re trying to solve.\\n\\uf0a1 The optimizer specifies the exact way in which the gradient of the loss will\\nbe used to update parameters: for instance, it could be the RMSProp opti-\\nmizer, SGD with momentum, and so on.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 69, 'page_label': '70'}, page_content='Getting started\\nwith neural networks\\nThis chapter is designed to get you starte d with using neural networks to solve real\\nproblems. You’ll consolidate the knowledg e you gained from our first practical\\nexample in chapter 2, and you’ll apply what you’ve learned to three new problems\\ncovering the three most common use cases of neural networks: binary classifica-\\ntion, multiclass classification, and scalar regression.\\n In this chapter, we’ll take a closer look at the core components of neural networks\\nthat we introduced in chapter 2: layers, networks, objective functions, and optimiz-\\ners. We’ll give you a quick introduction to Keras, the Python deep-learning library\\nthat we’ll use throughout the book. You’ll  set up a deep-learning workstation, with\\nThis chapter covers\\n\\uf0a1 Core components of neural networks\\n\\uf0a1 An introduction to Keras\\n\\uf0a1 Setting up a deep-learning workstation\\n\\uf0a1 Using neural networks to solve basic \\nclassification and regression problems'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 70, 'page_label': '71'}, page_content='57\\nTensorFlow, Keras, and GPU support. We’ll dive into th ree introductory examples of\\nhow to use neural networks to address real problems: \\n\\uf0a1 Classifying movie reviews as positive or negative (binary classification) \\n\\uf0a1 Classifying news wires by topic (multiclass classification) \\n\\uf0a1 Estimating the price of a house, given real-estate data (regression)\\nBy the end of this chapter, you’ll be ab le to use neural networks to solve simple\\nmachine problems such as classification and regression over vector data. You’ll then\\nbe ready to start building a more principled, theory-driven understanding of machine\\nlearning in chapter 4.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 71, 'page_label': '72'}, page_content=\"58 CHAPTER 3 Getting started with neural networks\\n3.1 Anatomy of a neural network\\nAs you saw in the previous chapters, training a neural network revolves around the fol-\\nlowing objects:\\n\\uf0a1 Layers, which are combined into a network (or model)\\n\\uf0a1 The input data and corresponding targets\\n\\uf0a1 The loss function, which defines the feedback signal used for learning\\n\\uf0a1 The optimizer, which determines how learning proceeds\\nYou can visualize their interaction as illust rated in figure 3.1: the network, composed\\nof layers that are chained together, maps the input data to predictions. The loss func-\\ntion then compares these predictions to the targets, producing a loss value: a measure\\nof how well the network’s pr edictions match what was ex pected. The optimizer uses\\nthis loss value to update the network’s weights.\\nLet’s take a closer look at layers, networks, loss functions, and optimizers.\\n3.1.1 Layers: the building blocks of deep learning\\nThe fundamental data structure in neural networks is the layer, to which you were\\nintroduced in chapter 2. A layer is a data-processing module that takes as input one or\\nmore tensors and that output s one or more tensors. Some  layers are stateless, but\\nmore frequently layers have a state: the layer’s weights, one or several tensors learned\\nwith stochastic gradient descent, which together contain the network’s knowledge.\\n Different layers are appropriate for different tensor formats and different types of data\\nprocessing. For instance, simple vector data, stored in 2D tensors of shape (samples,\\nfeatures), is often processed by densely connected layers, also called fully connected or dense\\nlayers (the Dense class in Keras). Sequence data, stored in 3D tensors of shape (samples,\\ntimesteps, features), is typically processed by recurrent l a y e r s  s u c h  a s  a n  LSTM layer.\\nImage data, stored in 4D tensors, is usually processed by 2D convolution layers (Conv2D).\\nLayer\\n(data transformation)\\nInput X\\nWeights\\nLayer\\n(data transformation)\\nPredictions\\nY'\\nWeight\\nupdate\\nTrue targets\\nY\\nWeights\\nLoss functionOptimizer\\nLoss score Figure 3.1 Relationship between the \\nnetwork, layers, loss function, and optimizer\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 72, 'page_label': '73'}, page_content='59Anatomy of a neural network\\n You can think of layers as the LEGO bricks of deep learning, a metaphor that is\\nmade explicit by frameworks like Keras. Building deep-learning models in Keras is\\ndone by clipping together compatible layers  to form useful data-transformation pipe-\\nlines. The notion of layer compatibility here refers specifically to the fact that every layer\\nwill only accept input tensors of a certain shape and will return output tensors of a cer-\\ntain shape. Consider the following example:\\nfrom keras import layers\\nlayer = layers.Dense(32, input_shape=(784,))\\nWe’re creating a layer that will only accept as input 2D tensors where the first dimen-\\nsion is 784 (axis 0, the batch dimension, is unspecified, and thus any value would be\\naccepted). This layer will return a tensor  where the first dimension has been trans-\\nformed to be 32.\\n Thus this layer can only be connected to a downstream layer that expects 32-\\ndimensional vectors as its input. When us ing Keras, you don’t have to worry about\\ncompatibility, because the layers you add to your models are dynamically built to\\nmatch the shape of the incoming layer. For instance, suppose you write the following:\\nfrom keras import models\\nfrom keras import layers\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(32, input_shape=(784,)))\\nmodel.add(layers.Dense(32))\\nThe second layer didn’t receive an input shape argument—instead , it automatically\\ninferred its input shape as being the output shape of the layer that came before. \\n3.1.2 Models: networks of layers\\nA deep-learning model is a directed, ac yclic graph of layers. The most common\\ninstance is a linear stack of layers, mapping a single input to a single output.\\n But as you move forward, you’ll be exposed to a much  broader variety of network\\ntopologies. Some common ones include the following:\\n\\uf0a1 Two-branch networks\\n\\uf0a1 Multihead networks\\n\\uf0a1 Inception blocks\\nThe topology of a network defines a hypothesis space. You may remember that in chap-\\nter 1, we defined machine learning as “sea rching for useful representations of some\\ninput data, within a predefined space of po ssibilities, using guidance from a feedback\\nsignal.” By choosing a networ k topology, you constrain your space of possibilities\\n(hypothesis space) to a specific series of tensor operations, mapping input data to out-\\nput data. What you’ll then be searching for is  a good set of values for the weight ten-\\nsors involved in these tensor operations.\\nA dense layer with 32 \\noutput units\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 73, 'page_label': '74'}, page_content='60 CHAPTER 3 Getting started with neural networks\\n Picking the right network architecture is more an art than a science; and although\\nthere are some best practices and principles you can rely on, only practice can help\\nyou become a proper neural-network architec t. The next few chapters will both teach\\nyou explicit principles for building neural networks and help you develop intuition as\\nto what works or doesn’t work for specific problems. \\n3.1.3 Loss functions and optimizers: \\nkeys to configuring the learning process\\nOnce the network architecture is defined, you still have to choose two more things:\\n\\uf0a1 Loss function (objective function) —The quantity that will be minimized during\\ntraining. It represents a measure of success for the task at hand.\\n\\uf0a1 Optimizer—Determines how the network will be updated based on the loss func-\\ntion. It implements a specific variant of stochastic gradient descent (SGD).\\nA neural network that has multiple outputs may have multiple loss functions (one per\\noutput). But the gradient-descent  process must be based on a single scalar loss value;\\nso, for multiloss networks, all losses are combin ed (via averaging) into a single scalar\\nquantity.\\n Choosing the right objective function fo r the right problem is extremely import-\\nant: your network will take any shortcut it can, to minimize the loss; so if the objective\\ndoesn’t fully correlate with success for the task at hand, your network will end up\\ndoing things you may not have want ed. Imagine a stupid, omnipotent AI trained via\\nSGD, with this poorly chosen objective fu nction: “maximizing the average well-being\\nof all humans alive.” To make its job easier, this AI might choose to kill all humans\\nexcept a few and focus on the well-being  of the remaining ones—because average\\nwell-being isn’t affected by how many humans are left. That might not be what you\\nintended! Just remember that all neural networks you build will be just as ruthless in\\nlowering their loss function—so choose the objective wisely, or you’ll have to face\\nunintended side effects.\\n Fortunately, when it comes to common problems such as classification, regression,\\nand sequence prediction, ther e are simple guidelines you can follow to choose the\\ncorrect loss. For instance, you’ll use binary  crossentropy for a tw o-class classification\\nproblem, categorical crossentropy for a many-class classification problem, mean-\\nsquared error for a regressi on problem, connectionist temporal classification ( CTC)\\nfor a sequence-learning problem, and so on. Only when you’re working on truly new\\nresearch problems will you have to develop your ow n objective functions. In the next\\nfew chapters, we’ll detail explicitly which loss functions to choose for a wide range of\\ncommon tasks. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 74, 'page_label': '75'}, page_content='61Introduction to Keras\\n3.2 Introduction to Keras\\nThroughout this book, the code examples use Keras ( https:/ /keras.io). Keras is a\\ndeep-learning framework for Python that provides a convenient way to define and\\ntrain almost any kind of deep-learning model. Keras was initially developed for\\nresearchers, with the aim of enabling fast experimentation.\\n Keras has the following key features:\\n\\uf0a1 It allows the same code to run seamlessly on CPU or GPU.\\n\\uf0a1 It has a user-friendly API that makes it easy to qu ickly prototype deep-learning\\nmodels.\\n\\uf0a1 It has built-in support for convolutional networks (for computer vision), recur-\\nrent networks (for sequence processing), and any combination of both.\\n\\uf0a1 It supports arbitrary network architectures: multi-input or multi-output models,\\nlayer sharing, mo del sharing, and so on. This means Keras is appropriate for\\nbuilding essentially any deep-learning model, from a generative adversarial net-\\nwork to a neural Turing machine.\\nKeras is distributed under the permissive MIT license, which means it can be freely\\nused in commercial projects. It’s compatible with any version of Python from 2.7 to 3.6\\n(as of mid-2017).\\n Keras has well over 200,000 users, rang ing from academic researchers and engi-\\nneers at both startups and large companies to graduate students and hobbyists. Keras\\nis used at Google, Netflix, Uber, CERN, Yelp, Square, and hundreds of startups work-\\ning on a wide range of problems. Keras is  also a popular framework on Kaggle, the\\nmachine-learning competition website, where almost every recent deep-learning com-\\npetition has been won using Keras models.\\nFigure 3.2 Google web search interest for di fferent deep-learning frameworks over time\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 75, 'page_label': '76'}, page_content='62 CHAPTER 3 Getting started with neural networks\\n3.2.1 Keras, TensorFlow, Theano, and CNTK\\nKeras is a model-level library, providing high-level building blocks for developing\\ndeep-learning models. It doesn’t handle low-level operations such as tensor manipula-\\ntion and differentiation. Instead, it relie s on a specialized, well-optimized tensor\\nlibrary to do so, serving as the backend engine of Keras. Rather than choosing a single\\ntensor library and tying the implementation of Keras to that library, Keras handles the\\nproblem in a modular way (see figure 3.3); thus several different backend engines can\\nbe plugged seamlessly into Keras. Currentl y, the three existing backend implementa-\\ntions are the TensorFlow backend, the Thea no backend, and the Microsoft Cognitive\\nToolkit (CNTK) backend. In the future, it’s likely that Keras will be extended to work\\nwith even more deep-learning execution engines.\\nTensorFlow, CNTK, and Theano are some of the pr imary platforms for deep learning\\ntoday. Theano (http:/ /deeplearning.net/software/theano) is developed by the MILA\\nlab at Université de Montréal, TensorFlow (www.tensorflow.org) is developed by Google,\\nand CNTK ( https:/ /github.com/Microsoft/CNTK) is developed by Microsoft. Any\\npiece of code that you write with Keras can be run with any of these backends without\\nhaving to change anything in the code: you can seamlessly switch between the two\\nduring development, which often proves useful—for instance, if one of these backends\\nproves to be faster for a specific task. We recommend using the TensorFlow backend as\\nthe default for most of your deep-learning needs, because it’s the most widely adopted,\\nscalable, and production ready.\\n Via TensorFlow (or Theano, or CNTK), Keras is able to run seamlessly on both\\nCPUs and GPUs. When running on CPU, TensorFlow is itself wrapping a low-level\\nlibrary for tensor operations called Eigen ( http:/ /eigen.tuxfamily.org). On GPU,\\nTensorFlow wraps a library of  well-optimized deep-learn ing operations called the\\nNVIDIA CUDA Deep Neural Network library (cuDNN). \\n3.2.2 Developing with Keras: a quick overview\\nYou’ve already seen one example of a Keras model: the MNIST example. The typical\\nKeras workflow looks just like that example:\\n1 Define your training data: input tensors and target tensors.\\n2 Define a network of layers (or model ) that maps your inputs to your targets.\\nFigure 3.3 The deep-learning \\nsoftware and hardware stack\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 76, 'page_label': '77'}, page_content=\"63Introduction to Keras\\n3 Configure the learning process by choos ing a loss function, an optimizer, and\\nsome metrics to monitor.\\n4 Iterate on your training data by calling the fit() method of your model.\\nThere are two ways to define a model: using the Sequential class (only for linear\\nstacks of layers, which is the most comm on network architectu re by far) or the func-\\ntional API (for directed acyclic grap hs of layers, which lets you build completely arbi-\\ntrary architectures).\\n As a refresher, here’s a two-layer model defined using the Sequential class (note\\nthat we’re passing the expected shape of the input data to the first layer):\\nfrom keras import models\\nfrom keras import layers\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(32, activation='relu', input_shape=(784,)))\\nmodel.add(layers.Dense(10, activation='softmax'))\\nAnd here’s the same model defined using the functional API:\\ninput_tensor = layers.Input(shape=(784,))\\nx = layers.Dense(32, activation='relu')(input_tensor)\\noutput_tensor = layers.Dense(10, activation='softmax')(x)\\nmodel = models.Model(inputs=input_tensor, outputs=output_tensor)\\nWith the functional API, you’re manipulating the data tensors that the model pro-\\ncesses and applying layers to this tensor as if they were functions.\\nNOTE A detailed guide to what you can do with the functional API can be\\nfound in chapter 7. Until chapte r 7, we’ll only be using the Sequential class\\nin our code examples.\\nOnce your model architecture is define d, it doesn’t matter whether you used a\\nSequential model or the functional API. All of the following steps are the same.\\n The learning process is configured in the compilation step, where you specify the\\noptimizer and loss function(s) that the model should use, as well as the metrics you\\nwant to monitor during training. Here’s an example with a single loss function, which\\nis by far the most common case:\\nfrom keras import optimizers\\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\\nloss='mse',\\nmetrics=['accuracy'])\\nFinally, the learning process consists of pa ssing Numpy arrays of input data (and the\\ncorresponding target data) to the model via the fit() method, similar to what you\\nwould do in Scikit-Learn and several other machine-learning libraries:\\nmodel.fit(input_tensor, target_tensor, batch_size=128, epochs=10)\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 77, 'page_label': '78'}, page_content='64 CHAPTER 3 Getting started with neural networks\\nOver the next few chapters, you’ll build a solid intuition about what type of network\\narchitectures work for different kinds of problems, how to pick the right learning con-\\nfiguration, and how to tweak a model until it  gives the results you want to see. We’ll\\nlook at three basic examples in sections 3.4, 3.5, and 3.6: a two-class classification\\nexample, a many-class classification example, and a regression example. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 78, 'page_label': '79'}, page_content='65Setting up a deep-learning workstation\\n3.3 Setting up a deep-learning workstation\\nBefore you can get started developing deep-learning applications, you need to set up\\nyour workstation. It’s highly  recommended, although not strictly necessary, that you\\nrun deep-learning code on a modern NVIDIA GPU. Some applications—in particular,\\nimage processing with convolutional networ ks and sequence processing with recur-\\nrent neural networks—will be  excruciatingly slow on CPU, even a fast multicore CPU.\\nAnd even for applications that can realistically be run on CPU, you’ll generally see\\nspeed increase by a factor or 5 or 10 by using a modern GPU. If you don’t want to\\ninstall a GPU on your machine, you can alternat ively consider running your experi-\\nments on an AWS EC2 GPU instance or on Google Cloud Platform. But note that cloud\\nGPU instances can become expensive over time.\\n Whether you’re running locally or in the cloud, it’s better to be using a Unix work-\\nstation. Although it’s technically possible to use Keras on Windows (all three Keras\\nbackends support Windows), We don’t recomme nd it. In the installation instructions\\nin appendix A, we’ll consider an Ubuntu machine. If you’re a Windows user, the sim-\\nplest solution to get everything running is to set up an Ubuntu dual boot on your\\nmachine. It may seem like a hassle, but us ing Ubuntu will save you a lot of time and\\ntrouble in the long run.\\n Note that in order to use Keras, you need to install TensorFlow or CNTK or Theano\\n(or all of them, if you want to be able to  switch back and forth among the three back-\\nends). In this book, we’ll focus on TensorFlow, with some light instructions relative to\\nTheano. We won’t cover CNTK.\\n3.3.1 Jupyter notebooks: the preferred way \\nto run deep-learning experiments\\nJupyter notebooks are a great way to run deep-learning experiments—in particular,\\nthe many code examples in this book. They ’re widely used in the data-science and\\nmachine-learning communities. A notebook is a file generated by the Jupyter Notebook\\napp (https:/ /jupyter.org), which you can edit in your browser. It mixes the ability to\\nexecute Python code with rich text-editing  capabilities for annotating what you’re\\ndoing. A notebook also allows you to br eak up long experiments into smaller pieces\\nthat can be executed independently, which makes development interactive and means\\nyou don’t have to rerun all of your previous  code if something goes wrong late in an\\nexperiment.\\n We recommend using Jupyter notebooks to get started with Keras, although that\\nisn’t a requirement: you can also run standalone Python scripts or run code from within\\nan IDE such as PyCharm. All the code examples in this book are available as open source\\nnotebooks; you can download them from the book’s website at www.manning\\n.com/books/deep-learning-with-python. \\n \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 79, 'page_label': '80'}, page_content='66 CHAPTER 3 Getting started with neural networks\\n3.3.2 Getting Keras running: two options\\nTo get started in practice, we recommend one of the following two options:\\n\\uf0a1 Use the official EC2 Deep Learning AMI ( https:/ /aws.amazon.com/amazon-\\nai/amis), and run Keras experiment s as Jupyter notebooks on EC2. Do this if\\nyou don’t already have a GPU on your local machine. Appendix B provides a\\nstep-by-step guide.\\n\\uf0a1 Install everything from scratch on a lo cal Unix workstation. You can then run\\neither local Jupyter notebooks or a regu lar Python codebase. Do this if you\\nalready have a high-end NVIDIA GPU. Appendix A provides an Ubuntu-specific,\\nstep-by-step guide.\\nLet’s take a closer look at some of the compromises involved in picking one option\\nover the other. \\n3.3.3 Running deep-learning jobs  in the cloud: pros and cons\\nIf you don’t already have a GPU that you can use for deep learning (a recent, high-end\\nNVIDIA GPU), then running deep-learning experi ments in the cloud is a simple, low-\\ncost way for you to get started without having to buy any additional hardware. If you’re\\nusing Jupyter notebooks, the experience of running in the cloud is no different from\\nrunning locally. As of mid-2017, the cloud o ffering that makes it easiest to get started\\nwith deep learning is definitely \\nAWS EC2. Appendix B provides a step-by-step guide to\\nrunning Jupyter notebooks on a EC2 GPU instance.\\n But if you’re a heavy user of deep learni ng, this setup isn’t sustainable in the long\\nterm—or even for more than a few weeks. EC2 instances are expensive: the instance\\ntype recommended in appendix B (the p2.xlarge instance, which won’t provide you\\nwith much power) costs $0.90 per hour as of mid-2017. Meanwhile, a solid consumer-\\nclass GPU will cost you somewher e between $1,000 and $1, 500—a price that has been\\nfairly stable over time, even as the specs of these GPUs keep improving. If you’re serious\\nabout deep learning, you should set up a local workstation with one or more GPUs.\\n In short, EC2 is a great way to get started. Yo u could follow the code examples in\\nthis book entirely on an EC2 GPU instance. But if you’re going to be a power user of\\ndeep learning, get your own GPUs. \\n3.3.4 What is the best GPU for deep learning?\\nIf you’re going to buy a GPU, which one should you choose? The first thing to note is\\nthat it must be an NVIDIA GPU. NVIDIA is the only graphics  computing company that\\nhas invested heavily in deep learning so  far, and modern deep-learning frameworks\\ncan only run on NVIDIA cards.\\n As of mid-2017, we recommend the NVIDIA TITAN Xp as the best card on the mar-\\nket for deep learning. For lower budg ets, you may want to consider the GTX 1060. If\\nyou’re reading these pages in 2018 or later,  take the time to look online for fresher\\nrecommendations, because new models come out every year.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 80, 'page_label': '81'}, page_content='67Setting up a deep-learning workstation\\n From this section onward , we’ll assume that you ha ve access to a machine with\\nKeras and its dependencies installed—preferably with GPU support. Make sure you\\nfinish this step before you proceed. Go th rough the step-by-step guides in the appen-\\ndixes, and look online if you need further help. There is no shortage of tutorials on\\nhow to install Keras and common deep-learning dependencies.\\n We can now dive into practical Keras examples. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 81, 'page_label': '82'}, page_content='68 CHAPTER 3 Getting started with neural networks\\n3.4 Classifying movie reviews: \\na binary classification example\\nTwo-class classification, or binary classifi cation, may be the most  widely applied kind\\nof machine-learning problem. In this example, you’ll learn to classify movie reviews as\\npositive or negative, based on the text content of the reviews.\\n3.4.1 The IMDB dataset\\nYou’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the\\nInternet Movie Database. They’re split into  25,000 reviews for training and 25,000\\nreviews for testing, each set consisting of 50% negative and 50% positive reviews.\\n Why use separate training and test sets ? Because you should never test a machine-\\nlearning model on the same data that you used to train it! Just because a model per-\\nforms well on its training data doesn’t mean it will perform well on data it has never\\nseen; and what you care about is your model’s performance on new data (because you\\nalready know the labels of your training data—obviously you don’t need your model\\nto predict those). For instance, it’s possible that your model could end up merely mem-\\norizing a mapping between your training samp les and their targets, which would be\\nuseless for the task of pred icting targets for data the model has never seen before.\\nWe’ll go over this point in much more detail in the next chapter.\\n Just like the \\nMNIST dataset, the IMDB dataset comes packaged with Keras. It has\\nalready been preprocessed: the reviews (seq uences of words) have been turned into\\nsequences of integers, where each integer stands for a specific word in a dictionary.\\n The following code will load the datase t (when you run it the first time, about\\n80 MB of data will be downloaded to your machine).\\nfrom keras.datasets import imdb\\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\\nnum_words=10000)\\nThe argument num_words=10000 means you’ll only keep the top 10,000 most fre-\\nquently occurring words in the training data. Rare words will be discarded. This allows\\nyou to work with vector data of manageable size.\\n The variables train_data and test_data are lists of reviews; each review is a list of\\nword indices (encoding a sequence of words). train_labels and test_labels are\\nlists of 0s and 1s, where 0 stands for negative and 1 stands for positive:\\n>>> train_data[0]\\n[1, 14, 22, 16, ... 178, 32]\\n>>> train_labels[0]\\n1\\nListing 3.1 Loading the IMDB dataset\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 82, 'page_label': '83'}, page_content=\"69Classifying movie reviews: a binary classification example\\nBecause you’re restricting yourself to th e top 10,000 most freq uent words, no word\\nindex will exceed 10,000:\\n>>> max([max(sequence) for sequence in train_data])\\n9999\\nFor kicks, here’s how you can quickly deco de one of these reviews back to English\\nwords:\\nword_index = imdb.get_word_index()\\nreverse_word_index = dict(\\n[(value, key) for (key, value) in word_index.items()])\\ndecoded_review = ' '.join(\\n[reverse_word_index.get(i - 3, '?') for i in train_data[0]])\\n3.4.2 Preparing the data\\nYou can’t feed lists of integers into a neural network. You have to turn your lists into\\ntensors. There are two ways to do that:\\n\\uf0a1 Pad your lists so that they all have the same length, turn them into an integer\\ntensor of shape \\n(samples, word_indices), and then use as the first layer in\\nyour network a layer capable of ha ndling such integer tensors (the Embedding\\nlayer, which we’ll cover in detail later in the book).\\n\\uf0a1 One-hot encode your lists to turn them into vectors of 0s and 1s. This would\\nmean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vec-\\ntor that would be all 0s except for indices 3 and 5, which would be 1s. Then you\\ncould use as the first layer in your network a Dense layer, capable of handling\\nfloating-point vector data.\\nLet’s go with the latter solution to vector ize the data, which you’ll do manually for\\nmaximum clarity.\\nimport numpy as np\\ndef vectorize_sequences(sequences, dimension=10000):\\nresults = np.zeros((len(sequences), dimension))\\nfor i, sequence in enumerate(sequences):\\nresults[i, sequence] = 1.\\nreturn results\\nx_train = vectorize_sequences(train_data)\\nx_test = vectorize_sequences(test_data)\\n \\nListing 3.2 Encoding the integer sequences into a binary matrix\\nword_index is a dictionary mapping \\nwords to an integer index.\\nReverses it, mapping \\ninteger indices to words Decodes the review. Note that the indices\\nare offset by 3 because 0, 1, and 2 are\\nreserved indices for “padding,” “start of\\nsequence,” and “unknown.”\\nCreates an all-zero matrix \\nof shape (len(sequences), \\ndimension)\\nSets specific indices \\nof results[i] to 1s\\nVectorized training data\\nVectorized test data\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 83, 'page_label': '84'}, page_content=\"70 CHAPTER 3 Getting started with neural networks\\nHere’s what the samples look like now:\\n>>> x_train[0]\\narray([ 0., 1., 1., ..., 0., 0., 0.])\\nYou should also vectorize your labels, which is straightforward:\\ny_train = np.asarray(train_labels).astype('float32')\\ny_test = np.asarray(test_labels).astype('float32')\\nNow the data is ready to be fed into a neural network.\\n3.4.3 Building your network\\nThe input data is vectors, and the labels are scalars (1s and 0s): this is the easiest setup\\nyou’ll ever encounter. A type of networ k that performs well on such a problem is\\na simple stack of fully connected ( Dense) layers with relu activations: Dense(16,\\nactivation='relu').\\n The argument being passed to each Dense l a y e r  ( 1 6 )  i s  t h e  n u m b e r  o f  h i d d e n\\nunits of the layer. A hidden unit is a dimension in the representation space of the layer.\\nYou may remember from ch apter 2 that each such Dense layer with a relu activation\\nimplements the following chain of tensor operations:\\noutput = relu(dot(W, input) + b)\\nHaving 16 hidden units means the weight matrix W will have shape (input_dimension,\\n16): the dot product with W will project the input data onto a 16-dimensional represen-\\ntation space (and then you’ll add the bias vector b and apply the relu operation). You\\ncan intuitively understand the dimensionali ty of your representation space as “how\\nmuch freedom you’re allowing the network to  have when learning internal represen-\\ntations.” Having more hidden units (a higher-dimensional representation space)\\nallows your network to learn more-complex representations, but it makes the network\\nmore computationally expensive and may lead  to learning unwanted patterns (pat-\\nterns that will improve performance on the training data but not on the test data).\\n There are two key architecture decisions to be made about such a stack of Dense layers:\\n\\uf0a1 How many layers to use\\n\\uf0a1 How many hidden units to choose for each layer\\nIn chapter 4, you’ll learn formal principles to guide you in making these choices. For\\nthe time being, you’ll have to trust me with the following architecture choice: \\n\\uf0a1 Two intermediate layers with 16 hidden units each \\n\\uf0a1 A third layer that will output the scalar  prediction regarding the sentiment of\\nthe current review\\nThe intermediate layers will use relu as their activation function, and the final layer\\nwill use a sigmoid activation so as to ou tput a probability (a score between 0 and 1,\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 84, 'page_label': '85'}, page_content='71Classifying movie reviews: a binary classification example\\nindicating how likely the sample is to have the target “1”: how likely the review is to be\\npositive). A relu (rectified linear unit) is a function meant to zero out negative values\\n(see figure 3.4), whereas a sigmoid “squashes” arbitrary values into the [0, 1] interval\\n(see figure 3.5), outputting something that can be interpreted as a probability.\\n \\nFigure 3.4 The rectified linear unit function\\nFigure 3.5 The sigmoid function\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 85, 'page_label': '86'}, page_content=\"72 CHAPTER 3 Getting started with neural networks\\nFigure 3.6 shows what the network looks li ke. And here’s the Keras implementation,\\nsimilar to the MNIST example you saw previously.\\nfrom keras import models\\nfrom keras import layers\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(16, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nFinally, you need to choose a loss function and an optimizer. Because you’re facing a\\nbinary classification problem and the output of your network is a probability (you end\\nyour network with a single-unit layer with a sigmoid activation), it’s best to use the\\nListing 3.3 The model definition\\nDense (units=1)\\nOutput\\n(probability)\\nInput\\n(vectorized text)\\nSequential\\nDense (units=16)\\nDense (units=16)\\nFigure 3.6 The three-layer network\\nWhat are activation functions, and why are they necessary?\\nWithout an activation function like relu (also called a non-linearity), the Dense layer\\nwould consist of two linear operations—a dot product and an addition:\\noutput = dot(W, input) + b\\nSo the layer could only learn linear transformations (affine transformations) of the\\ninput data: the hypothesis space of the layer would be the set of all possible linear\\ntransformations of the input data into a 16-dimensional space. Such a hypothesis\\nspace is too restricted and wouldn’t benefit from multiple layers of representations,\\nbecause a deep stack of linear layers would still implement a linear operation: adding\\nmore layers wouldn’t extend the hypothesis space.\\nIn order to get access to a much richer hypothesis space that would benefit from\\ndeep representations, you need a non-lin earity, or activation function. relu is the\\nmost popular activation function in deep learning, but there are many other candi-\\ndates, which all come with similarly strange names: prelu, elu, and so on.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 86, 'page_label': '87'}, page_content=\"73Classifying movie reviews: a binary classification example\\nbinary_crossentropy loss. It isn’t the only viable ch oice: you could use, for instance,\\nmean_squared_error. But crossentropy is usually the best choice when you’re dealing\\nwith models that output probabilities. Crossentropy is a quantity from the field of Infor-\\nmation Theory that measures the distance between probability distributions or, in this\\ncase, between the ground-truth distribution and your predictions.\\n Here’s the step where you configure the model with the rmsprop optimizer and\\nthe binary_crossentropy loss function. Note that yo u’ll also monitor accuracy\\nduring training.\\nmodel.compile(optimizer='rmsprop',\\nloss='binary_crossentropy',\\nmetrics=['accuracy'])\\nYou’re passing your optimizer,  loss function, and metrics as  strings, which is possible\\nbecause rmsprop, binary_crossentropy, and accuracy are packaged as part of Keras.\\nSometimes you may want to configure the para meters of your optimizer or pass a cus-\\ntom loss function or metric function. The former can be done by passing an optimizer\\nclass instance as the optimizer argument, as shown in listing 3.5; the latter can be\\ndone by passing function objects as the loss and/or metrics arguments, as shown in\\nlisting 3.6.\\nfrom keras import optimizers\\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\\nloss='binary_crossentropy',\\nmetrics=['accuracy'])\\nfrom keras import losses\\nfrom keras import metrics\\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\\nloss=losses.binary_crossentropy,\\nmetrics=[metrics.binary_accuracy])\\n3.4.4 Validating your approach\\nIn order to monitor during training the ac curacy of the model on data it has never\\nseen before, you’ll create a validation set by setting apart 10,0 00 samples from the\\noriginal training data.\\nx_val = x_train[:10000]\\npartial_x_train = x_train[10000:]\\nListing 3.4 Compiling the model\\nListing 3.5 Configuring the optimizer\\nListing 3.6 Using custom losses and metrics\\nListing 3.7 Setting aside a validation set\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 87, 'page_label': '88'}, page_content=\"74 CHAPTER 3 Getting started with neural networks\\ny_val = y_train[:10000]\\npartial_y_train = y_train[10000:]\\nYou’ll now train the model for 20 epochs (20 iterations over all samples in the\\nx_train and y_train tensors), in mini-batches of 512 samples. At the same time,\\nyou’ll monitor loss and accuracy on the 10,000 samples that you set apart. You do so by\\npassing the validation data as the validation_data argument.\\nmodel.compile(optimizer='rmsprop',\\nloss='binary_crossentropy',\\nmetrics=['acc'])\\nhistory = model.fit(partial_x_train,\\npartial_y_train,\\nepochs=20,\\nbatch_size=512,\\nvalidation_data=(x_val, y_val))\\nOn CPU, this will take less than 2 seconds per epoch—training is over in 20 seconds.\\nAt the end of every epoch, there is a slight pause as the model computes its loss and\\naccuracy on the 10,000 samples of the validation data.\\n Note that the call to \\nmodel.fit() returns a History object. This object has a mem-\\nber history, which is a dictionary containing data about everything that happened\\nduring training. Let’s look at it:\\n>>> history_dict = history.history\\n>>> history_dict.keys()\\n[u'acc', u'loss', u'val_acc', u'val_loss']\\nThe dictionary contains four entries: one per metric that was being monitored during\\ntraining and during validation. In the following two listing, let’s use Matplotlib to plot\\nthe training and validation loss side by side (see figure 3.7), as well as the training and\\nvalidation accuracy (see figure 3.8). Note that your own results may vary slightly due to\\na different random initialization of your network.\\nimport matplotlib.pyplot as plt\\nhistory_dict = history.history\\nloss_values = history_dict['loss']\\nval_loss_values = history_dict['val_loss']\\nepochs = range(1, len(acc) + 1)\\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\\nplt.title('Training and validation loss')\\nplt.xlabel('Epochs')\\nplt.ylabel('Loss')\\nplt.legend()\\nplt.show()\\nListing 3.8 Training your model\\nListing 3.9 Plotting the training and validation loss\\n“bo” is for \\n“blue dot.”\\n“b” is for “solid\\nblue line.”\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 88, 'page_label': '89'}, page_content=\"75Classifying movie reviews: a binary classification example\\nplt.clf()\\nacc_values = history_dict['acc']\\nval_acc_values = history_dict['val_acc']\\nplt.plot(epochs, acc, 'bo', label='Training acc')\\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\\nplt.title('Training and validation accuracy')\\nplt.xlabel('Epochs')\\nplt.ylabel('Loss')\\nplt.legend()\\nplt.show()\\nListing 3.10 Plotting the training and validation accuracy\\nFigure 3.7 Training and validation loss\\nClears the figure\\nFigure 3.8 Training and validation accuracy\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 89, 'page_label': '90'}, page_content=\"76 CHAPTER 3 Getting started with neural networks\\nAs you can see, the training loss decreases with every epoch, and the training accuracy\\nincreases with every epoch. That’s what you would expect when running gradient-\\ndescent optimization—the quantity you’re trying to minimize should be less with\\nevery iteration. But that isn’t the case for the validation loss and accuracy: they seem to\\npeak at the fourth epoch. This is an exam ple of what we warned against earlier: a\\nmodel that performs better on the training data isn’t necessarily a model that will do\\nbetter on data it has never seen before. In precise terms, what you’re seeing is overfit-\\nting: after the second epoch, you’re overoptimizing on the training data, and you end\\nup learning representations that are specific to the training data and don’t generalize\\nto data outside of the training set.\\n In this case, to prevent overfitting, yo u could stop training after three epochs. In\\ngeneral, you can use a range of techniques to mitigate overfitting, which we’ll cover in\\nchapter 4.\\n Let’s train a new network from scratch for four epochs and then evaluate it on the\\ntest data.\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(16, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='rmsprop',\\nloss='binary_crossentropy',\\nmetrics=['accuracy'])\\nmodel.fit(x_train, y_train, epochs=4, batch_size=512)\\nresults = model.evaluate(x_test, y_test)\\nThe final results are as follows:\\n>>> results\\n[0.2929924130630493, 0.88327999999999995]\\nThis fairly naive approach achieves an  accuracy of 88%. With state-of-the-art\\napproaches, you should be able to get close to 95%.\\n3.4.5 Using a trained network to generate predictions on new data\\nAfter having trained a network, you’ll want to use it in a practical setting. You can gen-\\nerate the likelihood of reviews being positive by using the predict method:\\n>>> model.predict(x_test)\\narray([[ 0.98006207]\\n[ 0.99758697]\\n[ 0.99975556]\\n...,\\n[ 0.82167041]\\n[ 0.02885115]\\n[ 0.65371346]], dtype=float32)\\nListing 3.11 Retraining a model from scratch\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 90, 'page_label': '91'}, page_content='77Classifying movie reviews: a binary classification example\\nAs you can see, the network is confident for some samples (0.99 or more, or 0.01 or\\nless) but less confident for others (0.6, 0.4).\\n3.4.6 Further experiments\\nThe following experiments will help convince you that the architecture choices you’ve\\nmade are all fairly reasonable, although there’s still room for improvement:\\n\\uf0a1 You used two hidden layers. Try using one or three hidden layers, and see how\\ndoing so affects validation and test accuracy.\\n\\uf0a1 Try using layers with more hidden units or fewer hidden units: 32 units, 64 units,\\nand so on.\\n\\uf0a1 Try using the mse loss function instead of binary_crossentropy.\\n\\uf0a1 Try using the tanh activation (an activation that was popular in the early days of\\nneural networks) instead of relu.\\n3.4.7 Wrapping up\\nHere’s what you should take away from this example:\\n\\uf0a1 You usually need to do quite a bit of preprocessing on your raw data in order to\\nbe able to feed it—as tensors—into a neural network. Sequences of words can\\nbe encoded as binary vectors, but there are other encoding options, too.\\n\\uf0a1 Stacks of Dense layers with relu activations can solve a wide range of problems\\n(including sentiment classification), and you’ll likely use them frequently.\\n\\uf0a1 In a binary classification  problem (two output cla sses), your network should\\nend with a Dense layer with one unit and a sigmoid activation: the output of\\nyour network should be a scalar between 0 and 1, encoding a probability.\\n\\uf0a1 With such a scalar sigmoid output on a binary classi fication problem, the loss\\nfunction you should use is binary_crossentropy.\\n\\uf0a1 The rmsprop optimizer is generally a good enough choice, whatever your prob-\\nlem. That’s one less thing for you to worry about.\\n\\uf0a1 As they get better on their training data, neural networks eventually start over-\\nfitting and end up obtaining increasingly  worse results on data they’ve never\\nseen before. Be sure to always monitor performance on data that is outside of\\nthe training set. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 91, 'page_label': '92'}, page_content=\"78 CHAPTER 3 Getting started with neural networks\\n3.5 Classifying newswires: \\na multiclass classification example\\nIn the previous section, you saw how to classify vector inputs into two mutually exclu-\\nsive classes using a densely connected neural ne twork. But what happens when you\\nhave more than two classes?\\n In this section, you’ll build a network to classify Reuters newswires into 46 mutually\\nexclusive topics. Because you have many cl asses, this problem is an instance of multi-\\nclass classification; and because each data point should be classified into only one cate-\\ngory, the problem is more specifically an instance of single-label, multiclass classification.\\nIf each data point could belong to multiple  categories (in this case, topics), you’d be\\nfacing a multilabel, multiclass classification problem.\\n3.5.1 The Reuters dataset\\nYou’ll work with the Reuters dataset, a set of short newswires and their topics, published\\nby Reuters in 1986. It’s a simple, widely us ed toy dataset for text classification. There\\nare 46 different topics; some topics are mo re represented than others, but each topic\\nhas at least 10 examples in the training set.\\n Like IMDB and MNIST, the Reuters dataset comes packag ed as part of Keras. Let’s\\ntake a look.\\nfrom keras.datasets import reuters\\n(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\\nnum_words=10000)\\nAs with the IMDB dataset, the argument num_words=10000 restricts the data to the\\n10,000 most frequently occurring words found in the data.\\n You have 8,982 training examples and 2,246 test examples:\\n>>> len(train_data)\\n8982\\n>>> len(test_data)\\n2246\\nAs with the IMDB reviews, each example is a list of integers (word indices):\\n>>> train_data[10]\\n[1, 245, 273, 207, 156, 53, 74, 160, 26, 14, 46, 296, 26, 39, 74, 2979,\\n3554, 14, 46, 4689, 4329, 86, 61, 3499, 4795, 14, 61, 451, 4329, 17, 12]\\nHere’s how you can decode it back to words, in case you’re curious.\\nword_index = reuters.get_word_index()\\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\\ndecoded_newswire='' . join([reverse_word_index.get(i - 3, '?') for i in\\ntrain_data[0]])\\nListing 3.12 Loading the Reuters dataset\\nListing 3.13 Decoding newswires back to text\\nNote that the indices are offset by 3 because 0, 1, and 2 are reserved \\nindices for “padding,” “start of sequence,” and “unknown.”\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 92, 'page_label': '93'}, page_content='79Classifying newswires: a multiclass classification example\\nThe label associated with an example is an integer between 0 and 45—a topic index:\\n>>> train_labels[10]\\n3\\n3.5.2 Preparing the data\\nYou can vectorize the data with the exact same code as in the previous example.\\nimport numpy as np\\ndef vectorize_sequences(sequences, dimension=10000):\\nresults = np.zeros((len(sequences), dimension))\\nfor i, sequence in enumerate(sequences):\\nresults[i, sequence] = 1.\\nreturn results\\nx_train = vectorize_sequences(train_data)\\nx_test = vectorize_sequences(test_data)\\nTo vectorize the labels, there are two possibilities: you can cast the label list as an inte-\\nger tensor, or you can use one-hot encoding . One-hot encoding is a widely used for-\\nmat for categorical data, also called categorical encoding . For a more detailed\\nexplanation of one-hot encoding, see sectio n 6.1. In this case, one-hot encoding of\\nthe labels consists of embedding each label as an all-zero vector with a 1 in the place of\\nthe label index. Here’s an example:\\ndef to_one_hot(labels, dimension=46):\\nresults = np.zeros((len(labels), dimension))\\nfor i, label in enumerate(labels):\\nresults[i, label] = 1.\\nreturn results\\none_hot_train_labels = to_one_hot(train_labels)\\none_hot_test_labels = to_one_hot(test_labels)\\nNote that there is a built-in way to do this in Keras, which you’ve already seen in action\\nin the MNIST example:\\nfrom keras.utils.np_utils import to_categorical\\none_hot_train_labels = to_categorical(train_labels)\\none_hot_test_labels = to_categorical(test_labels)\\n3.5.3 Building your network\\nThis topic-classification prob lem looks similar to the previous movie-review classifica-\\ntion problem: in both cases, you’re trying to classify short snippets of text. But there is\\na new constraint here: the number of outp ut classes has gone from 2 to 46. The\\ndimensionality of the output space is much larger.\\n In a stack of Dense layers like that you’ve been using, each layer can only access infor-\\nmation present in the output of the previous layer. If one layer drops some information\\nListing 3.14 Encoding the data\\nVectorized training data\\nVectorized test data\\nVectorized training labels\\nVectorized test labels\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 93, 'page_label': '94'}, page_content=\"80 CHAPTER 3 Getting started with neural networks\\nrelevant to the classification problem, this information can never be recovered by later\\nlayers: each layer can potentially become an information bottleneck. In the previous\\nexample, you used 16-dimensional intermediate layers, but a 16-dimensional space may\\nbe too limited to learn to separate 46 different classes: such small layers may act as infor-\\nmation bottlenecks, permanently dropping relevant information.\\n For this reason you’ll use larger layers. Let’s go with 64 units.\\nfrom keras import models\\nfrom keras import layers\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(64, activation='relu'))\\nmodel.add(layers.Dense(46, activation='softmax'))\\nThere are two other things you should note about this architecture:\\n\\uf0a1 You end the network with a Dense layer of size 46. This means for each input\\nsample, the network will output a 46-dimensional vector. Each entry in this vec-\\ntor (each dimension) will encode a different output class.\\n\\uf0a1 The last layer uses a softmax activation. You saw this pattern in the MNIST\\nexample. It means the network will output a probability distribution over the 46\\ndifferent output classes—for every input sample, the network will produce a 46-\\ndimensional output vector, where output[i] is the probability that the sample\\nbelongs to class i. The 46 scores will sum to 1.\\nThe best loss function to use in this case is categorical_crossentropy. It measures\\nthe distance between two probability distri butions: here, between the probability dis-\\ntribution output by the network and the true distribution of the labels. By minimizing\\nthe distance between these two distribution s, you train the network to output some-\\nthing as close as possible to the true labels.\\nmodel.compile(optimizer='rmsprop',\\nloss='categorical_crossentropy',\\nmetrics=['accuracy'])\\n3.5.4 Validating your approach\\nLet’s set apart 1,000 samples in the training data to use as a validation set.\\nx_val = x_train[:1000]\\npartial_x_train = x_train[1000:]\\ny_val = one_hot_train_labels[:1000]\\npartial_y_train = one_hot_train_labels[1000:]\\nListing 3.15 Model definition\\nListing 3.16 Compiling the model\\nListing 3.17 Setting aside a validation set\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 94, 'page_label': '95'}, page_content=\"81Classifying newswires: a multiclass classification example\\nNow, let’s train the network for 20 epochs.\\nhistory = model.fit(partial_x_train,\\npartial_y_train,\\nepochs=20,\\nbatch_size=512,\\nvalidation_data=(x_val, y_val))\\nAnd finally, let’s display its loss and accuracy curves (see figures 3.9 and 3.10).\\nimport matplotlib.pyplot as plt\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\nepochs = range(1, len(loss) + 1)\\nplt.plot(epochs, loss, 'bo', label='Training loss')\\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\\nplt.title('Training and validation loss')\\nplt.xlabel('Epochs')\\nplt.ylabel('Loss')\\nplt.legend()\\nplt.show()\\nplt.clf()\\nacc = history.history['acc']\\nval_acc = history.history['val_acc']\\nplt.plot(epochs, acc, 'bo', label='Training acc')\\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\\nplt.title('Training and validation accuracy')\\nplt.xlabel('Epochs')\\nplt.ylabel('Loss')\\nplt.legend()\\nplt.show()\\nListing 3.18 Training the model\\nListing 3.19 Plotting the training and validation loss\\nListing 3.20 Plotting the training and validation accuracy\\nClears the figure\\nFigure 3.9 Training and validation loss\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 95, 'page_label': '96'}, page_content=\"82 CHAPTER 3 Getting started with neural networks\\nThe network begins to overfit after nine  epochs. Let’s train a new network from\\nscratch for nine epochs and then evaluate it on the test set.\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(64, activation='relu'))\\nmodel.add(layers.Dense(46, activation='softmax'))\\nmodel.compile(optimizer='rmsprop',\\nloss='categorical_crossentropy',\\nmetrics=['accuracy'])\\nmodel.fit(partial_x_train,\\npartial_y_train,\\nepochs=9,\\nbatch_size=512,\\nvalidation_data=(x_val, y_val))\\nresults = model.evaluate(x_test, one_hot_test_labels)\\nHere are the final results:\\n>>> results\\n[0.9565213431445807, 0.79697239536954589]\\nThis approach reaches an accuracy of ~80 %. With a balanced binary classification\\nproblem, the accuracy reached by a purely random classifier would be 50%. But in\\nthis case it’s closer to 19%, so the results seem pretty good, at least when compared to\\na random baseline:\\n>>> import copy\\n>>> test_labels_copy = copy.copy(test_labels)\\n>>> np.random.shuffle(test_labels_copy)\\n>>> hits_array = np.array(test_labels) == np.array(test_labels_copy)\\n>>> float(np.sum(hits_array)) / len(test_labels)\\n0.18655387355298308\\nListing 3.21 Retraining a model from scratch\\nFigure 3.10 Training and \\nvalidation accuracy\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 96, 'page_label': '97'}, page_content=\"83Classifying newswires: a multiclass classification example\\n3.5.5 Generating predictions on new data\\nYou can verify that the predict method of the model instance returns a probability\\ndistribution over all 46 topics. Let’s generate topic predictions for all of the test data.\\npredictions = model.predict(x_test)\\nEach entry in predictions is a vector of length 46:\\n>>> predictions[0].shape\\n(46,)\\nThe coefficients in this vector sum to 1:\\n>>> np.sum(predictions[0])\\n1.0\\nThe largest entry is the predicted class—the class with the highest probability:\\n>>> np.argmax(predictions[0])\\n4\\n3.5.6 A different way to handle the labels and the loss\\nWe mentioned earlier that another way to en code the labels would be to cast them as\\nan integer tensor, like this:\\ny_train = np.array(train_labels)\\ny_test = np.array(test_labels)\\nThe only thing this approach would change is the choice of the loss function. The loss\\nfunction used in listing 3.21, categorical_crossentropy, expects the labels to follow\\na categorical encoding. With integer labels, you should use sparse_categorical_\\ncrossentropy:\\nmodel.compile(optimizer='rmsprop',\\nloss='sparse_categorical_crossentropy',\\nmetrics=['acc'])\\nThis new loss function is still mathematically the same as categorical_crossentropy;\\nit just has a different interface.\\n3.5.7 The importance of having su fficiently large intermediate layers\\nWe mentioned earlier that because the final outputs are 46-dimensional, you should\\navoid intermediate layers with many fewer than 46 hidden units. Now let’s see what\\nhappens when you introduce an information bottleneck by having intermediate layers\\nthat are significantly less than 46-dimensional: for example, 4-dimensional.\\nListing 3.22 Generating predictions for new data\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 97, 'page_label': '98'}, page_content=\"84 CHAPTER 3 Getting started with neural networks\\n \\nmodel = models.Sequential()\\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(4, activation='relu'))\\nmodel.add(layers.Dense(46, activation='softmax'))\\nmodel.compile(optimizer='rmsprop',\\nloss='categorical_crossentropy',\\nmetrics=['accuracy'])\\nmodel.fit(partial_x_train,\\npartial_y_train,\\nepochs=20,\\nbatch_size=128,\\nvalidation_data=(x_val, y_val))\\nThe network now peaks at ~71% validation accuracy, an 8% absolute drop. This drop\\nis mostly due to the fact that you’re trying to compress a lot of information (enough\\ninformation to recover the separation hyperplanes of 46 classes) into an intermediate\\nspace that is too low-dimensional.  The network is able to cram most of the necessary\\ninformation into these eight-dimensional representations, but not all of it.\\n3.5.8 Further experiments\\n\\uf0a1 Try using larger or smaller layers: 32 units, 128 units, and so on.\\n\\uf0a1 You used two hidden layers. Now try using a single hidden layer, or three hid-\\nden layers.\\n3.5.9 Wrapping up\\nHere’s what you should take away from this example:\\n\\uf0a1 If you’re trying to classify data points among N classes, your network should end\\nwith a Dense layer of size N.\\n\\uf0a1 In a single-label, multiclass classifica tion problem, your network should end\\nwith a softmax activation so that it will output a probability distribution over the\\nN output classes.\\n\\uf0a1 Categorical crossentropy is  almost always the loss fu nction you should use for\\nsuch problems. It minimizes the distance  between the probability distributions\\noutput by the network and the true distribution of the targets.\\n\\uf0a1 There are two ways to handle labels in multiclass classification:\\n– Encoding the labels via categorical encoding (also known as one-hot encod-\\ning) and using categorical_crossentropy as a loss function\\n– Encoding the labels as integers and using the sparse_categorical_crossentropy\\nloss function\\n\\uf0a1 If you need to classify data into a larg e number of categories, you should avoid\\ncreating information bottlenecks in your network due to intermediate layers\\nthat are too small. \\nListing 3.23 A model with an information bottleneck\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 98, 'page_label': '99'}, page_content='85Predicting house prices: a regression example\\n3.6 Predicting house prices: a regression example\\nThe two previous examples we re considered classificati on problems, where the goal\\nwas to predict a single discrete label of an  input data point. Another common type of\\nmachine-learning problem is regression, which consists of predicting a continuous\\nvalue instead of a discrete label: for inst ance, predicting the temperature tomorrow,\\ngiven meteorological data; or predicting the time that a software project will take to\\ncomplete, given its specifications.\\nNOTE Don’t confuse regression and the algorithm logistic regression . Con-\\nfusingly, logistic regression isn’t a re gression algo rithm—it’s a classification\\nalgorithm.\\n3.6.1 The Boston Housing Price dataset\\nYou’ll attempt to predict the median price of  homes in a given Boston suburb in the\\nmid-1970s, given data points about the suburb  at the time, such as the crime rate, the\\nlocal property tax rate, and so on. The data set you’ll use has an interesting difference\\nfrom the two previous examples. It has relatively few data points: only 506, split\\nbetween 404 training samples and 102 test samples. And each feature in the input data\\n(for example, the crime rate) has a different  scale. For instance, some values are pro-\\nportions, which take values between 0 and 1; others take values between 1 and 12, oth-\\ners between 0 and 100, and so on.\\nfrom keras.datasets import boston_housing\\n(train_data, train_targets), (test_data, test_targets) =\\n➥ boston_housing.load_data()\\nLet’s look at the data:\\n>>> train_data.shape\\n(404, 13)\\n>>> test_data.shape\\n(102, 13)\\nAs you can see, you have 404 training sa mples and 102 test samples, each with 13\\nnumerical features, such as per capita crime rate, average number of rooms per dwell-\\ning, accessibility to highways, and so on.\\n The targets are the median values of owner-occupied homes, in thousands of\\ndollars:\\n>>> train_targets\\n[ 15.2, 42.3, 50. ... 19.4, 19.4, 29.1]\\nThe prices are typically between $10,000 an d $50,000. If that sounds cheap, remem-\\nber that this was the mid-1970s, and these prices aren’t adjusted for inflation.\\nListing 3.24 Loading the Boston housing dataset\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 99, 'page_label': '100'}, page_content=\"86 CHAPTER 3 Getting started with neural networks\\n3.6.2 Preparing the data\\nIt would be problematic to feed into a neural network values that all take wildly differ-\\nent ranges. The network might be able to automatically adapt to such heterogeneous\\ndata, but it would definitely make learning more difficult. A widespread best practice\\nto deal with such data is to do feature-wise normalization: for each feature in the input\\ndata (a column in the input data matrix),  you subtract the mean of the feature and\\ndivide by the standard deviation, so that the feature is centered around 0 and has a\\nunit standard deviation. This is easily done in Numpy.\\nmean = train_data.mean(axis=0)\\ntrain_data -= mean\\nstd = train_data.std(axis=0)\\ntrain_data /= std\\ntest_data -= mean\\ntest_data /= std\\nNote that the quantities us ed for normalizing the test data are computed using the\\ntraining data. You should never use in your  workflow any quantity computed on the\\ntest data, even for something as simple as data normalization.\\n3.6.3 Building your network\\nBecause so few samples are available, you’ll  use a very small network with two hidden\\nlayers, each with 64 units. In general, the less training data you have, the worse overfit-\\nting will be, and using a small network is one way to mitigate overfitting.\\nfrom keras import models\\nfrom keras import layers\\ndef build_model():\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(64, activation='relu',\\ninput_shape=(train_data.shape[1],)))\\nmodel.add(layers.Dense(64, activation='relu'))\\nmodel.add(layers.Dense(1))\\nmodel.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\\nreturn model\\nThe network ends with a single unit and no activation (it will be a linear layer). This is\\na typical setup for scalar regression (a regression where you’re trying to predict a single\\ncontinuous value). Applying an activation function would constrain the range the out-\\nput can take; for instance, if you applied a sigmoid activation function to the last layer,\\nthe network could only learn to predict values between 0 and 1. Here, because the last\\nlayer is purely linear, the network is free to learn to predict values in any range.\\nListing 3.25 Normalizing the data\\nListing 3.26 Model definition\\nBecause you’ll need to instantiate \\nthe same model multiple times, you \\nuse a function to construct it.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 100, 'page_label': '101'}, page_content='87Predicting house prices: a regression example\\n Note that you compile the network with the mse loss function—mean squared error,\\nthe square of the difference between the predictions and the targets. This is a widely\\nused loss function for regression problems.\\n You’re also monitoring a new metric during training: mean absolute error (MAE). It’s\\nthe absolute value of the difference betw een the predictions and the targets. For\\ninstance, an MAE of 0.5 on this problem would mean your predictions are off by $500\\non average.\\n3.6.4 Validating your approach using K-fold validation\\nTo evaluate your network while you keep adjusting its parameters (such as the number\\nof epochs used for training), you could spli t the data into a training set and a valida-\\ntion set, as you did in the previous examples. But because you have so few data points,\\nthe validation set would end up being very small (for instance, about 100 examples).\\nAs a consequence, the validation scores mi ght change a lot depending on which data\\npoints you chose to use for validation and which you chose for training: the validation\\nscores might have a high variance with regard to the validation split. This would pre-\\nvent you from reliably evaluating your model.\\n The best practice in such situations is to use K-fold cross-validation (see figure 3.11).\\nIt consists of splitting the available data into K partitions (typically K = 4 or 5), instanti-\\nating K identical models, and training each one on K – 1 partitions while evaluating on\\nthe remaining partition. The validation score for the model used is then the average of\\nthe K validation scores obtained. In terms of code, this is straightforward.\\nimport numpy as np\\nk=4\\nnum_val_samples = len(train_data) // k\\nnum_epochs = 100\\nall_scores = []\\nListing 3.27 K-fold validation\\nData split into 3 partitions\\nValidation Training Training Validation\\nscore #1Fold 1\\nValidation Validation Training Validation\\nscore #2\\nFinal score:\\naverageFold 2\\nValidation Training Validation Validation\\nscore #3Fold 3\\nFigure 3.11 3-fold cross-validation\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 101, 'page_label': '102'}, page_content=\"88 CHAPTER 3 Getting started with neural networks\\nfor i in range(k):\\nprint('processing fold #', i)\\nval_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\\nval_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\\npartial_train_data = np.concatenate(\\n[train_data[:i * num_val_samples],\\ntrain_data[(i + 1) * num_val_samples:]],\\naxis=0)\\npartial_train_targets = np.concatenate(\\n[train_targets[:i * num_val_samples],\\ntrain_targets[(i + 1) * num_val_samples:]],\\naxis=0)\\nmodel = build_model()\\nmodel.fit(partial_train_data, partial_train_targets,\\nepochs=num_epochs, batch_size=1, verbose=0)\\nval_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\\nall_scores.append(val_mae)\\nRunning this with num_epochs = 100 yields the following results:\\n>>> all_scores\\n[2.588258957792037, 3.1289568449719116, 3.1856116051248984, 3.0763342615401386]\\n>>> np.mean(all_scores)\\n2.9947904173572462\\nThe different runs do indeed show rather different validation scores, from 2.6 to 3.2.\\nThe average (3.0) is a much more reliable metric than any sing le score—that’s the\\nentire point of K-fold cross-validation. In this case, you’re off by $3,000 on average,\\nwhich is significant considering that the prices range from $10,000 to $50,000.\\n Let’s try training the network a bit long er: 500 epochs. To keep a record of how\\nwell the model does at each epoch, you’ll modify the training loop to save the per-\\nepoch validation score log.\\nnum_epochs = 500\\nall_mae_histories = []\\nfor i in range(k):\\nprint('processing fold #', i)\\nval_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\\nval_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\\npartial_train_data = np.concatenate(\\n[train_data[:i * num_val_samples],\\ntrain_data[(i + 1) * num_val_samples:]],\\naxis=0)\\nListing 3.28 Saving the validation logs at each fold\\nPrepares the validation data: \\ndata from partition #k\\nPrepares the training data:\\ndata from all other partitions\\nBuilds the Keras model \\n(already compiled)\\nTrains the model \\n(in silent mode, \\nverbose = 0)\\nEvaluates the model\\non the validation data\\nPrepares the validation data:\\ndata from partition #k\\nPrepares the training \\ndata: data from all \\nother partitions\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 102, 'page_label': '103'}, page_content=\"89Predicting house prices: a regression example\\npartial_train_targets = np.concatenate(\\n[train_targets[:i * num_val_samples],\\ntrain_targets[(i + 1) * num_val_samples:]],\\naxis=0)\\nmodel = build_model()\\nhistory = model.fit(partial_train_data, partial_train_targets,\\nvalidation_data=(val_data, val_targets),\\nepochs=num_epochs, batch_size=1, verbose=0)\\nmae_history = history.history['val_mean_absolute_error']\\nall_mae_histories.append(mae_history)\\nYou can then compute the average of the per-epoch MAE scores for all folds.\\naverage_mae_history = [\\nnp.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\\nLet’s plot this; see figure 3.12.\\nimport matplotlib.pyplot as plt\\nplt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\\nplt.xlabel('Epochs')\\nplt.ylabel('Validation MAE')\\nplt.show()\\nIt may be a little difficult to see the plot, due  to scaling issues and relatively high vari-\\nance. Let’s do the following:\\n\\uf0a1 Omit the first 10 data points, which are on a different scale than the rest of the curve.\\n\\uf0a1 Replace each point with an exponential moving average of the previous points,\\nto obtain a smooth curve.\\nListing 3.29 Building the history of successive mean K-fold validation scores\\nListing 3.30 Plotting validation scores\\nBuilds the Keras model \\n(already compiled)\\nTrains the model\\n(in silent mode, verbose=0)\\nFigure 3.12 Validation \\nMAE by epoch\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 103, 'page_label': '104'}, page_content=\"90 CHAPTER 3 Getting started with neural networks\\nThe result is shown in figure 3.13.\\ndef smooth_curve(points, factor=0.9):\\nsmoothed_points = []\\nfor point in points:\\nif smoothed_points:\\nprevious = smoothed_points[-1]\\nsmoothed_points.append(previous * factor + point * (1 - factor))\\nelse:\\nsmoothed_points.append(point)\\nreturn smoothed_points\\nsmooth_mae_history = smooth_curve(average_mae_history[10:])\\nplt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\\nplt.xlabel('Epochs')\\nplt.ylabel('Validation MAE')\\nplt.show()\\nAccording to this plot, validation MAE stops improving signific antly after 80 epochs.\\nPast that point, you start overfitting.\\n Once you’re finished tuning other para meters of the model (in addition to the\\nnumber of epochs, you could also adjust the size of the hidden layers), you can train a\\nfinal production model on all of the training data, with the best parameters, and then\\nlook at its performance on the test data.\\nmodel = build_model()\\nmodel.fit(train_data, train_targets,\\nepochs=80, batch_size=16, verbose=0)\\ntest_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\\nListing 3.31 Plotting validation scores, excluding the first 10 data points\\nListing 3.32 Training the final model\\nFigure 3.13 Validation MAE \\nby epoch, excluding the first \\n10 data points\\nGets a fresh, compiled model\\nTrains it on the entirety of the data\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 104, 'page_label': '105'}, page_content='91Predicting house prices: a regression example\\nHere’s the final result:\\n>>> test_mae_score\\n2.5532484335057877\\nYou’re still off by about $2,550.\\n3.6.5 Wrapping up\\nHere’s what you should take away from this example:\\n\\uf0a1 Regression is done using different loss fu nctions than what we used for classifi-\\ncation. Mean squared error ( MSE) is a loss function co mmonly used for regres-\\nsion.\\n\\uf0a1 Similarly, evaluation metrics to be used for regression differ from those used for\\nclassification; naturally, the concept of accuracy doesn’t apply for regression. A\\ncommon regression metric is mean absolute error (MAE).\\n\\uf0a1 When features in the input data have va lues in different ranges, each feature\\nshould be scaled independently as a preprocessing step.\\n\\uf0a1 When there is little data available, using K-fold validation is a great way to reli-\\nably evaluate a model.\\n\\uf0a1 When little training data is available, it’s preferable to use a small network with\\nfew hidden layers (typically only one or two), in order to avoid severe overfitting. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 105, 'page_label': '106'}, page_content='92 CHAPTER 3 Getting started with neural networks\\nChapter summary\\n\\uf0a1 You’re now able to handle the mo st common kinds of machine-learning\\ntasks on vector data: binary classification, multiclass classification, and sca-\\nlar regression. The “Wrapping up” sections earlier in the chapter summa-\\nrize the important points you’ve learned regarding these types of tasks.\\n\\uf0a1 You’ll usually need to preprocess raw data before feeding it into a neural\\nnetwork.\\n\\uf0a1 W h e n  y o u r  d a t a  h a s  f e a t u r e s  w i t h  different ranges, scale each feature\\nindependently as part of preprocessing.\\n\\uf0a1 As training progresses, neural networ ks eventually begin to overfit and\\nobtain worse results on never-before-seen data.\\n\\uf0a1 If you don’t have much training data, use a small network with only one or\\ntwo hidden layers, to avoid severe overfitting.\\n\\uf0a1 If your data is divided into many categories, you may cause information\\nbottlenecks if you make the intermediate layers too small.\\n\\uf0a1 Regression uses different loss functi ons and different evaluation metrics\\nthan classification.\\n\\uf0a1 When you’re working with little data, K-fold validation can help reliably\\nevaluate your model.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 106, 'page_label': '107'}, page_content='Fundamentals of\\nmachine learning\\nAfter the three practical examples in chapter 3, you should be starting to feel famil-\\niar with how to approach classification and regression proble ms using neural net-\\nworks, and you’ve witnesse d the central problem of ma chine learning: overfitting.\\nThis chapter will formalize some of yo ur new intuition into  a solid conceptual\\nframework for attacking and solving deep-l earning problems. We’ll consolidate all\\nof these concepts—model evaluation, data preprocessing and feature engineering,\\nand tackling overfitting—into a detailed seven-step workflow for tackling any\\nmachine-learning task.\\nThis chapter covers\\n\\uf0a1 Forms of machine learning beyond classification \\nand regression\\n\\uf0a1 Formal evaluation procedures for machine-\\nlearning models\\n\\uf0a1 Preparing data for deep learning\\n\\uf0a1 Feature engineering\\n\\uf0a1 Tackling overfitting\\n\\uf0a1 The universal workflow for approaching machine-\\nlearning problems'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 107, 'page_label': '108'}, page_content='94 CHAPTER 4 Fundamentals of machine learning\\n4.1 Four branches of machine learning\\nIn our previous examples, you’ve become  familiar with three specific types of\\nmachine-learning problems: binary classifi cation, multiclass cla ssification, and scalar\\nregression. All three are instances of supervised learning, where the goal is to learn the\\nrelationship between training inputs and training targets.\\n Supervised learning is just the tip of th e iceberg—machine learning is a vast field\\nwith a complex subfield taxonomy. Machin e-learning algorithms generally fall into\\nfour broad categories, described in the following sections.\\n4.1.1 Supervised learning\\nThis is by far the most common case. It co nsists of learning to map input data to\\nknown targets (also called annotations), given a set of examples (often annotated by\\nhumans). All four examples you’ve encounte red in this book so far were canonical\\nexamples of supervised learning. Generally,  almost all applications of deep learning\\nthat are in the spotlight these days belong in this category, such as optical character\\nrecognition, speech recognition, image classification, and language translation.\\n Although supervised learning mostly consists of classification and regression, there\\nare more exotic variants as well, including the following (with examples):\\n\\uf0a1 Sequence generation —Given a picture, predict a ca ption describing it. Sequence\\ngeneration can sometimes be reformulated as a series of classification problems\\n(such as repeatedly predicting a word or token in a sequence).\\n\\uf0a1 Syntax tree prediction —Given a sentence, predict its decomposition into a syntax\\ntree.\\n\\uf0a1 Object detection —Given a picture, draw a boundi ng box around certain objects\\ninside the picture. This can also be expressed as a classification problem (given\\nmany candidate bounding boxes, classify the contents of each one) or as a joint\\nclassification and regression problem, where the bounding-box coordinates are\\npredicted via vector regression.\\n\\uf0a1 Image segmentation—Given a picture, draw a pixel-level mask on a specific object. \\n4.1.2 Unsupervised learning\\nThis branch of machine learning consists of finding interesting transformations of the\\ninput data without the help of any targets, for the purposes of data visualization, data\\ncompression, or data denoising, or to be tter understand the correlations present in\\nthe data at hand. Unsupervised learning is the bread and butter of data analytics, and\\nit’s often a necessary step in better understanding a dataset before attempting to solve\\na supervised-learning problem. Dimensionality reduction  and clustering are well-known\\ncategories of unsupervised learning. \\n4.1.3 Self-supervised learning\\nThis is a specific instance of supervised learning, but it’s different enough that it\\ndeserves its own category. Self-supervised learning is supervised learning without\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 108, 'page_label': '109'}, page_content='95Four branches of machine learning\\nhuman-annotated labels—you can think of it as supervised learning without any\\nhumans in the loop. There are still labels involved (because the learning has to be\\nsupervised by something), but they’re generated from the input data, typically using a\\nheuristic algorithm.\\n For instance, autoencoders are a well-known instance of  self-supervised learning,\\nwhere the generated targets are the input, unmodified. In the same way, trying to pre-\\ndict the next frame in a video, given past frames, or the next word in a text, given previ-\\nous words, are instances of self-supervised learning (temporally supervised learning, in this\\ncase: supervision comes from future input data). Note that the distinction between\\nsupervised, self-supervised, and unsupervised learning can be blurry sometimes—these\\ncategories are more of a continuum without solid borders. Self-supervised learning can\\nbe reinterpreted as either supervised or unsupervised learning, depending on whether\\nyou pay attention to the learning mechanism or to the context of its application.\\nNOTE In this book, we’ll focus specifically  on supervised learning, because\\nit’s by far the dominant form of deep learning today, with a wide range of\\nindustry applications. We’ll also take a briefer look at self-supervised learning\\nin later chapters. \\n4.1.4 Reinforcement learning\\nLong overlooked, this branch of machine learning recently started to get a lot of\\nattention after Google DeepMind successfully applied it  to learning to play Atari\\ngames (and, later, learning to play Go at the highest level). In reinforcement learning,\\nan agent receives information about its environment and learns to choose actions that\\nwill maximize some reward. For instance, a neur al network that “looks” at a video-\\ngame screen and outputs game actions in or der to maximize its score can be trained\\nvia reinforcement learning.\\n Currently, reinforcement learning is mostly a research area and hasn’t yet had sig-\\nnificant practical succ esses beyond games. In time, ho wever, we expect to see rein-\\nforcement learning take over an increasing ly large range of real -world applications:\\nself-driving cars, robotics, resource management, educatio n, and so on. It’s an idea\\nwhose time has come, or will come soon.  \\nClassification and regression glossary\\nClassification and regression involve many specialized terms. You’ve come across\\nsome of them in earlier examples, and you’ll see more of them in future chapters.\\nThey have precise, machine-learning-specific definitions, and you should be familiar\\nwith them:\\n\\uf0a1 Sample or input—One data point that goes into your model.\\n\\uf0a1 Prediction or output—What comes out of your model.\\n\\uf0a1 Target—The truth. What your model should ideally have predicted, according\\nto an external source of data.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 109, 'page_label': '110'}, page_content='96 CHAPTER 4 Fundamentals of machine learning\\n \\n(continued)\\n\\uf0a1 Prediction error  or loss value —A measure of the distance between your\\nmodel’s prediction and the target.\\n\\uf0a1 Classes—A set of possible labels to choose from in a classification problem.\\nFor example, when classifying cat and dog pictures, “dog” and “cat” are the\\ntwo classes.\\n\\uf0a1 Label—A specific instance of a class anno tation in a classification problem.\\nFor instance, if picture #1234 is annota ted as containing the class “dog,”\\nthen “dog” is a label of picture #1234.\\n\\uf0a1 Ground-truth or annotations—All targets for a dataset,  typically collected by\\nhumans.\\n\\uf0a1 Binary classification—A classification task where each input sample should\\nbe categorized into two exclusive categories.\\n\\uf0a1 Multiclass classification —A classification task where each input sample\\nshould be categorized into more than two categories: for instance, classifying\\nhandwritten digits.\\n\\uf0a1 Multilabel classification—A classification task where each input sample can\\nbe assigned multiple labels. For instance, a given image may contain both a\\ncat and a dog and should be annotated both with the “cat” label and the\\n“dog” label. The number of labels per image is usually variable.\\n\\uf0a1 Scalar regression—A task where the target is a continuous scalar value. Pre-\\ndicting house prices is a good example: the different target prices form a con-\\ntinuous space.\\n\\uf0a1 Vector regression—A task where the target is a set of continuous values: for\\nexample, a continuous vector. If you’re doing regression against multiple val-\\nues (such as the coordinates of a bounding box in an image), then you’re\\ndoing vector regression.\\n\\uf0a1 Mini-batch or batch—A small set of samples (t ypically between 8 and 128)\\nthat are processed simultaneously by the model. The number of samples is\\noften a power of 2, to facilitate memory alloca tion on GPU. When training, a\\nmini-batch is used to compute a si ngle gradient-descent update applied to\\nthe weights of the model. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 110, 'page_label': '111'}, page_content='97Evaluating machine-learning models\\n4.2 Evaluating machine-learning models\\nIn the three examples presented in chapter 3,  we split the data into a training set, a\\nvalidation set, and a test set. The reason not to evaluate the models on the same data\\nthey were trained on quickly became eviden t: after just a few epochs, all three models\\nbegan to overfit. That is, their performance on neve r-before-seen data started stalling\\n(or worsening) compared to their perfor mance on the training data—which always\\nimproves as training progresses.\\n In machine learning, the goal is to achieve models that generalize—that perform\\nwell on never-before-seen data—and overfi tting is the central obstacle. You can only\\ncontrol that which you can observe, so it’s crucial to be able to reliably measure the\\ngeneralization power of your model. The following sections look at strategies for miti-\\ngating overfitting and maximizing generalizat ion. In this section, we’ll focus on how\\nto measure generalization: how to evaluate machine-learning models.\\n4.2.1 Training, validation, and test sets\\nEvaluating a model always boils down to sp litting the available data into three sets:\\ntraining, validation, and test. You train on the training data and evaluate your model\\non the validation data. Once your model is ready for prime time, you test it one final\\ntime on the test data.\\n You may ask, why not have two sets: a training set and a test set? You’d train on the\\ntraining data and evaluate on the test data. Much simpler!\\n The reason is that developing a model alwa ys involves tuning its configuration: for\\ne x a m p l e ,  c h o o s i n g  t h e  n u m b e r  o f  l a y e r s  o r  t h e  s i z e  o f  t h e  l a y e r s  ( c a l l e d  t h e  hyper-\\nparameters of the model, to distinguish them from the parameters, which are the net-\\nwork’s weights). You do this tuning by using as a feedback signal the performance of\\nthe model on the validation data. In essence, this tuning is a form of learning: a search\\nfor a good configuration in some parameter space. As a result, tuning the configura-\\ntion of the model based on its performance on the validation set can quickly result in\\noverfitting to the validation set, even though your model is never directly trained on it.\\n Central to this phenomenon is the notion of information leaks. Every time you tune\\na hyperparameter of your model based on the model’s performance on the validation\\nset, some information about the validation data leaks into the model. If you do this\\nonly once, for one parameter, then very few bits of information will leak, and your val-\\nidation set will remain reliable to evalua te the model. But if you repeat this many\\ntimes—running one experiment, evaluating on the validation set, and modifying your\\nmodel as a result—then you’ll leak an incr easingly significant amount of information\\nabout the validation set into the model.\\n At the end of the day, you’ll end up with  a model that performs artificially well on\\nthe validation data, because that’s what you optimized it for. You care about perfor-\\nmance on completely new data, not the vali dation data, so you need to use a com-\\npletely different, never-before-seen dataset to evaluate the model: the test dataset. Your\\nmodel shouldn’t have had access to any information about the test set, even indirectly.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 111, 'page_label': '112'}, page_content='98 CHAPTER 4 Fundamentals of machine learning\\nIf anything about the model has been tuned based on test set performance, then your\\nmeasure of generalization will be flawed.\\n Splitting your data into training, validation, and test sets may seem straightforward,\\nbut there are a few advanced ways to do it that can come in handy when little data is\\navailable. Let’s review three classic evaluati on recipes: simple ho ld-out validation, K-\\nfold validation, and iterated K-fold validation with shuffling.\\nSIMPLE HOLD-OUT VALIDATION\\nSet apart some fraction of your data as your test set. Train on the remaining data, and\\nevaluate on the test set. As you saw in the previous sections, in order to prevent infor-\\nmation leaks, you shouldn’t tune your model based on the test set, and therefore you\\nshould also reserve a validation set.\\n Schematically, hold-out validation looks like figure 4.1. The following listing shows\\na simple implementation.\\nnum_validation_samples = 10000\\nnp.random.shuffle(data)\\nvalidation_data = data[:num_validation_samples]\\ndata = data[num_validation_samples:]\\ntraining_data = data[:]\\nmodel = get_model()\\nmodel.train(training_data)\\nvalidation_score = model.evaluate(validation_data)\\n# At this point you can tune your model,\\n# retrain it, evaluate it, tune it again...\\nmodel = get_model()\\nmodel.train(np.concatenate([training_data,\\nvalidation_data]))\\ntest_score = model.evaluate(test_data)\\nListing 4.1 Hold-out validation\\nTraining set\\nTotal available labeled data\\nTrain on this Evaluate\\non this\\nHeld-out\\nvalidation\\nset\\nFigure 4.1 Simple hold-\\nout validation split\\nShuffling the data is \\nusually appropriate. Defines the \\nvalidation set\\nDefines the training set\\nTrains a model on the training \\ndata, and evaluates it on the \\nvalidation data\\nOnce you’ve tuned your \\nhyperparameters, it’s common to \\ntrain your final model from scratch \\non all non-test data available.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 112, 'page_label': '113'}, page_content='99Evaluating machine-learning models\\nThis is the simplest evaluation protocol, an d it suffers from one flaw: if little data is\\navailable, then your validation and test sets  may contain too few samples to be statisti-\\ncally representative of the data at hand. This  is easy to recognize: if different random\\nshuffling rounds of the data before splitti ng end up yielding very different measures\\nof model performance, then you’re having this issue. K-fold validation and iterated\\nK-fold validation are two ways to address this, as discussed next. \\nK-FOLD VALIDATION\\nWith this approach, you split your data into K partitions of equal size. For each parti-\\ntion i, train a model on the remaining K – 1 partitions, and evaluate it on partition i.\\nYour final score is then  the averages of the K scores obtained. This method is helpful\\nwhen the performance of your model shows significant variance based on your train-\\ntest split. Like hold-out validation, this method doesn’t exempt you from using a dis-\\ntinct validation set for model calibration.\\n Schematically, K-fold cross-validation looks like figure 4.2. Listing 4.2 shows a simple\\nimplementation.\\nk=4\\nnum_validation_samples = len(data) // k\\nnp.random.shuffle(data)\\nvalidation_scores = []\\nfor fold in range(k):\\nvalidation_data = data[num_validation_samples * fold:\\nnum_validation_samples * (fold + 1)]\\ntraining_data = data[:num_validation_samples * fold] +\\ndata[num_validation_samples * (fold + 1):]\\nmodel = get_model()\\nmodel.train(training_data)\\nvalidation_score = model.evaluate(validation_data)\\nvalidation_scores.append(validation_score)\\nListing 4.2 K-fold cross-validation\\nData split into 3 partitions\\nValidation Training Training Validation\\nscore #1Fold 1\\nValidation Validation Training Validation\\nscore #2\\nFinal score:\\naverageFold 2\\nValidation Training Validation Validation\\nscore #3Fold 3\\nFigure 4.2 Three-fold validation\\nSelects the validation-\\ndata partition\\nUses the remainder of the data \\nas training data. Note that the \\n+ operator is list concatenation, \\nnot summation.\\nCreates a brand-new instance \\nof the model (untrained)\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 113, 'page_label': '114'}, page_content='100 CHAPTER 4 Fundamentals of machine learning\\nvalidation_score = np.average(validation_scores)\\nmodel = get_model()\\nmodel.train(data)\\ntest_score = model.evaluate(test_data)\\nITERATED K-FOLD VALIDATION WITH SHUFFLING\\nThis one is for situations in which you have relatively little data available and you need\\nto evaluate your model as precisely as possible. I’ve found it to be extremely helpful in\\nKaggle competitions. It consists of applying K-fold validation multiple times, shuffling\\nthe data every time before splitting it K ways. The final score is the average of the\\nscores obtained at each run of K-fold validation. Note that you end up training and\\nevaluating P × K models (where P is the number of iterations you use), which can very\\nexpensive. \\n4.2.2 Things to keep in mind\\nKeep an eye out for the following when you’re choosing an evaluation protocol:\\n\\uf0a1 Data representativeness —You want both your training set and test set to be repre-\\nsentative of the data at hand. For instance , if you’re trying to classify images of\\ndigits, and you’re starting from an array of samples where the samples are\\nordered by their class, taking the first 80%  of the array as your training set and\\nthe remaining 20% as your test set will result in your training set containing\\nonly classes 0–7, whereas your test set co ntains only classes 8–9. This seems like\\na ridiculous mistake, but it’s surprising ly common. For this reason, you usually\\nshould randomly shuffle your data before splitting it into training and test sets.\\n\\uf0a1 The arrow of time—If you’re trying to predict the future given the past (for exam-\\nple, tomorrow’s weathe r, stock movements, and so on), you should not ran-\\ndomly shuffle your data before splitting  it, because doing so will create a\\ntemporal leak: your model will effectively be trained on data from the future. In\\nsuch situations, you should always make  sure all data in your test set is posterior\\nto the data in the training set.\\n\\uf0a1 Redundancy in your data —If some data points in your data appear twice (fairly\\ncommon with real-world data ), then shuffling the data and splitting it into a\\ntraining set and a validation set will result in redundancy between the training\\nand validation sets. In effect, you’ll be testing on part of your training data,\\nwhich is the worst thing you can do! Make  sure your training set and validation\\nset are disjoint. \\nValidation score: \\naverage of the \\nvalidation scores \\nof the k folds\\nTrains the final \\nmodel on all non-\\ntest data available\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 114, 'page_label': '115'}, page_content='101Data preprocessing, feature engineering, and feature learning\\n4.3 Data preprocessing, feature engineering, \\nand feature learning\\nIn addition to model evaluation, an important question we must tackle before we dive\\ndeeper into model development is the foll owing: how do you prepare the input data\\nand targets before feeding them into a neur al network? Many data-preprocessing and\\nfeature-engineering techniques are domain specific (for example, specific to text data\\nor image data); we’ll cover those in the fo llowing chapters as we encounter them in\\npractical examples. For now, we’ll review  the basics that are common to all data\\ndomains.\\n4.3.1 Data preprocessing for neural networks\\nData preprocessing aims at making the raw data at hand more amenable to neural\\nnetworks. This includes vectorization, no rmalization, handling missing values, and\\nfeature extraction.\\nVECTORIZATION\\nAll inputs and targets in a neural network must be tensors of floating-point data (or, in\\nspecific cases, tensors of integers). Whatever data you need to process—sound,\\nimages, text—you must first turn  into tensors, a step called data vectorization . For\\ninstance, in the two previous text-classifica tion examples, we started from text repre-\\nsented as lists of integers (standing for sequences of words), and we used one-hot\\nencoding to turn them into a tensor of float32 data. In the examples of classifying\\ndigits and predicting house prices, the data  already came in vectorized form, so you\\nwere able to skip this step. \\nVALUE NORMALIZATION\\nIn the digit-classification example, you started from image data encoded as integers in\\nthe 0–255 range, encoding grayscale values. Before you fed this data into your net-\\nwork, you had to cast it to float32 and divide by 255 so you’d end up with floating-\\npoint values in the 0–1 rang e. Similarly, when predicti ng house prices, you started\\nfrom features that took a variety of ranges—some features had small floating-point val-\\nues, others had fairly large integer values. Before you fed this data into your network,\\nyou had to normalize each feature independen tly so that it had a standard deviation\\nof 1 and a mean of 0.\\n In general, it isn’t safe to feed into a neural network data that takes relatively large val-\\nues (for example, multidigit integers, which are much larger than the initial values taken\\nby the weights of a network) or data that is heterogeneous (for example, data where one\\nfeature is in the range 0–1 and another is in the range 100–200). Doing so can trigger\\nlarge gradient updates that will prevent the network from converging. To make learning\\neasier for your network, your data should have the following characteristics:\\n\\uf0a1 Take small values—Typically, most values should be in the 0–1 range.\\n\\uf0a1 Be homogenous —That is, all features should take values in roughly the same\\nrange.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 115, 'page_label': '116'}, page_content='102 CHAPTER 4 Fundamentals of machine learning\\nAdditionally, the following stricter normal ization practice is common and can help,\\nalthough it isn’t always necessary (for example, you didn’t do this in the digit-classification\\nexample):\\n\\uf0a1 Normalize each feature independently to have a mean of 0.\\n\\uf0a1 Normalize each feature independently to have a standard deviation of 1.\\nThis is easy to do with Numpy arrays:\\nx -= x.mean(axis=0)\\nx /= x.std(axis=0)\\nHANDLING MISSING VALUES\\nYou may sometimes have missing values in yo ur data. For instance, in the house-price\\nexample, the first feature (the column of index 0 in the data) was the per capita crime\\nrate. What if this feature wasn’t available for all samples? You’d then have missing val-\\nues in the training or test data.\\n In general, with neural networks, it’s safe to input missing values as 0, with the con-\\ndition that 0 isn’t already a meaningful value. The network will learn from exposure to\\nthe data that the value 0 means missing data and will start ignoring the value.\\n Note that if you’re expecting missing values in the test data, but the network was\\ntrained on data without any missing values, the network won’t have learned to ignore\\nmissing values! In this situation, you should  artificially generate training samples with\\nmissing entries: copy some training samples several ti mes, and drop some of the fea-\\ntures that you expect are likely to be missing in the test data. \\n4.3.2 Feature engineering\\nFeature engineering is the process of using your own knowledge about the data and about\\nthe machine-learning algorithm at hand (in this case, a neural network) to make the\\nalgorithm work better by applying\\nhardcoded (nonlearned) transfor-\\nmations to the data before it goes\\ninto the model. In many cases, it isn’t\\nreasonable to expect a machine-\\nlearning model to be able to learn\\nfrom completely arbitrary data. The\\ndata needs to be presented to the\\nmodel in a way that will make the\\nmodel’s job easier.\\n Let’s look at an intuitive example.\\nSuppose you’re trying to develop a\\nmodel that can take as input an\\nimage of a clock and can output the\\ntime of day (see figure 4.3).\\nAssuming x is a 2D data matrix \\nof shape (samples, features) \\nRaw data:\\npixel grid\\nBetter\\nfeatures:\\nclock hands’\\ncoordinates\\n{x1: 0.7,\\ny1: 0.7}\\n{x2: 0.5,\\ny2: 0.0}\\n{x1: 0.0,\\ny2: 1.0}\\n{x2: -0.38,\\n2: 0.32}\\nEven better\\nfeatures:\\nangles of\\nclock hands\\ntheta1: 45\\ntheta2: 0\\ntheta1: 90\\ntheta2: 140\\nFigure 4.3 Feature engineering for reading the time on \\na clock\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 116, 'page_label': '117'}, page_content='103Data preprocessing, feature engineering, and feature learning\\nIf you choose to use the raw pixels of the image as input data, then you have a difficult\\nmachine-learning problem on your hands. You’ll need a convolutional neural net-\\nwork to solve it, and you’ll have to expend  quite a bit of com putational resources to\\ntrain the network.\\n But if you already understa nd the problem at a high level (you understand how\\nhumans read time on a clock face), then you can come up with much better input fea-\\ntures for a machine-learning algorithm: for instance, it’s easy to write a five-line\\nPython script to follow the black pixels of  the clock hands and ou tput the (x, y) coor-\\ndinates of the tip of each hand. Then a si mple machine-learning algorithm can learn\\nto associate these coordinates with the appropriate time of day.\\n You can go even further: do a coordina te change, and express the (x, y) coordi-\\nnates as polar coordinates with regard to the center of the image. Your input will\\nbecome the angle theta of each clock hand. At this point, your features are making\\nthe problem so easy that no machine learni ng is required; a simple rounding opera-\\ntion and dictionary lookup are enough to recover the approximate time of day.\\n That’s the essence of feature engineerin g: making a problem easier by expressing\\nit in a simpler way. It usually requires understanding the problem in depth.\\n Before deep learning, feature engineering used to be critical, because classical\\nshallow algorithms didn’t have hypothesis sp aces rich enough to learn useful features\\nby themselves. The way you presented the data to the algorithm was essential to its suc-\\ncess. For instance, before convolutional ne ural networks became successful on the\\nMNIST digit-classification probl em, solutions were typically  based on hardcoded fea-\\ntures such as the number of loops in a digit image, the height of each digit in an\\nimage, a histogram of pixel values, and so on.\\n Fortunately, modern deep learning remo ves the need for most feature engineer-\\ning, because neural networks are capable of  automatically extrac ting useful features\\nfrom raw data. Does this mean you don’t have to worry about feature engineering as\\nlong as you’re using deep neural networks? No, for two reasons:\\n\\uf0a1 Good features still allow you to solve problems more elegantly while using fewer\\nresources. For instance, it would be ridiculous to solve the problem of reading a\\nclock face using a convolutional neural network.\\n\\uf0a1 Good features let you solve a problem wi th far less data. The ability of deep-\\nlearning models to learn fe atures on their own relies on having lots of training\\ndata available; if you have only a few samples, then the information value in\\ntheir features becomes critical. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 117, 'page_label': '118'}, page_content='104 CHAPTER 4 Fundamentals of machine learning\\n4.4 Overfitting and underfitting\\nIn all three examples in the previous chapter—predicting movie reviews, topic classifi-\\ncation, and house-price regression—the performance of the model on the held-out\\nvalidation data always peaked after a fe w epochs and then began to degrade: the\\nmodel quickly started to overfit to the training data. Ov erfitting happens in every\\nmachine-learning problem. Learning how to de al with overfitting is essential to mas-\\ntering machine learning.\\n The fundamental issue in machine learni ng is the tension between optimization\\nand generalization. Optimization refers to the process of adjusting a model to get the\\nbest performance possible on the training data (the learning in machine learning ),\\nwhereas generalization refers to how well the trained model performs on data it has\\nnever seen before. The goal of the game is to get good generalization, of course, but\\nyou don’t control generalization; you can on ly adjust the model based on its training\\ndata.\\n At the beginning of training, optimizati on and generalization are correlated: the\\nlower the loss on training data, the lower the loss on test data. While this is happening,\\nyour model is said to be underfit: there is still progress to be made; the network hasn’t\\nyet modeled all relevant patterns in the tr aining data. But after a certain number of\\niterations on the training data, generalization stops improving, and validation metrics\\nstall and then begin to degrade: the model is starting to overfit. That is, it’s beginning\\nto learn patterns that are specific to the training data but that are misleading or irrele-\\nvant when it comes to new data.\\n To prevent a model from le arning misleading or irrelevant patterns found in the\\ntraining data, the best solution is to get more training data . A model trained on more data\\nwill naturally generalize better. When that isn’t possible, the next-best solution is to\\nmodulate the quantity of information that yo ur model is allowed to store or to add\\nconstraints on what information it’s allowed to store. If a network can only afford to\\nmemorize a small number of patterns, the opti mization process will force it to focus\\non the most prominent patterns, which have a better chance of generalizing well.\\n The processing of fighting ov erfitting this way is called regularization. Let’s review\\nsome of the most common regularization techniques and apply them in practice to\\nimprove the movie-classification model from section 3.4.\\n4.4.1 Reducing the network’s size\\nThe simplest way to prevent overfitting is to reduce the size of the model: the number\\nof learnable parameters in the model (which  is determined by the number of layers\\nand the number of units per layer). In deep learning, the number of learnable param-\\neters in a model is often referred to as the model’s capacity. Intuitively, a model with\\nmore parameters has more memorization capacity and therefore can easily learn a per-\\nfect dictionary-like mapping between trai ning samples and their targets—a mapping\\nwithout any generalization power. For instan ce, a model with 500,000 binary parame-\\nters could easily be made to lear n the class of every digit in the MNIST training set:\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 118, 'page_label': '119'}, page_content=\"105Overfitting and underfitting\\nwe’d need only 10 binary parameters for each of the 50,000 digits. But such a model\\nwould be useless for cl assifying new digit sa mples. Always keep this in mind: deep-\\nlearning models tend to be good at fitting to the training data, but the real challenge\\nis generalization, not fitting.\\n On the other hand, if the network has limited memorization resources, it won’t be\\nable to learn this mapping as easily; thus, in  order to minimize it s loss, it will have to\\nresort to learning compressed representati ons that have predictive power regarding\\nthe targets—precisely the type of represen tations we’re interested in. At the same\\ntime, keep in mind that you should use models that have enough parameters that they\\ndon’t underfit: your model shouldn’t be starved for memorization resources. There is\\na compromise to be found between too much capacity and not enough capacity.\\n Unfortunately, there is no magical form ula to determine the right number of lay-\\ners or the right size for each layer. You mu st evaluate an array of different architec-\\ntures (on your validation set, not on your test set, of course) in order to find the\\ncorrect model size for your data. The genera l workflow to find an appropriate model\\nsize is to start with relatively few layers and parameters, and increase the size of the lay-\\ners or add new layers until you see diminishing returns with regard to validation loss.\\n Let’s try this on the movie-review classi fication network. The original network is\\nshown next.\\nfrom keras import models\\nfrom keras import layers\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(16, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nNow let’s try to replace it with this smaller network.\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(4, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nFigure 4.4 shows a comparison of the validation losses of the original network and the\\nsmaller network. The dots are the validation  loss values of the smaller network, and\\nthe crosses are the initial ne twork (remember, a lower valid ation loss signals a better\\nmodel).\\nListing 4.3 Original model\\nListing 4.4 Version of the model with lower capacity\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 119, 'page_label': '120'}, page_content=\"106 CHAPTER 4 Fundamentals of machine learning\\n \\nAs you can see, the smaller network starts overfitting later than the reference network\\n(after six epochs rather than four), and its performance degrades more slowly once it\\nstarts overfitting.\\n Now, for kicks, let’s add to this benc hmark a network that has much more capac-\\nity—far more than the problem warrants.\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(512, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(512, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nFigure 4.5 shows how the bigger network fa res compared to the reference network.\\nThe dots are the validation loss values of the bigger network, and the crosses are the\\ninitial network.\\nListing 4.5 Version of the model with higher capacity\\nFigure 4.4 Effect of model \\ncapacity on validation loss: trying \\na smaller model\\nFigure 4.5 Effect of model \\ncapacity on validation loss: \\ntrying a bigger model\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 120, 'page_label': '121'}, page_content='107Overfitting and underfitting\\nThe bigger network starts over fitting almost immediately, after just one epoch, and it\\noverfits much more severely. Its validation loss is also noisier.\\n Meanwhile, figure 4.6 shows the training  losses for the two networks. As you can\\nsee, the bigger network gets its training loss near zero very quickly. The more capacity\\nthe network has, the more quickly it can model the training data (resulting in a low\\ntraining loss), but the more su sceptible it is to overfittin g (resulting in a large differ-\\nence between the training and validation loss).  \\n4.4.2 Adding weight regularization\\nYou may be familiar wi th the principle of Occam’s razor : given two explanations for\\nsomething, the explanation most likely to be correct is the simplest one—the one that\\nmakes fewer assumptions. This idea also ap plies to the models le arned by neural net-\\nworks: given some training data and a netw ork architecture, multiple sets of weight\\nvalues (multiple models) could explain the data. Simpler models are less likely to over-\\nfit than complex ones.\\n A simple model in this context is a model where the distribution of parameter values\\nhas less entropy (or a model with fewer para meters, as you saw in the previous sec-\\ntion). Thus a common way to mitigate overfitting is to put constraints on the complex-\\nity of a network by forcing its weights to take only small values, which makes the\\ndistribution of weight values more regular. This is called weight regularization, and it’s\\ndone by adding to the loss function of the network a cost associated with having large\\nweights. This cost comes in two flavors:\\n\\uf0a1 L1 regularization —The cost added is proportional to the absolute value of the\\nweight coefficients (the L1 norm of the weights).\\n\\uf0a1 L2 regularization —The cost added is proportional to the square of the value of the\\nweight coefficients  (the L2 norm  of the weights). L2 regularization is also called\\nweight decay in the context of neural networks. Don’t let the different name con-\\nfuse you: weight decay is mathematically the same as L2 regularization.\\nFigure 4.6 Effect of model \\ncapacity on training loss: \\ntrying a bigger model\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 121, 'page_label': '122'}, page_content=\"108 CHAPTER 4 Fundamentals of machine learning\\nIn Keras, weight regulariza tion is added by passing weight regularizer instances to layers\\nas keyword arguments. Let’s add L2 weight regularization to the movie-review classifi-\\ncation network.\\nfrom keras import regularizers\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\\nactivation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\\nactivation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nl2(0.001) means every coefficient in the weight matrix of the layer will add 0.001 *\\nweight_coefficient_value to the total loss of the network. Note that because this\\npenalty is only added at training time , the loss for this network will be much higher at\\ntraining than at test time.\\n Figure 4.7 shows the impact of the L2 regularization penalty. As you can see, the\\nmodel with L2 regularization (dots) has become much more resistant to overfitting\\nthan the reference model (crosses), even though both models have the same number\\nof parameters.\\nAs an alternative to L2 regularization, you can use one of the following Keras weight\\nregularizers.\\nfrom keras import regularizers\\nregularizers.l1(0.001)\\nregularizers.l1_l2(l1=0.001, l2=0.001)\\nListing 4.6 Adding L2 weight regularization to the model\\nListing 4.7 Different weight regularizers available in Keras\\nFigure 4.7 Effect of L2 weight \\nregularization on validation loss\\nL1 regularization Simultaneous L1 and \\nL2 regularization \\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 122, 'page_label': '123'}, page_content='109Overfitting and underfitting\\n4.4.3 Adding dropout\\nDropout is one of the most effective and mo st commonly used regularization tech-\\nniques for neural networks, developed by Geoff Hinton and his students at the Uni-\\nversity of Toronto. Dropout, applied to a layer, consists of randomly dropping out\\n(setting to zero) a number of output featur es of the layer during training. Let’s say a\\ngiven layer would normally return a vector [0 .2, 0.5, 1.3, 0.8, 1.1] for a given input\\nsample during training. After applying dropout, this vector will have a few zero entries\\ndistributed at random: for example, [0, 0.5, 1.3, 0, 1.1]. The dropout rate is the fraction\\nof the features that are zeroed out; it’s usually set between 0.2 and 0.5. At test time, no\\nunits are dropped out; instead, the layer’s output values are scaled down by a factor\\nequal to the dropout rate, to balance for th e fact that more units are active than at\\ntraining time.\\n Consider a Numpy matrix contai ning the output of a layer, \\nlayer_output, of\\nshape (batch_size, features). At training time, we zero out at random a fraction of\\nthe values in the matrix:\\nlayer_output *= np.random.randint(0, high=2, size=layer_output.shape)\\nAt test time, we scale down the output by the dropout rate. Here, we scale by 0.5\\n(because we previously dropped half the units):\\nlayer_output *= 0.5\\nNote that this process can be implemented by doing both operations at training time\\nand leaving the output unchanged at test time, which is often the way it’s imple-\\nmented in practice (see figure 4.8):\\nlayer_output *= np.random.randint(0, high=2, size=layer_output.shape)\\nlayer_output /= 0.5\\nThis technique may seem strange and arbitr ary. Why would this help reduce overfit-\\nting? Hinton says he was inspired by, am ong other things, a fr aud-prevention mecha-\\nnism used by banks. In his own words, “I went to my bank. The tellers kept changing\\nand I asked one of them why. He said he didn’t know but they got moved around a lot.\\nAt training time, drops out 50%\\nof the units in the output\\nAt test time\\nAt training timeNote that we’re scaling up rather \\nscaling down in this case.\\n0.3\\n* 2\\n0.6\\n0.2\\n0.7\\n0.2\\n0.1\\n1.9\\n0.5\\n1.5\\n0.0\\n0.3\\n1.0\\n0.0\\n0.3\\n1.2\\n0.0\\n0.0\\n50%\\ndropout 0.6\\n0.0\\n0.7\\n0.2\\n0.1\\n1.9\\n0.0\\n1.5\\n0.0\\n0.3\\n0.0\\n0.0\\n0.3\\n0.0\\n0.0\\nFigure 4.8 Dropout applied to an \\nactivation matrix at training time, \\nwith rescaling happening during \\ntraining. At test time, the activation \\nmatrix is unchanged.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 123, 'page_label': '124'}, page_content=\"110 CHAPTER 4 Fundamentals of machine learning\\nI figured it must be becaus e it would require cooperation between employees to suc-\\ncessfully defraud the bank. This made me re alize that randomly removing a different\\nsubset of neurons on each example would prevent conspiracies and thus reduce over-\\nfitting.”\\n1 The core idea is that introducing nois e in the output values of a layer can\\nbreak up happenstance patterns that aren’t  significant (what Hinton refers to as con-\\nspiracies), which the network will start memorizing if no noise is present.\\n In Keras, you can introduce dropout in a network via the Dropout layer, which is\\napplied to the output of the layer right before it:\\nmodel.add(layers.Dropout(0.5))\\nLet’s add two Dropout layers in the IMDB network to see how well they do at reducing\\noverfitting.\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dropout(0.5))\\nmodel.add(layers.Dense(16, activation='relu'))\\nmodel.add(layers.Dropout(0.5))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nFigure 4.9 shows a plot of the results. Again, this is a clear improvement over the refer-\\nence network.\\nTo recap, these are the most common ways to prevent overfitting in neural networks:\\n\\uf0a1 Get more training data.\\n\\uf0a1 Reduce the capacity of the network.\\n\\uf0a1 Add weight regularization.\\n\\uf0a1 Add dropout. \\n1 See the Reddit thread “AMA: We are the Google Brain team. We’d love to answer your questions about\\nmachine learning,” http:/ /mng.bz/XrsS.\\nListing 4.8 Adding dropout to the IMDB network\\nFigure 4.9 Effect of dropout \\non validation loss\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 124, 'page_label': '125'}, page_content='111The universal workflow of machine learning\\n4.5 The universal workfl ow of machine learning\\nIn this section, we’ll present a universal bl ueprint that you can use to attack and solve\\nany machine-learning problem. The bluepr int ties together the concepts you’ve\\nlearned about in this chapter: problem de finition, evaluation, feature engineering,\\nand fighting overfitting.\\n4.5.1 Defining the problem and assembling a dataset\\nFirst, you must define the problem at hand:\\n\\uf0a1 What will your input data be? What are you trying to predict? You can only learn\\nto predict something if you have availa ble training data: for example, you can\\nonly learn to classify the sentiment of  movie reviews if you have both movie\\nreviews and sentiment annotations available.  As such, data availability is usually\\nthe limiting factor at this stage (unless you have the means to pay people to col-\\nlect data for you).\\n\\uf0a1 What type of problem are you facing? Is it binary classification? Multiclass classi-\\nfication? Scalar regression? Vector regression? Multicla ss, multilabel classifica-\\ntion? Something else, like clustering, ge neration, or reinforcement learning?\\nIdentifying the problem type will guide yo ur choice of model architecture, loss\\nfunction, and so on.\\nYou can’t move to the next stage until you know what your inputs and outputs are, and\\nwhat data you’ll use. Be aware of the hypotheses you make at this stage:\\n\\uf0a1 You hypothesize that your outputs can be predicted given your inputs.\\n\\uf0a1 You hypothesize that your available data is sufficiently informative to learn the\\nrelationship between inputs and outputs.\\nUntil you have a working model, these are merely hypotheses, waiting to be validated\\nor invalidated. Not all problems can be solv ed; just because you’ve assembled exam-\\nples of inputs X and targets Y doesn’t mean X contains enough information to predict\\nY. For instance, if you’re trying to predic t the movements of a stock on the stock mar-\\nket given its recent price hi story, you’re unlikely to succeed, because price history\\ndoesn’t contain much predictive information.\\n One class of unsolvable problems you should be aware of is nonstationary problems.\\nSuppose you’re trying to build a recommendation engine for clothing, you’re training\\nit on one month of data (August), and you want to start generating recommendations\\nin the winter. One big issue is that the kinds of clothes people buy change from season\\nto season: clothes buying is a nonstation ary phenomenon over the scale of a few\\nmonths. What you’re trying to model changes over time. In this case, the right move is\\nt o  c o n s t a n t l y  r e t r a i n  y o u r  m o d e l  o n  d a t a  f r o m  t h e  r e c e n t  p a s t ,  o r  g a t h e r  d a t a  a t  a\\ntimescale where the problem is stationary. For a cyclical problem like clothes buying, a\\nfew years’ worth of data will suffice to capture seasonal variation—but remember to\\nmake the time of the year an input of your model!\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 125, 'page_label': '126'}, page_content='112 CHAPTER 4 Fundamentals of machine learning\\n Keep in mind that machine learning can only be used to memorize patterns that\\nare present in your training data. You can only recognize what you’ve seen before.\\nUsing machine learning trained on past da ta to predict the future is making the\\nassumption that the future will behave like the past. That often isn’t the case. \\n4.5.2 Choosing a measure of success\\nTo control something, you need to be able to observe it. To achieve success, you must\\ndefine what you mean by success—accuracy? Precision and recall? Customer-retention\\nrate? Your metric for success will guide the choice of a loss function: what your model\\nwill optimize. It should direct ly align with your higher-level  goals, such as the success\\nof your business.\\n For balanced-classification problems, where every class is equally likely, accuracy and\\narea under the receiver operating characteristic curve  (ROC AUC) are common metrics. For\\nclass-imbalanced problems, you can use precision and recall. For ranking problems or\\nmultilabel classification, you can use mean  average precision. And it isn’t uncommon\\nto have to define your own custom metric by which to measure success. To get a sense\\nof the diversity of machine-learning success metrics and how they relate to different\\nproblem domains, it’s helpfu l to browse the data scie nce competitions on Kaggle\\n(https:/ /kaggle.com); they showcase a wide range of problems and evaluation metrics. \\n4.5.3 Deciding on an evaluation protocol\\nOnce you know what you’re aiming for, you must establish ho w you’ll measure your\\ncurrent progress. We’ve previously reviewed three common evaluation protocols:\\n\\uf0a1 Maintaining a hold-out validation set —The way to go when you have plenty of\\ndata\\n\\uf0a1 Doing K-fold cross-validation —The right choice when you have too few samples\\nfor hold-out validation to be reliable\\n\\uf0a1 Doing iterated K-fold validation —For performing highly accurate model evalua-\\ntion when little data is available\\nJust pick one of these. In most cases, the first will work well enough. \\n4.5.4 Preparing your data\\nOnce you know what you’re training on, what you’re optimizing for, and how to evalu-\\nate your approach, you’re almost ready to begin training models. But first, you should\\nformat your data in a way that can be fed into a machine-learning model—here, we’ll\\nassume a deep neural network:\\n\\uf0a1 As you saw previously, your data should be formatted as tensors.\\n\\uf0a1 The values taken by these tensors should usually be scaled to small values: for\\nexample, in the [-1, 1] range or [0, 1] range.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 126, 'page_label': '127'}, page_content='113The universal workflow of machine learning\\n\\uf0a1 If different features take values in di fferent ranges (heterogeneous data), then\\nthe data should be normalized.\\n\\uf0a1 You may want to do some feature engineering, especially for small-data problems.\\nOnce your tensors of input data and target data are ready, you can begin to train models. \\n4.5.5 Developing a model that does better than a baseline\\nYour goal at this stage is to achieve statistical power : that is, to develop a small model\\nthat is capable of beating a dumb baseline. In the MNIST digit-classification example,\\nanything that achieves an a ccuracy greater than 0.1 can be said to have statistical\\npower; in the IMDB example, it’s anything with an accuracy greater than 0.5.\\n Note that it’s not always possible to achieve statistical power. If you can’t beat a ran-\\ndom baseline after trying multiple reasonable architectures, it may be that the answer\\nto the question you’re asking isn’t present in the input data. Remember that you make\\ntwo hypotheses:\\n\\uf0a1 You hypothesize that your outputs can be predicted given your inputs.\\n\\uf0a1 You hypothesize that the available data is sufficiently informative to learn the\\nrelationship between inputs and outputs.\\nIt may well be that these hypotheses are false, in which case you must go back to the\\ndrawing board.\\n Assuming that things go well, you need  to make three key choices to build your\\nfirst working model:\\n\\uf0a1 Last-layer activation —This establishes useful cons traints on the network’s out-\\nput. For instance, the IMDB classification example used sigmoid in the last\\nlayer; the regression example didn’t use any last-layer activation; and so on.\\n\\uf0a1 Loss function—This should match the type of problem you’re trying to solve. For\\ninstance, the IMDB example used binary_crossentropy, the regression exam-\\nple used mse, and so on.\\n\\uf0a1 Optimization configuration —What optimizer will you use? What will its learning\\nrate be? In most cases, it’s safe to go with rmsprop and its default learning rate.\\nRegarding the choice of a loss function, note  that it isn’t always possible to directly\\noptimize for the metric that measures su ccess on a problem. Sometimes there is no\\neasy way to turn a metric into a loss function ; loss functions, after all, need to be com-\\nputable given only a mini-batch of data (ideally, a loss function should be computable\\nfor as little as a single data point) and must be differentiable (otherwise, you can’t use\\nbackpropagation to train your  network). For instance, the widely used classification\\nmetric ROC AUC can’t be directly optimi zed. Hence, in classifi cation tasks, it’s com-\\nmon to optimize for a proxy metric of ROC AUC, such as crossentropy. In general, you\\ncan hope that the lower the crossentropy gets, the higher the ROC AUC will be.\\n Table 4.1 can help you choose a last-layer  activation and a loss function for a few\\ncommon problem types.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 127, 'page_label': '128'}, page_content='114 CHAPTER 4 Fundamentals of machine learning\\n \\n4.5.6 Scaling up: developing a model that overfits\\nOnce you’ve obtained a model that has statistical power, the question becomes, is your\\nmodel sufficiently powerful? Does it have enough layers and parameters to properly\\nmodel the problem at hand? For instance, a network with a single hidden layer with\\ntwo units would have statistical power on MNIST but wouldn’t be sufficient to solve the\\nproblem well. Remember that the universa l tension in machine learning is between\\noptimization and generalization; the ideal model is one that stands right at the border\\nbetween underfitting and overfitting; between undercapacity and overcapacity. To fig-\\nure out where this border lies, first you must cross it.\\n To figure out how big a model you’ll need, you must develop a model that overfits.\\nThis is fairly easy:\\n1 Add layers.\\n2 Make the layers bigger.\\n3 Train for more epochs.\\nAlways monitor the training loss and validati on loss, as well as the training and valida-\\ntion values for any metrics you care abou t. When you see that the model’s perfor-\\nmance on the validation data begins to degrade, you’ve achieved overfitting.\\n The next stage is to start regularizing and tuning the model, to get as close as pos-\\nsible to the ideal model that neither underfits nor overfits. \\n4.5.7 Regularizing your model and tuning your hyperparameters\\nThis step will take the most time: you’ll re peatedly modify your model, train it, evalu-\\nate on your validation data (not the test da ta, at this point), modify it again, and\\nrepeat, until the model is as good as it can get. These are some things you should try:\\n\\uf0a1 Add dropout.\\n\\uf0a1 Try different architectures: add or remove layers.\\n\\uf0a1 Add L1 and/or L2 regularization.\\nTable 4.1 Choosing the right last-layer activation and loss function for your model\\nProblem type Last-layer activation Loss function\\nBinary classification sigmoid binary_crossentropy\\nMulticlass, single-label classification softmax categorical_crossentropy\\nMulticlass, multilabel classification sigmoid binary_crossentropy\\nRegression to arbitrary values None mse\\nRegression to values between 0 and 1 sigmoid mse  or binary_crossentropy\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 128, 'page_label': '129'}, page_content='115The universal workflow of machine learning\\n\\uf0a1 Try different hyperparameter s (such as the number of units per layer or the\\nlearning rate of the optimizer) to find the optimal configuration.\\n\\uf0a1 Optionally, iterate on feature engineer ing: add new features, or remove fea-\\ntures that don’t seem to be informative.\\nBe mindful of the following: every time you use feedback from your validation process\\nto tune your model, you leak information about the validation process into the model.\\nRepeated just a few times, th is is innocuous; but done sy stematically over many itera-\\ntions, it will eventually cause your mode l to overfit to the validation process (even\\nthough no model is directly trained on an y of the validation data). This makes the\\nevaluation process less reliable.\\n Once you’ve developed a satisfactory model configuration, you can train your final\\nproduction model on all the available data  (training and validation) and evaluate it\\none last time on the test set.  If it turns out that performance on the test set is signifi-\\ncantly worse than the performance measured on the validation data, this may mean\\neither that your validation pr ocedure wasn’t reliable after all, or that you began over-\\nfitting to the validation data while tuning the parameters of the model. In this case,\\nyou may want to switch to a more reliabl e evaluation protocol (such as iterated K-fold\\nvalidation). \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 129, 'page_label': '130'}, page_content='116 CHAPTER 4 Fundamentals of machine learning\\nChapter summary\\n\\uf0a1 Define the problem at hand and the data on which you’ll train. Collect\\nthis data, or annotate it with labels if need be.\\n\\uf0a1 Choose how you’ll meas ure success on your probl em. Which metrics will\\nyou monitor on your validation data?\\n\\uf0a1 Determine your evaluation protocol: hold-out validation? K-fold valida-\\ntion? Which portion of the data should you use for validation?\\n\\uf0a1 Develop a first model that does better  than a basic baseline: a model with\\nstatistical power.\\n\\uf0a1 Develop a model that overfits.\\n\\uf0a1 Regularize your model and tune its hyperparameters, based on perfor-\\nmance on the validation data. A lot of machine-learning research tends to\\nfocus only on this step—but keep the big picture in mind.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 130, 'page_label': '131'}, page_content='Part 2\\nDeep learning in practice\\nChapters 5–9 will help you gain practical intuition about how to solve real-\\nworld problems using deep learning, and will familiarize you with essential deep-\\nlearning best practices. Most of the code examples in the book are concentrated\\nin this second half.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 131, 'page_label': '132'}, page_content='Deep learning\\nfor computer vision\\nThis chapter introduces convolutiona l neural networks, also known as convnets, a\\ntype of deep-learning model almost univer sally used in computer vision applica-\\ntions. You’ll learn to apply convnets to image-classification problems—in particular\\nthose involving small training datasets, which are the most common use case if you\\naren’t a large tech company.\\nThis chapter covers\\n\\uf0a1 Understanding convolutional neural networks \\n(convnets)\\n\\uf0a1 Using data augmentation to mitigate overfitting\\n\\uf0a1 Using a pretrained convnet to do feature \\nextraction\\n\\uf0a1 Fine-tuning a pretrained convnet\\n\\uf0a1 Visualizing what convnets learn and how they \\nmake classification decisions'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 132, 'page_label': '133'}, page_content=\"120 CHAPTER 5 Deep learning for computer vision\\n5.1 Introduction to convnets\\nWe’re about to dive into the theory of what convnets are and why they have been so\\nsuccessful at computer vision tasks. But first, let’s take a practical look at a simple conv-\\nnet example. It uses a convnet to classify MNIST digits, a task we performed in chapter\\n2 using a densely connected network (our test accuracy then was 97.8%). Even though\\nthe convnet will be basic, its accuracy will  blow out of the water that of the densely\\nconnected model from chapter 2.\\n The following lines of code show you what a basic convnet looks like. It’s a stack of\\nConv2D and MaxPooling2D layers. You’ll see in a minute exactly what they do.\\nfrom keras import layers\\nfrom keras import models\\nmodel = models.Sequential()\\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\\nImportantly, a convnet takes as input tensors of shape (image_height, image_width,\\nimage_channels) (not including the batch dimension). In this case, we’ll configure\\nthe convnet to process inputs of size (28, 28, 1) , which is the format of MNIST\\nimages. We’ll do this by passing the argument input_shape=(28, 28, 1) to the first\\nlayer.\\n Let’s display the architecture of the convnet so far:\\n>>> model.summary()\\n________________________________________________________________\\nLayer (type) Output Shape Param #\\n================================================================\\nconv2d_1 (Conv2D) (None, 26, 26, 32) 320\\n________________________________________________________________\\nmaxpooling2d_1 (MaxPooling2D) (None, 13, 13, 32) 0\\n________________________________________________________________\\nconv2d_2 (Conv2D) (None, 11, 11, 64) 18496\\n________________________________________________________________\\nmaxpooling2d_2 (MaxPooling2D) (None, 5, 5, 64) 0\\n________________________________________________________________\\nconv2d_3 (Conv2D) (None, 3, 3, 64) 36928\\n================================================================\\nTotal params: 55,744\\nTrainable params: 55,744\\nNon-trainable params: 0\\nYou can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of\\nshape (height, width, channels). The width and height dimensions tend to shrink\\nListing 5.1 Instantiating a small convnet\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 133, 'page_label': '134'}, page_content=\"121Introduction to convnets\\nas you go deeper in the netw ork. The number of channels is controlled by the first\\nargument passed to the Conv2D layers (32 or 64).\\n The next step is to feed the last output tensor (of shape (3, 3, 64)) into a densely\\nconnected classifier network like those yo u’re already familiar with: a stack of Dense\\nlayers. These classifiers process vectors, which are 1D, whereas the current output is a\\n3D tensor. First we have to flatten the 3D outputs to 1D, and then add a few Dense lay-\\ners on top.\\nmodel.add(layers.Flatten())\\nmodel.add(layers.Dense(64, activation='relu'))\\nmodel.add(layers.Dense(10, activation='softmax'))\\nWe’ll do 10-way classification, using a final layer with 10 outputs and a softmax activa-\\ntion. Here’s what the network looks like now:\\n>>> model.summary()\\nLayer (type) Output Shape Param #\\n================================================================\\nconv2d_1 (Conv2D) (None, 26, 26, 32) 320\\n________________________________________________________________\\nmaxpooling2d_1 (MaxPooling2D) (None, 13, 13, 32) 0\\n________________________________________________________________\\nconv2d_2 (Conv2D) (None, 11, 11, 64) 18496\\n________________________________________________________________\\nmaxpooling2d_2 (MaxPooling2D) (None, 5, 5, 64) 0\\n________________________________________________________________\\nconv2d_3 (Conv2D) (None, 3, 3, 64) 36928\\n________________________________________________________________\\nflatten_1 (Flatten) (None, 576) 0\\n________________________________________________________________\\ndense_1 (Dense) (None, 64) 36928\\n________________________________________________________________\\ndense_2 (Dense) (None, 10) 650\\n================================================================\\nTotal params: 93,322\\nTrainable params: 93,322\\nNon-trainable params: 0\\nAs you can see, the (3, 3, 64) outputs are flattened into vectors of shape (576,)\\nbefore going through two Dense layers.\\n Now, let’s train the convnet on the MNIST digits. We’ll reuse a lot of the code from\\nthe MNIST example in chapter 2.\\nfrom keras.datasets import mnist\\nfrom keras.utils import to_categorical\\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\\nListing 5.2 Adding a classifier on top of the convnet\\nListing 5.3 Training the convnet on MNIST images\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 134, 'page_label': '135'}, page_content=\"122 CHAPTER 5 Deep learning for computer vision\\ntrain_images = train_images.reshape((60000, 28, 28, 1))\\ntrain_images = train_images.astype('float32') / 255\\ntest_images = test_images.reshape((10000, 28, 28, 1))\\ntest_images = test_images.astype('float32') / 255\\ntrain_labels = to_categorical(train_labels)\\ntest_labels = to_categorical(test_labels)\\nmodel.compile(optimizer='rmsprop',\\nloss='categorical_crossentropy',\\nmetrics=['accuracy'])\\nmodel.fit(train_images, train_labels, epochs=5, batch_size=64)\\nLet’s evaluate the model on the test data:\\n>>> test_loss, test_acc = model.evaluate(test_images, test_labels)\\n>>> test_acc\\n0.99080000000000001\\nWhereas the densely connected network from chapter 2 had a test accuracy of 97.8%,\\nthe basic convnet has a test accuracy of 99.3%: we decreased the error rate by 68%\\n(relative). Not bad!\\n But why does this simple convnet work so well, compared to a densely connected\\nmodel? To answer this, let’s dive into what the \\nConv2D and MaxPooling2D layers do.\\n5.1.1 The convolution operation\\nThe fundamental difference between a dens ely connected layer and a convolution\\nlayer is this: Dense layers learn global patterns in their input feature space (for exam-\\nple, for a MNIST digit, patterns involving all pixels), whereas convolution layers learn\\nlocal patterns (see figure 5.1): in the case of images, patterns found in small 2D win-\\ndows of the inputs. In the previous example, these windows were all 3 × 3.\\nFigure 5.1 Images can be broken \\ninto local patterns such as edges, \\ntextures, and so on.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 135, 'page_label': '136'}, page_content='123Introduction to convnets\\nThis key characteristic gives convnets two interesting properties:\\n\\uf0a1 The patterns they learn are translation invariant.  After learning a certain pattern in\\nthe lower-right corner of a picture, a convnet can recognize it anywhere: for\\nexample, in the upper-left corner. A densely connected network would have to\\nlearn the pattern anew if it appeared at a new location. This makes convnets\\ndata efficient when processing images (because the visual world is fundamentally\\ntranslation invariant): they need fewer training samples to learn representations\\nthat have generalization power.\\n\\uf0a1 They can learn spatial hierarchies of patterns (see figure 5.2). A first convolution layer\\nwill learn small local patterns such as  edges, a second convolution layer will\\nlearn larger patterns made of the features of the first layers, and so on. This\\nallows convnets to efficiently learn increasingly complex and abstract visual con-\\ncepts (because the visual world is fundamentally spatially hierarchical).\\nConvolutions operate over \\n3D tensors, called feature maps, with two spatial axes ( height\\nand width) as well as a depth axis (also called the channels axis). For an RGB image, the\\ndimension of the depth axis is 3, becaus e the image has three color channels: red,\\ngreen, and blue. For a black- and-white picture, like the MNIST digits, the depth is 1\\n(levels of gray). The convolution operatio n extracts patches from its input feature\\nmap and applies the same transformation to all of these patches, producing an output\\nfeature map. This output feature map is still a 3D tensor: it has a width and a height. Its\\ndepth can be arbitrary, because the output depth is a parameter of the layer, and the\\n“cat”\\nFigure 5.2 The visual world forms a spatial hierarchy of visual \\nmodules: hyperlocal edges combine into local objects such as eyes \\nor ears, which combine into high-level concepts such as “cat.”\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 136, 'page_label': '137'}, page_content='124 CHAPTER 5 Deep learning for computer vision\\ndifferent channels in that depth axis no longer stand for specific colors as in RGB\\ninput; rather, they stand for filters. Filters encode specific aspects of the input data: at a\\nhigh level, a single filter could encode th e concept “presence of a face in the input,”\\nfor instance.\\n In the MNIST example, the first convolution layer takes a feature map of size (28,\\n28, 1) and outputs a feature map of size (26, 26, 32): it computes 32 filters over its\\ninput. Each of these 32 output channels cont ains a 26 × 26 grid of values, which is a\\nresponse map of the filter over the input, indicating the response of that filter pattern at\\ndifferent locations in the input (see figure 5.3). That is what the term feature map\\nmeans: every dimension in the depth axis is a feature (or filter), and the 2D tensor\\noutput[:, :, n] is the 2D spatial map of the response of this filter over the input.\\nConvolutions are defined by two key parameters:\\n\\uf0a1 Size of the patches extracted from the inputs—These are typically 3 × 3 or 5 × 5. In the\\nexample, they were 3 × 3, which is a common choice.\\n\\uf0a1 Depth of the output feature map —The number of filters computed by the convolu-\\ntion. The example started with a depth of 32 and ended with a depth of 64.\\nIn Keras Conv2D layers, these parameters are the first arguments passed to the layer:\\nConv2D(output_depth, (window_height, window_width)).\\n A convolution works by sliding these windows of size 3 × 3 or 5 × 5 over the 3D input\\nfeature map, stopping at every po ssible location, and extracting the 3D patch of sur-\\nrounding features (shape (window_height, window_width, input_depth)). Each\\nsuch 3D patch is then transformed (via a tensor product with the same learned weight\\nmatrix, called the convolution kernel) into a 1D vector of shape (output_depth,). All of\\nthese vectors are then spatially reassembled into a 3D output map of shape (height,\\nwidth, output_depth). Every spatial location in the output feature map corresponds\\nto the same location in the input feature map (for example, the lower-right corner of\\nthe output contains information about th e lower-right corner of the input). For\\ninstance, with 3 × 3 windows, the vector output[i, j, :] comes from the 3D patch\\ninput[i-1:i+1, j-1:j+1, :]. The full process is detailed in figure 5.4.\\nResponse map,\\nquantifying the presence\\nof the filter’s pattern at \\ndifferent locationsOriginal input\\nSingle filter\\nFigure 5.3 The concept of a \\nresponse map: a 2D map of the \\npresence of a pattern at different \\nlocations in an input\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 137, 'page_label': '138'}, page_content='125Introduction to convnets\\n \\nNote that the output width and height ma y differ from the input width and height.\\nThey may differ for two reasons:\\n\\uf0a1 Border effects, which can be countered by padding the input feature map\\n\\uf0a1 The use of strides, which I’ll define in a second\\nLet’s take a deeper look at these notions.\\nUNDERSTANDING BORDER EFFECTS AND PADDING\\nConsider a 5 × 5 feature map (25 tiles total) . There are only 9 tiles around which you\\ncan center a 3 × 3 window, forming a 3 × 3 grid (see figure 5.5). Hence, the output fea-\\nture map will be 3 × 3. It shrinks a little: by exactly two tiles alongside each dimension,\\nin this case. You can see this border effect in action in the earlier example: you start\\nwith 28 × 28 inputs, which become 26 × 26 after the first convolution layer.\\nHeight\\nInput feature map\\nOutput feature map\\n3 × 3 input patches\\nTransformed patches\\nWidth\\nInput\\ndepth\\nDot product\\nwith kernel\\nOutput\\ndepth\\nOutput\\ndepth\\nFigure 5.4 How convolution works\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 138, 'page_label': '139'}, page_content='126 CHAPTER 5 Deep learning for computer vision\\n \\nIf you want to get an output feature map with the same spatial dimensions as the\\ninput, you can use padding. Padding consists of adding an appropriate number of rows\\nand columns on each side of the input feature map so as to make it possible to fit cen-\\nter convolution windows around every input tile. For a 3 × 3 window, you add one col-\\numn on the right, one column on the left,  one row at the top, and one row at the\\nbottom. For a 5 × 5 window, you add two rows (see figure 5.6).\\nIn Conv2D layers, padding is configurable via the padding argument, which takes two\\nvalues: \"valid\", which means no padding (only valid window locations will be used);\\nand \"same\", which means “pad in such a way as to have an output with the same width\\nand height as the input.” The padding argument defaults to \"valid\". \\n \\nFigure 5.5 Valid locations of 3 × 3 patches in a 5 × 5 input feature map\\netc.\\nFigure 5.6 Padding a 5 × 5 input in order to be able to extract 25 3 × 3 patches\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 139, 'page_label': '140'}, page_content='127Introduction to convnets\\nUNDERSTANDING CONVOLUTION STRIDES\\nThe other factor that can influence output size is the notion of strides. The description\\nof convolution so far has assumed that the center tiles of the convolution windows are\\nall contiguous. But the distance between two successive windows is a parameter of the\\nconvolution, called its stride, which defaults to 1. It’s possible to have strided convolu-\\ntions: convolutions with a stride higher than 1. In figure 5.7, you can see the patches\\nextracted by a 3 × 3 convolution with stride 2 over a 5 × 5 input (without padding).\\nUsing stride 2 means the width and height of the feature map are downsampled by a\\nfactor of 2 (in addition to any changes in duced by border effects). Strided convolu-\\ntions are rarely used in practice, although  they can come in handy for some types of\\nmodels; it’s good to be familiar with the concept.\\n To downsample feature maps, instea d of strides, we tend to use the max-pooling\\noperation, which you saw in action in the first convnet example. Let’s look at it in\\nmore depth. \\n5.1.2 The max-pooling operation\\nIn the convnet example, you may have noticed that the size of the feature maps is\\nhalved after every \\nMaxPooling2D layer. For instance, before the first MaxPooling2D lay-\\ners, the feature map is 26 × 26, but the max-pooling operation halves it to 13 × 13.\\nThat’s the role of max pooling: to aggre ssively downsample fe ature maps, much like\\nstrided convolutions.\\n Max pooling consists of extracting wind ows from the input feature maps and out-\\nputting the max value of each channel. It’s conceptually similar to convolution, except\\nthat instead of transforming local patches via a learned linear transformation (the con-\\nvolution kernel), they’re transformed via a hardcoded max tensor operation. A big dif-\\nference from convolution is that max poolin g is usually done with 2 × 2 windows and\\n1\\n1 2\\n3 4\\n2\\n34\\nFigure 5.7 3 × 3 convolution patches with 2 × 2 strides\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 140, 'page_label': '141'}, page_content=\"128 CHAPTER 5 Deep learning for computer vision\\nstride 2, in order to downsample the feature maps by a factor of 2. On the other hand,\\nconvolution is typically done with 3 × 3 windows and no stride (stride 1).\\n Why downsample feature maps this way?  Why not remove the max-pooling layers\\nand keep fairly large feature maps all the way up? Let’s look at this option. The convo-\\nlutional base of the model would then look like this:\\nmodel_no_max_pool = models.Sequential()\\nmodel_no_max_pool.add(layers.Conv2D(32, (3, 3), activation='relu',\\ninput_shape=(28, 28, 1)))\\nmodel_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu'))\\nmodel_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu'))\\nHere’s a summary of the model:\\n>>> model_no_max_pool.summary()\\nLayer (type) Output Shape Param #\\n================================================================\\nconv2d_4 (Conv2D) (None, 26, 26, 32) 320\\n________________________________________________________________\\nconv2d_5 (Conv2D) (None, 24, 24, 64) 18496\\n________________________________________________________________\\nconv2d_6 (Conv2D) (None, 22, 22, 64) 36928\\n================================================================\\nTotal params: 55,744\\nTrainable params: 55,744\\nNon-trainable params: 0\\nWhat’s wrong with this setup? Two things:\\n\\uf0a1 It isn’t conducive to learning a spatial hierarchy of features. The 3 × 3 windows\\nin the third layer will only contain information coming from 7 × 7 windows in\\nthe initial input. The high-level patterns learned by the convnet will still be very\\nsmall with regard to the initial input, which may not be enough to learn to clas-\\nsify digits (try recognizing a digit by only looking at it through windows that are\\n7 × 7 pixels!). We need the features from the last convolution layer to contain\\ninformation about the totality of the input.\\n\\uf0a1 The final feature map has 22 × 22 × 64 = 30,976 total coefficients per sample.\\nThis is huge. If you were to flatten it to stick a \\nDense layer of size 512 on top,\\nthat layer would have 15.8 million parameters. This is far too large for such a\\nsmall model and would result in intense overfitting.\\nIn short, the reason to use downsampling is to reduce the number of feature-map\\ncoefficients to process, as well as to induce  spatial-filter hierarchies by making succes-\\nsive convolution layers look at increasingly  large windows (in terms of the fraction of\\nthe original input they cover).\\n Note that max pooling isn’t the only way you can achieve such downsampling. As\\nyou already know, you can also use strides in the prior convolution layer. And you can\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 141, 'page_label': '142'}, page_content='129Introduction to convnets\\nuse average pooling instead of max pooling, where each local input patch is trans-\\nformed by taking the average value of each  channel over the patch, rather than the\\nmax. But max pooling tends to work better than these alternative solutions. In a nut-\\nshell, the reason is that features tend to  encode the spatial presence of some pattern\\nor concept over the different tiles of the feature map (hence, the term feature map),\\nand it’s more informative to look at the maximal presence of different features than at\\ntheir average presence. So the most reasonable subsampling strategy is to first produce\\ndense maps of features (via unstrided co nvolutions) and then look at the maximal\\nactivation of the features over small patches, rather than looking at sparser windows of\\nthe inputs (via strided convolutions) or av eraging input patches, which could cause\\nyou to miss or dilute feature-presence information.\\n At this point, you should understand the basics of convnets—feature maps, convo-\\nlution, and max pooling—and you know how to build a small convnet to solve a toy\\nproblem such as \\nMNIST digits classification. Now let’s move on to more useful, practi-\\ncal applications. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 142, 'page_label': '143'}, page_content='130 CHAPTER 5 Deep learning for computer vision\\n5.2 Training a convnet from scratch on a small dataset\\nHaving to train an image-classification mode l using very little data is a common situ-\\nation, which you’ll likely encounter in prac tice if you ever do computer vision in a\\nprofessional context. A “few” samples can mean anywhere from a few hundred to a\\nfew tens of thousands of images. As a prac tical example, we’ll focus on classifying\\nimages as dogs or cats, in a dataset contai ning 4,000 pictures of cats and dogs (2,000\\ncats, 2,000 dogs). We’ll use 2,000 pictures for training—1,000 for validation, and\\n1,000 for testing.\\n In this section, we’ll review one basic strategy to tackle this problem: training a new\\nmodel from scratch using what little data yo u have. You’ll start by naively training a\\nsmall convnet on the 2,000 training samples, without any regularization, to set a base-\\nline for what can be achieved. This will ge t you to a classification accuracy of 71%. At\\nthat point, the main issue will be overfitting. Then we’ll introduce data augmentation, a\\npowerful technique for mitigating overfitting in computer vision. By using data aug-\\nmentation, you’ll improve the network to reach an accuracy of 82%.\\n In the next section, we’ ll review two more essential techniques for applying deep\\nlearning to small datasets: feature extraction with a pretrained network  (which will get you\\nto an accuracy of 90% to 96%) and fine-tuning a pretrained network (this will get you to a\\nfinal accuracy of 97%). Together, these three strategies—training a small model from\\nscratch, doing feature extraction using a pretrained model, and fine-tuning a pre-\\ntrained model—will constitute your future  toolbox for tackling the problem of per-\\nforming image classification with small datasets.\\n5.2.1 The relevance of deep le arning for small-data problems\\nYou’ll sometimes hear th at deep learning only works when lots of data is available.\\nThis is valid in part: one fundamental characteristic of deep learning is that it can find\\ninteresting features in the training data on  its own, without any need for manual fea-\\nture engineering, and this can only be achi eved when lots of training examples are\\navailable. This is especially true for pr oblems where the input samples are very high-\\ndimensional, like images.\\n But what constitutes lots of samples is relative—relative to the size and depth of the\\nnetwork you’re trying to train, for starters. It isn’t possible to train a convnet to solve a\\ncomplex problem with just a few tens of samples, bu t a few hundred can potentially\\nsuffice if the model is small and well regularized and the task is simple. Because conv-\\nnets learn local, translation-invariant features, they’re highly data efficient on percep-\\ntual problems. Training a convnet from scratc h on a very small image dataset will still\\nyield reasonable results despite a relative lack of data, without the need for any custom\\nfeature engineering. You’ll see this in action in this section.\\n What’s more, deep-learning models are by nature highly repurposable: you can\\ntake, say, an image-classification or speech-to-text model trained on a large-scale dataset\\nand reuse it on a significantly different problem with only minor changes. Specifically,\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 143, 'page_label': '144'}, page_content='131Training a convnet from scratch on a small dataset\\nin the case of computer vision, many pretrained models (usually trained on the Image-\\nNet dataset) are now publicly available for download and can be used to bootstrap pow-\\nerful vision models out of very little data. That’s what you’ll do in the next section. Let’s\\nstart by getting your hands on the data. \\n5.2.2 Downloading the data\\nThe Dogs vs. Cats dataset that you’ll use isn’t packaged with Keras. It was made avail-\\na b l e  b y  K a g g l e  a s  p a r t  o f  a  c o m p u t e r - vision competition in late 2013, back when\\nconvnets weren’t mainstream. You can download the original dataset from www.kaggle\\n.com/c/dogs-vs-cats/data (you’ll need to create a Kaggle account if you don’t already\\nhave one—don’t worry, the process is painless).\\n The pictures are medium-resolution color \\nJPEGs. Figure 5.8 shows some examples.\\nUnsurprisingly, the dogs-versus-cats Kaggle  competition in 2013 was won by entrants\\nwho used convnets. The best entries achiev ed up to 95% accuracy. In this example,\\nyou’ll get fairly close to this accuracy (i n the next section), even though you’ll train\\nyour models on less than 10% of the data that was available to the competitors.\\n This dataset contains 25,000 images of dogs and cats (12,500 from each class) and\\nis 543 MB (compressed). After downloading and uncompressing it, you’ll create a new\\ndataset containing three subsets: a training se t with 1,000 samples of each class, a vali-\\ndation set with 500 samples of each class, and a test set with 500 samples of each class.\\nFigure 5.8 Samples from the Dogs vs. Cats dat aset. Sizes weren’t modified: the samples are \\nheterogeneous in size, appearance, and so on.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 144, 'page_label': '145'}, page_content=\"132 CHAPTER 5 Deep learning for computer vision\\n Following is the code to do this.\\nimport os, shutil\\noriginal_dataset_dir = '/Users/fchollet/Downloads/kaggle_original_data'\\nbase_dir = '/Users/fchollet/Downloads/cats_and_dogs_small'\\nos.mkdir(base_dir)\\ntrain_dir = os.path.join(base_dir, 'train')\\nos.mkdir(train_dir)\\nvalidation_dir = os.path.join(base_dir, 'validation')\\nos.mkdir(validation_dir)\\ntest_dir = os.path.join(base_dir, 'test')\\nos.mkdir(test_dir)\\ntrain_cats_dir = os.path.join(train_dir, 'cats')\\nos.mkdir(train_cats_dir)\\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\\nos.mkdir(train_dogs_dir)\\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')\\nos.mkdir(validation_cats_dir)\\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')\\nos.mkdir(validation_dogs_dir)\\ntest_cats_dir = os.path.join(test_dir, 'cats')\\nos.mkdir(test_cats_dir)\\ntest_dogs_dir = os.path.join(test_dir, 'dogs')\\nos.mkdir(test_dogs_dir)\\nfnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\\nfor fname in fnames:\\nsrc = os.path.join(original_dataset_dir, fname)\\ndst = os.path.join(train_cats_dir, fname)\\nshutil.copyfile(src, dst)\\nfnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\\nfor fname in fnames:\\nsrc = os.path.join(original_dataset_dir, fname)\\ndst = os.path.join(validation_cats_dir, fname)\\nshutil.copyfile(src, dst)\\nfnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\\nfor fname in fnames:\\nsrc = os.path.join(original_dataset_dir, fname)\\ndst = os.path.join(test_cats_dir, fname)\\nshutil.copyfile(src, dst)\\nListing 5.4 Copying images to training, validation, and test directories\\nPath to the directory where the \\noriginal dataset was uncompressed\\nDirectory where you’ll store\\nyour smaller dataset\\nDirectories for \\nthe training, \\nvalidation, and \\ntest splits\\nDirectory with \\ntraining cat pictures\\nDirectory with \\ntraining dog pictures\\nDirectory with \\nvalidation cat pictures\\nDirectory with \\nvalidation dog pictures\\nDirectory with test cat pictures\\nDirectory with test dog pictures\\nCopies the first \\n1,000 cat images \\nto train_cats_dir\\nCopies the next 500 \\ncat images to \\nvalidation_cats_dir\\nCopies the next 500 \\ncat images to \\ntest_cats_dir\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 145, 'page_label': '146'}, page_content=\"133Training a convnet from scratch on a small dataset\\nfnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\\nfor fname in fnames:\\nsrc = os.path.join(original_dataset_dir, fname)\\ndst = os.path.join(train_dogs_dir, fname)\\nshutil.copyfile(src, dst)\\nfnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\\nfor fname in fnames:\\nsrc = os.path.join(original_dataset_dir, fname)\\ndst = os.path.join(validation_dogs_dir, fname)\\nshutil.copyfile(src, dst)\\nfnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\\nfor fname in fnames:\\nsrc = os.path.join(original_dataset_dir, fname)\\ndst = os.path.join(test_dogs_dir, fname)\\nshutil.copyfile(src, dst)\\nAs a sanity check, let’s count how many pict ures are in each training split (train/vali-\\ndation/test):\\n>>> print('total training cat images:', len(os.listdir(train_cats_dir)))\\ntotal training cat images: 1000\\n>>> print('total training dog images:', len(os.listdir(train_dogs_dir)))\\ntotal training dog images: 1000\\n>>> print('total validation cat images:', len(os.listdir(validation_cats_dir)))\\ntotal validation cat images: 500\\n>>> print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\\ntotal validation dog images: 500\\n>>> print('total test cat images:', len(os.listdir(test_cats_dir)))\\ntotal test cat images: 500\\n>>> print('total test dog images:', len(os.listdir(test_dogs_dir)))\\ntotal test dog images: 500\\nSo you do indeed have 2,000 training imag es, 1,000 validation images, and 1,000 test\\nimages. Each split contains the same number of samples from each class: this is a bal-\\nanced binary-classification problem, which me ans classification accuracy will be an\\nappropriate measure of success. \\n5.2.3 Building your network\\nYou built a small convnet for MNIST in the previous example, so you should be famil-\\niar with such convnets. You’ll reuse the same  general structure: th e convnet will be a\\nstack of alternated Conv2D (with relu activation) and MaxPooling2D layers.\\n But because you’re dealing with bigger images and a more complex problem, you’ll\\nmake your network larger, accord ingly: it will have one more Conv2D + MaxPooling2D\\nstage. This serves both to augment the capa city of the network and to further reduce\\nthe size of the feature maps so they aren’t overly large when you reach the Flatten\\nlayer. Here, because you start from inputs of size 150 × 150 (a somewhat arbitrary\\nchoice), you end up with feature maps of size 7 × 7 just before the Flatten layer.\\nCopies the first \\n1,000 dog images \\nto train_dogs_dir\\nCopies the next 500 \\ndog images to \\nvalidation_dogs_dir\\nCopies the next 500 \\ndog images to \\ntest_dogs_dir\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 146, 'page_label': '147'}, page_content=\"134 CHAPTER 5 Deep learning for computer vision\\nNOTE The depth of the feature maps prog ressively increases in the network\\n(from 32 to 128), whereas the size of the feature maps decreases (from 148 ×\\n148 to 7 × 7). This is a pattern you’ll see in almost all convnets.\\nBecause you’re attacking a binary-classification problem, you’ll end the network with a\\nsingle unit (a Dense layer of size 1) and a sigmoid activation. This unit will encode the\\nprobability that the network is looking at one class or the other.\\nfrom keras import layers\\nfrom keras import models\\nmodel = models.Sequential()\\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu',\\ninput_shape=(150, 150, 3)))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Flatten())\\nmodel.add(layers.Dense(512, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nLet’s look at how the dimensions of the feature maps change with every successive\\nlayer:\\n>>> model.summary()\\nLayer (type) Output Shape Param #\\n================================================================\\nconv2d_1 (Conv2D) (None, 148, 148, 32) 896\\n________________________________________________________________\\nmaxpooling2d_1 (MaxPooling2D) (None, 74, 74, 32) 0\\n________________________________________________________________\\nconv2d_2 (Conv2D) (None, 72, 72, 64) 18496\\n________________________________________________________________\\nmaxpooling2d_2 (MaxPooling2D) (None, 36, 36, 64) 0\\n________________________________________________________________\\nconv2d_3 (Conv2D) (None, 34, 34, 128) 73856\\n________________________________________________________________\\nmaxpooling2d_3 (MaxPooling2D) (None, 17, 17, 128) 0\\n________________________________________________________________\\nconv2d_4 (Conv2D) (None, 15, 15, 128) 147584\\n________________________________________________________________\\nmaxpooling2d_4 (MaxPooling2D) (None, 7, 7, 128) 0\\n________________________________________________________________\\nflatten_1 (Flatten) (None, 6272) 0\\n________________________________________________________________\\ndense_1 (Dense) (None, 512) 3211776\\n________________________________________________________________\\nListing 5.5 Instantiating a small convnet for dogs vs. cats classification\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 147, 'page_label': '148'}, page_content=\"135Training a convnet from scratch on a small dataset\\ndense_2 (Dense) (None, 1) 513\\n================================================================\\nTotal params: 3,453,121\\nTrainable params: 3,453,121\\nNon-trainable params: 0\\nFor the compilation step, you’ll go with the RMSprop optimizer, as usual. Because you\\nended the network with a single sigmoid unit, you’ll use binary crossentropy as the\\nloss (as a reminder, check out table 4.1 for a cheatsheet on what loss function to use in\\nvarious situations).   \\nfrom keras import optimizers\\nmodel.compile(loss='binary_crossentropy',\\noptimizer=optimizers.RMSprop(lr=1e-4),\\nmetrics=['acc'])\\n5.2.4 Data preprocessing\\nAs you know by now, data should be formatted into appropriately preprocessed floating-\\npoint tensors before being fed into the network. Currently, the data sits on a drive as\\nJPEG files, so the steps for getting it into the network are roughly as follows:\\n1 Read the picture files.\\n2 Decode the JPEG content to RGB grids of pixels.\\n3 Convert these into floating-point tensors.\\n4 Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know,\\nneural networks prefer to deal with small input values).\\nIt may seem a bit daunting, bu t fortunately Keras has utilities to take care of these\\nsteps automatically. Keras has a module with image-processing helper tools, located at\\nkeras.preprocessing.image. In particular, it contains the class ImageDataGenerator,\\nwhich lets you quickly set up Python generators that can automatically turn image files\\non disk into batches of preprocessed tensors. This is what you’ll use here.\\nfrom keras.preprocessing.image import ImageDataGenerator\\ntrain_datagen = ImageDataGenerator(rescale=1./255)\\ntest_datagen = ImageDataGenerator(rescale=1./255)\\ntrain_generator = train_datagen.flow_from_directory(\\ntrain_dir,\\ntarget_size=(150, 150)\\nbatch_size=20,\\nclass_mode='binary')\\nvalidation_generator = test_datagen.flow_from_directory(\\nvalidation_dir,\\nListing 5.6 Configuring the model for training\\nListing 5.7 Using ImageDataGenerator to read images from directories\\nRescales all images by 1/255\\nTarget\\ndirectory\\nResizes all images to 150 × 150\\nBecause you use \\nbinary_crossentropy \\nloss, you need binary \\nlabels.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 148, 'page_label': '149'}, page_content=\"136 CHAPTER 5 Deep learning for computer vision\\ntarget_size=(150, 150),\\nbatch_size=20,\\nclass_mode='binary')\\nLet’s look at the output of one of these generators: it yields batches of 150 × 150 RGB\\nimages (shape (20, 150, 150, 3)) and binary labels (shape (20,)). There are 20 sam-\\nples in each batch (the batch size). Note that the generator yields these batches indef-\\ninitely: it loops endlessly over the images in the target folder. For this reason, you need\\nto break the iteration loop at some point:\\n>>> for data_batch, labels_batch in train_generator:\\n>>> print('data batch shape:', data_batch.shape)\\n>>> print('labels batch shape:', labels_batch.shape)\\n>>> break\\ndata batch shape: (20, 150, 150, 3)\\nlabels batch shape: (20,)\\nLet’s fit the model to the data using the generator. You do so using the fit_generator\\nmethod, the equivalent of fit for data generators like this one. It expects as its first\\nargument a Python generator that will yield batches of inputs and targets indefinitely,\\nlike this one does. Because the data is being generated endlessly, the Keras model\\nneeds to know how many samples to draw  from the generator before declaring an\\nepoch over. This is the role of the steps_per_epoch argument: after having drawn\\nsteps_per_epoch batches from the generator—that is, after having run for\\nUnderstanding Python generators\\nA Python generator is an object that acts as an iterator: it’s an object you can use\\nwith the for … in operator. Generators are built using the yield operator.\\nHere is an example of a generator that yields integers:\\ndef generator():\\ni=0\\nwhile True:\\ni+ =1\\nyield i\\nfor item in generator():\\nprint(item)\\nif item > 4:\\nbreak\\nIt prints this:\\n1\\n2\\n3\\n4\\n5\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 149, 'page_label': '150'}, page_content=\"137Training a convnet from scratch on a small dataset\\nsteps_per_epoch gradient descent steps—the fitt ing process will go to the next\\nepoch. In this case, batches are 20 samples,  so it will take 100 batches until you see\\nyour target of 2,000 samples.\\n When using fit_generator, you can pass a validation_data argument, much as\\nwith the fit method. It’s important to note that this argument is allowed to be a data\\ngenerator, but it could also be a tuple of Numpy arrays. If you pass a generator as\\nvalidation_data, then this generator is expected to yield batches of validation data\\nendlessly; thus you sh ould also specify the validation_steps argument, which tells\\nthe process how many batches to draw from the validation generator for evaluation.\\nhistory = model.fit_generator(\\ntrain_generator,\\nsteps_per_epoch=100,\\nepochs=30,\\nvalidation_data=validation_generator,\\nvalidation_steps=50)\\nIt’s good practice to always save your models after training.\\nmodel.save('cats_and_dogs_small_1.h5')\\nLet’s plot the loss and accuracy of the model over the training and validation data\\nduring training (see figures 5.9 and 5.10).\\nimport matplotlib.pyplot as plt\\nacc = history.history['acc']\\nval_acc = history.history['val_acc']\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\nepochs = range(1, len(acc) + 1)\\nplt.plot(epochs, acc, 'bo', label='Training acc')\\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\\nplt.title('Training and validation accuracy')\\nplt.legend()\\nplt.figure()\\nplt.plot(epochs, loss, 'bo', label='Training loss')\\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\\nplt.title('Training and validation loss')\\nplt.legend()\\nplt.show()\\nListing 5.8 Fitting the model using a batch generator\\nListing 5.9 Saving the model\\nListing 5.10 Displaying curves of loss and accuracy during training\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 150, 'page_label': '151'}, page_content='138 CHAPTER 5 Deep learning for computer vision\\nThese plots are characteristic of overfitting. The training accuracy increases linearly\\nover time, until it reaches nearly 100%, whereas the validation accuracy stalls at 70–72%.\\nThe validation loss reaches its minimum after only five epochs and then stalls, whereas\\nthe training loss keeps decreasing linearly until it reaches nearly 0.\\n Because you have relatively few training  samples (2,000), overfitting will be your\\nnumber-one concern. You alre ady know about a number of  techniques that can help\\nmitigate overfitting, such as dropout and weight decay (L2 regularization). We’re now\\ngoing to work with a new one, specific to computer vision and used almost universally\\nwhen processing images with deep-learning models: data augmentation. \\n5.2.5 Using data augmentation\\nOverfitting is caused by having too few samples to learn from, rendering you unable\\nto train a model that can generalize to new data. Given infinite data, your model\\nFigure 5.9 Training and \\nvalidation accuracy\\nFigure 5.10 Training and \\nvalidation loss\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 151, 'page_label': '152'}, page_content=\"139Training a convnet from scratch on a small dataset\\nwould be exposed to every possible aspect of the data distribution at hand: you would\\nnever overfit. Data augmentation takes the approach of generating more training data\\nfrom existing training samples, by augmenting the samples via a number of random\\ntransformations that yield believable-looking images. The goal is that at training time,\\nyour model will never see the exact same pi cture twice. This helps expose the model\\nto more aspects of the data and generalize better.\\n In Keras, this can be done by configur ing a number of random transformations to\\nbe performed on the images read by the ImageDataGenerator instance. Let’s get\\nstarted with an example.\\ndatagen = ImageDataGenerator(\\nrotation_range=40,\\nwidth_shift_range=0.2,\\nheight_shift_range=0.2,\\nshear_range=0.2,\\nzoom_range=0.2,\\nhorizontal_flip=True,\\nfill_mode='nearest')\\nThese are just a few of the options available (for more, see the Keras documentation).\\nLet’s quickly go over this code:\\n\\uf0a1 rotation_range is a value in degrees (0–180),  a range within which to ran-\\ndomly rotate pictures.\\n\\uf0a1 width_shift and height_shift are ranges (as a fraction of total width or\\nheight) within which to randomly translate pictures vertically or horizontally.\\n\\uf0a1 shear_range is for randomly applying shearing transformations.\\n\\uf0a1 zoom_range is for randomly zooming inside pictures.\\n\\uf0a1 horizontal_flip is for randomly flipping half the images horizontally—rele-\\nvant when there are no assumptions of  horizontal asymmetry (for example,\\nreal-world pictures).\\n\\uf0a1 fill_mode is the strategy used for filling in newly created pixels, which can\\nappear after a rotation or a width/height shift.\\nLet’s look at the augmented images (see figure 5.11).\\nfrom keras.preprocessing import image\\nfnames = [os.path.join(train_cats_dir, fname) for\\nfname in os.listdir(train_cats_dir)]\\nimg_path = fnames[3]\\nimg = image.load_img(img_path, target_size=(150, 150))\\nListing 5.11 Setting up a data augmentation configuration via ImageDataGenerator\\nListing 5.12 Displaying some randomly augmented training images\\nModule with image-\\npreprocessing utilities\\nChooses one image to augment\\nReads the image \\nand resizes it\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 152, 'page_label': '153'}, page_content='140 CHAPTER 5 Deep learning for computer vision\\nx = image.img_to_array(img)\\nx = x.reshape((1,) + x.shape)\\ni=0\\nfor batch in datagen.flow(x, batch_size=1):\\nplt.figure(i)\\nimgplot = plt.imshow(image.array_to_img(batch[0]))\\ni+ =1\\ni fi%4= =0 :\\nbreak\\nplt.show()\\nIf you train a new network using this data -augmentation configuration, the network\\nwill never see the same input twice. But the inputs it sees  are still heavily intercor-\\nrelated, because they come from a small nu mber of original images—you can’t pro-\\nduce new information, you can only remix existing information. As such, this may not\\nbe enough to completely get rid of overfitt ing. To further fight overfitting, you’ll also\\nadd a Dropout layer to your model, right before the densely connected classifier.\\nConverts it to a Numpy array with shape (150, 150, 3)\\nReshapes it to (1, 150, 150, 3)\\nGenerates batches of \\nrandomly transformed \\nimages. Loops indefinitely, \\nso you need to break the \\nloop at some point!\\nFigure 5.11 Generation of cat pictures via random data augmentation\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 153, 'page_label': '154'}, page_content=\"141Training a convnet from scratch on a small dataset\\n \\nmodel = models.Sequential()\\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu',\\ninput_shape=(150, 150, 3)))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Flatten())\\nmodel.add(layers.Dropout(0.5))\\nmodel.add(layers.Dense(512, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nmodel.compile(loss='binary_crossentropy',\\noptimizer=optimizers.RMSprop(lr=1e-4),\\nmetrics=['acc'])\\nLet’s train the network using data augmentation and dropout.\\ntrain_datagen = ImageDataGenerator(\\nrescale=1./255,\\nrotation_range=40,\\nwidth_shift_range=0.2,\\nheight_shift_range=0.2,\\nshear_range=0.2,\\nzoom_range=0.2,\\nhorizontal_flip=True,)\\ntest_datagen = ImageDataGenerator(rescale=1./255)\\ntrain_generator = train_datagen.flow_from_directory(\\ntrain_dir,\\ntarget_size=(150, 150),\\nbatch_size=32,\\nclass_mode='binary')\\nvalidation_generator = test_datagen.flow_from_directory(\\nvalidation_dir,\\ntarget_size=(150, 150),\\nbatch_size=32,\\nclass_mode='binary')\\nhistory = model.fit_generator(\\ntrain_generator,\\nsteps_per_epoch=100,\\nepochs=100,\\nvalidation_data=validation_generator,\\nvalidation_steps=50)\\nListing 5.13 Defining a new convnet that includes dropout\\nListing 5.14 Training the convnet using data-augmentation generators\\nNote that the \\nvalidation data \\nshouldn’t be \\naugmented!\\nTarget\\ndirectory Resizes all images to 150 × 150\\nBecause you use \\nbinary_crossentropy \\nloss, you need binary \\nlabels.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 154, 'page_label': '155'}, page_content=\"142 CHAPTER 5 Deep learning for computer vision\\nLet’s save the model—you’ll use it in section 5.4.\\nmodel.save('cats_and_dogs_small_2.h5')\\nAnd let’s plot the results again: see figures 5.12 and 5.13. Thanks to data augmenta-\\ntion and dropout, you’re no longer overfitting: the training curves are closely tracking\\nthe validation curves. You now reach an accuracy of 82%, a 15% relative improvement\\nover the non-regularized model.\\n \\nBy using regularization techniques even further, and by tuning the network’s parame-\\nters (such as the number of filters per convolution layer, or the number of layers in\\nthe network), you may be able to get an even better accuracy, likely up to 86% or 87%.\\nBut it would prove difficult to go any high er just by training your own convnet from\\nscratch, because you have so little data to  work with. As a next step to improve your\\naccuracy on this problem, you’ll have to us e a pretrained model, which is the focus of\\nthe next two sections. \\nListing 5.15 Saving the model\\nFigure 5.12 Training and validation \\naccuracy with data augmentation\\nFigure 5.13 Training and validation \\nloss with data augmentation\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 155, 'page_label': '156'}, page_content='143Using a pretrained convnet\\n5.3 Using a pretrained convnet\\nA common and highly effective approach to deep learning on small image datasets is\\nto use a pretrained network. A pretrained network is a saved network that was previously\\ntrained on a large dataset, typically on a la rge-scale image-classifi cation task. If this\\noriginal dataset is large enough and general enough, then the spatial hierarchy of fea-\\ntures learned by the pretrained network can effectively act as a generic model of the\\nvisual world, and hence its features can prove useful for many different computer-\\nvision problems, even though these new problems may involve completely different\\nclasses than those of the original task. For instance, you might train a network on\\nImageNet (where classes are mostly animal s and everyday objects) and then repur-\\npose this trained network for something as  remote as identifying furniture items in\\nimages. Such portability of learned features across differ ent problems is a key advan-\\ntage of deep learning compared to many older, shallow-learning approaches, and it\\nmakes deep learning very effective for small-data problems.\\n In this case, let’s consider a large convnet trained on the ImageNet dataset\\n(1.4 million labeled images and 1,000 different classes). ImageNet contains many ani-\\nmal classes, including different species of cats and dogs, and you can thus expect to\\nperform well on the dogs-versus-cats classification problem.\\n You’ll use the VGG16 architecture, developed by Karen Simonyan and Andrew\\nZisserman in 2014; it’s a si mple and widely used convne t architecture for ImageNet. 1\\nAlthough it’s an older model, far from the current state of the art and somewhat\\nheavier than many other recent models, I chose it because its architecture is similar to\\nwhat you’re already familiar with and is easy to understand without introducing any\\nnew concepts. This may be your first encounter with one of these cutesy model\\nnames—\\nVGG, ResNet, Inception, Ince ption-ResNet, Xception, and so on; you’ll get\\nused to them, because they will come up fr equently if you keep doing deep learning\\nfor computer vision.\\n There are two ways to us e a pretrained network: feature extraction and fine-tuning.\\nWe’ll cover both of them. Let’s start with feature extraction.\\n5.3.1 Feature extraction\\nFeature extraction consists of using the representations learned by a previous network\\nto extract interesting features from new samples. These features are then run through\\na new classifier, which is trained from scratch.\\n As you saw previously, convnets used fo r image classification  comprise two parts:\\nthey start with a series of pooling and conv olution layers, and they end with a densely\\nconnected classifier. The fi rst part is called the convolutional base of the model. In the\\ncase of convnets, feature extraction consists of taking the convolutional base of a\\n1 Karen Simonyan and Andrew Zisserman, “Very Deep Co nvolutional Networks for Large-Scale Image Recog-\\nnition,” arXiv (2014), https:/ /arxiv.org/abs/1409.1556.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 156, 'page_label': '157'}, page_content='144 CHAPTER 5 Deep learning for computer vision\\npreviously trained network, running the new data through it, and training a new clas-\\nsifier on top of the output (see figure 5.14).\\nWhy only reuse the convolutional base? Could you reuse the densely connected classi-\\nfier as well? In general, doing so should be avoided. The reason is that the representa-\\ntions learned by the convolutional base are likely to be more generic and therefore\\nmore reusable: the feature maps of a convnet are presence maps of generic concepts\\nover a picture, which is likely to be useful regardless of the computer-vision problem at\\nhand. But the representations learned by the classifier will necessarily be specific to the\\nset of classes on which the model was trained—they will only contain information about\\nthe presence probability of this or that cla ss in the entire picture. Additionally, repre-\\nsentations found in densely connected layers no longer contain any information about\\nwhere objects are located in the input image: these layers get rid of the notion of space,\\nwhereas the object location is still described by convolutional feature maps. For prob-\\nlems where object location matters, densely connected features are largely useless.\\n Note that the level of generality (and th erefore reusability) of the representations\\nextracted by specific convolution layers depends on the depth of the layer in the\\nmodel. Layers that come earlier in the mo del extract local, highly generic feature\\nmaps (such as visual edges, colors, and te xtures), whereas layers that are higher up\\nextract more-abstract concepts (such as “cat ear” or “dog eye”). So if your new dataset\\ndiffers a lot from the dataset on which the original model was trained, you may be bet-\\nter off using only the first few layers of the model to do feature extraction, rather than\\nusing the entire convolutional base.\\nPrediction\\nInput\\nTrained\\nclassifier\\nTrained\\nconvolutional\\nbase\\nPrediction\\nInput\\nTrained\\nclassifier\\nTrained\\nconvolutional\\nbase\\nPrediction\\nInput\\nNew classifier\\n(randomly initialized)\\nTrained\\nconvolutional\\nbase\\n(frozen)\\nFigure 5.14 Swapping classifiers while keeping the same convolutional base\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 157, 'page_label': '158'}, page_content=\"145Using a pretrained convnet\\n In this case, because the ImageNet class set contains multiple dog and cat classes,\\nit’s likely to be beneficial to reuse the information contained in the densely connected\\nlayers of the original model. But we’ll choo se not to, in order to cover the more gen-\\neral case where the class set of the new pr oblem doesn’t overlap the class set of the\\noriginal model. Let’s put this in practice by using the convolutional base of the VGG16\\nnetwork, trained on ImageNet, to extrac t interesting features from cat and dog\\nimages, and then train a dogs-versus-cats classifier on top of these features.\\n The VGG16 model, among others, comes prepac kaged with Keras. You can import\\nit from the keras.applications module. Here’s the list of image-classification\\nmodels (all pretrained on the ImageNet da taset) that are available as part of keras\\n.applications:\\n\\uf0a1 Xception\\n\\uf0a1 Inception V3\\n\\uf0a1 ResNet50\\n\\uf0a1 VGG16\\n\\uf0a1 VGG19\\n\\uf0a1 MobileNet\\nLet’s instantiate the VGG16 model.\\nfrom keras.applications import VGG16\\nconv_base = VGG16(weights='imagenet',\\ninclude_top=False,\\ninput_shape=(150, 150, 3))\\nYou pass three arguments to the constructor:\\n\\uf0a1 weights specifies the weight checkpoint from which to initialize the model.\\n\\uf0a1 include_top refers to including (or not) the densely connected classifier on\\ntop of the network. By default, this de nsely connected classifier corresponds to\\nthe 1,000 classes from ImageNet. Becaus e you intend to use your own densely\\nconnected classifier (w ith only two classes: cat and dog), you don’t need to\\ninclude it.\\n\\uf0a1 input_shape is the shape of the image tensors that you’ll feed to the network.\\nThis argument is purely optional: if you don’t pass it, the network will be able to\\nprocess inputs of any size.\\nHere’s the detail of the architecture of the VGG16 convolutional base. It’s similar to\\nthe simple convnets you’re already familiar with:\\n>>> conv_base.summary()\\nLayer (type) Output Shape Param #\\n================================================================\\ninput_1 (InputLayer) (None, 150, 150, 3) 0\\nListing 5.16 Instantiating the VGG16 convolutional base\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 158, 'page_label': '159'}, page_content='146 CHAPTER 5 Deep learning for computer vision\\n________________________________________________________________\\nblock1_conv1 (Convolution2D) (None, 150, 150, 64) 1792\\n________________________________________________________________\\nblock1_conv2 (Convolution2D) (None, 150, 150, 64) 36928\\n________________________________________________________________\\nblock1_pool (MaxPooling2D) (None, 75, 75, 64) 0\\n________________________________________________________________\\nblock2_conv1 (Convolution2D) (None, 75, 75, 128) 73856\\n________________________________________________________________\\nblock2_conv2 (Convolution2D) (None, 75, 75, 128) 147584\\n________________________________________________________________\\nblock2_pool (MaxPooling2D) (None, 37, 37, 128) 0\\n________________________________________________________________\\nblock3_conv1 (Convolution2D) (None, 37, 37, 256) 295168\\n________________________________________________________________\\nblock3_conv2 (Convolution2D) (None, 37, 37, 256) 590080\\n________________________________________________________________\\nblock3_conv3 (Convolution2D) (None, 37, 37, 256) 590080\\n________________________________________________________________\\nblock3_pool (MaxPooling2D) (None, 18, 18, 256) 0\\n________________________________________________________________\\nblock4_conv1 (Convolution2D) (None, 18, 18, 512) 1180160\\n________________________________________________________________\\nblock4_conv2 (Convolution2D) (None, 18, 18, 512) 2359808\\n________________________________________________________________\\nblock4_conv3 (Convolution2D) (None, 18, 18, 512) 2359808\\n________________________________________________________________\\nblock4_pool (MaxPooling2D) (None, 9, 9, 512) 0\\n________________________________________________________________\\nblock5_conv1 (Convolution2D) (None, 9, 9, 512) 2359808\\n________________________________________________________________\\nblock5_conv2 (Convolution2D) (None, 9, 9, 512) 2359808\\n________________________________________________________________\\nblock5_conv3 (Convolution2D) (None, 9, 9, 512) 2359808\\n________________________________________________________________\\nblock5_pool (MaxPooling2D) (None, 4, 4, 512) 0\\n================================================================\\nTotal params: 14,714,688\\nTrainable params: 14,714,688\\nNon-trainable params: 0\\nThe final feature map has shape (4, 4, 512). That’s the feature on top of which you’ll\\nstick a densely connected classifier.\\n At this point, there are two ways you could proceed:\\n\\uf0a1 Running the convolutional base over your  dataset, recording its output to a\\nNumpy array on disk, and then using this data as input to a standalone, densely\\nconnected classifier similar to those you saw in part 1 of this book. This solution\\nis fast and cheap to run, because it only requires running the convolutional\\nbase once for every input image, and the convolutional base is by far the most\\nexpensive part of the pipeline. But for the same reason, this technique won’t\\nallow you to use data augmentation.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 159, 'page_label': '160'}, page_content=\"147Using a pretrained convnet\\n\\uf0a1 Extending the model you have (conv_base) by adding Dense layers on top, and\\nrunning the whole thing end to end on the input data. This will allow you to use\\ndata augmentation, because every input image goes through the convolutional\\nbase every time it’s seen by the model. But for the same reason, this technique is\\nfar more expensive than the first.\\nWe’ll cover both techniques. Let’s walk throug h the code required to set up the first\\none: recording the output of conv_base on your data and using these outputs as\\ninputs to a new model.\\nFAST FEATURE EXTRACTION WITHOUT DATA AUGMENTATION\\nYou’ll start by running instances of the previously introduced ImageDataGenerator to\\nextract images as Numpy arrays as well as their labels. You’ll extract features from\\nthese images by calling the predict method of the conv_base model.\\nimport os\\nimport numpy as np\\nfrom keras.preprocessing.image import ImageDataGenerator\\nbase_dir = '/Users/fchollet/Downloads/cats_and_dogs_small'\\ntrain_dir = os.path.join(base_dir, 'train')\\nvalidation_dir = os.path.join(base_dir, 'validation')\\ntest_dir = os.path.join(base_dir, 'test')\\ndatagen = ImageDataGenerator(rescale=1./255)\\nbatch_size = 20\\ndef extract_features(directory, sample_count):\\nfeatures = np.zeros(shape=(sample_count, 4, 4, 512))\\nlabels = np.zeros(shape=(sample_count))\\ngenerator = datagen.flow_from_directory(\\ndirectory,\\ntarget_size=(150, 150),\\nbatch_size=batch_size,\\nclass_mode='binary')\\ni=0\\nfor inputs_batch, labels_batch in generator:\\nfeatures_batch = conv_base.predict(inputs_batch)\\nfeatures[i * batch_size : (i + 1) * batch_size] = features_batch\\nlabels[i * batch_size : (i + 1) * batch_size] = labels_batch\\ni+ =1\\nif i * batch_size >= sample_count:\\nbreak\\nreturn features, labels\\ntrain_features, train_labels = extract_features(train_dir, 2000)\\nvalidation_features, validation_labels = extract_features(validation_dir, 1000)\\ntest_features, test_labels = extract_features(test_dir, 1000)\\nThe extracted features are currently of shape (samples, 4, 4, 512). You’ll feed them\\nto a densely connected classifier, so first you must flatten them to (samples, 8192):\\nListing 5.17 Extracting features using the pretrained convolutional base\\nNote that because generators\\nyield data indefinitely in a loop,\\nyou must break after every\\nimage has been seen once.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 160, 'page_label': '161'}, page_content=\"148 CHAPTER 5 Deep learning for computer vision\\ntrain_features = np.reshape(train_features, (2000, 4*4* 512))\\nvalidation_features = np.reshape(validation_features, (1000, 4*4* 512))\\ntest_features = np.reshape(test_features, (1000, 4*4* 512))\\nAt this point, you can define your densely connected classifier (note the use of drop-\\nout for regularization) and train it on the data and labels that you just recorded.\\nfrom keras import models\\nfrom keras import layers\\nfrom keras import optimizers\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\\nmodel.add(layers.Dropout(0.5))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer=optimizers.RMSprop(lr=2e-5),\\nloss='binary_crossentropy',\\nmetrics=['acc'])\\nhistory = model.fit(train_features, train_labels,\\nepochs=30,\\nbatch_size=20,\\nvalidation_data=(validation_features, validation_labels))\\nTraining is very fast, because you only have to deal with two Dense layers—an epoch\\ntakes less than one second even on CPU.\\n Let’s look at the loss and accuracy curves  during training (see figures 5.15 and\\n5.16).\\nimport matplotlib.pyplot as plt\\nacc = history.history['acc']\\nval_acc = history.history['val_acc']\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\nepochs = range(1, len(acc) + 1)\\nplt.plot(epochs, acc, 'bo', label='Training acc')\\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\\nplt.title('Training and validation accuracy')\\nplt.legend()\\nplt.figure()\\nplt.plot(epochs, loss, 'bo', label='Training loss')\\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\\nplt.title('Training and validation loss')\\nplt.legend()\\nplt.show()\\nListing 5.18 Defining and training the densely connected classifier\\nListing 5.19 Plotting the results\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 161, 'page_label': '162'}, page_content='149Using a pretrained convnet\\nYou reach a validation accuracy of about 90 %—much better than you achieved in the\\nprevious section with the small model trained from scratch. But the plots also indicate\\nthat you’re overfitting almost from the start—despite using dropout with a fairly large\\nrate. That’s because this technique doesn’t use data augmentation, which is essential\\nfor preventing overfitting with small image datasets. \\nFEATURE EXTRACTION WITH DATA AUGMENTATION\\nNow, let’s review the second technique I mentioned for doing feature extraction,\\nwhich is much slower and more expensive, but which allows you to use data augmenta-\\ntion during traini ng: extending the conv_base model and running it end to end on\\nthe inputs.\\nNOTE This technique is so expensive that you should only attempt it if you\\nhave access to a GPU—it’s absolutely intractable on CPU. If you can’t run your\\ncode on GPU, then the previous technique is the way to go.\\nFigure 5.15 Training and validation \\naccuracy for simple feature extraction \\nFigure 5.16 Training and validation \\nloss for simple feature extraction \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 162, 'page_label': '163'}, page_content=\"150 CHAPTER 5 Deep learning for computer vision\\nBecause models behave just like layers, you can add a model (like conv_base) to a\\nSequential model just like you would add a layer.\\nfrom keras import models\\nfrom keras import layers\\nmodel = models.Sequential()\\nmodel.add(conv_base)\\nmodel.add(layers.Flatten())\\nmodel.add(layers.Dense(256, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nThis is what the model looks like now:\\n>>> model.summary()\\nLayer (type) Output Shape Param #\\n================================================================\\nvgg16 (Model) (None, 4, 4, 512) 14714688\\n________________________________________________________________\\nflatten_1 (Flatten) (None, 8192) 0\\n________________________________________________________________\\ndense_1 (Dense) (None, 256) 2097408\\n________________________________________________________________\\ndense_2 (Dense) (None, 1) 257\\n================================================================\\nTotal params: 16,812,353\\nTrainable params: 16,812,353\\nNon-trainable params: 0\\nAs you can see, the convolutional base of VGG16 has 14,714,688 parameters, which is\\nvery large. The classifier you’re adding on top has 2 million parameters.\\n Before you compile and train the model, it’s very important to freeze the convolu-\\ntional base. Freezing a layer or set of layers means pr eventing their weights from being\\nupdated during training. If you don’t do th is, then the representations that were pre-\\nviously learned by the convolutional base wi ll be modified during training. Because\\nthe Dense layers on top are randomly initialize d, very large weight updates would be\\npropagated through the network, effectively destroying the representations previously\\nlearned.\\n In Keras, you freeze a network by setting its trainable attribute to False:\\n>>> print('This is the number of trainable weights '\\n'before freezing the conv base:', len(model.trainable_weights))\\nThis is the number of trainable weights before freezing the conv base: 30\\n>>> conv_base.trainable = False\\n>>> print('This is the number of trainable weights '\\n'after freezing the conv base:', len(model.trainable_weights))\\nThis is the number of trainable weights after freezing the conv base: 4\\nListing 5.20 Adding a densely connected classifier on top of the convolutional base\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 163, 'page_label': '164'}, page_content=\"151Using a pretrained convnet\\nWith this setup, only the weights from the two Dense layers that you added will be\\ntrained. That’s a total of four weight tensors: two per layer (the main weight matrix\\nand the bias vector). Note that in order for these changes to take effect, you must first\\ncompile the model. If you ever modify weight trainability after compilation, you\\nshould then recompile the model, or these changes will be ignored.\\n Now you can start training your model, with the same data-augmentation configu-\\nration that you used in the previous example.\\nfrom keras.preprocessing.image import ImageDataGenerator\\nfrom keras import optimizers\\ntrain_datagen = ImageDataGenerator(\\nrescale=1./255,\\nrotation_range=40,\\nwidth_shift_range=0.2,\\nheight_shift_range=0.2,\\nshear_range=0.2,\\nzoom_range=0.2,\\nhorizontal_flip=True,\\nfill_mode='nearest')\\ntest_datagen = ImageDataGenerator(rescale=1./255)\\ntrain_generator = train_datagen.flow_from_directory(\\ntrain_dir,\\ntarget_size=(150, 150),\\nbatch_size=20,\\nclass_mode='binary')\\nvalidation_generator = test_datagen.flow_from_directory(\\nvalidation_dir,\\ntarget_size=(150, 150),\\nbatch_size=20,\\nclass_mode='binary')\\nmodel.compile(loss='binary_crossentropy',\\noptimizer=optimizers.RMSprop(lr=2e-5),\\nmetrics=['acc'])\\nhistory = model.fit_generator(\\ntrain_generator,\\nsteps_per_epoch=100,\\nepochs=30,\\nvalidation_data=validation_generator,\\nvalidation_steps=50)\\nLet’s plot the results again (see figures 5.17 and 5.18). As you can see, you reach a val-\\nidation accuracy of about 96%. This is mu ch better than you achieved with the small\\nconvnet trained from scratch.\\nListing 5.21 Training the model end to end with a frozen convolutional base\\nNote that the\\nvalidation data\\nshouldn’t be\\naugmented!\\nTarget\\ndirectory Resizes all images to 150 × 150\\nBecause you use \\nbinary_crossentropy \\nloss, you need binary \\nlabels.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 164, 'page_label': '165'}, page_content='152 CHAPTER 5 Deep learning for computer vision\\n     \\n5.3.2 Fine-tuning\\nAnother widely used technique for mo del reuse, complementary to feature\\nextraction, is fine-tuning (see figure 5.19). Fine-tuning consists of unfreezing a few of\\nthe top layers of a frozen model base used for feature extraction, and jointly training\\nboth the newly added part of the model (in this case, the fully connected classifier)\\nand these top layers. This is called fine-tuning because it slight ly adjusts the more\\nabstract representations of the model being reused, in order to make them more rele-\\nvant for the problem at hand.\\nFigure 5.17 Training and validation \\naccuracy for feature extraction with \\ndata augmentation\\nFigure 5.18 Training and validation \\nloss for feature extraction with data \\naugmentation\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 165, 'page_label': '166'}, page_content='153Using a pretrained convnet\\nDense\\nDense\\nFlatten\\nMaxPooling2D\\nConvolution2D\\nConvolution2D\\nConvolution2D\\nMaxPooling2D\\nConvolution2D\\nConvolution2D\\nConvolution2D\\nMaxPooling2D\\nConvolution2D\\nConvolution2D\\nConvolution2D\\nMaxPooling2D\\nConvolution2D\\nConvolution2D\\nMaxPooling2D\\nConvolution2D\\nConvolution2D\\nConv block 1: \\nfrozen\\nConv block 2: \\nfrozen\\nConv block 3: \\nfrozen\\nConv block 4: \\nfrozen\\nWe fine-tune\\nConv block 5.\\nWe fine-tune\\nour own fully \\nconnected \\nclassifier. Figure 5.19 Fine-tuning the last \\nconvolutional block of the VGG16 network\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 166, 'page_label': '167'}, page_content='154 CHAPTER 5 Deep learning for computer vision\\nI stated earlier that it’s necessary to freeze the convolution base of VGG16 in order to\\nbe able to train a randomly initialized classi fier on top. For the same reason, it’s only\\npossible to fine-tune the top layers of the convolutional base once the classifier on top\\nhas already been trained. If the classifier isn’t already tr ained, then the error signal\\npropagating through the network during traini ng will be too large, and the represen-\\ntations previously learned by the layers bein g fine-tuned will be destroyed. Thus the\\nsteps for fine-tuning a network are as follow:\\n1 Add your custom network on top of an already-trained base network.\\n2 Freeze the base network.\\n3 Train the part you added.\\n4 Unfreeze some layers in the base network.\\n5 Jointly train both these layers and the part you added.\\nYou already completed the first three steps when doing feature extraction. Let’s pro-\\nceed with step 4: yo u’ll unfreeze your conv_base and then freeze individual layers\\ninside it.\\n As a reminder, this is what your convolutional base looks like:\\n>>> conv_base.summary()\\nLayer (type) Output Shape Param #\\n================================================================\\ninput_1 (InputLayer) (None, 150, 150, 3) 0\\n________________________________________________________________\\nblock1_conv1 (Convolution2D) (None, 150, 150, 64) 1792\\n________________________________________________________________\\nblock1_conv2 (Convolution2D) (None, 150, 150, 64) 36928\\n________________________________________________________________\\nblock1_pool (MaxPooling2D) (None, 75, 75, 64) 0\\n________________________________________________________________\\nblock2_conv1 (Convolution2D) (None, 75, 75, 128) 73856\\n________________________________________________________________\\nblock2_conv2 (Convolution2D) (None, 75, 75, 128) 147584\\n________________________________________________________________\\nblock2_pool (MaxPooling2D) (None, 37, 37, 128) 0\\n________________________________________________________________\\nblock3_conv1 (Convolution2D) (None, 37, 37, 256) 295168\\n________________________________________________________________\\nblock3_conv2 (Convolution2D) (None, 37, 37, 256) 590080\\n________________________________________________________________\\nblock3_conv3 (Convolution2D) (None, 37, 37, 256) 590080\\n________________________________________________________________\\nblock3_pool (MaxPooling2D) (None, 18, 18, 256) 0\\n________________________________________________________________\\nblock4_conv1 (Convolution2D) (None, 18, 18, 512) 1180160\\n________________________________________________________________\\nblock4_conv2 (Convolution2D) (None, 18, 18, 512) 2359808\\n________________________________________________________________\\nblock4_conv3 (Convolution2D) (None, 18, 18, 512) 2359808\\n________________________________________________________________\\nblock4_pool (MaxPooling2D) (None, 9, 9, 512) 0\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 167, 'page_label': '168'}, page_content=\"155Using a pretrained convnet\\n________________________________________________________________\\nblock5_conv1 (Convolution2D) (None, 9, 9, 512) 2359808\\n________________________________________________________________\\nblock5_conv2 (Convolution2D) (None, 9, 9, 512) 2359808\\n________________________________________________________________\\nblock5_conv3 (Convolution2D) (None, 9, 9, 512) 2359808\\n________________________________________________________________\\nblock5_pool (MaxPooling2D) (None, 4, 4, 512) 0\\n================================================================\\nTotal params: 14714688\\nYou’ll fine-tune the last three convolutio nal layers, which means all layers up to\\nblock4_pool should be frozen, and the layers block5_conv1, block5_conv2, and\\nblock5_conv3 should be trainable.\\n Why not fine-tune more layers? Why not fine-tune the entire convolutional base?\\nYou could. But you need to consider the following:\\n\\uf0a1 Earlier layers in the convolutional base encode more-generic, reusable features,\\nwhereas layers higher up encode more-specialized features. It’s more useful to\\nfine-tune the more specialized features, because these are the ones that need to\\nbe repurposed on your new problem. There would be fast-decreasing returns in\\nfine-tuning lower layers.\\n\\uf0a1 The more parameters you’re training, the more you’re at risk of overfitting.\\nThe convolutional base has 15 million parameters, so it would be risky to\\nattempt to train it on your small dataset.\\nThus, in this situation, it’s a good strategy to fine-tune only the top two or three layers\\nin the convolutional base. Let’s set this up, starting from where you left off in the pre-\\nvious example.\\nconv_base.trainable = True\\nset_trainable = False\\nfor layer in conv_base.layers:\\nif layer.name == 'block5_conv1':\\nset_trainable = True\\nif set_trainable:\\nlayer.trainable = True\\nelse:\\nlayer.trainable = False\\nNow you can begin fine-tuning the ne twork. You’ll do this with the RMSProp opti-\\nmizer, using a very low learning rate. The re ason for using a low learning rate is that\\nyou want to limit the magnitude of the modifications you make to the representations\\nof the three layers you’re fine-tuning. Up dates that are too large may harm these rep-\\nresentations.\\n \\nListing 5.22 Freezing all layers up to a specific one\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 168, 'page_label': '169'}, page_content=\"156 CHAPTER 5 Deep learning for computer vision\\n \\nmodel.compile(loss='binary_crossentropy',\\noptimizer=optimizers.RMSprop(lr=1e-5),\\nmetrics=['acc'])\\nhistory = model.fit_generator(\\ntrain_generator,\\nsteps_per_epoch=100,\\nepochs=100,\\nvalidation_data=validation_generator,\\nvalidation_steps=50)\\nLet’s plot the results using the same plotting code as before (see figures 5.20 and 5.21).\\nThese curves look noisy. To make them more readable, you can smooth them by\\nreplacing every loss and accura cy with exponential moving averages of these quanti-\\nties. Here’s a trivial utility function to do this (see figures 5.22 and 5.23).\\nListing 5.23 Fine-tuning the model\\nFigure 5.20 Training and \\nvalidation accuracy for fine-tuning \\nFigure 5.21 Training and \\nvalidation loss for fine-tuning \\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 169, 'page_label': '170'}, page_content=\"157Using a pretrained convnet\\n \\ndef smooth_curve(points, factor=0.8):\\nsmoothed_points = []\\nfor point in points:\\nif smoothed_points:\\nprevious = smoothed_points[-1]\\nsmoothed_points.append(previous * factor + point * (1 - factor))\\nelse:\\nsmoothed_points.append(point)\\nreturn smoothed_points\\nplt.plot(epochs,\\nsmooth_curve(acc), 'bo', label='Smoothed training acc')\\nplt.plot(epochs,\\nsmooth_curve(val_acc), 'b', label='Smoothed validation acc')\\nplt.title('Training and validation accuracy')\\nplt.legend()\\nplt.figure()\\nplt.plot(epochs,\\nsmooth_curve(loss), 'bo', label='Smoothed training loss')\\nplt.plot(epochs,\\nsmooth_curve(val_loss), 'b', label='Smoothed validation loss')\\nplt.title('Training and validation loss')\\nplt.legend()\\nplt.show()\\nListing 5.24 Smoothing the plots\\nFigure 5.22 Smoothed curves for training and validation accuracy \\nfor fine-tuning\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 170, 'page_label': '171'}, page_content=\"158 CHAPTER 5 Deep learning for computer vision\\nThe validation accuracy curve look much cl eaner. You’re seeing a nice 1% absolute\\nimprovement in accuracy, from about 96% to above 97%.\\n Note that the loss curve doesn’t show any real improvement (in fact, it’s deteriorat-\\ning). You may wonder, how could accuracy st ay stable or improve if the loss isn’t\\ndecreasing? The answer is simple: what you display is an average of pointwise loss val-\\nues; but what matters for accuracy is the di stribution of the loss values, not their aver-\\nage, because accuracy is the result of a bi nary thresholding of the class probability\\npredicted by the model. The model may still be improving even if this isn’t reflected\\nin the average loss.\\n You can now finally evaluate this model on the test data:\\ntest_generator = test_datagen.flow_from_directory(\\ntest_dir,\\ntarget_size=(150, 150),\\nbatch_size=20,\\nclass_mode='binary')\\ntest_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\\nprint('test acc:', test_acc)\\nHere you get a test accuracy of 97%. In th e original Kaggle competition around this\\ndataset, this would have been one of the top results. But using modern deep-learning\\ntechniques, you managed to reach this result using only  a small fraction of the train-\\ning data available (about 10%). There is a huge difference between being able to train\\non 20,000 samples compared to 2,000 samples!\\n \\nFigure 5.23 Smoothed curves for training and validation loss for fine-tuning\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 171, 'page_label': '172'}, page_content='159Using a pretrained convnet\\n5.3.3 Wrapping up\\nHere’s what you should take away from the exercises in the past two sections:\\n\\uf0a1 Convnets are the best type of machin e-learning models for computer-vision\\ntasks. It’s possible to train one from sc ratch even on a very small dataset, with\\ndecent results.\\n\\uf0a1 On a small dataset, overfitting will be the main issue. Data augmentation is a\\npowerful way to fight overfitting when you’re working with image data.\\n\\uf0a1 It’s easy to reuse an existing convnet on a new dataset via feature extraction.\\nThis is a valuable technique for working with small image datasets.\\n\\uf0a1 As a complement to feature extraction, you can use fine-tuning, which adapts to\\na new problem some of the representations previously learned by an existing\\nmodel. This pushes performance a bit further.\\nNow you have a solid set of tools for deal ing with image-classi fication problems—in\\nparticular with small datasets. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 172, 'page_label': '173'}, page_content=\"160 CHAPTER 5 Deep learning for computer vision\\n5.4 Visualizing what convnets learn\\nIt’s often said that deep-learning models are “black boxes”: learning representations\\nthat are difficult to extract and present in  a human-readable form. Although this is\\npartially true for certain types of deep-lea rning models, it’s defi nitely not true for\\nconvnets. The representations learned by co nvnets are highly amenable to visualiza-\\ntion, in large part because they’re representations of visual concepts . Since 2013, a wide\\narray of techniques have been developed for visualizing and interpreting these repre-\\nsentations. We won’t survey all of them, bu t we’ll cover three of the most accessible\\nand useful ones:\\n\\uf0a1 Visualizing intermediate convnet outputs (intermediate activations) —Useful for\\nunderstanding how successive convnet layers transform their input, and for get-\\nting a first idea of the meaning of individual convnet filters.\\n\\uf0a1 Visualizing convnets filters —Useful for understanding precisely what visual pat-\\ntern or concept each filter in a convnet is receptive to.\\n\\uf0a1 Visualizing heatmaps of class activation in an image —Useful for understanding\\nwhich parts of an image were identified as belonging to a given class, thus allow-\\ning you to localize objects in images.\\nFor the first method—activation visualizat ion—you’ll use the small convnet that you\\ntrained from scratch on the dogs-versus-cats classification problem in section 5.2. For\\nthe next two methods, you’ll use the VGG16 model introduced in section 5.3.\\n5.4.1 Visualizing intermediate activations\\nVisualizing intermediate activations consists  of displaying the feature maps that are\\noutput by various convolution and pooling la yers in a network, given a certain input\\n(the output of a layer is often called its activation, the output of the activation func-\\ntion). This gives a view into how an inpu t is decomposed into the different filters\\nlearned by the network. You want to visualize feature maps with three dimensions:\\nwidth, height, and depth (channels). Each  channel encodes relatively independent\\nfeatures, so the proper way to visualize these feature maps is by independently plot-\\nting the contents of every channel as a \\n2D image. Let’s start by loading the model that\\nyou saved in section 5.2:\\n>>> from keras.models import load_model\\n>>> model = load_model('cats_and_dogs_small_2.h5')\\n>>> model.summary() <1> As a reminder.\\n________________________________________________________________\\nLayer (type) Output Shape Param #\\n================================================================\\nconv2d_5 (Conv2D) (None, 148, 148, 32) 896\\n________________________________________________________________\\nmaxpooling2d_5 (MaxPooling2D) (None, 74, 74, 32) 0\\n________________________________________________________________\\nconv2d_6 (Conv2D) (None, 72, 72, 64) 18496\\n________________________________________________________________\\nmaxpooling2d_6 (MaxPooling2D) (None, 36, 36, 64) 0\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 173, 'page_label': '174'}, page_content=\"161Visualizing what convnets learn\\n________________________________________________________________\\nconv2d_7 (Conv2D) (None, 34, 34, 128) 73856\\n________________________________________________________________\\nmaxpooling2d_7 (MaxPooling2D) (None, 17, 17, 128) 0\\n________________________________________________________________\\nconv2d_8 (Conv2D) (None, 15, 15, 128) 147584\\n________________________________________________________________\\nmaxpooling2d_8 (MaxPooling2D) (None, 7, 7, 128) 0\\n________________________________________________________________\\nflatten_2 (Flatten) (None, 6272) 0\\n________________________________________________________________\\ndropout_1 (Dropout) (None, 6272) 0\\n________________________________________________________________\\ndense_3 (Dense) (None, 512) 3211776\\n________________________________________________________________\\ndense_4 (Dense) (None, 1) 513\\n================================================================\\nTotal params: 3,453,121\\nTrainable params: 3,453,121\\nNon-trainable params: 0\\nNext, you’ll get an input image—a picture of a cat, not part of the images the network\\nwas trained on.\\nimg_path = '/Users/fchollet/Downloads/cats_and_dogs_small/test/cats/cat.1700.jpg'\\nfrom keras.preprocessing import image\\nimport numpy as np\\nimg = image.load_img(img_path, target_size=(150, 150))\\nimg_tensor = image.img_to_array(img)\\nimg_tensor = np.expand_dims(img_tensor, axis=0)\\nimg_tensor /= 255.\\n<1> Its shape is (1, 150, 150, 3)\\nprint(img_tensor.shape)\\nLet’s display the picture (see figure 5.24).\\nimport matplotlib.pyplot as plt\\nplt.imshow(img_tensor[0])\\nplt.show()\\nListing 5.25 Preprocessing a single image\\nListing 5.26 Displaying the test picture\\nPreprocesses the image \\ninto a 4D tensor\\nRemember that the model \\nwas trained on inputs that \\nwere preprocessed this way.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 174, 'page_label': '175'}, page_content='162 CHAPTER 5 Deep learning for computer vision\\nIn order to extract the feature maps you want  to look at, you’ll create a Keras model\\nthat takes batches of images as input, and outputs the activations of all convolution and\\npooling layers. To do this, you’ll use the Keras class Model. A model is instantiated\\nusing two arguments: an input tensor (or li st of input tensors) and an output tensor\\n(or list of output tensors). The resulting class is a Keras model, just like the Sequential\\nmodels you’re familiar with, mapping the specified inputs to the specified outputs.\\nWhat sets the Model class apart is that it allows for models with multiple outputs, unlike\\nSequential. For more information about the Model class, see section 7.1.\\nfrom keras import models\\nlayer_outputs = [layer.output for layer in model.layers[:8]]\\nactivation_model = models.Model(inputs=model.input, outputs=layer_outputs)\\nWhen fed an image input, this model returns the values of the layer activations in the\\noriginal model. This is the first time you’ve encountered a multi-output model in this\\nbook: until now, the models you’ve seen have had exactly one input and one output.\\nIn the general case, a model can have any number of inputs and outputs. This one has\\none input and eight outputs: one output per layer activation.\\n \\n \\n \\n \\nListing 5.27 Instantiating a model from an input tensor and a list of output tensors\\nFigure 5.24 The test cat picture\\nExtracts the outputs of \\nthe top eight layers\\nCreates a model that will return these\\noutputs, given the model input\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 175, 'page_label': '176'}, page_content=\"163Visualizing what convnets learn\\n \\nactivations = activation_model.predict(img_tensor)\\nFor instance, this is the activation of the first convolution layer for the cat image input:\\n>>> first_layer_activation = activations[0]\\n>>> print(first_layer_activation.shape)\\n(1, 148, 148, 32)\\nIt’s a 148 × 148 feature map with 32 channels. Let’s try plotting the fourth channel of\\nthe activation of the first layer of the original model (see figure 5.25).\\nimport matplotlib.pyplot as plt\\nplt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')\\nThis channel appears to encode a diagonal edge detector. Let’s try the seventh chan-\\nnel (see figure 5.26)—but note that your own channels may vary, because the specific\\nfilters learned by convolution layers aren’t deterministic.\\nplt.matshow(first_layer_activation[0, :, :, 7], cmap='viridis')\\nListing 5.28 Running the model in predict mode\\nListing 5.29 Visualizing the fourth channel\\nListing 5.30 Visualizing the seventh channel\\nReturns a list of five \\nNumpy arrays: one array \\nper layer activation\\nFigure 5.25 Fourth channel of the activation \\nof the first layer on the test cat picture\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 176, 'page_label': '177'}, page_content=\"164 CHAPTER 5 Deep learning for computer vision\\nThis one looks like a “bright green dot” dete ctor, useful to encode cat eyes. At this\\npoint, let’s plot a complete visualization of  all the activations in the network (see fig-\\nure 5.27). You’ll extract and plot every cha nnel in each of the eight activation maps,\\nand you’ll stack the results in one big image tensor, with channels stacked side by side.\\nlayer_names = []\\nfor layer in model.layers[:8]:\\nlayer_names.append(layer.name)\\nimages_per_row = 16\\nfor layer_name, layer_activation in zip(layer_names, activations):\\nn_features = layer_activation.shape[-1]\\nsize = layer_activation.shape[1]\\nn_cols = n_features // images_per_row\\ndisplay_grid = np.zeros((size * n_cols, images_per_row * size))\\nfor col in range(n_cols):\\nfor row in range(images_per_row):\\nchannel_image = layer_activation[0,\\n:, :,\\ncol * images_per_row + row]\\nchannel_image -= channel_image.mean()\\nchannel_image /= channel_image.std()\\nchannel_image *= 64\\nchannel_image += 128\\nchannel_image = np.clip(channel_image, 0, 255).astype('uint8')\\ndisplay_grid[col * size : (col + 1) * size,\\nrow * size : (row + 1) * size] = channel_image\\nscale = 1. / size\\nplt.figure(figsize=(scale * display_grid.shape[1],\\nscale * display_grid.shape[0]))\\nplt.title(layer_name)\\nplt.grid(False)\\nplt.imshow(display_grid, aspect='auto', cmap='viridis')\\nListing 5.31 Visualizing every channel in every intermediate activation\\nFigure 5.26 Seventh channel of the activation \\nof the first layer on the test cat picture\\nNames of the layers, so you can \\nhave them as part of your plot\\nDisplays the feature maps\\nNumber of\\nfeatures in the\\nfeature map\\nThe feature map has shape \\n(1, size, size, n_features).\\nTiles the\\nactivation\\nchannels in\\nthis matrix\\nTiles each filter into \\na big horizontal grid\\nPost-processes\\nthe feature to\\nmake it visually\\npalatable\\nDisplays the grid\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 177, 'page_label': '178'}, page_content='165Visualizing what convnets learn\\nFigure 5.27 Every channel of every layer activation on the test cat picture\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 178, 'page_label': '179'}, page_content='166 CHAPTER 5 Deep learning for computer vision\\nThere are a few things to note here:\\n\\uf0a1 The first layer acts as a coll ection of various edge detectors. At that stage, the\\nactivations retain almost all of the information present in the initial picture.\\n\\uf0a1 As you go higher, the activations become  increasingly abstract and less visually\\ninterpretable. They begin to encode higher-level concepts such as “cat ear” and\\n“cat eye.” Higher presentations carry increasingly less information about the\\nvisual contents of the image, and increa singly more information related to the\\nclass of the image.\\n\\uf0a1 The sparsity of the activations increases wi th the depth of the layer: in the first\\nlayer, all filters are activated by the in put image; but in the following layers,\\nmore and more filters are blank. This means the pattern encoded by the filter\\nisn’t found in the input image.\\nWe have just evidenced an important univer sal characteristic of the representations\\nlearned by deep neural networks: the feat ures extracted by a layer become increas-\\ningly abstract with the depth of the layer. The activations of higher layers carry less\\nand less information about the specific in put being seen, and more and more infor-\\nmation about the target (in this case, the cl ass of the image: cat or dog). A deep neu-\\nral network effectively acts as an information distillation pipeline, with raw data going in\\n(in this case, RGB pictures) and being repeatedly transformed so that irrelevant infor-\\nmation is filtered out (for example, the sp ecific visual appearance of the image), and\\nuseful information is magnified and refined (for example, the class of the image).\\n This is analogous to the way humans an d animals perceive the world: after observ-\\ning a scene for a few seconds, a human ca n remember which abstract objects were\\npresent in it (bicycle, tree) but can’t re member the specific appearance of these\\nobjects. In fact, if you tried to draw a generic bicycle from memory, chances are you\\ncouldn’t get it even remotely right, even though you’ve seen th ousands of bicycles in\\nyour lifetime (see, for example, figure 5.28). Try it right now: this effect is absolutely\\nreal. You brain has learned to completely abstract its visual input—to transform it into\\nhigh-level visual concepts while filtering out irrelevant visual details—making it tre-\\nmendously difficult to remember how things around you look.   \\nFigure 5.28 Left: attempts \\nto draw a bicycle from \\nmemory. Right: what a \\nschematic bicycle should \\nlook like.Licensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 179, 'page_label': '180'}, page_content=\"167Visualizing what convnets learn\\n5.4.2 Visualizing convnet filters\\nAnother easy way to inspect the filters learne d by convnets is to display the visual pat-\\ntern that each filter is meant to respond to. This can be done with gradient ascent in\\ninput space : applying gradient descent to the value of the input image of a convnet so as\\nto maximize the response of a specif ic filter, starting from a blank input image. The\\nresulting input image will be one that the chosen filter is maximally responsive to.\\n The process is simple: you’ ll build a loss function that  maximizes the value of a\\ngiven filter in a given convolution layer,  and then you’ll use stochastic gradient\\ndescent to adjust the values of the input image so as to maximize this activation value.\\nFor instance, here’s a loss for the activation of filter 0 in the layer block3_conv1 of the\\nVGG16 network, pretrained on ImageNet.\\nfrom keras.applications import VGG16\\nfrom keras import backend as K\\nmodel = VGG16(weights='imagenet',\\ninclude_top=False)\\nlayer_name = 'block3_conv1'\\nfilter_index = 0\\nlayer_output = model.get_layer(layer_name).output\\nloss = K.mean(layer_output[:, :, :, filter_index])\\nTo implement gradient descent, you’ll need  the gradient of this loss with respect to\\nthe model’s input. To do this, you’ll use the gradients function packaged with the\\nbackend module of Keras.\\ngrads = K.gradients(loss, model.input)[0]\\nA non-obvious trick to use to help the grad ient-descent process go smoothly is to nor-\\nmalize the gradient tensor by dividing it by its L2 norm (the square root of the average\\nof the square of the values in the tensor ). This ensures that the magnitude of the\\nupdates done to the input image is always within the same range.\\ngrads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\\nNow you need a way to compute the value of the loss tensor and the gradient tensor,\\ngiven an input image. You can define a Keras backend function to do this: iterate is\\nListing 5.32 Defining the loss tensor for filter visualization\\nListing 5.33 Obtaining the gradient of the loss with regard to the input\\nListing 5.34 Gradient-normalization trick\\nThe call to gradients returns a list of \\ntensors (of size 1 in this case). Hence, \\nyou keep only the first element—\\nwhich is a tensor.\\nAdd 1e–5 before dividing \\nto avoid accidentally \\ndividing by 0.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 180, 'page_label': '181'}, page_content=\"168 CHAPTER 5 Deep learning for computer vision\\na function that takes a Numpy tensor (as a list of tensors of size 1) and returns a list of\\ntwo Numpy tensors: the loss value and the gradient value.\\niterate = K.function([model.input], [loss, grads])\\nimport numpy as np\\nloss_value, grads_value = iterate([np.zeros((1, 150, 150, 3))])\\nAt this point, you can define a Python loop to do stochastic gradient descent.\\ninput_img_data = np.random.random((1, 150, 150, 3)) * 20 + 128.\\nstep = 1.\\nfor i in range(40):\\nloss_value, grads_value = iterate([input_img_data])\\ninput_img_data += grads_value * step\\nThe resulting image tensor is a floating-point tensor of shape (1, 150, 150, 3), with\\nvalues that may not be integers within [0 , 255]. Hence, you need to postprocess this\\ntensor to turn it into a displayable image. You do so with the following straightforward\\nutility function.\\ndef deprocess_image(x):\\nx -= x.mean()\\nx /= (x.std() + 1e-5)\\nx* =0 . 1\\nx+ =0 . 5\\nx = np.clip(x, 0, 1)\\nx* =2 5 5\\nx = np.clip(x, 0, 255).astype('uint8')\\nreturn x\\nNow you have all the pieces. Let’s put them together into a Python function that takes\\nas input a layer name and a filter index, and returns a valid image tensor representing\\nthe pattern that maximizes the activation of the specified filter.\\nListing 5.35 Fetching Numpy output values given Numpy input values\\nListing 5.36 Loss maximization via stochastic gradient descent\\nListing 5.37 Utility function to convert a tensor into a valid image\\nStarts from a gray image \\nwith some noise\\nRuns gradient \\nascent for 40 \\nsteps\\nComputes the loss value \\nand gradient value\\nAdjusts the input image in the \\ndirection that maximizes the loss\\nMagnitude of each gradient update\\nNormalizes the tensor: \\ncenters on 0, ensures \\nthat std is 0.1\\nClips to [0, 1]\\nConverts to an RGB array\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 181, 'page_label': '182'}, page_content=\"169Visualizing what convnets learn\\n \\ndef generate_pattern(layer_name, filter_index, size=150):\\nlayer_output = model.get_layer(layer_name).output\\nloss = K.mean(layer_output[:, :, :, filter_index])\\ngrads = K.gradients(loss, model.input)[0]\\ngrads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\\niterate = K.function([model.input], [loss, grads])\\ninput_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\\nstep = 1.\\nfor i in range(40):\\nloss_value, grads_value = iterate([input_img_data])\\ninput_img_data += grads_value * step\\nimg = input_img_data[0]\\nreturn deprocess_image(img)\\nLet’s try it (see figure 5.29):\\n>>> plt.imshow(generate_pattern('block3_conv1', 0))\\nIt seems that filter 0 in layer block3_conv1 is responsive to a polka-dot pattern. Now\\nthe fun part: you can start visualizing every f ilter in every layer. For simplicity, you’ll\\nonly look at the first 64 filters in each laye r, and you’ll only look at the first layer of\\neach convolution block ( block1_conv1, block2_conv1, block3_conv1, block4_\\nconv1, block5_conv1). You’ll arrange the outputs on an 8 × 8 grid of 64 × 64 filter pat-\\nterns, with some black margins between each filter pattern (see figures 5.30–5.33).\\nListing 5.38 Function to generate filter visualizations\\nRuns\\ngradient\\nascent for\\n40 steps\\nBuilds a loss function that maximizes \\nthe activation of the nth filter of the \\nlayer under consideration\\nComputes the \\ngradient of the \\ninput picture with \\nregard to this loss\\nNormalization \\ntrick: normalizes \\nthe gradient\\nReturns the loss \\nand grads given \\nthe input picture\\nStarts from a\\ngray image with\\nsome noise\\nFigure 5.29 Pattern that the zeroth \\nchannel in layer block3_conv1 \\nresponds to maximally\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 182, 'page_label': '183'}, page_content=\"170 CHAPTER 5 Deep learning for computer vision\\n \\nlayer_name = 'block1_conv1'\\nsize = 64\\nmargin = 5\\nresults = np.zeros((8 * size+7* margin, 8 * size+7* margin, 3))\\nfor i in range(8):\\nfor j in range(8):\\nfilter_img = generate_pattern(layer_name, i + (j * 8), size=size)\\nhorizontal_start =i*s i z e+i* margin\\nhorizontal_end = horizontal_start + size\\nvertical_start =j*s i z e+j* margin\\nvertical_end = vertical_start + size\\nresults[horizontal_start: horizontal_end,\\nvertical_start: vertical_end, :] = filter_img\\nplt.figure(figsize=(20, 20))\\nplt.imshow(results)\\nListing 5.39 Generating a grid of all filter response patterns in a layer\\nEmpty (black) image\\nto store results\\nIterates over the rows of the results grid\\nIterates over the columns of the results grid\\nGenerates the\\npattern for\\nfilter i + (j * 8)\\nin layer_name\\nPuts the result \\nin the square \\n(i, j) of the \\nresults grid\\nDisplays the results grid\\nFigure 5.30 Filter patterns for layer block1_conv1\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 183, 'page_label': '184'}, page_content='171Visualizing what convnets learn\\nFigure 5.31 Filter patterns for layer block2_conv1\\nFigure 5.32 Filter patterns for layer block3_conv1\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 184, 'page_label': '185'}, page_content='172 CHAPTER 5 Deep learning for computer vision\\nThese filter visualizations tell you a lot about how convnet layers see the world: each\\nlayer in a convnet learns a collection of filt ers such that their inputs can be expressed\\nas a combination of the filters. This is similar to how the Fourier transform decom-\\nposes signals onto a bank of cosine function s. The filters in these convnet filter banks\\nget increasingly complex and refined as you go higher in the model:\\n\\uf0a1 The filters from the first layer in the model ( block1_conv1) encode simple\\ndirectional edges and colors (or colored edges, in some cases).\\n\\uf0a1 The filters from block2_conv1 encode simple textures made from combina-\\ntions of edges and colors.\\n\\uf0a1 The filters in higher layers begin to resemble textures found in natural images:\\nfeathers, eyes, leaves, and so on. \\n5.4.3 Visualizing heatmaps of class activation\\nI’ll introduce one more visualization techni que: one that is useful for understanding\\nwhich parts of a given image led a convnet to  its final classification decision. This is\\nhelpful for debugging th e decision process of a convnet, particularly in the case of a\\nclassification mistake. It also allows you to locate specific objects in an image.\\n This general category of techniques is called class activation map (CAM) visualization,\\nand it consists of producing heatmaps of class activation over input images. A class acti-\\nvation heatmap is a 2D grid of scores associated with a specific output class, computed\\nfor every location in any input image, indi cating how important each location is with\\nFigure 5.33 Filter patterns for layer block4_conv1\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 185, 'page_label': '186'}, page_content=\"173Visualizing what convnets learn\\nrespect to the class under consideration. For instance, given an image fed into a dogs-\\nversus-cats convnet, CAM visualization allows you to ge nerate a heatmap for the class\\n“cat,” indicating how cat-like different parts of the image are, and also a heatmap for the\\nclass “dog,” indicating how dog-like parts of the image are.\\n The specific implementation you’ll  use is the one described in “Grad- CAM: Visual\\nExplanations from Deep Networks via Gradient-based Localization.” 2 It’s very simple:\\nit consists of taking the output feature map of a convolution layer, given an input\\nimage, and weighing every channel in that feature map by the gradient of the class\\nwith respect to the channel. Intuitively, one way to understand this trick is that you’re\\nweighting a spatial map of “how intensely the input image activates different chan-\\nnels” by “how important each channel is with regard to the class,” resulting in a spatial\\nmap of “how intensely the input image activates the class.”\\n We’ll demonstrate this technique using the pretrained VGG16 network again.\\nfrom keras.applications.vgg16 import VGG16\\nmodel = VGG16(weights='imagenet')\\nConsider the image of two African elephant s shown in figure 5.34 (under a Creative\\nCommons license), possibly a mother and her calf, strolling on the savanna. Let’s con-\\nvert this image into something the VGG 16 model can read: the model was trained on\\nimages of size 224 × 244, preprocessed acco rding to a few rules that are packaged in\\nthe utility function keras.applications.vgg16.preprocess_input. So you need to\\nload the image, resize it to 224 × 224, convert it to a Numpy float32 tensor, and apply\\nthese preprocessing rules.\\n2 Ramprasaath R. Selvaraju et al., arXiv (2017), https:/ /arxiv.org/abs/ 1610.02391.\\nListing 5.40 Loading the VGG16 network with pretrained weights\\nNote that you include the densely \\nconnected classifier on top; in all \\nprevious cases, you discarded it.\\nFigure 5.34 Test picture of African elephants\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 186, 'page_label': '187'}, page_content=\"174 CHAPTER 5 Deep learning for computer vision\\nfrom keras.preprocessing import image\\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\\nimport numpy as np\\nimg_path = '/Users/fchollet/Downloads/creative_commons_elephant.jpg'\\nimg = image.load_img(img_path, target_size=(224, 224))\\nx = image.img_to_array(img)\\nx = np.expand_dims(x, axis=0)\\nx = preprocess_input(x)\\nYou can now run the pretrained network on the image and decode its prediction vec-\\ntor back to a human-readable format:\\n>>> preds = model.predict(x)\\n>>> print('Predicted:', decode_predictions(preds, top=3)[0])\\nPredicted:', [(u'n02504458', u'African_elephant', 0.92546833),\\n(u'n01871265', u'tusker', 0.070257246),\\n(u'n02504013', u'Indian_elephant', 0.0042589349)]\\nThe top three classes predicted for this image are as follows:\\n\\uf0a1 African elephant (with 92.5% probability)\\n\\uf0a1 Tusker (with 7% probability)\\n\\uf0a1 Indian elephant (with 0.4% probability)\\nThe network has recognized the image as containing an undetermined quantity of\\nAfrican elephants. The entry in the predicti on vector that was maximally activated is\\nthe one corresponding to the “African elephant” class, at index 386:\\n>>> np.argmax(preds[0])\\n386\\nTo visualize which parts of the image are th e most African elephant–like, let’s set up\\nthe Grad-CAM process.\\nafrican_e66lephant_output = model.output[:, 386]\\nlast_conv_layer = model.get_layer('block5_conv3')\\nListing 5.41 Preprocessing an input image for VGG16\\nListing 5.42 Setting up the Grad-CAM algorithm\\nPython Imaging Library (PIL) image \\nof size 224 × 224\\nLocal path to the target image\\nfloat32 Numpy array of shape \\n(224, 224, 3)\\nAdds a dimension to transform the array \\ninto a batch of size (1, 224, 224, 3)\\nPreprocesses the batch (this does \\nchannel-wise color normalization)\\n“African elephant” entry in the \\nprediction vector Output feature map of \\nthe block5_conv3 layer, \\nthe last convolutional \\nlayer in VGG16\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 187, 'page_label': '188'}, page_content='175Visualizing what convnets learn\\ngrads = K.gradients(african_elephant_output, last_conv_layer.output)[0]\\npooled_grads = K.mean(grads, axis=(0, 1, 2))\\niterate = K.function([model.input],\\n[pooled_grads, last_conv_layer.output[0]])\\npooled_grads_value, conv_layer_output_value = iterate([x])\\nfor i in range(512):\\nconv_layer_output_value[:, :, i] *= pooled_grads_value[i]\\nheatmap = np.mean(conv_layer_output_value, axis=-1)\\nFor visualization purposes, yo u’ll also normalize the he atmap between 0 and 1. The\\nresult is shown in figure 5.35.\\nheatmap = np.maximum(heatmap, 0)\\nheatmap /= np.max(heatmap)\\nplt.matshow(heatmap)\\nListing 5.43 Heatmap post-processing\\nGradient of the “African \\nelephant” class with regard to \\nthe output feature map of \\nblock5_conv3\\nVector of shape (512,), where each entry\\nis the mean intensity of the gradient\\nover a specific feature-map channel\\nValues of these two quantities, as \\nNumpy arrays, given the sample image \\nof two elephants\\nLets you access the values of the quantities \\nyou just defined: pooled_grads and the \\noutput feature map of block5_conv3, given \\na sample image\\nThe channel-wise mean of\\nthe resulting feature map\\nis the heatmap of the\\nclass activation.\\nMultiplies each\\nchannel in the\\nfeature-map array\\nby “how\\nimportant this\\nchannel is” with\\nregard to the\\n“elephant” class\\n0\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n2 4 6 8 10 12\\nFigure 5.35 African elephant class  \\nactivation heatmap over the test picture\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 188, 'page_label': '189'}, page_content=\"176 CHAPTER 5 Deep learning for computer vision\\nFinally, you’ll use OpenCV to generate an image that superimposes the original image\\non the heatmap you just obtained (see figure 5.36).\\nimport cv2\\nimg = cv2.imread(img_path)\\nheatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\\nheatmap = np.uint8(255 * heatmap)\\nheatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\\nsuperimposed_img = heatmap * 0.4 + img\\ncv2.imwrite('/Users/fchollet/Downloads/elephant_cam.jpg', superimposed_img)\\nThis visualization technique answers two important questions:\\n\\uf0a1 Why did the network think this image contained an African elephant?\\n\\uf0a1 Where is the African elephant located in the picture?\\nIn particular, it’s interesting to note that the ears of the elephant calf are strongly acti-\\nvated: this is probably how the network can tell the difference between African and\\nIndian elephants. \\nListing 5.44 Superimposing the heatmap with the original picture\\nUses cv2 to load the \\noriginal image\\nResizes the heatmap to\\nbe the same size as the\\noriginal image\\nApplies the heatmap to the \\noriginal image\\nConverts the \\nheatmap to RGB\\n0.4 here is a heatmap \\nintensity factor.\\nSaves the image to disk\\nFigure 5.36 Superimposing the class activation heatmap on the original picture\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 189, 'page_label': '190'}, page_content='177Visualizing what convnets learn\\nChapter summary\\n\\uf0a1 Convnets are the best tool for attacking visual-classification problems.\\n\\uf0a1 Convnets work by learning a hierar chy of modular patterns and concepts\\nto represent the visual world.\\n\\uf0a1 The representations they learn are easy to inspect—convnets are the\\nopposite of black boxes!\\n\\uf0a1 You’re now capable of training your own convnet from scratch to solve an\\nimage-classification problem.\\n\\uf0a1 You understand how to use visual data augmentation to fight overfitting.\\n\\uf0a1 You know how to use a pretrained co nvnet to do feature extraction and\\nfine-tuning.\\n\\uf0a1 You can generate visualizations of th e filters learned by your convnets, as\\nwell as heatmaps of class activity.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 190, 'page_label': '191'}, page_content='Deep learning for\\ntext and sequences\\nThis chapter explores deep-learning mode ls that can process text (understood as\\nsequences of word or sequences of charac ters), timeseries, and sequence data in\\ngeneral. The two fundamental deep-learning algorithms for sequence processing\\nare recurrent neural networks  and 1D convnets, the one-dimensional version of the 2D\\nconvnets that we covered in the previous chapters. We’ll discuss both of these\\napproaches in this chapter.\\n Applications of these algorithms include the following:\\n\\uf0a1 Document classification and timeseries classification, such as identifying the\\ntopic of an article or the author of a book\\n\\uf0a1 Timeseries comparisons, such as esti mating how closely related two docu-\\nments or two stock tickers are\\nThis chapter covers\\n\\uf0a1 Preprocessing text data into useful \\nrepresentations\\n\\uf0a1 Working with recurrent neural networks\\n\\uf0a1 Using 1D convnets for sequence processing'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 191, 'page_label': '192'}, page_content='179\\n\\uf0a1 Sequence-to-sequence learning, such as  decoding an English sentence into\\nFrench\\n\\uf0a1 Sentiment analysis, such as classifying th e sentiment of tweets or movie reviews\\nas positive or negative\\n\\uf0a1 Timeseries forecasting, such  as predicting the future weather at a certain loca-\\ntion, given recent weather data\\nThis chapter’s examples focus on two na rrow tasks: sentiment analysis on the IMDB\\ndataset, a task we approached earlier in the book, and temperature forecasting. But\\nthe techniques demonstrated for these two tasks are relevant to all the applications\\njust listed, and many more.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 192, 'page_label': '193'}, page_content='180 CHAPTER 6 Deep learning for text and sequences\\n6.1 Working with text data\\nText is one of the most widespread forms of sequence data. It can be understood as\\neither a sequence of characters or a sequence of words, but it’s most common to work\\nat the level of words. The deep-learning sequence-processing mo dels introduced in\\nthe following sections can use text to produce a basic form of natural-language under-\\nstanding, sufficient for applications incl uding document classi fication, sentiment\\nanalysis, author identification, and even question-answering ( QA) (in a constrained\\ncontext). Of course, keep in mind throughout this chapter that none of these deep-\\nlearning models truly understand text in  a human sense; rather, these models can\\nmap the statistical structure of written lang uage, which is sufficient to solve many sim-\\nple textual tasks. Deep learning for natural-language processing is pattern recognition\\napplied to words, sentences, and paragrap hs, in much the same way that computer\\nvision is pattern recognition applied to pixels.\\n Like all other neural netw orks, deep-learning models don’t take as input raw text:\\nthey only work with numeric tensors. Vectorizing text is the process of transforming text\\ninto numeric tensors. This can be done in multiple ways:\\n\\uf0a1 Segment text into words, and transform each word into a vector.\\n\\uf0a1 Segment text into characters, and transform each character into a vector.\\n\\uf0a1 Extract n-grams of words or characters, and transform each n-gram into a vector.\\nN-grams are overlapping groups of multiple consecutive words or characters.\\nCollectively, the different units into which you can break down text (words, charac-\\nters, or n-grams) are called tokens, and breaking text into such tokens is called tokeniza-\\ntion. All text-vectorization processes consist of applying some tokenization scheme and\\nthen associating numeric vectors with the generated tokens. These vectors, packed\\ninto sequence tensors, are fed into deep neural networks. There are multiple ways to\\nassociate a vector with a to ken. In this section, I’ ll present two major ones: one-hot\\nencoding of tokens, and token embedding (typically used exclusively for words, and called\\nword embedding). The remainder of this section ex plains these techniques and shows\\nhow to use them to go from raw text to a Numpy tensor that you can send to a Keras\\nnetwork.\\nText\\n“The cat sat on the mat.”\\nTokens\\n“the”, “cat”, “sat”, “on”, “the”, “mat”, “.”\\nVector encoding of the tokens\\n0.0 0.0 0.4 0.0 0.0 1.0 0.0\\n0.5 1.0 0.5 0.2 0.5 0.5 0.0\\n1.0 0.2 1.0 1.0 1.0 0.0 0.0\\nthe cat sat on the mat .\\nFigure 6.1 From text \\nto tokens to vectors\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 193, 'page_label': '194'}, page_content='181Working with text data\\n \\n6.1.1 One-hot encoding of words and characters\\nOne-hot encoding is the most common, most basic way to turn a token into a vector.\\nYou saw it in action in the initial IMDB and Reuters examples in chapter 3 (done with\\nwords, in that case). It consists of asso ciating a unique integer index with every word\\nand then turning this integer index i into a binary vector of size N (the size of the\\nvocabulary); the vector is all zeros except for the i th entry, which is 1.\\n Of course, one-hot encoding can be done at the character level, as well. To unam-\\nbiguously drive home what one-hot encoding  is and how to implement it, listings 6.1\\nand 6.2 show two toy examples: one for words, the other for characters.\\nUnderstanding n-grams and bag-of-words\\nWord n-grams are groups of N (or fewer) consecutive words that you can extract from\\na sentence. The same concept may also be applied to characters instead of words.\\nHere’s a simple example. Consider the sentence “The cat sat on the mat.” It may be\\ndecomposed into the following set of 2-grams:\\n{\"The\", \"The cat\", \"cat\", \"cat sat\", \"sat\",\\n\"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}\\nIt may also be decomposed into the following set of 3-grams:\\n{\"The\", \"The cat\", \"cat\", \"cat sat\", \"The cat sat\",\\n\"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"the\",\\n\"sat on the\", \"the mat\", \"mat\", \"on the mat\"}\\nSuch a set is called a bag-of-2-grams or bag-of-3-grams, respectively. The term bag\\nhere refers to the fact that you’re dealing with a set of tokens rather than a list or\\nsequence: the tokens have no specific order. This family of tokenization methods is\\ncalled bag-of-words.\\nBecause bag-of-words isn’t an order-preserving tokenization method (the tokens gen-\\nerated are understood as a set, not a sequence, and the general structure of the sen-\\ntences is lost), it tends to be used in shallow language-processing models rather than\\nin deep-learning models. Extracting n-grams is a form of feature engineering, and\\ndeep learning does away with this kind of rigid, brittle approach, replacing it with hier-\\narchical feature learning. One-dimensional convnets and recurrent neural networks,\\nintroduced later in this chapter, are capable of learning representations for groups of\\nwords and characters without being explicitly told about the existence of such groups,\\nby looking at continuous word or character sequences. For this reason, we won’t\\ncover n-grams any further in this book. But do keep in mind that they’re a powerful,\\nunavoidable feature-engineering tool when using lightweight, shallow text-processing\\nmodels such as logistic regression and random forests.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 194, 'page_label': '195'}, page_content=\"182 CHAPTER 6 Deep learning for text and sequences\\n \\nimport numpy as np\\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\\ntoken_index = {}\\nfor sample in samples:\\nfor word in sample.split():\\nif word not in token_index:\\ntoken_index[word] = len(token_index) + 1\\nmax_length = 10\\nresults = np.zeros(shape=(len(samples),\\nmax_length,\\nmax(token_index.values()) + 1))\\nfor i, sample in enumerate(samples):\\nfor j, word in list(enumerate(sample.split()))[:max_length]:\\nindex = token_index.get(word)\\nresults[i, j, index] = 1.\\nimport string\\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\\ncharacters = string.printable\\ntoken_index = dict(zip(range(1, len(characters) + 1), characters))\\nmax_length = 50\\nresults = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\\nfor i, sample in enumerate(samples):\\nfor j, character in enumerate(sample):\\nindex = token_index.get(character)\\nresults[i, j, index] = 1.\\nNote that Keras has built-in utilities for doing one-hot encoding of text at the word level\\nor character level, starting from raw text data. You should use these utilities, because\\nthey take care of a number of important fe atures such as strippi ng special characters\\nfrom strings and only taking into account the N most common words in your dataset (a\\ncommon restriction, to avoid dealing with very large input vector spaces).\\nListing 6.1 Word-level one-hot encoding (toy example)\\nListing 6.2 Character-level one-hot encoding (toy example)\\nInitial data: one entry per sample (in \\nthis example, a sample is a sentence, \\nbut it could be an entire document)\\nBuilds an index of all tokens in the data\\nTokenizes the samples via the split\\nmethod. In real life, you’d also strip\\npunctuation and special characters\\nfrom the samples.\\nAssigns a unique index to each \\nunique word. Note that you don’t \\nattribute index 0 to anything.\\nThis is where you\\nstore the results.\\nVectorizes the samples. You’ll only\\nconsider the first max_length\\nwords in each sample.\\nAll printable ASCII\\ncharacters\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 195, 'page_label': '196'}, page_content=\"183Working with text data\\n \\nfrom keras.preprocessing.text import Tokenizer\\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\\ntokenizer = Tokenizer(num_words=1000)\\ntokenizer.fit_on_texts(samples)\\nsequences = tokenizer.texts_to_sequences(samples)\\none_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\\nword_index = tokenizer.word_index\\nprint('Found %s unique tokens.' % len(word_index))\\nA variant of one-hot encoding is the so-called one-hot hashing trick , which you can use\\nwhen the number of unique tokens in your vocabulary is too large to handle explicitly.\\nInstead of explicitly assigning an index to each word and keeping a reference of these\\nindices in a dictionary, you can hash words in to vectors of fixed size. This is typically\\ndone with a very lightweight hashing functi on. The main advantage of this method is\\nthat it does away with maintaining an ex plicit word index, wh ich saves memory and\\nallows online encoding of the data (you can generate token vectors right away, before\\nyou’ve seen all of the available data). The one drawback of this approach is that it’s\\nsusceptible to hash collisions : two different words may end up with the same hash, and\\nsubsequently any machine-learning model looking at these hashes won’t be able to tell\\nthe difference between these words. The li kelihood of hash collisions decreases when\\nthe dimensionality of the hashing space is  much larger than the total number of\\nunique tokens being hashed.\\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\\ndimensionality = 1000\\nmax_length = 10\\nresults = np.zeros((len(samples), max_length, dimensionality))\\nfor i, sample in enumerate(samples):\\nfor j, word in list(enumerate(sample.split()))[:max_length]:\\nindex = abs(hash(word)) % dimensionality\\nresults[i, j, index] = 1.\\nListing 6.3 Using Keras for word-level one-hot encoding\\nListing 6.4 Word-level one-hot encoding with hashing trick (toy example)\\nCreates a tokenizer, configured\\nto only take into account the\\n1,000 most common words\\nTurns strings into lists \\nof integer indices\\nHow you can recover \\nthe word index that \\nwas computedYou could also directly get the one-hot \\nbinary representations. Vectorization \\nmodes other than one-hot encoding \\nare supported by this tokenizer.\\nBuilds\\nthe\\nword\\nindex\\nStores the words as vectors of size 1,000. If you have close \\nto 1,000 words (or more), you’ll see many hash collisions, \\nwhich will decrease the accuracy of this encoding method.\\nHashes the word into a \\nrandom integer index \\nbetween 0 and 1,000\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 196, 'page_label': '197'}, page_content='184 CHAPTER 6 Deep learning for text and sequences\\n6.1.2 Using word embeddings\\nAnother popular and powerful way to associate a vector with a word is the use of dense\\nword vectors, also called word embeddings. Whereas the vectors obtained through one-hot\\nencoding are binary, sparse (mostly made of zeros), and very high-dimensional (same\\ndimensionality as the number of words in the vocabulary), word embeddings are low-\\ndimensional floating-point ve ctors (that is, dense vectors,  as opposed to sparse vec-\\ntors); see figure 6.2. Unlike the word vectors obtained via one-hot encoding, word\\nembeddings are learned from data. It’s common to see word embeddings that are\\n256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large\\nvocabularies. On the other hand, one-hot en coding words generall y leads to vectors\\nthat are 20,000-dimensional or greater (capturing a vocabulary of 20,000 tokens, in\\nthis case). So, word embeddings pack more information into far fewer dimensions.\\nThere are two ways to obtain word embeddings:\\n\\uf0a1 Learn word embeddings jointly with the main task you care about (such as doc-\\nument classification or sent iment prediction). In this setup, you start with ran-\\ndom word vectors and then learn word vectors in the same way you learn the\\nweights of a neural network.\\n\\uf0a1 Load into your model word embeddings  that were precomputed using a differ-\\nent machine-learning task than the one yo u’re trying to solve. These are called\\npretrained word embeddings.\\nLet’s look at both.\\nOne-hot word vectors:\\n - Sparse\\n - High-dimensional\\n - Hardcoded\\nWord embeddings:\\n - Dense\\n - Lower-dimensional\\n - Learned from data\\nFigure 6.2 Whereas word representations \\nobtained from one-hot encoding or hashing are \\nsparse, high-dimensional, and hardcoded, word \\nembeddings are dense, relatively low-\\ndimensional, and learned from data.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 197, 'page_label': '198'}, page_content='185Working with text data\\nLEARNING WORD EMBEDDINGS WITH THE EMBEDDING LAYER\\nThe simplest way to associate a dense vector  with a word is to choose the vector at\\nrandom. The problem with this approach is  that the resulting embedding space has\\nno structure: for instance, the words accurate and exact may end up with completely\\ndifferent embeddings, even though they’re interchangeable in mo st sentences. It’s\\ndifficult for a deep neural network to ma ke sense of such a noisy, unstructured\\nembedding space.\\n To get a bit more abstract, the geomet ric relationships between word vectors\\nshould reflect the semantic relationships between these words. Word embeddings are\\nmeant to map human language into a geomet ric space. For instan ce, in a reasonable\\nembedding space, you would expect synonyms to be embedded into similar word vec-\\ntors; and in general, you would expect the geometric distance (such as L2 distance)\\nbetween any two word vectors to relate to the semantic distance between the associ-\\nated words (words meaning different things  are embedded at points far away from\\neach other, whereas related words are closer ). In addition to distance, you may want\\nspecific directions in the embedding space to be meaningful. To make this clearer, let’s\\nlook at a concrete example.\\n In figure 6.3, four words are embedded on a 2D plane:\\ncat, dog, wolf, and tiger. With the vector representations we\\nchose here, some semantic relationships between these\\nwords can be encoded as geometric transformations. For\\ninstance, the same vector a llo ws  us  t o go  fr om  cat to tiger\\nand from dog to wolf: this vector could be interpreted as the\\n“from pet to wild animal” vector. Similarly, another vector\\nlets us go from dog to cat and from wolf to tiger, which could\\nbe interpreted as a “from canine to feline” vector.\\n In real-world word-embed ding spaces, common exam-\\nples of meaningful geometric transformations are “gender”\\nvectors and “plural” vectors. For instance, by adding a “female” vector to the vector\\n“king,” we obtain the vector “queen.” By ad ding a “plural” vector, we obtain “kings.”\\nWord-embedding spaces typically feature th ousands of such inte rpretable and poten-\\ntially useful vectors.\\n Is there some ideal word-embedding sp ace that would perf ectly map human lan-\\nguage and could be used for any natural-language-processing task? Possibly, but we\\nhave yet to compute anything of the so rt. Also, there is no such a thing as human lan-\\nguage—there are many different languages, and they aren’t isomorphic, because a lan-\\nguage is the reflection of a specific culture and a specific  context. But more\\npragmatically, what makes a good word-embedding space depends heavily on your task:\\nthe perfect word-embedding space for an English-language movie-review sentiment-\\nanalysis model may look different from th e perfect embedding space for an English-\\nlanguage legal-document-classification model, because the importance of certain\\nsemantic relationships varies from task to task.\\n1\\n0\\n10\\nWolf Tiger\\nCat\\nDog\\nX\\nFigure 6.3 A toy example \\nof a word-embedding space\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 198, 'page_label': '199'}, page_content='186 CHAPTER 6 Deep learning for text and sequences\\n It’s thus reasonable to learn a new embedding space wi th every new task. Fortu-\\nnately, backpropagation makes this easy, an d Keras makes it even easier. It’s about\\nlearning the weights of a layer: the Embedding layer.\\nfrom keras.layers import Embedding\\nembedding_layer = Embedding(1000, 64)\\nThe Embedding l a y e r  i s  b e s t  u n d e r s t o o d  a s  a  d i c t i o n a r y  t h a t  m a p s  i n t e g e r  i n d i c e s\\n(which stand for specific words) to dense vectors. It takes integers as input, it looks up\\nthese integers in an internal dictionary, and it returns the associated vectors. It’s effec-\\ntively a dictionary lookup (see figure 6.4).\\nThe Embedding layer takes as input a 2D tensor of integers, of shape (samples,\\nsequence_length), where each entry is a sequence of integers. It can embed\\nsequences of variable lengths: for instance, you could feed into the Embedding layer in\\nthe previous example batches with shapes (32, 10) (batch of 32 sequences of length\\n10) or (64, 15) (batch of 64 sequences of length  15). All sequences in a batch must\\nhave the same length, though (because you need to pack them into a single tensor),\\nso sequences that are shorter than others should be padded with zeros, and sequences\\nthat are longer should be truncated.\\n This layer returns a 3D floating-point tensor of shape (samples, sequence_\\nlength, embedding_dimensionality). Such a 3D tensor can then be processed by\\nan RNN layer or a 1D convolution layer (both will be  introduced in the following\\nsections).\\n When you instantiate an Embedding layer, its weights (its internal dictionary of\\ntoken vectors) are initially random, just as with any other layer. During training, these\\nword vectors are grad ually adjusted via backpropagat ion, structuring the space into\\nsomething the downstream model can exploi t. Once fully trained, the embedding\\nspace will show a lot of structure—a kind of structure specialized for the specific prob-\\nlem for which you’re training your model.\\n Let’s apply this idea to the IMDB movie-review sentimen t-prediction task that\\nyou’re already familiar with. First, you’ll qu ickly prepare the data. You’ll restrict the\\nmovie reviews to the top 10 ,000 most common words (as you did the first time you\\nworked with this dataset) and cut off the reviews after only 20 words. The network will\\nlearn 8-dimensional embeddings for each of the 10,000 words, turn the input integer\\nListing 6.5 Instantiating an Embedding layer\\nThe Embedding layer takes at least two \\narguments: the number of possible tokens \\n(here, 1,000: 1 + maximum word index) \\nand the dimensionality of the embeddings \\n(here, 64).\\nWord index Embedding layer Corresponding word vecto r\\nFigure 6.4 The Embedding layer\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 199, 'page_label': '200'}, page_content=\"187Working with text data\\nsequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the\\ntensor to 2D, and train a single Dense layer on top for classification.\\nfrom keras.datasets import imdb\\nfrom keras import preprocessing\\nmax_features = 10000\\nmaxlen = 20\\n(x_train, y_train), (x_test, y_test) = imdb.load_data(\\nnum_words=max_features)\\nx_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen\\nx_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\\nfrom keras.models import Sequential\\nfrom keras.layers import Flatten, Dense\\nmodel = Sequential()\\nmodel.add(Embedding(10000, 8, input_length=maxlen))\\nmodel.add(Flatten())\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\\nmodel.summary()\\nhistory = model.fit(x_train, y_train,\\nepochs=10,\\nbatch_size=32,\\nvalidation_split=0.2)\\nYou get to a validation accuracy of ~76%, which is pretty good considering that you’re\\nonly looking at the first 20 words in every review. But note that merely flattening the\\nembedded sequences and training a single Dense layer on top leads to a model that\\ntreats each word in the input sequence separately, without considering inter-word\\nrelationships and sentence structure (for ex ample, this model would likely treat both\\n“this movie is a bomb” and “this movie is the bomb” as being ne gative reviews). It’s\\nmuch better to add recurrent layers or 1D convolutional layers on top of the embed-\\nded sequences to learn features that take into account each sequence as a whole.\\nThat’s what we’ll focus on in the next few sections. \\nListing 6.6 Loading the IMDB data for use with an Embedding layer\\nListing 6.7 Using an Embedding layer and classifier on the IMDB data\\nNumber of words to \\nconsider as features\\nCuts off the text after this \\nnumber of words (among \\nthe max_features most \\ncommon words)\\nLoads the data as lists of integers\\nTurns the lists of integers into\\na 2D integer tensor of shape\\n(samples, maxlen)\\nSpecifies the maximum input length to the \\nEmbedding layer so you can later flatten the \\nembedded inputs. After the Embedding layer, \\nthe activations have shape (samples, maxlen, 8).\\nFlattens the 3D tensor of \\nembeddings into a 2D \\ntensor of shape (samples, \\nmaxlen * 8)\\nAdds the \\nclassifier on top\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 200, 'page_label': '201'}, page_content='188 CHAPTER 6 Deep learning for text and sequences\\nUSING PRETRAINED WORD EMBEDDINGS\\nSometimes, you have so litt le training data available that you can’t use your data\\nalone to learn an appropriat e task-specific embedding of your vocabulary. What do\\nyou do then?\\n Instead of learning word embeddings jo intly with the problem you want to solve,\\nyou can load embedding vectors from a precomputed embedding space that you\\nknow is highly structured and exhibits useful properties—that captures generic\\naspects of language structure. The ration ale behind using pretrained word embed-\\ndings in natural-language processing is mu ch the same as for using pretrained conv-\\nnets in image classification: you don’t ha ve enough data available to learn truly\\npowerful features on your own, but you expe ct the features that you need to be fairly\\ngeneric—that is, common visual features or semantic features. In this case, it makes\\nsense to reuse features learned on a different problem.\\n Such word embeddings are generally co mputed using word-occurrence statistics\\n(observations about what words co-occur in sentences or documents), using a variety of\\ntechniques, some involving ne ural networks, others not. The idea of a dense, low-\\ndimensional embedding space fo r words, computed in an unsupervised way, was ini-\\ntially explored by Bengio et al. in the early 2000s,\\n1 but it only started to take off in\\nresearch and industry applications after the release of one of the most famous and suc-\\ncessful word-embedding schemes: the Word2vec algorithm (https:/ /code.google.com/\\narchive/p/word2vec), developed by Tomas Mikolov at Google in 2013. Word2vec\\ndimensions capture specific semantic properties, such as gender.\\n There are various precomputed databases of word embeddings that you can down-\\nload and use in a Keras Embedding layer. Word2vec is one of them. Another popular\\none is called Global Vectors for Word Representation (GloVe, https:/ /nlp.stanford\\n.edu/projects/glove), which was developed by Stanford researchers in 2014. This\\nembedding technique is based on factorizin g a matrix of word co-occurrence statis-\\ntics. Its developers have made availabl e precomputed embeddings for millions of\\nEnglish tokens, obtained from Wikipedia data and Common Crawl data.\\n Let’s look at how you can get started using GloVe embeddings in a Keras model.\\nThe same method is valid for Word2vec embeddings or any other word-embedding\\ndatabase. You’ll also use this example to  refresh the text-tokenization techniques\\nintroduced a few paragraphs ago: you’ll start from raw text and work your way up. \\n6.1.3 Putting it all together: fr om raw text to word embeddings\\nYou’ll use a model similar to the one we just went over: embedding sentences in\\nsequences of vectors, flattening them, and training a Dense layer on top. But you’ll do\\nso using pretrained word embeddings; and instead of using the pretokenized IMDB\\ndata packaged in Keras, you’ll start from scratch by downloading the original text data.\\n1 Yoshua Bengio et al., Neural Probabilistic Language Models (Springer, 2003).\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 201, 'page_label': '202'}, page_content=\"189Working with text data\\nDOWNLOADING THE IMDB DATA AS RAW TEXT\\nFirst, head to http:/ /mng.bz/0tIo and download the raw IMDB dataset. Uncompress it.\\n Now, let’s collect the individual training reviews into a list of strings, one string per\\nreview. You’ll also collect the review labels (positive/negative) into a labels list.  \\nimport os\\nimdb_dir = '/Users/fchollet/Downloads/aclImdb'\\ntrain_dir = os.path.join(imdb_dir, 'train')\\nlabels = []\\ntexts = []\\nfor label_type in ['neg', 'pos']:\\ndir_name = os.path.join(train_dir, label_type)\\nfor fname in os.listdir(dir_name):\\nif fname[-4:] == '.txt':\\nf = open(os.path.join(dir_name, fname))\\ntexts.append(f.read())\\nf.close()\\nif label_type == 'neg':\\nlabels.append(0)\\nelse:\\nlabels.append(1)\\nTOKENIZING THE DATA\\nLet’s vectorize the text and prepare a training and validation split, using the concepts\\nintroduced earlier in this section. Because pretrained word embeddings are meant to\\nbe particularly useful on problems where li ttle training data is available (otherwise,\\ntask-specific embeddings are likely to outperform them), we’ll add the following twist:\\nrestricting the training data to the first 200 samples. So you’ll le arn to classify movie\\nreviews after looking at just 200 examples.\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nimport numpy as np\\nmaxlen = 100\\ntraining_samples = 200\\nvalidation_samples = 10000\\nmax_words = 10000\\ntokenizer = Tokenizer(num_words=max_words)\\ntokenizer.fit_on_texts(texts)\\nsequences = tokenizer.texts_to_sequences(texts)\\nListing 6.8 Processing the labels of the raw IMDB data\\nListing 6.9 Tokenizing the text of the raw IMDB data\\nCuts off reviews after 100 words\\nTrains on 200 samples\\nValidates on 10,000 samples\\nConsiders only the top \\n10,000 words in the dataset\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 202, 'page_label': '203'}, page_content=\"190 CHAPTER 6 Deep learning for text and sequences\\nword_index = tokenizer.word_index\\nprint('Found %s unique tokens.' % len(word_index))\\ndata = pad_sequences(sequences, maxlen=maxlen)\\nlabels = np.asarray(labels)\\nprint('Shape of data tensor:', data.shape)\\nprint('Shape of label tensor:', labels.shape)\\nindices = np.arange(data.shape[0])\\nnp.random.shuffle(indices)\\ndata = data[indices]\\nlabels = labels[indices]\\nx_train = data[:training_samples]\\ny_train = labels[:training_samples]\\nx_val = data[training_samples: training_samples + validation_samples]\\ny_val = labels[training_samples: training_samples + validation_samples]\\nDOWNLOADING THE GLOVE WORD EMBEDDINGS\\nGo to https:/ /nlp.stanford.edu/projects/glove, and download the precomputed\\nembeddings from 2014 English Wikipedia. It’s an 822 MB zip file called glove.6B.zip,\\ncontaining 100-dimensional embedding vectors for 400,000 words (or nonword\\ntokens). Unzip it. \\nPREPROCESSING THE EMBEDDINGS\\nLet’s parse the unzipped file (a .txt file) to build an index that maps words (as strings)\\nto their vector representation (as number vectors).\\nglove_dir = '/Users/fchollet/Downloads/glove.6B'\\nembeddings_index = {}\\nf = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\\nfor line in f:\\nvalues = line.split()\\nword = values[0]\\ncoefs = np.asarray(values[1:], dtype='float32')\\nembeddings_index[word] = coefs\\nf.close()\\nprint('Found %s word vectors.' % len(embeddings_index))\\nNext, you’ll build an embedding ma trix that you can load into an Embedding layer. It\\nmust be a matrix of shape (max_words, embedding_dim), where each entry i contains\\nthe embedding_dim-dimensional vector for the word of index i in the reference word\\nindex (built during tokenization). Note that  index 0 isn’t supposed to stand for any\\nword or token—it’s a placeholder.\\n \\nListing 6.10 Parsing the GloVe word-embeddings file\\nSplits the data into a training set and a \\nvalidation set, but first shuffles the data, \\nbecause you’re starting with data in which \\nsamples are ordered (all negative first, then \\nall positive) \\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 203, 'page_label': '204'}, page_content=\"191Working with text data\\n \\nembedding_dim = 100\\nembedding_matrix = np.zeros((max_words, embedding_dim))\\nfor word, i in word_index.items():\\nif i < max_words:\\nembedding_vector = embeddings_index.get(word)\\nif embedding_vector is not None:\\nembedding_matrix[i] = embedding_vector\\nDEFINING A MODEL\\nYou’ll use the same model architecture as before.  \\nfrom keras.models import Sequential\\nfrom keras.layers import Embedding, Flatten, Dense\\nmodel = Sequential()\\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\\nmodel.add(Flatten())\\nmodel.add(Dense(32, activation='relu'))\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.summary()\\nLOADING THE GLOVE EMBEDDINGS IN THE MODEL\\nThe Embedding layer has a single weight matrix: a 2D float matrix where each entry i is\\nthe word vector meant to be associated with index i. Simple enough. Load the GloVe\\nmatrix you prepared into the Embedding layer, the first layer in the model.\\nmodel.layers[0].set_weights([embedding_matrix])\\nmodel.layers[0].trainable = False\\nAdditionally, you’ll freeze the Embedding layer (set its trainable attribute to False),\\nfollowing the same rationale you’re already familiar with in the context of pretrained\\nconvnet features: when parts of a model are pretrained (like your Embedding layer)\\nand parts are randomly initialized (like your classifier), the pretrained parts shouldn’t\\nbe updated during training, to avoid forgetting what they already know. The large gra-\\ndient updates triggered by the randomly init ialized layers would be disruptive to the\\nalready-learned features. \\n \\n \\n \\nListing 6.11 Preparing the GloVe word-embeddings matrix\\nListing 6.12 Model definition\\nListing 6.13 Loading pretrained word embeddings into the Embedding layer\\nWords not found in the \\nembedding index will \\nbe all zeros. \\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 204, 'page_label': '205'}, page_content=\"192 CHAPTER 6 Deep learning for text and sequences\\nTRAINING AND EVALUATING THE MODEL\\nCompile and train the model.\\nmodel.compile(optimizer='rmsprop',\\nloss='binary_crossentropy',\\nmetrics=['acc'])\\nhistory = model.fit(x_train, y_train,\\nepochs=10,\\nbatch_size=32,\\nvalidation_data=(x_val, y_val))\\nmodel.save_weights('pre_trained_glove_model.h5')\\nNow, plot the model’s performance over time (see figures 6.5 and 6.6).\\nimport matplotlib.pyplot as plt\\nacc = history.history['acc']\\nval_acc = history.history['val_acc']\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\nepochs = range(1, len(acc) + 1)\\nplt.plot(epochs, acc, 'bo', label='Training acc')\\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\\nplt.title('Training and validation accuracy')\\nplt.legend()\\nplt.figure()\\nplt.plot(epochs, loss, 'bo', label='Training loss')\\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\\nplt.title('Training and validation loss')\\nplt.legend()\\nplt.show()\\nListing 6.14 Training and evaluation\\nListing 6.15 Plotting the results\\nFigure 6.5 Training and validation loss \\nwhen using pretrained word embeddings\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 205, 'page_label': '206'}, page_content=\"193Working with text data\\nThe model quickly starts overfitting, which is unsurprising given the small number of\\ntraining samples. Validation accuracy has high variance for the same reason, but it\\nseems to reach the high 50s.\\n Note that your mileage may vary: because you have so few training samples, perfor-\\nmance is heavily dependent on exactly which 200 samples you choose—and you’re\\nchoosing them at random. If this works po orly for you, try choosing a different ran-\\ndom set of 200 samples, for the sake of the exercise (in real life, you don’t get to\\nchoose your training data).\\n You can also train the same model with out loading the pretrained word embed-\\ndings and without freezing the embedding laye r. In that case, you’ll learn a task-\\nspecific embedding of the input tokens, which is generally more powerful than\\npretrained word embeddings when lots of da ta is available. But in this case, you have\\nonly 200 training samples. Let’s try it (see figures 6.7 and 6.8).\\nfrom keras.models import Sequential\\nfrom keras.layers import Embedding, Flatten, Dense\\nmodel = Sequential()\\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\\nmodel.add(Flatten())\\nmodel.add(Dense(32, activation='relu'))\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.summary()\\nmodel.compile(optimizer='rmsprop',\\nloss='binary_crossentropy',\\nmetrics=['acc'])\\nhistory = model.fit(x_train, y_train,\\nepochs=10,\\nbatch_size=32,\\nvalidation_data=(x_val, y_val))\\nListing 6.16 Training the same model without pretrained word embeddings\\nFigure 6.6 Training and \\nvalidation accuracy when using \\npretrained word embeddings\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 206, 'page_label': '207'}, page_content=\"194 CHAPTER 6 Deep learning for text and sequences\\nValidation accuracy stalls in the low 50s. So  in this case, pretrained word embeddings\\noutperform jointly learned embeddings. If you increase the number of training sam-\\nples, this will quickly stop being the case—try it as an exercise.\\n Finally, let’s evaluate the model on the test data. First, you need to tokenize the test\\ndata.\\ntest_dir = os.path.join(imdb_dir, 'test')\\nlabels = []\\ntexts = []\\nfor label_type in ['neg', 'pos']:\\ndir_name = os.path.join(test_dir, label_type)\\nfor fname in sorted(os.listdir(dir_name)):\\nif fname[-4:] == '.txt':\\nf = open(os.path.join(dir_name, fname))\\ntexts.append(f.read())\\nListing 6.17 Tokenizing the data of the test set\\nFigure 6.7 Training and \\nvalidation loss without using \\npretrained word embeddings\\nFigure 6.8 Training and validation \\naccuracy without using pretrained \\nword embeddings\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 207, 'page_label': '208'}, page_content=\"195Working with text data\\nf.close()\\nif label_type == 'neg':\\nlabels.append(0)\\nelse:\\nlabels.append(1)\\nsequences = tokenizer.texts_to_sequences(texts)\\nx_test = pad_sequences(sequences, maxlen=maxlen)\\ny_test = np.asarray(labels)\\nNext, load and evaluate the first model.\\nmodel.load_weights('pre_trained_glove_model.h5')\\nmodel.evaluate(x_test, y_test)\\nYou get an appalling test accuracy of 56%.  Working with just a handful of training\\nsamples is difficult!\\n6.1.4 Wrapping up\\nNow you’re able to do the following:\\n\\uf0a1 Turn raw text into something a neural network can process\\n\\uf0a1 Use the Embedding layer in a Keras model to learn task-specific token embed-\\ndings\\n\\uf0a1 Use pretrained word embeddings to get an extra boost on small natural-\\nlanguage-processing problems \\nListing 6.18 Evaluating the model on the test set\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 208, 'page_label': '209'}, page_content='196 CHAPTER 6 Deep learning for text and sequences\\n6.2 Understanding recurrent neural networks\\nA major characteristic of all neural networks you’ve seen so far, such as densely con-\\nnected networks and convnets, is that th ey have no memory. Each input shown to\\nthem is processed independently, with no state kept in between inputs. With such net-\\nworks, in order to pr ocess a sequence or a temporal se ries of data points, you have to\\nshow the entire sequence to the network at on ce: turn it into a single data point. For\\ninstance, this is what you did in the IMDB example: an entire movie review was trans-\\nformed into a single large vector and processed in one go. Such networks are called\\nfeedforward networks.\\n In contrast, as you’re reading the presen t sentence, you’re pr ocessing it word by\\nword—or rather, eye saccade by eye saccade—while keeping memories of what came\\nbefore; this gives you a fluid representation of the meaning conveyed by this sentence.\\nBiological intelligence processes information incrementally while maintaining an\\ninternal model of what it’s processing, built from past information and constantly\\nupdated as new information comes in.\\n A recurrent neural network  (RNN) adopts the same principle, albeit in an extremely\\nsimplified version: it processes sequences by iterating through the sequence elements\\nand maintaining a state containing information relative\\nto what it has seen so far. In effect, an RNN is a type of\\nneural network that has an internal loop (see figure 6.9).\\nThe state of the RNN is reset between processing two dif-\\nferent, independent sequences (such as two different\\nIMDB reviews), so you still co nsider one sequence a sin-\\ngle data point: a single input to the network. What\\nchanges is that this data point is no longer processed in a\\nsingle step; rather, the ne twork internally loops over\\nsequence elements.\\n To make these notions of loop and state clear, let’s implement the forward pass of a\\ntoy \\nRNN in Numpy. This RNN takes as input a sequence of vectors, which you’ll encode\\nas a 2D tensor of size (timesteps, input_features). It loops over timesteps, and at\\neach timestep, it consid ers its current state at t and the input at t (of shape (input_\\nfeatures,), and combines them to obtain the output at t. You’ll then set the state for\\nthe next step to be this previous output. For the first timestep, the previous output\\nisn’t defined; hence, there is no current stat e. So, you’ll initialize the state as an all-\\nzero vector called the initial state of the network.\\n In pseudocode, this is the RNN.\\nstate_t = 0\\nfor input_t in input_sequence:\\noutput_t = f(input_t, state_t)\\nstate_t = output_t\\nListing 6.19 Pseudocode RNN\\nThe state at t\\nIterates over sequence elements\\nThe previous output becomes the state for the next iteration.\\nRNN\\nInput\\nOutput\\nRecurrent\\nconnection\\nFigure 6.9 A recurrent \\nnetwork: a network with a loop\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 209, 'page_label': '210'}, page_content='197Understanding recurrent neural networks\\nYou can even flesh out the function f: the transformation of the input and state into an\\noutput will be parameterized by two matrices, W and U, and a bias vector. It’s similar to\\nthe transformation operated by a densely connected layer in a feedforward network.\\nstate_t = 0\\nfor input_t in input_sequence:\\noutput_t = activation(dot(W, input_t) + dot(U, state_t) + b)\\nstate_t = output_t\\nTo make these notions absolutely unambiguous, let’s write a naive Numpy implemen-\\ntation of the forward pass of the simple RNN.\\nimport numpy as np\\ntimesteps = 100\\ninput_features = 32\\noutput_features = 64\\ninputs = np.random.random((timesteps, input_features))\\nstate_t = np.zeros((output_features,))\\nW = np.random.random((output_features, input_features))\\nU = np.random.random((output_features, output_features))\\nb = np.random.random((output_features,))\\nsuccessive_outputs = []\\nfor input_t in inputs:\\noutput_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\\nsuccessive_outputs.append(output_t)\\nstate_t = output_t\\nfinal_output_sequence = np.concatenate(successive_outputs, axis=0)\\nEasy enough: in summary, an RNN is a for loop that reuses quantities computed\\nduring the previous iteration of the loop , nothing more. Of course, there are many\\ndifferent RNNs fitting this definition that you co uld build—this example is one of the\\nsimplest RNN formulations. RNNs are characterized by their step function, such as the\\nfollowing function in this case (see figure 6.10):\\noutput_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\\nListing 6.20 More detailed pseudocode for the RNN\\nListing 6.21 Numpy implementation of a simple RNN\\nNumber of timesteps in \\nthe input sequence Dimensionality of the \\ninput feature space\\nDimensionality of the \\noutput feature space\\nInput data: random \\nnoise for the sake of \\nthe example\\nInitial state: an \\nall-zero vector\\nCreates random \\nweight matrices\\ninput_t is a vector of \\nshape (input_features,).\\nCombines the input with the current \\nstate (the previous output) to obtain \\nthe current output\\nStores this output in a list\\nUpdates the state of the\\nnetwork for the next timestep\\nThe final output is a 2D tensor of\\nshape (timesteps, output_features).\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 210, 'page_label': '211'}, page_content='198 CHAPTER 6 Deep learning for text and sequences\\nNOTE In this example, the final outp ut is a 2D tensor of shape (timesteps,\\noutput_features), where each timestep is the output of the loop at time t.\\nEach timestep t in the output tensor contai ns information about timesteps 0\\nto t in the input sequence—about the entire past. For this reason, in many\\ncases, you don’t need this full sequence of outputs; you just need the last out-\\nput (output_t at the end of the loop), because it already contains informa-\\ntion about the entire sequence.\\n6.2.1 A recurrent layer in Keras\\nThe process you just naively implemented in  Numpy corresponds to an actual Keras\\nlayer—the SimpleRNN layer:\\nfrom keras.layers import SimpleRNN\\nThere is one minor difference: SimpleRNN processes batches of sequences, like all other\\nKeras layers, not a single sequence as in the Numpy example. This means it takes inputs\\nof shape (batch_size, timesteps, input_features), rather than (timesteps,\\ninput_features).\\n Like all recurrent layers in Keras, SimpleRNN can be run in two different modes: it\\ncan return either the full sequences of successive outputs for each timestep (a 3D ten-\\nsor of shape (batch_size, timesteps, output_features)) or only the last output for\\neach input sequence (a 2D tensor of shape (batch_size, output_features)). These\\ntwo modes are controlled by the return_sequences constructor argument. Let’s look\\nat an example that uses SimpleRNN and returns only the output at the last timestep:\\n>>> from keras.models import Sequential\\n>>> from keras.layers import Embedding, SimpleRNN\\n>>> model = Sequential()\\n>>> model.add(Embedding(10000, 32))\\n>>> model.add(SimpleRNN(32))\\n>>> model.summary()\\n...\\noutput t-1 output t output t+1\\ninput t-1 input t input t+1\\n...\\nState t State t+1\\noutput_t =\\n activation(\\n  W•input_t +\\n  U•state_t +\\n  bo)\\nFigure 6.10 A simple RNN, unrolled over time\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 211, 'page_label': '212'}, page_content='199Understanding recurrent neural networks\\n________________________________________________________________\\nLayer (type) Output Shape Param #\\n================================================================\\nembedding_22 (Embedding) (None, None, 32) 320000\\n________________________________________________________________\\nsimplernn_10 (SimpleRNN) (None, 32) 2080\\n================================================================\\nTotal params: 322,080\\nTrainable params: 322,080\\nNon-trainable params: 0\\nThe following example returns the full state sequence:\\n>>> model = Sequential()\\n>>> model.add(Embedding(10000, 32))\\n>>> model.add(SimpleRNN(32, return_sequences=True))\\n>>> model.summary()\\n________________________________________________________________\\nLayer (type) Output Shape Param #\\n================================================================\\nembedding_23 (Embedding) (None, None, 32) 320000\\n________________________________________________________________\\nsimplernn_11 (SimpleRNN) (None, None, 32) 2080\\n================================================================\\nTotal params: 322,080\\nTrainable params: 322,080\\nNon-trainable params: 0\\nIt’s sometimes useful to stack several recu rrent layers one after the other in order to\\nincrease the representational power of a netw ork. In such a setup, you have to get all\\nof the intermediate layers to return full sequence of outputs:\\n>>> model = Sequential()\\n>>> model.add(Embedding(10000, 32))\\n>>> model.add(SimpleRNN(32, return_sequences=True))\\n>>> model.add(SimpleRNN(32, return_sequences=True))\\n>>> model.add(SimpleRNN(32, return_sequences=True))\\n>>> model.add(SimpleRNN(32))\\n>>> model.summary()\\n________________________________________________________________\\nLayer (type) Output Shape Param #\\n================================================================\\nembedding_24 (Embedding) (None, None, 32) 320000\\n________________________________________________________________\\nsimplernn_12 (SimpleRNN) (None, None, 32) 2080\\n________________________________________________________________\\nsimplernn_13 (SimpleRNN) (None, None, 32) 2080\\n________________________________________________________________\\nsimplernn_14 (SimpleRNN) (None, None, 32) 2080\\n________________________________________________________________\\nsimplernn_15 (SimpleRNN) (None, 32) 2080\\n================================================================\\nTotal params: 328,320\\nTrainable params: 328,320\\nNon-trainable params: 0\\nLast layer only returns \\nthe last output\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 212, 'page_label': '213'}, page_content=\"200 CHAPTER 6 Deep learning for text and sequences\\nNow, let’s use such  a model on the IMDB movie-review-classifica tion problem. First,\\npreprocess the data.\\nfrom keras.datasets import imdb\\nfrom keras.preprocessing import sequence\\nmax_features = 10000\\nmaxlen = 500\\nbatch_size = 32\\nprint('Loading data...')\\n(input_train, y_train), (input_test, y_test) = imdb.load_data(\\nnum_words=max_features)\\nprint(len(input_train), 'train sequences')\\nprint(len(input_test), 'test sequences')\\nprint('Pad sequences (samples x time)')\\ninput_train = sequence.pad_sequences(input_train, maxlen=maxlen)\\ninput_test = sequence.pad_sequences(input_test, maxlen=maxlen)\\nprint('input_train shape:', input_train.shape)\\nprint('input_test shape:', input_test.shape)\\nLet’s train a simple recu rrent network using an Embedding layer and a SimpleRNN\\nlayer.\\nfrom keras.layers import Dense\\nmodel = Sequential()\\nmodel.add(Embedding(max_features, 32))\\nmodel.add(SimpleRNN(32))\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\\nhistory = model.fit(input_train, y_train,\\nepochs=10,\\nbatch_size=128,\\nvalidation_split=0.2)\\nNow, let’s display the training and validation loss and accuracy (see figures 6.11 and 6.12).\\nimport matplotlib.pyplot as plt\\nacc = history.history['acc']\\nval_acc = history.history['val_acc']\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\nepochs = range(1, len(acc) + 1)\\nplt.plot(epochs, acc, 'bo', label='Training acc')\\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\\nListing 6.22 Preparing the IMDB data\\nListing 6.23 Training the model with Embedding and SimpleRNN layers\\nListing 6.24 Plotting results\\nNumber of words to \\nconsider as features\\nCuts off texts after this many words (among \\nthe max_features most common words)\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 213, 'page_label': '214'}, page_content=\"201Understanding recurrent neural networks\\nplt.title('Training and validation accuracy')\\nplt.legend()\\nplt.figure()\\nplt.plot(epochs, loss, 'bo', label='Training loss')\\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\\nplt.title('Training and validation loss')\\nplt.legend()\\nplt.show()\\nAs a reminder, in chapter 3, the first naive approach to this dataset got you to a test\\naccuracy of 88%. Unfortunately, this sma ll recurrent network doesn’t perform well\\ncompared to this baseline (only 85% validat ion accuracy). Part of the problem is that\\nyour inputs only consider the first 500 word s, rather than full sequences—hence, the\\nRNN has access to less information than the earlier baseline model. The remainder of\\nthe problem is that SimpleRNN isn’t good at processing lo ng sequences, such as text.\\nFigure 6.11 Training and validation \\nloss on IMDB with SimpleRNN\\nFigure 6.12 Training and validation \\naccuracy on IMDB with SimpleRNN\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 214, 'page_label': '215'}, page_content='202 CHAPTER 6 Deep learning for text and sequences\\nOther types of recurrent layers perform much better. Let’s l ook at some more-\\nadvanced layers. \\n6.2.2 Understanding the LSTM and GRU layers\\nSimpleRNN isn’t the only recurrent layer available in Keras. There are two others: LSTM\\nand GRU. In practice, you’ll always use one of these, because SimpleRNN is generally too\\nsimplistic to be of real use. SimpleRNN has a major issue: although it should theoretically\\nbe able to retain at time t information about inputs s een many timesteps before, in\\npractice, such long-term dependencies are impossible to learn. This is due to the van-\\nishing gradient problem , an effect that is similar to what is observed with non-recurrent\\nnetworks (feedforward networks) that are many layers deep: as you keep adding layers\\nto a network, the network eventually becomes untrainable. The theoretical reasons for\\nthis effect were studied by Hochreiter, Sc hmidhuber, and Bengio in the early 1990s. 2\\nThe LSTM and GRU layers are designed to solve this problem.\\n Let’s consider the LSTM layer. The underlying Lo ng Short-Term Memory ( LSTM)\\nalgorithm was developed by Hoch reiter and Schmidhuber in 1997; 3 it was the culmi-\\nnation of their research on the vanishing gradient problem.\\n This layer is a variant of the SimpleRNN layer you already know about; it adds a way\\nto carry information across many timesteps.  Imagine a conveyor belt running parallel\\nto the sequence you’re processing. Information from the sequence can jump onto the\\nconveyor belt at any point, be transported to a later time step, and jump off, intact,\\nwhen you need it. This is essentially what LSTM does: it saves information for later,\\nthus preventing older signals from gradually vanishing during processing.\\n To understand this in detail, let’s start from the SimpleRNN cell (see figure 6.13).\\nBecause you’ll have a lot of weight matrices, index the W and U matrices in the cell with\\nthe letter o (Wo and Uo) for output.\\n2 See, for example, Yoshua Bengio, Patrice Simard, and Paolo Frasconi, “Learning Long-Term Dependencies\\nwith Gradient Descent Is Difficult,” IEEE Transactions on Neural Networks 5, no. 2 (1994).\\n3 Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” Neural Computation 9, no. 8 (1997).\\n...\\noutput t-1 output t output t+1\\ninput t-1 input t input t+1\\n...\\nState t State t+1\\noutput_t =\\n activation(\\n  Wo•input_t +\\n  Uo•state_t +\\n  bo)\\nFigure 6.13 The starting point of an LSTM layer: a SimpleRNN\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 215, 'page_label': '216'}, page_content='203Understanding recurrent neural networks\\nLet’s add to this picture an additional data  flow that carries information across time-\\nsteps. Call its values at different timesteps Ct, where C stands for carry. This informa-\\nt i o n  w i l l  h a v e  t h e  f o l l o w i n g  i m p a c t  o n  t h e  c e l l :  i t  w i l l  b e  c o m b i n e d  w i t h  t h e  i n p u t\\nconnection and the recurrent connection (via  a dense transformation: a dot product\\nwith a weight matrix followed  by a bias add and the applic ation of an activation func-\\ntion), and it will affect the state being sent  to the next timestep (via an activation\\nfunction an a multiplication  operation). Conceptually, th e carry dataflow is a way to\\nmodulate the next output and the next state (see figure 6.14). Simple so far.\\nNow the subtlety: the way the next value of the carry dataflow is computed. It involves\\nthree distinct transformations. All three have the form of a \\nSimpleRNN cell:\\ny = activation(dot(state_t, U) + dot(input_t, W) + b)\\nBut all three transformations have their ow n weight matrices, which you’ll index with\\nthe letters i, f, and k. Here’s what you have so far (it may seem a bit arbitrary, but bear\\nwith me).\\noutput_t = activation(dot(state_t, Uo) + dot(input_t, Wo) + dot(C_t, Vo) + bo)\\ni_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)\\nf_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)\\nk_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)\\nYou obtain the new carry state (the next c_t) by combining i_t, f_t, and k_t.\\nc_t+1 = i_t * k_t + c_t * f_t\\nAdd this as shown in figure 6.15. And that’s it. Not so complicated—merely a tad\\ncomplex.\\nListing 6.25 Pseudocode details of the LSTM architecture (1/2)\\nListing 6.26 Pseudocode details of the LSTM architecture (2/2)\\n...\\noutput t-1 output t output t+1\\ninput t-1 input t input t+1\\n...\\nState t State t+1\\nCarry trackc t+1c t\\nc t c t\\nc t-1\\noutput_t =\\n activation(\\n  Wo•input_t +\\n  Uo•state_t +\\n  Vo•c_t + \\n  bo)\\nFigure 6.14 Going from a SimpleRNN to an LSTM: adding a carry track\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 216, 'page_label': '217'}, page_content='204 CHAPTER 6 Deep learning for text and sequences\\n \\nIf you want to get philosophical, you can interpret what each of these operations is\\nmeant to do. For instance, you can say that multiplying c_t and f_t is a way to deliber-\\nately forget irrelevant information in the carry dataflow. Meanwhile, i_t and k_t pro-\\nvide information about the present, updat ing the carry track with new information.\\nBut at the end of the day, these interpretations don’t mean much, because what these\\noperations actually d o  i s  d e t e r m i n e d  b y  t h e  c o n t e n t s  o f  t h e  w e i g h t s  p a r a m e t e r i z i n g\\nthem; and the weights are learned in an end- to-end fashion, starting over with each\\ntraining round, making it impossible to credit this or that operation with a specific\\npurpose. The specification of an RNN cell (as just described) determines your hypoth-\\nesis space—the space in which you’ll sear ch for a good model configuration during\\ntraining—but it doesn’t determine what the cell does; that is up to the cell weights.\\nThe same cell with different weights can be doing very different things. So the combi-\\nnation of operations making up an RNN cell is better interpreted as a set of constraints\\non your search, not as a design in an engineering sense.\\n To a researcher, it seems that the choice of such constraints—the question of how to\\nimplement RNN cells—is better left to optimization algorithms (like genetic algorithms\\nor reinforcement learning processes) than to human engineers. And in the future,\\nthat’s how we’ll build networks. In summary : you don’t need to understand anything\\nabout the specific architecture of an LSTM cell; as a human, it shouldn’t be your job to\\nunderstand it. Just keep in mind what the LSTM cell is meant to do: allow past informa-\\ntion to be reinjected at a later time, thus fighting the vanishing-gradient problem. \\n6.2.3 A concrete LSTM example in Keras\\nNow let’s switch to more practical co ncerns: you’ll set up a model using an LSTM layer\\nand train it on the IMDB data (see figures 6.16 and 6.17). The network is similar to the\\none with SimpleRNN that was just presented. You on ly specify the output dimensional-\\nity of the LSTM layer; leave every other argument (there are many) at the Keras\\n...\\noutput t-1 output t output t+1\\ninput t-1 input t input t+1\\n...\\nState t State t+1\\nCarry trackc t+1c t\\nc t c t\\nc t-1\\noutput_t =\\n activation(\\n  Wo•input_t +\\n  Uo•state_t +\\n  Vo•c_t + \\n  bo)\\nCompute \\nnew \\ncarry\\nCompute \\nnew \\ncarry\\nFigure 6.15 Anatomy of an LSTM\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 217, 'page_label': '218'}, page_content=\"205Understanding recurrent neural networks\\ndefaults. Keras has good defaults, and things will almost always “just work” without you\\nhaving to spend time tuning parameters by hand.\\nfrom keras.layers import LSTM\\nmodel = Sequential()\\nmodel.add(Embedding(max_features, 32))\\nmodel.add(LSTM(32))\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='rmsprop',\\nloss='binary_crossentropy',\\nmetrics=['acc'])\\nhistory = model.fit(input_train, y_train,\\nepochs=10,\\nbatch_size=128,\\nvalidation_split=0.2)\\nListing 6.27 Using the LSTM layer in Keras\\nFigure 6.16 Training and validation \\nloss on IMDB with LSTM\\nFigure 6.17 Training and validation \\naccuracy on IMDB with LSTM\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 218, 'page_label': '219'}, page_content='206 CHAPTER 6 Deep learning for text and sequences\\nThis time, you achieve up to 89% validation  accuracy. Not bad: certainly much better\\nthan the SimpleRNN network—that’s largely because LSTM suffers much less from the\\nvanishing-gradient problem—and slightly better than the fully connected approach\\nfrom chapter 3, even though you’re looking at less data than you were in chapter 3.\\nYou’re truncating sequences after 500 timesteps, whereas in chapter 3, you were con-\\nsidering full sequences.\\n But this result isn’t groundbreaking  for such a computationally intensive\\napproach. Why isn’t \\nLSTM performing better? One reason is that you made no effort\\nto tune hyperparameters such as th e embeddings dimensionality or the LSTM output\\ndimensionality. Another may be lack of re gularization. But honestly, the primary rea-\\nson is that analyzing the global, long-term structure of the reviews (what LSTM is good\\nat) isn’t helpful for a sentiment-analysis pr oblem. Such a basic problem is well solved\\nby looking at what words occur in each review, and at what frequency. That’s what the\\nfirst fully connected approach looked at. But there are far more difficult natural-\\nlanguage-processing pr oblems out there, where the strength of LSTM will become\\napparent: in particular, question-answering and machine translation.\\n6.2.4 Wrapping up\\nNow you understand the following:\\n\\uf0a1 What RNNs are and how they work\\n\\uf0a1 What LSTM is, and why it works better on long sequences than a naive RNN\\n\\uf0a1 How to use Keras RNN layers to process sequence data\\nNext, we’ll review a number of  more advanced features of RNNs, which can help you\\nget the most out of your deep-learning sequence models. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 219, 'page_label': '220'}, page_content='207Advanced use of recurrent neural networks\\n6.3 Advanced use of recurrent neural networks\\nIn this section, we’ll revi ew three advanced techniques for improving the perfor-\\nmance and generalization power of recurren t neural networks. By the end of the sec-\\ntion, you’ll know most of what there is to know about using recurrent networks with\\nKeras. We’ll demonstrate all three concep ts on a temperature-forecasting problem,\\nwhere you have access to a timeseries of da ta points coming from sensors installed on\\nthe roof of a building, such as temperature, air pressure, and humidity, which you use\\nto predict what the temperature will be 24 ho urs after the last data point. This is a\\nfairly challenging problem that exemplifie s many common difficulties encountered\\nwhen working with timeseries.\\n We’ll cover the following techniques:\\n\\uf0a1 Recurrent dropout —This is a specific, built-in way to use dropout to fight overfit-\\nting in recurrent layers.\\n\\uf0a1 Stacking recurrent layers —This increases the represen tational power of the net-\\nwork (at the cost of higher computational loads).\\n\\uf0a1 Bidirectional recurrent layers —These present the same information to a recurrent\\nnetwork in different ways, increasing accuracy and mitigating forgetting issues.\\n6.3.1 A temperature-forecasting problem\\nUntil now, the only sequence data we’ve covered has been text data, such as the IMDB\\ndataset and the Reuters dataset. But sequence data is found in many more problems\\nthan just language processing. In all the examples in this section, you’ll play with a\\nweather timeseries dataset re corded at the Weather Statio n at the Max Planck Insti-\\ntute for Biogeochemistry in Jena, Germany.4\\n In this dataset, 14 diff erent quantities (such air te mperature, atmospheric pres-\\nsure, humidity, wind direction,  and so on) were recorded every 10 minutes, over sev-\\neral years. The original data  goes back to 2003, but this example is limited to data\\nfrom 2009–2016. This dataset is perfec t for learning to work with numerical\\ntimeseries. You’ll use it to build a model that takes as input some data from the recent\\npast (a few days’ worth of data points) and predicts the air temperature 24 hours in\\nthe future.\\n Download and uncompress the data as follows:\\ncd ~/Downloads\\nmkdir jena_climate\\ncd jena_climate\\nwget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\\nunzip jena_climate_2009_2016.csv.zip\\nLet’s look at the data.\\n4 Olaf Kolle, www.bgc-jena.mpg.de/wetter.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 220, 'page_label': '221'}, page_content='208 CHAPTER 6 Deep learning for text and sequences\\n \\nimport os\\ndata_dir = \\'/users/fchollet/Downloads/jena_climate\\'\\nfname = os.path.join(data_dir, \\'jena_climate_2009_2016.csv\\')\\nf = open(fname)\\ndata = f.read()\\nf.close()\\nlines = data.split(\\'\\\\n\\')\\nheader = lines[0].split(\\',\\')\\nlines = lines[1:]\\nprint(header)\\nprint(len(lines))\\nThis outputs a count of 420,551 lines of data (each line is a timestep: a record of a\\ndate and 14 weather-related values), as well as the following header:\\n[\"Date Time\",\\n\"p (mbar)\",\\n\"T (degC)\",\\n\"Tpot (K)\",\\n\"Tdew (degC)\",\\n\"rh (%)\",\\n\"VPmax (mbar)\",\\n\"VPact (mbar)\",\\n\"VPdef (mbar)\",\\n\"sh (g/kg)\",\\n\"H2OC (mmol/mol)\",\\n\"rho (g/m**3)\",\\n\"wv (m/s)\",\\n\"max. wv (m/s)\",\\n\"wd (deg)\"]\\nNow, convert all 420,551 lines of data into a Numpy array.\\nimport numpy as np\\nfloat_data = np.zeros((len(lines), len(header) - 1))\\nfor i, line in enumerate(lines):\\nvalues = [float(x) for x in line.split(\\',\\')[1:]]\\nfloat_data[i, :] = values\\nFor instance, here is the plot of temperature (in degrees Celsius) over time (see figure\\n6.18). On this plot, you can clearly see the yearly periodicity of temperature.\\nListing 6.28 Inspecting the data of the Jena weather dataset\\nListing 6.29 Parsing the data\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 221, 'page_label': '222'}, page_content='209Advanced use of recurrent neural networks\\n \\nfrom matplotlib import pyplot as plt\\ntemp = float_data[:, 1] <1> temperature (in degrees Celsius)\\nplt.plot(range(len(temp)), temp)\\nHere is a more narrow plot of the first 10 days of temperature data (see figure 6.19).\\nBecause the data is recorded every 10 minutes, you get 144 data points per day.\\nplt.plot(range(1440), temp[:1440])\\nListing 6.30 Plotting the temperature timeseries\\nListing 6.31 Plotting the first 10 days of the temperature timeseries\\nFigure 6.18 Temperature \\nover the full temporal range of \\nthe dataset (ºC)\\nFigure 6.19 Temperature \\nover the first 10 days of the \\ndataset (ºC)\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 222, 'page_label': '223'}, page_content='210 CHAPTER 6 Deep learning for text and sequences\\nOn this plot, you can see daily periodicity, especially evident for the last 4 days. Also\\nnote that this 10-day period must be coming from a fairly cold winter month.\\n If you were trying to predict average temperature for the next month given a few\\nmonths of past data, the problem would be ea sy, due to the reliable year-scale period-\\nicity of the data. But looking at the data ov er a scale of days, the temperature looks a\\nlot more chaotic. Is this timeseries predictable at a daily scale? Let’s find out.\\n6.3.2 Preparing the data\\nThe exact formulation of the problem will be as follows: given data going as far back\\nas lookback timesteps (a timestep is 10  minutes) and sampled every steps timesteps,\\ncan you predict the temperature in delay timesteps? You’ll use the following parame-\\nter values:\\n\\uf0a1 lookback = 720—Observations will go back 5 days.\\n\\uf0a1 steps = 6—Observations will be sampled at one data point per hour.\\n\\uf0a1 delay = 144—Targets will be 24 hours in the future.\\nTo get started, you need to do two things:\\n\\uf0a1 Preprocess the data to a format a neural  network can ingest. This is easy: the\\ndata is already numerical, so you don’t need to do any vect orization. But each\\ntimeseries in the data is on a different scale (for example, temperature is typi-\\ncally between -20 and +30, but atmosphe ric pressure, measured in mbar, is\\naround 1,000). You’ll normalize each time series independently so that they all\\ntake small values on a similar scale.\\n\\uf0a1 Write a Python generator that takes the current array of float data and yields\\nbatches of data from the recent past, along with a target temperature in the\\nfuture. Because the samples in the dataset are highly redundant (sample N and\\nsample N + 1 will have most of their timesteps in common), it would be wasteful\\nto explicitly allocate every sample. Inst ead, you’ll generate the samples on the\\nfly using the original data.\\nYou’ll preprocess the data by subtracting the mean of each timeseries and dividing by\\nthe standard deviation. You’re going to use the first 200,000 timesteps as training data,\\nso compute the mean and standard deviation only on this fraction of the data.\\nmean = float_data[:200000].mean(axis=0)\\nfloat_data -= mean\\nstd = float_data[:200000].std(axis=0)\\nfloat_data /= std\\nListing 6.33 shows the data generator you’ll use. It yields a tuple (samples, targets),\\nwhere samples is one batch of input data and targets is the corresponding array of\\ntarget temperatures. It takes the following arguments:\\nListing 6.32 Normalizing the data\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 223, 'page_label': '224'}, page_content='211Advanced use of recurrent neural networks\\n\\uf0a1 data—The original array of floating-point data, which you normalized in listing 6.32.\\n\\uf0a1 lookback—How many timesteps back the input data should go.\\n\\uf0a1 delay—How many timesteps in the future the target should be.\\n\\uf0a1 min_index and max_index—Indices in the data array that delimit which time-\\nsteps to draw from. This is useful for keeping a segment of the data for valida-\\ntion and another for testing.\\n\\uf0a1 shuffle—Whether to shuffle the samples or draw them in chronological order.\\n\\uf0a1 batch_size—The number of samples per batch.\\n\\uf0a1 step—The period, in timesteps, at which yo u sample data. You’ ll set it to 6 in\\norder to draw one data point every hour.\\ndef generator(data, lookback, delay, min_index, max_index,\\nshuffle=False, batch_size=128, step=6):\\nif max_index is None:\\nmax_index = len(data) - delay - 1\\ni = min_index + lookback\\nwhile 1:\\nif shuffle:\\nrows = np.random.randint(\\nmin_index + lookback, max_index, size=batch_size)\\nelse:\\nif i + batch_size >= max_index:\\ni = min_index + lookback\\nrows = np.arange(i, min(i + batch_size, max_index))\\ni += len(rows)\\nsamples = np.zeros((len(rows),\\nlookback // step,\\ndata.shape[-1]))\\ntargets = np.zeros((len(rows),))\\nfor j, row in enumerate(rows):\\nindices = range(rows[j] - lookback, rows[j], step)\\nsamples[j] = data[indices]\\ntargets[j] = data[rows[j] + delay][1]\\nyield samples, targets\\nNow, let’s use the abstract generator function to instantiate three generators: one for\\ntraining, one for validation, and one for test ing. Each will look at different temporal\\nsegments of the original data: the training  generator looks at the first 200,000 time-\\nsteps, the validation generator looks at th e following 100,000, and the test generator\\nlooks at the remainder.\\nlookback = 1440\\nstep = 6\\ndelay = 144\\nbatch_size = 128\\nListing 6.33 Generator yielding timeseries samples and their targets\\nListing 6.34 Preparing the training, validation, and test generators\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 224, 'page_label': '225'}, page_content='212 CHAPTER 6 Deep learning for text and sequences\\ntrain_gen = generator(float_data,\\nlookback=lookback,\\ndelay=delay,\\nmin_index=0,\\nmax_index=200000,\\nshuffle=True,\\nstep=step,\\nbatch_size=batch_size)\\nval_gen = generator(float_data,\\nlookback=lookback,\\ndelay=delay,\\nmin_index=200001,\\nmax_index=300000,\\nstep=step,\\nbatch_size=batch_size)\\ntest_gen = generator(float_data,\\nlookback=lookback,\\ndelay=delay,\\nmin_index=300001,\\nmax_index=None,\\nstep=step,\\nbatch_size=batch_size)\\nval_steps = (300000 - 200001 - lookback)\\ntest_steps = (len(float_data) - 300001 - lookback)\\n6.3.3 A common-sense, non-machine-learning baseline\\nBefore you start using black-box deep-learning models to solve the temperature-\\nprediction problem, let’s try a simple, common-sense approach. It will serve as a sanity\\ncheck, and it will establish a baseline that you’ll have to beat in  order to demonstrate\\nthe usefulness of more-advanced machine-learning models. Such common-sense base-\\nlines can be useful when you’re approa ching a new problem for which there is no\\nknown solution (yet). A classic example is that of unbalanced classification tasks,\\nwhere some classes are much more common than others. If your dataset contains 90%\\ninstances of class A and 10% instances of class B, then a common-sense approach to\\nthe classification task is to always predict “A” when presented with a new sample. Such\\na classifier is 90% accurate overall, and any learning-based approach should therefore\\nbeat this 90% score in order to demonstr ate usefulness. Sometimes, such elementary\\nbaselines can prove surprisingly hard to beat.\\n In this case, the temperature timeseries  can safely be assumed to be continuous\\n(the temperatures tomorrow are likely to be  close to the temperatures today) as well\\nas periodical with a daily period. Thus a common-sense approach is to always predict\\nthat the temperature 24 hours from now wi ll be equal to the temperature right now.\\nLet’s evaluate this approach, using the mean absolute error (MAE) metric:\\nnp.mean(np.abs(preds - targets))\\nHow many steps to draw from \\nval_gen in order to see the \\nentire validation set\\nHow many steps to draw \\nfrom test_gen in order to \\nsee the entire test set \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 225, 'page_label': '226'}, page_content=\"213Advanced use of recurrent neural networks\\nHere’s the evaluation loop.\\ndef evaluate_naive_method():\\nbatch_maes = []\\nfor step in range(val_steps):\\nsamples, targets = next(val_gen)\\npreds = samples[:, -1, 1]\\nmae = np.mean(np.abs(preds - targets))\\nbatch_maes.append(mae)\\nprint(np.mean(batch_maes))\\nevaluate_naive_method()\\nThis yields an MAE of 0.29. Because the temperature data has been normalized to be\\ncentered on 0 and have a standard deviation of 1, this number isn’t immediately inter-\\npretable. It translates to an average absolute error of 0.29 × temperature_std degrees\\nCelsius: 2.57˚C.\\ncelsius_mae = 0.29 * std[1]\\nThat’s a fairly large average absolute error. Now the game is to use your knowledge of\\ndeep learning to do better. \\n6.3.4 A basic machine-learning approach\\nIn the same way that it’s useful to establish a common-sense baseline before trying\\nmachine-learning approaches, it’s useful to try simple, cheap machine-learning mod-\\nels (such as small, densely connected networ ks) before looking into complicated and\\ncomputationally expensive models such as RNNs. This is the best way to make sure any\\nfurther complexity you throw at the problem is legitimate and delivers real benefits.\\n The following listing shows a fully connec ted model that starts by flattening the\\ndata and then runs it through two Dense layers. Note the lack of activation function on\\nthe last Dense layer, which is typical for a regression problem. You use MAE as the loss.\\nBecause you evaluate on the exact same data  and with the exact same metric you did\\nwith the common-sense approach, the results will be directly comparable.\\nfrom keras.models import Sequential\\nfrom keras import layers\\nfrom keras.optimizers import RMSprop\\nmodel = Sequential()\\nmodel.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))\\nmodel.add(layers.Dense(32, activation='relu'))\\nmodel.add(layers.Dense(1))\\nListing 6.35 Computing the common-sense baseline MAE\\nListing 6.36 Converting the MAE back to a Celsius error\\nListing 6.37 Training and evaluating a densely connected model\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 226, 'page_label': '227'}, page_content=\"214 CHAPTER 6 Deep learning for text and sequences\\nmodel.compile(optimizer=RMSprop(), loss='mae')\\nhistory = model.fit_generator(train_gen,\\nsteps_per_epoch=500,\\nepochs=20,\\nvalidation_data=val_gen,\\nvalidation_steps=val_steps)\\nLet’s display the loss curves for validation and training (see figure 6.20).\\nimport matplotlib.pyplot as plt\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\nepochs = range(1, len(loss) + 1)\\nplt.figure()\\nplt.plot(epochs, loss, 'bo', label='Training loss')\\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\\nplt.title('Training and validation loss')\\nplt.legend()\\nplt.show()\\nSome of the validation losses are close to the no-learning baseline, but not reliably.\\nThis goes to show the merit of having this baseline in the first place: it turns out to be\\nnot easy to outperform. Your common sens e contains a lot of valuable information\\nthat a machine-learning model doesn’t have access to.\\n You may wonder, if a simple , well-performing model exists to go from the data to\\nthe targets (the common-sense baseline), why doesn’t the model you’re training find it\\nand improve on it? Because this simple soluti on isn’t what your training setup is look-\\ning for. The space of models in which yo u’re searching for a solution—that is, your\\nhypothesis space—is the space of all possible two-layer networks with the configuration\\nyou defined. These networks are already fairly complicated. When you’re looking for a\\nListing 6.38 Plotting results\\nFigure 6.20 Training and validation \\nloss on the Jena temperature- \\nforecasting task with a simple, densely \\nconnected network\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 227, 'page_label': '228'}, page_content=\"215Advanced use of recurrent neural networks\\nsolution with a space of complicated models, the simple, well-performing baseline may\\nbe unlearnable, even if it’s technically part of the hypothesis space. That is a pretty sig-\\nnificant limitation of machine learning in  general: unless the learning algorithm is\\nhardcoded to look for a specif ic kind of simple model, parameter learning can some-\\ntimes fail to find a simple solution to a simple problem. \\n6.3.5 A first recurrent baseline\\nThe first fully connected approach didn’t do well, but that doesn’t mean machine\\nlearning isn’t applicable to this problem.  The previous approach first flattened the\\ntimeseries, which removed the notion of ti me from the input data. Let’s instead look\\nat the data as what it is: a sequence, where causality and order matter. You’ll try a\\nrecurrent-sequence processing model—it should be the pe rfect fit for such sequence\\ndata, precisely because it exploits the temporal ordering of data points, unlike the first\\napproach.\\n Instead of the LSTM layer introduced in the previous section, you’ll use the GRU\\nlayer, developed by Chung et al. in 2014. 5 Gated recurrent unit ( GRU) layers work\\nusing the same principle as LSTM, but they’re somewhat  streamlined and thus\\ncheaper to run (although they may not ha ve as much representational power as\\nLSTM). This trade-off between computatio nal expensiveness and representational\\npower is seen everywhere in machine learning.\\nfrom keras.models import Sequential\\nfrom keras import layers\\nfrom keras.optimizers import RMSprop\\nmodel = Sequential()\\nmodel.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\\nmodel.add(layers.Dense(1))\\nmodel.compile(optimizer=RMSprop(), loss='mae')\\nhistory = model.fit_generator(train_gen,\\nsteps_per_epoch=500,\\nepochs=20,\\nvalidation_data=val_gen,\\nvalidation_steps=val_steps)\\nFigure 6.21 shows the results.  Much better! You can significantly beat the common-\\nsense baseline, demonstrating the value of machine learning as well as the superiority\\nof recurrent networks compared to sequen ce-flattening dense networks on this type\\nof task.\\n5 Junyoung Chung et al., “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling,”\\nConference on Neural Information Processing Systems (2014), https:/ /arxiv.org/abs/1412.3555.\\nListing 6.39 Training and evaluating a GRU-based model\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 228, 'page_label': '229'}, page_content='216 CHAPTER 6 Deep learning for text and sequences\\n \\nThe new validation MAE of ~0.265 (before you start significantly overfitting) translates\\nto a mean absolute error of 2.35˚C after de normalization. That’s a solid gain on the\\ninitial error of 2.57˚C, but you probably still have a bit of a margin for improvement. \\n6.3.6 Using recurrent dropout to fight overfitting\\nIt’s evident from the training and validation  curves that the model is overfitting: the\\ntraining and validation losses start to diverge considerably after a few epochs. You’re\\nalready familiar with a classic techniqu e for fighting this phenomenon: dropout,\\nwhich randomly zeros out input units of a layer in order to break happenstance cor-\\nrelations in the training data that the laye r is exposed to. But how to correctly apply\\ndropout in recurrent networks  isn’t a trivial question. It has long been known that\\napplying dropout before a recurrent layer hinders learning rather than helping with\\nregularization. In 2015, Yarin Gal, as part of his PhD thesis on Bayesian deep learn-\\ning,6 determined the proper way to use drop out with a recurrent network: the same\\ndropout mask (the same pattern of dropped units) should be applied at every time-\\nstep, instead of a dropout ma sk that varies randomly fr om timestep to timestep.\\nWhat’s more, in order to regularize the representations formed by the recurrent gates\\nof layers such as \\nGRU and LSTM, a temporally constant dropout mask should be applied\\nto the inner recurrent activations of the layer (a recurrent dropout mask). Using the\\nsame dropout mask at every timestep allo ws the network to pr operly propagate its\\nlearning error through time; a temporally  random dropout mask would disrupt this\\nerror signal and be harmful to the learning process.\\n Yarin Gal did his research using Keras and helped build this mechanism directly\\ninto Keras recurrent layers. Every recurrent layer in Keras has two dropout-related\\narguments: \\ndropout, a float specifying th e dropout rate for input units of the layer,\\n6 See Yarin Gal, “Uncertainty in Deep Le arning (PhD Thesis),” October 13, 2016, http:/ /mlg.eng.cam.ac.uk/\\nyarin/blog_2248.html.\\nFigure 6.21 Training and validation \\nloss on the Jena temperature-\\nforecasting task with a GRU\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 229, 'page_label': '230'}, page_content=\"217Advanced use of recurrent neural networks\\nand recurrent_dropout, specifying the dropout rate of the recurrent units. Let’s add\\ndropout and recurrent dropout to the GRU layer and see how doing so impacts overfit-\\nting. Because networks being regularized with dropout always take longer to fully con-\\nverge, you’ll train the network for twice as many epochs.\\nfrom keras.models import Sequential\\nfrom keras import layers\\nfrom keras.optimizers import RMSprop\\nmodel = Sequential()\\nmodel.add(layers.GRU(32,\\ndropout=0.2,\\nrecurrent_dropout=0.2,\\ninput_shape=(None, float_data.shape[-1])))\\nmodel.add(layers.Dense(1))\\nmodel.compile(optimizer=RMSprop(), loss='mae')\\nhistory = model.fit_generator(train_gen,\\nsteps_per_epoch=500,\\nepochs=40,\\nvalidation_data=val_gen,\\nvalidation_steps=val_steps)\\nFigure 6.22 shows the results. Success! You’re no longer overfitting during the first 30\\nepochs. But although you have more stable evaluation scores, your best scores aren’t\\nmuch lower than they were previously.   \\n6.3.7 Stacking recurrent layers\\nBecause you’re no longer overfitting but seem to have hit a performance bottleneck,\\nyou should consider increasing the capacity  of the network. Recall the description of\\nthe universal machine-learning workflow: it’s  generally a good idea to increase the\\ncapacity of your network until overfitt ing becomes the primary obstacle (assuming\\nListing 6.40 Training and evaluating a dropout-regularized GRU-based model\\nFigure 6.22 Training and validation \\nloss on the Jena temperature-\\nforecasting task with a dropout-\\nregularized GRU\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 230, 'page_label': '231'}, page_content=\"218 CHAPTER 6 Deep learning for text and sequences\\nyou’re already taking basic steps to mitiga te overfitting, such as using dropout). As\\nlong as you aren’t overfitting too badly, you’re likely under capacity.\\n Increasing network capacity is typically done by increasing the number of units in\\nt h e  l a y e r s  o r  a d d i n g  m o r e  l a y e r s .  R e c u r r e nt layer stacking is a classic way to build\\nmore-powerful recurrent networks: for inst ance, what currently powers the Google\\nTranslate algorithm is a stack of seven large LSTM layers—that’s huge.\\n To stack recurrent layers on top of each  other in Keras, all intermediate layers\\nshould return their full sequence of outputs (a 3D tensor) rather than their output at\\nthe last timestep. This is done by specifying return_sequences=True.\\nfrom keras.models import Sequential\\nfrom keras import layers\\nfrom keras.optimizers import RMSprop\\nmodel = Sequential()\\nmodel.add(layers.GRU(32,\\ndropout=0.1,\\nrecurrent_dropout=0.5,\\nreturn_sequences=True,\\ninput_shape=(None, float_data.shape[-1])))\\nmodel.add(layers.GRU(64, activation='relu',\\ndropout=0.1,\\nrecurrent_dropout=0.5))\\nmodel.add(layers.Dense(1))\\nmodel.compile(optimizer=RMSprop(), loss='mae')\\nhistory = model.fit_generator(train_gen,\\nsteps_per_epoch=500,\\nepochs=40,\\nvalidation_data=val_gen,\\nvalidation_steps=val_steps)\\nFigure 6.23 shows th e results. You can see that the added layer does improve the\\nresults a bit, though not significantly. You can draw two conclusions:\\n\\uf0a1 Because you’re still not overfitting too badly, you could safely increase the size of\\nyour layers in a quest for validation-loss improvement. This has a non-negligible\\ncomputational cost, though.\\n\\uf0a1 Adding a layer didn’t help by a significant factor, so you may be seeing diminish-\\ning returns from increasing network capacity at this point.\\nListing 6.41 Training and evaluating a dropout-regularized, stacked GRU model\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 231, 'page_label': '232'}, page_content='219Advanced use of recurrent neural networks\\n   \\n6.3.8 Using bidirectional RNNs\\nThe last technique introduced in this section is called bidirectional RNNs. A bidirec-\\ntional RNN is a common RNN variant that can offer grea ter performance than a regu-\\nlar RNN on certain tasks. It’s frequently us ed in natural-language processing—you\\ncould call it the Swiss Army knife of deep learning for natural-language processing.\\n RNNs are notably order dependent, or time dependent: they process the timesteps\\nof their input sequences in order, and sh uffling or reversing the timesteps can com-\\npletely change the representations the RNN extracts from the sequence. This is pre-\\ncisely the reason they perform well on problems where order is  meaningful, such as\\nthe temperature-forecasting problem. A bidirectional RNN exploits the order sensitiv-\\nity of RNNs :  i t  c o n s i s t s  o f  u s i n g  t w o  r e g u l a r  RNNs ,  s u c h  a s  t h e  GRU and LSTM layers\\nyou’re already familiar with, each of which processes the input sequence in one direc-\\ntion (chronologically and an tichronologically), and then merging their representa-\\ntions. By processing a sequence both ways, a bidirectional RNN can catch patterns that\\nmay be overlooked by a unidirectional RNN.\\n Remarkably, the fact that the RNN layers in this section have processed sequences in\\nchronological order (older timesteps first) may have been an arbitrary decision. At least,\\nit’s a decision we made no attempt to question so far. Could the RNNs have performed\\nwell enough if they processed input sequences in antichronological order, for instance\\n(newer timesteps first)? Let’s try this in practice and see what happens. All you need to\\ndo is write a variant of the data generator where the input sequences are reverted along\\nthe time dimension (replace the last line with yield samples[:, ::-1, :], targets).\\nTraining the same one-GRU-layer network that you used in the first experiment in this\\nsection, you get the results shown in figure 6.24.\\nFigure 6.23 Training and validation \\nloss on the Jena temperature-\\nforecasting task with a stacked \\nGRU network\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 232, 'page_label': '233'}, page_content=\"220 CHAPTER 6 Deep learning for text and sequences\\n \\nThe reversed-order GRU strongly underper forms even the common-sense baseline,\\nindicating that in this case, chronological processing is important to the success of your\\napproach. This makes perfect sense: the underlying GRU layer will typically be better at\\nremembering the recent past than the di stant past, and naturally the more recent\\nweather data points are more predictive than older data points for the problem (that’s\\nwhat makes the common-sense baseline fairly strong). Thus the chronological version\\nof the layer is bound to outperform the reve rsed-order version. Importantly, this isn’t\\ntrue for many other problems, including natural language: intuitively, the importance\\nof a word in understanding a sentence isn’t usually dependent on its position in the sen-\\ntence. Let’s try the same trick on the LSTM IMDB example from section 6.2.\\nfrom keras.datasets import imdb\\nfrom keras.preprocessing import sequence\\nfrom keras import layers\\nfrom keras.models import Sequential\\nmax_features = 10000\\nmaxlen = 500\\n(x_train, y_train), (x_test, y_test) = imdb.load_data(\\nnum_words=max_features)\\nx_train = [x[::-1] for x in x_train]\\nx_test = [x[::-1] for x in x_test]\\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\\nmodel = Sequential()\\nmodel.add(layers.Embedding(max_features, 128))\\nmodel.add(layers.LSTM(32))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='rmsprop',\\nloss='binary_crossentropy',\\nmetrics=['acc'])\\nListing 6.42 Training and evaluating an LSTM using reversed sequences\\nFigure 6.24 Training and validation \\nloss on the Jena temperature-\\nforecasting task with a GRU trained \\non reversed sequences\\nNumber of words \\nto consider as \\nfeatures\\nCuts off texts after this \\nnumber of words (among \\nthe max_features most \\ncommon words)\\nLoads\\ndata Reverses \\nsequences\\nPads \\nsequences\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 233, 'page_label': '234'}, page_content=\"221Advanced use of recurrent neural networks\\nhistory = model.fit(x_train, y_train,\\nepochs=10,\\nbatch_size=128,\\nvalidation_split=0.2)\\nYou get performance nearly identical to that of the chronological-order LSTM.\\nRemarkably, on such a text dataset, revers ed-order processing works just as well as\\nchronological processing, confirming the hypothesis that, although word order does\\nmatter in understanding language, which order you use isn’t crucial. Importantly, an\\nRNN trained on reversed sequences will le arn different representations than one\\ntrained on the original sequences, much as you would have different mental models if\\ntime flowed backward in the real world—if you lived a life where you died on your first\\nday and were born on your last day. In machine learning, representations that are dif-\\nferent yet useful are always worth exploiting, and th e more they differ, the better: they\\noffer a new angle from which to  look at your data, capturing aspects of the data that\\nwere missed by other approaches, and thus  they can help boost performance on a\\ntask. This is the intuition behind ensembling, a concept we’ll explore in chapter 7.\\n A bidirectional \\nRNN exploits this idea to improve on the performance of chronological-\\norder RNNs. It looks at its input sequence both ways (see figure 6.25), obtaining poten-\\ntially richer representations and capturing patterns that may have been missed by the\\nchronological-order version alone.\\nTo instantiate a bidirectional RNN in Keras, you use the Bidirectional layer, which takes\\nas its first argument a recurrent layer instance. Bidirectional creates a second, separate\\ninstance of this recurrent layer and uses one instance for processing the input sequences\\nin chronological order and the other instan ce for processing the input sequences in\\nreversed order. Let’s try it on the IMDB sentiment-analysis task.\\nmodel = Sequential()\\nmodel.add(layers.Embedding(max_features, 32))\\nmodel.add(layers.Bidirectional(layers.LSTM(32)))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nListing 6.43 Training and evaluating a bidirectional LSTM\\nChronological\\nsequence\\nReversed\\nsequence\\nMerge (add,\\nconcatenate)\\nInput data\\na, b, c, d, e\\na, b, c, d, e\\ne, d, c, b, a\\nRNN RNN\\nFigure 6.25 How a \\nbidirectional RNN layer works\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 234, 'page_label': '235'}, page_content=\"222 CHAPTER 6 Deep learning for text and sequences\\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\\nhistory = model.fit(x_train, y_train,\\nepochs=10,\\nbatch_size=128,\\nvalidation_split=0.2)\\nIt performs slightly better than the regular LSTM you tried in the previous section,\\nachieving over 89% validation accuracy. It also seems to overfit more quickly, which is\\nunsurprising because a bidirectional layer ha s twice as many parameters as a chrono-\\nlogical LSTM. With some regularization, the bidi rectional approach would likely be a\\nstrong performer on this task.\\n Now let’s try the same approach on the temperature-prediction task.\\nfrom keras.models import Sequential\\nfrom keras import layers\\nfrom keras.optimizers import RMSprop\\nmodel = Sequential()\\nmodel.add(layers.Bidirectional(\\nlayers.GRU(32), input_shape=(None, float_data.shape[-1])))\\nmodel.add(layers.Dense(1))\\nmodel.compile(optimizer=RMSprop(), loss='mae')\\nhistory = model.fit_generator(train_gen,\\nsteps_per_epoch=500,\\nepochs=40,\\nvalidation_data=val_gen,\\nvalidation_steps=val_steps)\\nThis performs about as well as the regular GRU layer. It’s easy to understand why: all the\\npredictive capacity must come from the chronological half of the network, because the\\nantichronological half is known to be seve rely underperforming on this task (again,\\nbecause the recent past matters much more than the distant past in this case). \\n6.3.9 Going even further\\nThere are many other things you could try,  in order to improve performance on the\\ntemperature-forecasting problem:\\n\\uf0a1 Adjust the number of units in each re current layer in the stacked setup. The\\ncurrent choices are largely arbitrary and thus probably suboptimal.\\n\\uf0a1 Adjust the learning rate used by the RMSprop optimizer.\\n\\uf0a1 Try using LSTM layers instead of GRU layers.\\n\\uf0a1 Try using a bigger densely connected regr essor on top of the recurrent layers:\\nthat is, a bigger Dense layer or even a stack of Dense layers.\\n\\uf0a1 Don’t forget to eventually run the best -performing models (in terms of valida-\\ntion MAE) on the test set! Otherwise, you’ll develop architectures that are over-\\nfitting to the validation set.\\nListing 6.44 Training a bidirectional GRU\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 235, 'page_label': '236'}, page_content='223Advanced use of recurrent neural networks\\nAs always, deep learning is more an art than a science. We can provide guidelines that\\nsuggest what is likely to work or not work on a given problem, but, ultimately, every\\nproblem is unique; you’ll have to evaluate  different strategies empirically. There is\\ncurrently no theory that will tell you in adva nce precisely what you should do to opti-\\nmally solve a problem. You must iterate.\\n6.3.10 Wrapping up\\nHere’s what you should take away from this section:\\n\\uf0a1 As you first learned in chapter 4, when approaching a new problem, it’s good to\\nfirst establish common-sense baselines for your metric of choice. If you don’t\\nhave a baseline to beat, you can’t tell whether you’re making real progress.\\n\\uf0a1 Try simple models before expensive on es, to justify the additional expense.\\nSometimes a simple model will turn out to be your best option.\\n\\uf0a1 When you have data where temporal or dering matters, recurrent networks are\\na great fit and easily outperform models that first flatten the temporal data.\\n\\uf0a1 To use dropout with recurrent networks , you should use a time-constant drop-\\nout mask and recurrent dropout mask. These are built into Keras recurrent lay-\\ners, so all you have to do is use the dropout and recurrent_dropout arguments\\nof recurrent layers.\\n\\uf0a1 Stacked RNNs provide more representational power than a single RNN layer.\\nThey’re also much more expensive and th us not always worth it. Although they\\noffer clear gains on complex problems (such as machine translation), they may\\nnot always be relevant to smaller, simpler problems.\\n\\uf0a1 Bidirectional RNNs, which look at a sequence bo th ways, are useful on natural-\\nlanguage processing problems. But they aren’t strong performers on sequence\\ndata where the recent past is much more informative than the beginning of the\\nsequence.\\nNOTE There are two important concepts we won’t cover in detail here: recur-\\nrent attention and sequence masking. Both tend to be especially relevant for\\nnatural-language processing, and they ar en’t particularly applicable to the\\ntemperature-forecasting problem. We’ll leave them for future study outside of\\nthis book.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 236, 'page_label': '237'}, page_content='224 CHAPTER 6 Deep learning for text and sequences\\nMarkets and machine learning\\nSome readers are bound to want to take the techniques we’ve introduced here and\\ntry them on the problem of forecasting the future price of securities on the stock mar-\\nket (or currency exchange rate s, and so on). Markets have very different statistical\\ncharacteristics than natural phenomena such as weather patterns. Trying to use\\nmachine learning to beat markets, when you only have access to publicly available\\ndata, is a difficult endeavor, and you’re likely to waste your time and resources with\\nnothing to show for it.\\nAlways remember that when it comes to markets, past performance is not a good\\npredictor of future returns— looking in the rear-view mirr or is a bad way to drive.\\nMachine learning, on the other hand, is applicable to datasets where the past is a\\ngood predictor of the future. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 237, 'page_label': '238'}, page_content='225Sequence processing with convnets\\n6.4 Sequence processing with convnets\\nIn chapter 5, you learned about convolutio nal neural networks (convnets) and how\\nthey perform particularly well on computer vision problems, due to their ability to\\noperate convolutionally, extracting features from loca l input patches and allowing for\\nrepresentation modularity and data efficiency. The same properties that make conv-\\nnets excel at computer vision also make them highly relevant to sequence processing.\\nTime can be treated as a spatial dimension, like the height or width of a \\n2D image.\\n Such 1D convnets can be competitive with RNNs on certain se quence-processing\\nproblems, usually at a considerably cheaper computational cost. Recently, 1D conv-\\nnets, typically used with dilated kernels, ha ve been used with great success for audio\\ngeneration and machine translation. In addition to these specific successes, it has long\\nbeen known that small 1D convnets can offer a fast alternative to RNNs for simple tasks\\nsuch as text classification and timeseries forecasting.\\n6.4.1 Understanding 1D convolution for sequence data\\nThe convolution layers introduced previously were 2D convolutions, extracting 2D\\npatches from image tensors and applying an  identical transforma tion to every patch.\\nIn the same way, you can use 1D convolutions, extracting local 1D patches (subse-\\nquences) from sequences (see figure 6.26).\\nSuch 1D convolution layers can recognize local patterns in a sequence. Because the\\nsame input transformation is performed on every patch, a pattern learned at a certain\\nposition in a sentence can later be recognized at a different position, making 1D conv-\\nnets translation invariant (for temp oral translations). For instance, a 1D convnet pro-\\ncessing sequences of characters using convolution windows of size 5 should be able to\\nlearn words or word fragments of length 5 or less, and it should be able to recognize\\nInput\\nfeaturesInput\\nOutput\\nExtracted\\npatch\\nWindow of\\nsize 5\\nDot product\\nwith weights\\nOutput \\nfeatures\\nTime\\nFigure 6.26 How 1D convolution \\nworks: each output timestep is \\nobtained from a temporal patch in \\nthe input sequence.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 238, 'page_label': '239'}, page_content=\"226 CHAPTER 6 Deep learning for text and sequences\\nthese words in any context in an input sequence. A character-level 1D convnet is thus\\nable to learn about word morphology. \\n6.4.2 1D pooling for sequence data\\nYou’re already familiar with 2D pooling operations, such as 2D average pooling and\\nmax pooling, used in convnets to spatially downsample image tensors. The 2D pooling\\noperation has a 1D equivalent: extracting 1D patches (subsequences) from an input\\nand outputting the maximum value (max pooling) or average value (average pooling).\\nJust as with 2D convnets, this is used for reducing the length of 1D inputs (subsampling). \\n6.4.3 Implementing a 1D convnet\\nIn Keras, you use a 1D convnet via the Conv1D layer, which has an interface similar to\\nConv2D. It takes as input 3D tensors with shape (samples, time, features) and\\nreturns similarly shaped 3D tensors. The convolution window is a 1D window on the\\ntemporal axis: axis 1 in the input tensor.\\n Let’s build a simple two-layer 1D convnet and apply it to the IMDB sentiment-\\nclassification task you’re al ready familiar with. As a reminder, this is the code for\\nobtaining and preprocessing the data.\\nfrom keras.datasets import imdb\\nfrom keras.preprocessing import sequence\\nmax_features = 10000\\nmax_len = 500\\nprint('Loading data...')\\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\\nprint(len(x_train), 'train sequences')\\nprint(len(x_test), 'test sequences')\\nprint('Pad sequences (samples x time)')\\nx_train = sequence.pad_sequences(x_train, maxlen=max_len)\\nx_test = sequence.pad_sequences(x_test, maxlen=max_len)\\nprint('x_train shape:', x_train.shape)\\nprint('x_test shape:', x_test.shape)\\n1D convnets are stru ctured in the same way as their 2D counterparts, which you used\\nin chapter 5: they consist of a stack of Conv1D and MaxPooling1D layers, ending in\\neither a global pooling layer or a Flatten layer, that turn the 3D outputs into 2D out-\\nputs, allowing you to add one or more Dense layers to the model for classification or\\nregression.\\n One difference, though, is the fact that you can afford to use larger convolution\\nwindows with 1D convnets. With a 2D convolution layer, a 3 × 3 convolution window\\ncontains 3 × 3 = 9 feature vectors; but with a 1D convolution layer, a convolution win-\\ndow of size 3 contains only 3 feature vectors. You can thus easily afford 1D convolution\\nwindows of size 7 or 9.\\nListing 6.45 Preparing the IMDB data\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 239, 'page_label': '240'}, page_content=\"227Sequence processing with convnets\\n This is the example 1D convnet for the IMDB dataset.\\nfrom keras.models import Sequential\\nfrom keras import layers\\nfrom keras.optimizers import RMSprop\\nmodel = Sequential()\\nmodel.add(layers.Embedding(max_features, 128, input_length=max_len))\\nmodel.add(layers.Conv1D(32, 7, activation='relu'))\\nmodel.add(layers.MaxPooling1D(5))\\nmodel.add(layers.Conv1D(32, 7, activation='relu'))\\nmodel.add(layers.GlobalMaxPooling1D())\\nmodel.add(layers.Dense(1))\\nmodel.summary()\\nmodel.compile(optimizer=RMSprop(lr=1e-4),\\nloss='binary_crossentropy',\\nmetrics=['acc'])\\nhistory = model.fit(x_train, y_train,\\nepochs=10,\\nbatch_size=128,\\nvalidation_split=0.2)\\nFigures 6.27 and 6.28 show the training and validation results. Validation accuracy is\\nsomewhat less than that of the LSTM, but runtime is faster on both CPU and GPU (the\\nexact increase in speed will vary greatly depending on your exact configuration). At this\\npoint, you could retrain this model for the right number of epochs (eight) and run it\\non the test set. This is a convincing demonstration that a 1D convnet can offer a fast,\\ncheap alternative to a recurrent network on a word-level sentiment-classification task.  \\nListing 6.46 Training and evaluating a simple 1D convnet on the IMDB data\\nFigure 6.27 Training and \\nvalidation loss on IMDB with a \\nsimple 1D convnet\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 240, 'page_label': '241'}, page_content=\"228 CHAPTER 6 Deep learning for text and sequences\\n6.4.4 Combining CNNs and RNNs to process long sequences\\nBecause 1D convnets process input patches independently, they aren’t sensitive to the\\norder of the timesteps (beyond a local scal e, the size of the convolution windows),\\nunlike RNNs. Of course, to recognize longer-term patterns, you can stack many convo-\\nlution layers and pooling layers, resulting in  upper layers that will see long chunks of\\nthe original inputs—but that’s still a fairly  weak way to induce order sensitivity. One\\nway to evidence this weakness is to try 1D convnets on the temperature-forecasting\\nproblem, where order-sensitivity is key to producing good predictions. The following\\nexample reuses the following variables defined previously: float_data, train_gen,\\nval_gen, and val_steps.\\nfrom keras.models import Sequential\\nfrom keras import layers\\nfrom keras.optimizers import RMSprop\\nmodel = Sequential()\\nmodel.add(layers.Conv1D(32, 5, activation='relu',\\ninput_shape=(None, float_data.shape[-1])))\\nmodel.add(layers.MaxPooling1D(3))\\nmodel.add(layers.Conv1D(32, 5, activation='relu'))\\nmodel.add(layers.MaxPooling1D(3))\\nmodel.add(layers.Conv1D(32, 5, activation='relu'))\\nmodel.add(layers.GlobalMaxPooling1D())\\nmodel.add(layers.Dense(1))\\nmodel.compile(optimizer=RMSprop(), loss='mae')\\nhistory = model.fit_generator(train_gen,\\nsteps_per_epoch=500,\\nepochs=20,\\nvalidation_data=val_gen,\\nvalidation_steps=val_steps)\\nListing 6.47 Training and evaluating a simple 1D convnet on the Jena data\\nFigure 6.28 Training and \\nvalidation accuracy on IMDB \\nwith a simple 1D convnet\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 241, 'page_label': '242'}, page_content='229Sequence processing with convnets\\nFigure 6.29 shows the training and validation MAEs.\\nThe validation MAE stays in the 0.40s: you can’t even beat the common-sense baseline\\nusing the small convnet. Again, this is because the convnet looks for patterns any-\\nwhere in the input timeseries and has no knowledge of the temporal position of a pat-\\ntern it sees (toward the beginning, toward the end, and so on). Because more recent\\ndata points should be interpreted differently from older data points in the case of this\\nspecific forecasting problem,  the convnet fails at produc ing meaningful results. This\\nlimitation of convnets isn’t an issue with the \\nIMDB data, because patterns of keywords\\nassociated with a positive or negative sentiment are informative independently of\\nwhere they’re found in the input sentences.\\n One strategy to combine the speed and lightness of convnets with the order-sensitivity\\nof RNNs is to use a 1D convnet as a preprocessing step before an RNN (see figure 6.30).\\nThis is especially beneficial when you’re deal-\\ning with sequences that are so long they can’t\\nrealistically be processed with \\nRNNs, such as\\nsequences with thousands of steps. The conv-\\nnet will turn the long input sequence into\\nmuch shorter (downsampled) sequences of\\nhigher-level features. This sequence of\\nextracted features then becomes the input to\\nthe \\nRNN part of the network.\\n This technique isn’t seen often in\\nresearch papers and pr actical applications,\\npossibly because it isn’t well known. It’s effec-\\ntive and ought to be more common. Let’s try\\nit on the temperature-forecasting dataset.\\nBecause this strategy allows you to manipu-\\nlate much longer sequences, you can either\\nFigure 6.29 Training and \\nvalidation loss on the Jena \\ntemperature-forecasting task \\nwith a simple 1D convnet\\nRNN\\n1D CNN\\nLong sequence\\nShorter\\nsequence CNN features\\nFigure 6.30 Combining a 1D convnet and \\nan RNN for processing long sequences\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 242, 'page_label': '243'}, page_content=\"230 CHAPTER 6 Deep learning for text and sequences\\nlook at data from longer  ago (by increasing the lookback parameter of the data gen-\\nerator) or look at high-resolution timeseries (by decreasing the step parameter of the\\ngenerator). Here, somewhat arbitrarily, you’ll use a step that’s half as large, resulting\\nin a timeseries twice as long, where the temperature data is sampled at a rate of\\n1 point per 30 minutes. The example reuses the generator function defined earlier.\\nstep = 3\\nlookback = 720\\ndelay = 144\\ntrain_gen = generator(float_data,\\nlookback=lookback,\\ndelay=delay,\\nmin_index=0,\\nmax_index=200000,\\nshuffle=True,\\nstep=step)\\nval_gen = generator(float_data,\\nlookback=lookback,\\ndelay=delay,\\nmin_index=200001,\\nmax_index=300000,\\nstep=step)\\ntest_gen = generator(float_data,\\nlookback=lookback,\\ndelay=delay,\\nmin_index=300001,\\nmax_index=None,\\nstep=step)\\nval_steps = (300000 - 200001 - lookback) // 128\\ntest_steps = (len(float_data) - 300001 - lookback) // 128\\nThis is the model, starting with two Conv1D layers and following up with a GRU layer.\\nFigure 6.31 shows the results.\\nfrom keras.models import Sequential\\nfrom keras import layers\\nfrom keras.optimizers import RMSprop\\nmodel = Sequential()\\nmodel.add(layers.Conv1D(32, 5, activation='relu',\\ninput_shape=(None, float_data.shape[-1])))\\nmodel.add(layers.MaxPooling1D(3))\\nmodel.add(layers.Conv1D(32, 5, activation='relu'))\\nmodel.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5))\\nmodel.add(layers.Dense(1))\\nmodel.summary()\\nmodel.compile(optimizer=RMSprop(), loss='mae')\\nListing 6.48 Preparing higher-resolution data generators for the Jena dataset\\nListing 6.49 Model combining a 1D convolutional base and a GRU layer\\nPreviously set to 6 (1 point per hour); \\nnow 3 (1 point per 30 min)Unchanged\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 243, 'page_label': '244'}, page_content='231Sequence processing with convnets\\nhistory = model.fit_generator(train_gen,\\nsteps_per_epoch=500,\\nepochs=20,\\nvalidation_data=val_gen,\\nvalidation_steps=val_steps)\\nJudging from the validation loss, this setup isn’t as good as the regularized GRU alone,\\nbut it’s significantly faster. It  looks at twice as much data, which in this case doesn’t\\nappear to be hugely helpful but may be important for other datasets. \\n6.4.5 Wrapping up\\nHere’s what you should take away from this section:\\n\\uf0a1 In the same way that 2D convnets perform well for pr ocessing visual patterns in\\n2D space, 1D convnets perform well for proc essing temporal patterns. They\\noffer a faster alternative to RNNs on some problems, in  particular natural-\\nlanguage processing tasks.\\n\\uf0a1 Typically, 1D convnets are structured much like their 2D equivalents from the\\nworld of computer vision: they consist of stacks of Conv1D layers and Max-\\nPooling1D layers, ending in a global pooling operation or flattening operation.\\n\\uf0a1 Because RNNs are extremely expensive for proce ssing very long sequences, but\\n1D convnets are cheap, it can be a good idea to use a 1D convnet as a prepro-\\ncessing step before an RNN, shortening the sequence and extracting useful rep-\\nresentations for the RNN to process. \\n \\n \\n \\n \\n \\n  \\nFigure 6.31 Training and validation \\nloss on the Jena temperature-\\nforecasting task with a 1D convnet \\nfollowed by a \\nGRU\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 244, 'page_label': '245'}, page_content='232 CHAPTER 6 Deep learning for text and sequences\\nChapter summary\\n\\uf0a1 In this chapter, you learned the fo llowing techniques, which are widely\\napplicable to any dataset of sequence data, from text to timeseries:\\n– How to tokenize text\\n– What word embeddings are, and how to use them\\n– What recurrent networks are, and how to use them\\n– How to stack RNN layers and use bidirectional RNNs to build more-power-\\nful sequence-processing models\\n– How to use 1D convnets for sequence processing\\n– How to combine 1D convnets and RNNs to process long sequences\\n\\uf0a1 You can use RNNs for timeseries regr ession (“predicti ng the future”),\\ntimeseries classification, anomaly de tection in timeseries, and sequence\\nlabeling (such as identifying names or dates in sentences).\\n\\uf0a1 Similarly, you can use 1D convnets for machine translation (sequence-to-\\nsequence convolutional models, like SliceNet a), document classification,\\nand spelling correction.\\n\\uf0a1 If global order matters  in your sequence data, then it’s preferable to use a\\nrecurrent network to process it. This is typically the case for timeseries,\\nwhere the recent past is likely to be more informative than the distant\\npast.\\n\\uf0a1 If global ordering isn’t fundamentally meaningful , then 1D convnets will turn\\nout to work at least as well and are ch eaper. This is often the case for text\\ndata, where a keyword found at the be ginning of a sentence is just as\\nmeaningful as a keyword found at the end.\\na See https://arxiv.org/abs/1706.03059.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 245, 'page_label': '246'}, page_content='Advanced deep-learning\\nbest practices\\nThis chapter explores a number of powerful tools that will bring you closer to\\nbeing able to develop state-of-the-art models on difficult problems. Using the Keras\\nfunctional API, you can build graph-like models , share a layer across different\\ninputs, and use Keras models just like Py thon functions. Keras callbacks and the\\nTensorBoard browser-based visualization tool let you monitor models during train-\\ning. We’ll also discuss several other best  practices including batch normalization,\\nresidual connections, hyperparameter optimization, and model ensembling.\\nThis chapter covers\\n\\uf0a1 The Keras functional API\\n\\uf0a1 Using Keras callbacks\\n\\uf0a1 Working with the TensorBoard visualization tool\\n\\uf0a1 Important best practices for developing state-of-\\nthe-art models'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 246, 'page_label': '247'}, page_content='234 CHAPTER 7 Advanced deep-learning best practices\\n7.1 Going beyond the Sequential model: \\nthe Keras functional API\\nUntil now, all neural networks introduced in this book\\nhave been implemented using the Sequential model.\\nThe Sequential model makes the assumption that the\\nnetwork has exactly one input and exactly one output, and\\nthat it consists of a linear stack of layers (see figure 7.1).\\n This is a commonly verifi ed assumption; the configu-\\nration is so common that we’ve been able to cover many\\ntopics and practical applications in these pages so far\\nusing only the \\nSequential model class. But this set of\\nassumptions is too in flexible in a numb er of cases. Some\\nnetworks require several independent inputs, others\\nrequire multiple outputs, an d some networks have inter-\\nnal branching between layers that makes them look like\\ngraphs of layers rather than linear stacks of layers.\\n Some tasks, for instance, require multimodal inputs: they merge data coming from\\ndifferent input sources, processing each ty pe of data using different kinds of neural\\nlayers. Imagine a deep-learning model trying to predict the most likely market price of\\na second-hand piece of clothing, using th e following inputs: user-provided metadata\\n(such as the item’s brand, age, and so on), a user-provided text description, and a pic-\\nture of the item. If you had only the metadata available, you could one-hot encode it\\nand use a densely connected network to pred ict the price. If you had only the text\\ndescription available, you could use an \\nRNN or a 1D convnet. If you had only the pic-\\nture, you could use a 2D convnet. But how can you use all three at the same time? A\\nnaive approach would be to train three sepa rate models and then do a weighted aver-\\nage of their predictions. But this may be suboptimal, because the information\\nextracted by the models may be redundant. A better way is to jointly learn a more accu-\\nrate model of the data by using a model that can see all available input modalities\\nsimultaneously: a model with three input branches (see figure 7.2).\\nMerging\\nmodule\\nPrice prediction\\nText descriptionMetadata Picture\\nDense module RNN module Convnet module\\nFigure 7.2 A multi-input model\\nLayer\\nOutput\\nInput\\nSequential\\nLayer\\nLayer\\nFigure 7.1 A Sequential \\nmodel: a linear stack of layers\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 247, 'page_label': '248'}, page_content='235Going beyond the Sequential model: the Keras functional API\\nSimilarly, some tasks need to predict multiple target attributes of input data. Given the\\ntext of a novel or short story, you might want to automatically classify it by genre (such\\nas romance or thriller) but also predict the approximate date it was written. Of course,\\nyou could train two separate models: one for the genre and one for the date. But\\nbecause these attributes aren’t statistically independent, you could build a better\\nmodel by learning to jointly predict both genre and date at th e same time. Such a\\njoint model would then have two outputs, or heads (see figure 7.3). Due to correla-\\ntions between genre and date, knowing the date of a novel would help the model\\nlearn rich, accurate representations of the space of novel genres, and vice versa.\\nAdditionally, many recently developed ne ural architectures require nonlinear net-\\nwork topology: networks structured as directed acyclic graphs. The Inception family of\\nnetworks (developed by Sz egedy et al. at Google), 1 for instance, relies on Inception\\nmodules, where the input is processed by several parallel convolutional branches whose\\noutputs are then merged back into a single tensor (see figure 7.4). There’s also the\\nrecent trend of adding residual connections to a model, which started with the ResNet\\nfamily of networks (developed by He et al. at Microsoft). 2 A residual connection con-\\nsists of reinjecting pr evious representations into the downstream flow of data by add-\\ning a past output tensor to a later output tensor (see figure 7.5), which helps prevent\\ninformation loss along the data-processing flow. There are many other examples of\\nsuch graph-like networks.\\n1 Christian Szegedy et al., “Going Deeper with Convolut ions,” Conference on Computer Vision and Pattern\\nRecognition (2014), https:/ /arxiv.org/abs/1409.4842.\\n2 Kaiming He et al., “Deep Residual Learning for Image Recognition,” Conference on Computer Vision and\\nPattern Recognition (2015), https:/ /arxiv.org/abs/1512.03385.\\nDate\\nDate\\nregressor\\nGenre\\nGenre\\nclassifier\\nText-processing\\nmodule\\nNovel text\\nFigure 7.3 A multi-output (or multihead) model\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 248, 'page_label': '249'}, page_content='236 CHAPTER 7 Advanced deep-learning best practices\\n \\nThese three important use cases—multi-input models, multi-output models, and\\ngraph-like models—aren’t possible when using only the Sequential model class in\\nKeras. But there’s another far more general and flexible way to use Keras: the func-\\ntional API. This section explains in detail what it is, what it can do, and how to use it.\\n7.1.1 Introduction to the functional API\\nIn the functional API, you directly manipulate tensors, and you use layers as functions\\nthat take tensors and return tensors (hence, the name functional API):\\nfrom keras import Input, layers\\ninput_tensor = Input(shape=(32,))\\nConv2D\\n3 × 3, strides=2\\nConv2D\\n3 × 3, strides=2\\nConv2D\\n3 × 3\\nConv2D\\n3 × 3\\nConv2D\\n1 × 1, strides=2\\nConv2D\\n1 × 1\\nAvgPool2D\\n3 × 3, strides=2\\nConv2D\\n1 × 1\\nConcatenate\\nOutput\\nInput\\nFigure 7.4 An Inception module: a subgraph of layers with several \\nparallel convolutional branches\\nLayer\\nResidual\\nconnection\\n+\\nLayer\\nLayer\\nLayer\\nFigure 7.5 A residual connection: \\nreinjection of prior information \\ndownstream via feature-map addition\\nA tensor\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 249, 'page_label': '250'}, page_content=\"237Going beyond the Sequential model: the Keras functional API\\ndense = layers.Dense(32, activation='relu')\\noutput_tensor = dense(input_tensor)\\nLet’s start with a minimal example that shows side by side a simple Sequential model\\nand its equivalent in the functional API:\\nfrom keras.models import Sequential, Model\\nfrom keras import layers\\nfrom keras import Input\\nseq_model = Sequential()\\nseq_model.add(layers.Dense(32, activation='relu', input_shape=(64,)))\\nseq_model.add(layers.Dense(32, activation='relu'))\\nseq_model.add(layers.Dense(10, activation='softmax'))\\ninput_tensor = Input(shape=(64,))\\nx = layers.Dense(32, activation='relu')(input_tensor)\\nx = layers.Dense(32, activation='relu')(x)\\noutput_tensor = layers.Dense(10, activation='softmax')(x)\\nmodel = Model(input_tensor, output_tensor)\\nmodel.summary()\\nThis is what the call to model.summary() displays:\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\ninput_1 (InputLayer) (None, 64) 0\\n_________________________________________________________________\\ndense_1 (Dense) (None, 32) 2080\\n_________________________________________________________________\\ndense_2 (Dense) (None, 32) 1056\\n_________________________________________________________________\\ndense_3 (Dense) (None, 10) 330\\n=================================================================\\nTotal params: 3,466\\nTrainable params: 3,466\\nNon-trainable params: 0\\n_________________________________________________________________\\nThe only part that may seem a bit magical at this point is instantiating a Model object\\nusing only an input tensor and an output tensor. Behind the scenes, Keras retrieves\\nevery layer involved in going from input_tensor to output_tensor, bringing them\\ntogether into a graph-like data structure—a Model. Of course, the reason it works is\\nthat output_tensor was obtained by repeatedly transforming input_tensor. If you\\ntried to build a model from inputs and outputs that weren’t related, you’d get a Run-\\ntimeError:\\n>>> unrelated_input = Input(shape=(32,))\\n>>> bad_model = model = Model(unrelated_input, output_tensor)\\nA layer is a function.\\nA layer may be called on a \\ntensor, and it returns a tensor.\\nSequential model, which \\nyou already know about\\nIts functional \\nequivalent\\nThe Model class turns an input tensor \\nand output tensor into a model.Let’s look at it!\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 250, 'page_label': '251'}, page_content='238 CHAPTER 7 Advanced deep-learning best practices\\nRuntimeError: Graph disconnected: cannot\\nobtain value for tensor\\n➥ Tensor(\"input_1:0\", shape=(?, 64), dtype=float32) at layer \"input_1\".\\nThis error tells you, in essenc e, that Keras couldn’t reach input_1 from the provided\\noutput tensor.\\n When it comes to compiling, training, or evaluating such an instance of Model, the\\nAPI is the same as that of Sequential:\\nmodel.compile(optimizer=\\'rmsprop\\', loss=\\'categorical_crossentropy\\')\\nimport numpy as np\\nx_train = np.random.random((1000, 64))\\ny_train = np.random.random((1000, 10))\\nmodel.fit(x_train, y_train, epochs=10, batch_size=128)\\nscore = model.evaluate(x_train, y_train)\\n7.1.2 Multi-input models\\nThe functional API can be used to build models that have multiple inputs. Typically,\\nsuch models at some point merge their different input branches using a layer that can\\ncombine several tensors: by adding them, concatenating them, and so on. This is usu-\\nally done via a Keras me rge operation such as keras.layers.add, keras.layers\\n.concatenate, and so on. Let’s look at a very si mple example of a multi-input model:\\na question-answering model.\\n A typical question-answering model has two inputs: a natural-language question\\nand a text snippet (such as a news articl e) providing information to be used for\\nanswering the question. The model must then produce an answer: in the simplest pos-\\nsible setup, this is a one- word answer obtained via a softmax over some predefined\\nvocabulary (see figure 7.6).\\nCompiles \\nthe modelGenerates dummy Numpy \\ndata to train on\\nTrains the model \\nfor 10 epochsEvaluates \\nthe model \\nDense\\nAnswer\\nConcatenate\\nLSTM LSTM\\nReference text Question\\nEmbedding Embedding\\nFigure 7.6 A question-answering model\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 251, 'page_label': '252'}, page_content=\"239Going beyond the Sequential model: the Keras functional API\\nFollowing is an example of how you can build such a model with the functional API.\\nYou set up two independent branches, encoding the text input and the question input\\nas representation vectors; then, concaten ate these vectors; and finally, add a softmax\\nclassifier on top of the concatenated representations.\\nfrom keras.models import Model\\nfrom keras import layers\\nfrom keras import Input\\ntext_vocabulary_size = 10000\\nquestion_vocabulary_size = 10000\\nanswer_vocabulary_size = 500\\ntext_input = Input(shape=(None,), dtype='int32', name='text')\\nembedded_text = layers.Embedding(\\n64, text_vocabulary_size)(text_input)\\nencoded_text = layers.LSTM(32)(embedded_text)\\nquestion_input = Input(shape=(None,),\\ndtype='int32',\\nname='question')\\nembedded_question = layers.Embedding(\\n32, question_vocabulary_size)(question_input)\\nencoded_question = layers.LSTM(16)(embedded_question)\\nconcatenated = layers.concatenate([encoded_text, encoded_question],\\naxis=-1)\\nanswer = layers.Dense(answer_vocabulary_size,\\nactivation='softmax')(concatenated)\\nmodel = Model([text_input, question_input], answer)\\nmodel.compile(optimizer='rmsprop',\\nloss='categorical_crossentropy',\\nmetrics=['acc'])\\nNow, how do you train this two-input model? There are two possible APIs: you can feed\\nthe model a list of Numpy arrays as inputs, or you can feed it a dictionary that maps\\ninput names to Numpy arrays. Naturally, the latter option is available only if you give\\nnames to your inputs.\\nimport numpy as np\\nnum_samples = 1000\\nmax_length = 100\\ntext = np.random.randint(1, text_vocabulary_size,\\nsize=(num_samples, max_length))\\nListing 7.1 Functional API implementation of a two-input question-answering model\\nListing 7.2 Feeding data to a multi-input model\\nThe text input is a variable-\\nlength sequence of integers.\\nNote that you can optionally\\nname the inputs.\\nEmbeds the inputs \\ninto a sequence of \\nvectors of size 64\\nEncodes the vectors in a \\nsingle vector via an LSTM\\nSame process (with different layer \\ninstances) for the question\\nConcatenates the encoded \\nquestion and encoded text\\nAdds a softmax \\nclassifier on top\\nAt model instantiation, you specify \\nthe two inputs and the output.\\nGenerates dummy\\nNumpy data\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 252, 'page_label': '253'}, page_content=\"240 CHAPTER 7 Advanced deep-learning best practices\\nquestion = np.random.randint(1, question_vocabulary_size,\\nsize=(num_samples, max_length))\\nanswers = np.random.randint(0, 1,\\nsize=(num_samples, answer_vocabulary_size))\\nmodel.fit([text, question], answers, epochs=10, batch_size=128)\\nmodel.fit({'text': text, 'question': question}, answers,\\nepochs=10, batch_size=128)\\n7.1.3 Multi-output models\\nIn the same way, you can use the functional API to build models with multiple outputs\\n(or multiple heads). A simple example is a network that attempts to simultaneously\\npredict different properties of  the data, such as a network that takes as input a series\\nof social media posts from a single anonymous person and tries to predict attributes of\\nthat person, such as age, gender, and income level (see figure 7.7).\\nfrom keras import layers\\nfrom keras import Input\\nfrom keras.models import Model\\nvocabulary_size = 50000\\nnum_income_groups = 10\\nposts_input = Input(shape=(None,), dtype='int32', name='posts')\\nembedded_posts = layers.Embedding(256, vocabulary_size)(posts_input)\\nx = layers.Conv1D(128, 5, activation='relu')(embedded_posts)\\nx = layers.MaxPooling1D(5)(x)\\nx = layers.Conv1D(256, 5, activation='relu')(x)\\nx = layers.Conv1D(256, 5, activation='relu')(x)\\nx = layers.MaxPooling1D(5)(x)\\nx = layers.Conv1D(256, 5, activation='relu')(x)\\nx = layers.Conv1D(256, 5, activation='relu')(x)\\nx = layers.GlobalMaxPooling1D()(x)\\nx = layers.Dense(128, activation='relu')(x)\\nage_prediction = layers.Dense(1, name='age')(x)\\nincome_prediction = layers.Dense(num_income_groups,\\nactivation='softmax',\\nname='income')(x)\\ngender_prediction = layers.Dense(1, activation='sigmoid', name='gender')(x)\\nmodel = Model(posts_input,\\n[age_prediction, income_prediction, gender_prediction])\\nListing 7.3 Functional API implementation of a three-output model\\nAnswers are one-\\nhot encoded,\\nnot integers\\nFitting using a list of inputs Fitting using a dictionary of\\ninputs (only if inputs are named)\\nNote that the output \\nlayers are given names.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 253, 'page_label': '254'}, page_content=\"241Going beyond the Sequential model: the Keras functional API\\nImportantly, training such a mo del requires the ability to specify different loss func-\\ntions for different heads of the network: for instance, age prediction is a scalar regres-\\nsion task, but gender predicti on is a binary classification  task, requiring a different\\ntraining procedure. But because gradient descent requires you to minimize a scalar,\\nyou must combine these losses into a single  value in order to train the model. The\\nsimplest way to combine different losses is to sum them all. In Keras, you can use\\neither a list or a dictionary of losses in compile to specify different objects for different\\noutputs; the resulting loss va lues are summed into a glob al loss, which is minimized\\nduring training.\\nmodel.compile(optimizer='rmsprop',\\nloss=['mse', 'categorical_crossentropy', 'binary_crossentropy'])\\nmodel.compile(optimizer='rmsprop',\\nloss={'age': 'mse',\\n'income': 'categorical_crossentropy',\\n'gender': 'binary_crossentropy'})\\nNote that very imbalanced loss contributi ons will cause the model representations to\\nbe optimized preferentially for the task with the largest individual loss, at the expense\\nof the other tasks. To remedy this, you can assign different levels of importance to the\\nloss values in their contribution to the final loss. This is useful in particular if the\\nlosses’ values use different scales. For instance, the mean squared error (\\nMSE) loss\\nused for the age-regression task typically takes a value around 3–5, whereas the cross-\\nentropy loss used for the gender-classification task can be as low as 0.1. In such a situa-\\ntion, to balance the contributi on of the different losses, yo u can assign a weight of 10\\nto the crossentropy loss and a weight of 0.25 to the MSE loss.\\nmodel.compile(optimizer='rmsprop',\\nloss=['mse', 'categorical_crossentropy', 'binary_crossentropy'],\\nloss_weights=[0.25, 1., 10.])\\nListing 7.4 Compilation options of a multi-output model: multiple losses\\nListing 7.5 Compilation options of a multi-output model: loss weighting\\nGender\\nSocial media posts\\nDense\\nIncome\\nDense\\nAge\\nDense\\n1D convnet\\nFigure 7.7 A social media \\nmodel with three heads\\nEquivalent (possible \\nonly if you give names \\nto the output layers)\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 254, 'page_label': '255'}, page_content=\"242 CHAPTER 7 Advanced deep-learning best practices\\nmodel.compile(optimizer='rmsprop',\\nloss={'age': 'mse',\\n'income': 'categorical_crossentropy',\\n'gender': 'binary_crossentropy'},\\nloss_weights={'age': 0.25,\\n'income': 1.,\\n'gender': 10.})\\nMuch as in the case of multi-input models, you can pass Numpy data to the model for\\ntraining either via a list of arrays or via a dictionary of arrays.\\nmodel.fit(posts, [age_targets, income_targets, gender_targets],\\nepochs=10, batch_size=64)\\nmodel.fit(posts, {'age': age_targets,\\n'income': income_targets,\\n'gender': gender_targets},\\nepochs=10, batch_size=64)\\n7.1.4 Directed acyclic graphs of layers\\nWith the functional API, not only can you build models with multiple inputs and mul-\\ntiple outputs, but you can also implement networks with a complex internal topology.\\nNeural networks in Keras are allowed to be arbitrary directed acyclic graphs of layers. The\\nqualifier acyclic is important: these graphs can’t have cycles. It’s impossible for a tensor\\nx to become the input of one of the layers that generated x. The only processing loops\\nthat are allowed (that is, recurrent connections) are those internal to recurrent layers.\\n Several common neural-network compon ents are implemented as graphs. Two\\nnotable ones are Inception mo dules and residual connecti ons. To better understand\\nhow the functional API can be used to build graphs of layers, let’s take a look at how\\nyou can implement both of them in Keras.\\nINCEPTION MODULES\\nInception3 is a popular type of network architecture for convolutional neural networks;\\nit was developed by Christian Szegedy and his colleagues at Google in 2013–2014,\\ninspired by the earlier network-in-network architecture.4 It consists of a stack of modules\\nthat themselves look like small independ ent networks, split into several parallel\\nbranches. The most basic form of an Ince ption module has three to four branches\\nstarting with a 1 × 1 convolution, followed by a 3 × 3 convolution, and ending with the\\nconcatenation of the resulting features. This  setup helps the network separately learn\\nListing 7.6 Feeding data to a multi-output model\\n3 https://arxiv.org/abs/1409.4842.\\n4 Min Lin, Qiang Chen, and Shuicheng Yan, “Network in Network,” International Conference on Learning\\nRepresentations (2013), https:/ /arxiv.org/abs/1312.4400.\\nEquivalent (possible \\nonly if you give names \\nto the output layers)\\nage_targets, income_targets, and \\ngender_targets are assumed to be \\nNumpy arrays.\\nEquivalent (possible only if you \\ngive names to the output layers) \\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 255, 'page_label': '256'}, page_content='243Going beyond the Sequential model: the Keras functional API\\nspatial features and channel-wise features, which is more efficient than learning them\\njointly. More-complex versions of an Ince ption module are also  possible, typically\\ninvolving pooling operations, different spatial convolution sizes (for example, 5 × 5\\ninstead of 3 × 3 on some branches), and br anches without a spatial convolution (only\\na 1 × 1 convolution). An example of such  a module, taken from Inception V3, is\\nshown in figure 7.8.\\nHere’s how you’d implement the module feat ured in figure 7.8 using the functional\\nAPI. This example assumes the existence of a 4D input tensor x:\\nConv2D\\n3 × 3, strides=2\\nConv2D\\n3 × 3, strides=2\\nConv2D\\n3 × 3\\nConv2D\\n3 × 3\\nConv2D\\n1 × 1, strides=2\\nConv2D\\n1 × 1\\nAvgPool2D\\n3 × 3, strides=2\\nConv2D\\n1 × 1\\nConcatenate\\nOutput\\nInput\\nFigure 7.8 An Inception \\nmodule\\nThe purpose of 1 × 1 convolutions\\nYou already know that convol utions extract spatial patc hes around every tile in an\\ninput tensor and apply the same transformation to each patch. An edge case is when\\nthe patches extracted consist of a single tile. The convolution operation then\\nbecomes equivalent to running each tile vector through a Dense layer: it will compute\\nfeatures that mix together information from  the channels of the input tensor, but it\\nwon’t mix information across space (because it’s looking at one tile at a time). Such\\n1 × 1 convolutions (also called pointwise convolutions) are featured in Inception mod-\\nules, where they contribute to factoring out channel-wise feature learning and space-\\nwise feature learning—a reasonable thing to do if you assume that each channel is\\nhighly autocorrelated across space, but different channels may not be highly cor-\\nrelated with each other.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 256, 'page_label': '257'}, page_content=\"244 CHAPTER 7 Advanced deep-learning best practices\\nfrom keras import layers\\nbranch_a = layers.Conv2D(128, 1,\\nactivation='relu', strides=2)(x)\\nbranch_b = layers.Conv2D(128, 1, activation='relu')(x)\\nbranch_b = layers.Conv2D(128, 3, activation='relu', strides=2)(branch_b)\\nbranch_c = layers.AveragePooling2D(3, strides=2)(x)\\nbranch_c = layers.Conv2D(128, 3, activation='relu')(branch_c)\\nbranch_d = layers.Conv2D(128, 1, activation='relu')(x)\\nbranch_d = layers.Conv2D(128, 3, activation='relu')(branch_d)\\nbranch_d = layers.Conv2D(128, 3, activation='relu', strides=2)(branch_d)\\noutput = layers.concatenate(\\n[branch_a, branch_b, branch_c, branch_d], axis=-1)\\nNote that the full Inception V3 architecture is available in Keras as keras.applications\\n.inception_v3.InceptionV3, including weights pretrained on the ImageNet dataset.\\nAnother closely related model available as part of the Keras applications module is\\nXception.5 Xception, which stands for extreme inception, is a convnet architecture loosely\\ninspired by Inception. It takes the idea of separating the learning of channel-wise and\\nspace-wise features to its logical extreme, and replaces Inception modules with depth-\\nwise separable convolutions consisting of a depthwise convolution (a spatial convolu-\\ntion where every input channel is handle d separately) followed by a pointwise\\nconvolution (a 1 × 1 convolution)—effectively, an extreme form of an Inception mod-\\nule, where spatial features and channel-wise features are fully separated. Xception has\\nroughly the same number of  parameters as Inception V3, but it shows better runtime\\nperformance and higher accura cy on ImageNet as well as  other large-scale datasets,\\ndue to a more efficient use of model parameters. \\nRESIDUAL CONNECTIONS\\nResidual connections are a common graph-like network component found in many post-\\n2015 network architectures, including Xception. They were introduced by He et al.\\nfrom Microsoft in their winning entry in the \\nILSVRC ImageNet challenge in late 2015.6\\nThey tackle two common pr oblems that plague any larg e-scale deep-learning model:\\nvanishing gradients and representational bottlenecks. In general, adding residual con-\\nnections to any model that has more than 10 layers is likely to be beneficial.\\n5 François Chollet, “Xception: Deep Learning with De pthwise Separable Convolutions,” Conference on Com-\\nputer Vision and Pattern Recognition (2017), https:/ /arxiv.org/abs/1610.02357.\\n6 He et al., “Deep Residual Learning for Image Recognition,” https://arxiv.org/abs/1512.03385.\\nEvery branch has the same stride value (2), \\nwhich is necessary to keep all branch outputs \\nthe same size so you can concatenate them.\\nIn this branch, the striding occurs\\nin the spatial convolution layer.\\nIn this branch, the striding occurs \\nin the average pooling layer.\\nConcatenates the \\nbranch outputs to \\nobtain the module \\noutput\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 257, 'page_label': '258'}, page_content=\"245Going beyond the Sequential model: the Keras functional API\\n A residual connection consists of making the output of an earlier layer available as\\ninput to a later layer, effectively creating a shortcut in a sequen tial network. Rather\\nthan being concatenated to the later activation, the earlier output is summed with the\\nlater activation, which assumes that both activations are the same size. If they’re differ-\\nent sizes, you can use a linear transformation to reshape the earlier activation into the\\ntarget shape (for example, a Dense layer without an activati on or, for convolutional\\nfeature maps, a 1 × 1 convolution without an activation).\\n Here’s how to implement a residual co nnection in Keras when the feature-map\\nsizes are the same, using identity residual connections. This example assumes the exis-\\ntence of a 4D input tensor x:\\nfrom keras import layers\\nx = ...\\ny = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\\ny = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\\ny = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\\ny = layers.add([y, x])\\nAnd the following implements a residual connection when the feature-map sizes dif-\\nfer, using a linear residual connectio n (again, assuming the existence of a 4D input\\ntensor x):\\nfrom keras import layers\\nx = ...\\ny = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\\ny = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\\ny = layers.MaxPooling2D(2, strides=2)(y)\\nresidual = layers.Conv2D(128, 1, strides=2, padding='same')(x)\\ny = layers.add([y, residual])\\nApplies a transformation to x\\nAdds the original x back to \\nthe output features\\nUses a 1 × 1 convolution to\\nlinearly downsample the original\\nx tensor to the same shape as y\\nAdds the residual tensor \\nback to the output features\\nRepresentational bottlenecks in deep learning\\nIn a Sequential model, each successive representation layer is built on top of the\\nprevious one, which means it only has access to information contained in the activa-\\ntion of the previous layer. If one layer is too small (for example, it has features that\\nare too low-dimensional), then the model will be constr ained by how much informa-\\ntion can be crammed into the activations of this layer.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 258, 'page_label': '259'}, page_content='246 CHAPTER 7 Advanced deep-learning best practices\\n7.1.5 Layer weight sharing\\nOne more important feature of the functional API is the ability to reuse a layer\\ninstance several times. When you call a layer instance twice, instead of instantiating a\\nnew layer for each call, you reuse the same weights with every call. This allows you to\\nbuild models that have shared branches—s everal branches that all share the same\\nknowledge and perform the same operations. That is, they share the same representa-\\ntions and learn these representations simultaneously for different sets of inputs.\\n For example, consider a model that a ttempts to assess the semantic similarity\\nbetween two sentences. The model has two inputs (the two sentences to compare)\\nand outputs a score between 0 and 1, where 0 means unrelated sentences and 1 means\\nsentences that are either identical or reformulations of each other. Such a model\\ncould be useful in many applications, in cluding deduplicating natural-language que-\\nries in a dialog system.\\n In this setup, the two input sentences ar e interchangeable, because semantic simi-\\nlarity is a symmetrical relationship: the simila rity of A to B is identical to the similarity\\nof \\nB to A. For this reason, it wouldn’t make sense to learn two independent models for\\n(continued)\\nYou can grasp this concept with a signal-processing analogy: if you have an audio-\\nprocessing pipeline that consists of a series of operations, each of which takes as\\ninput the output of the previous operation, then if one operation crops your signal to\\na low-frequency range (for example, 0–15 kHz), the operations downstream will never\\nbe able to recover the dropped frequencies. Any loss of information is permanent.\\nResidual connections, by reinjecting earlier information downstream, partially solve\\nthis issue for deep-learning models.\\nVanishing gradients in deep learning\\nBackpropagation, the master algorithm used to train deep neural networks, works by\\npropagating a feedback signal from the output loss down to earlier layers. If this feed-\\nback signal has to be propagated through a deep stack of layers, the signal may\\nbecome tenuous or even be lost entirely, rendering the network untrainable. This\\nissue is known as vanishing gradients.\\nThis problem occurs both with deep networks and with recurrent networks over very\\nlong sequences—in both cases, a feedback signal must be propagated through a\\nlong series of operations. You’re already familiar with the solution that the \\nLSTM layer\\nuses to address this problem in recurrent networks: it introduces a carry track that\\npropagates information parallel to the main processing track. Residual connections\\nwork in a similar way in feedforward deep networks, but they’re even simpler: they\\nintroduce a purely linear information carry track parallel to the main layer stack, thus\\nhelping to propagate gradients through arbitrarily deep stacks of layers. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 259, 'page_label': '260'}, page_content=\"247Going beyond the Sequential model: the Keras functional API\\nprocessing each input sent ence. Rather, you want to process both with a single LSTM\\nlayer. The representations of this LSTM layer (its weights) are learned based on both\\ninputs simultaneously. This is what we call a Siamese LSTM model or a shared LSTM.\\n Here’s how to implement such a model using layer sharing (layer reuse) in the\\nKeras functional API:\\nfrom keras import layers\\nfrom keras import Input\\nfrom keras.models import Model\\nlstm = layers.LSTM(32)\\nleft_input = Input(shape=(None, 128))\\nleft_output = lstm(left_input)\\nright_input = Input(shape=(None, 128))\\nright_output = lstm(right_input)\\nmerged = layers.concatenate([left_output, right_output], axis=-1)\\npredictions = layers.Dense(1, activation='sigmoid')(merged)\\nmodel = Model([left_input, right_input], predictions)\\nmodel.fit([left_data, right_data], targets)\\nNaturally, a layer instance ma y be used more than once—it can be called arbitrarily\\nmany times, reusing the same set of weights every time. \\n7.1.6 Models as layers\\nImportantly, in the functional API, models can be used as you’d use layers—effectively,\\nyou can think of a model as a “bigger layer.” This is true of both the Sequential and\\nModel classes. This means you can call a model on an input tensor and retrieve an out-\\nput tensor:\\ny = model(x)\\nIf the model has multiple input tensors an d multiple output te nsors, it should be\\ncalled with a list of tensors:\\ny1, y2 = model([x1, x2])\\nWhen you call a model instance, you’re reusing the weights of the model—exactly like\\nwhat happens when you call a layer instance. Calling an in stance, whether it’s a layer\\ninstance or a model instance, will always re use the existing learned representations of\\nthe instance—which is intuitive.\\n One simple practical example of what you can build by reusing a model instance is\\na vision model that uses a dual camera as its input: two parallel cameras, a few centi-\\nmeters (one inch) apart. Su ch a model can perceive dept h, which can be useful in\\nmany applications. You shouldn’t need tw o independent models to extract visual\\nInstantiates a single \\nLSTM layer, once\\nBuilding the left branch of the \\nmodel: inputs are variable-length \\nsequences of vectors of size 128.\\nBuilding the right branch of the model: \\nwhen you call an existing layer \\ninstance, you reuse its weights.\\nBuilds the classifier on top Instantiating and training the model: when you\\ntrain such a model, the weights of the LSTM layer\\nare updated based on both inputs.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 260, 'page_label': '261'}, page_content='248 CHAPTER 7 Advanced deep-learning best practices\\nfeatures from the left camera and the ri ght camera before merging the two feeds.\\nSuch low-level processing can be shared across the two inpu ts: that is, done via layers\\nthat use the same weights and thus share th e same representations. Here’s how you’d\\nimplement a Siamese vision model (shared convolutional base) in Keras:\\nfrom keras import layers\\nfrom keras import applications\\nfrom keras import Input\\nxception_base = applications.Xception(weights=None,\\ninclude_top=False)\\nleft_input = Input(shape=(250, 250, 3))\\nright_input = Input(shape=(250, 250, 3))\\nleft_features = xception_base(left_input)\\nright_input = xception_base(right_input)\\nmerged_features = layers.concatenate(\\n[left_features, right_input], axis=-1)\\n7.1.7 Wrapping up\\nThis concludes our introducti on to the Keras functional API—an essential tool for\\nbuilding advanced deep neural network architectures. Now you know the following:\\n\\uf0a1 To step out of the Sequential API whenever you need anything more than a lin-\\near stack of layers\\n\\uf0a1 How to build Keras models with severa l inputs, several outputs, and complex\\ninternal network topology, using the Keras functional API\\n\\uf0a1 How to reuse the weights of a layer or model across di fferent processing\\nbranches, by calling the same layer or model instance several times \\nThe base image-processing\\nmodel is the Xception network\\n(convolutional base only).\\nThe inputs are 250 × 250 \\nRGB images.\\nCalls the same vision \\nmodel twice\\nThe merged features contain \\ninformation from the right visual \\nfeed and the left visual feed. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 261, 'page_label': '262'}, page_content='249Inspecting and monitoring deep-learning models using Keras callbacks and TensorBoard\\n7.2 Inspecting and monitoring deep-learning models using \\nKeras callbacks and TensorBoard\\nIn this section, we’ll review ways to gain  greater access to and control over what goes\\non inside your model during training. Launching a training run on a large dataset for\\ntens of epochs using model.fit() or model.fit_generator() can be a bit like\\nlaunching a paper airplane: past the initia l impulse, you don’t have any control over\\nits trajectory or its landing spot. If you want to avoid bad outcomes (and thus wasted\\npaper airplanes), it’s smarter to use not a paper plane, but a drone that can sense its\\nenvironment, send data back to its oper ator, and automatically make steering deci-\\nsions based on its current state. The techniques we present here will transform the call\\nto model.fit() from a paper airplane into a smart , autonomous drone that can self-\\nintrospect and dynamically take action.\\n7.2.1 Using callbacks to act on a model during training\\nWhen you’re training a model, there are many things you can’t predict from the start.\\nIn particular, you can’t tell how many epochs will be needed to get to an optimal vali-\\ndation loss. The examples so far have ad opted the strategy of training for enough\\nepochs that you begin overfitting, using the first run to figure out the proper number\\nof epochs to train for, and then finally launching a new training run from scratch\\nusing this optimal number. Of course, this approach is wasteful.\\n A much better way to handle this is to stop training when you measure that the val-\\nidation loss in no longer improving. This can be achieved using a Keras callback. A\\ncallback is an object (a class instance implemen ting specific methods) that is passed to\\nthe model in the call to \\nfit and that is called by the model at various points during\\ntraining. It has access to all the available data about the state of the model and its per-\\nformance, and it can take action: interrupt training, save a model, load a different\\nweight set, or otherwise alter the state of the model.\\n Here are some examples of ways you can use callbacks:\\n\\uf0a1 Model checkpointing—Saving the current weights of the model at different points\\nduring training.\\n\\uf0a1 Early stopping —Interrupting training when the validation loss is no longer\\nimproving (and of course, saving the best model obtained during training).\\n\\uf0a1 Dynamically adjusting the value of certain parameters during training —Such as the\\nlearning rate of the optimizer.\\n\\uf0a1 Logging training and validation metrics duri ng training, or visualizing the representa-\\ntions learned by the model as they’re updated —The Keras progress bar that you’re\\nfamiliar with is a callback!\\nThe keras.callbacks module includes a number of bu ilt-in callbacks (this is not an\\nexhaustive list):\\nkeras.callbacks.ModelCheckpoint\\nkeras.callbacks.EarlyStopping\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 262, 'page_label': '263'}, page_content=\"250 CHAPTER 7 Advanced deep-learning best practices\\nkeras.callbacks.LearningRateScheduler\\nkeras.callbacks.ReduceLROnPlateau\\nkeras.callbacks.CSVLogger\\nLet’s review a few of them to give you an idea of how to use them: ModelCheckpoint,\\nEarlyStopping, and ReduceLROnPlateau.\\nTHE MODELCHECKPOINT AND EARLYSTOPPING CALLBACKS\\nYou can use the EarlyStopping callback to interrupt training once a target metric\\nbeing monitored has stopped improving for a fixed number of epochs. For instance,\\nthis callback allows you to interrupt trai ning as soon as you start overfitting, thus\\navoiding having to retrain your model for a smaller number of epochs. This callback is\\ntypically used in combination with ModelCheckpoint, which lets you continually save\\nthe model during training (and, optionally, save only the current best model so far:\\nthe version of the model that achieved the best performance at the end of an epoch):\\nimport keras\\ncallbacks_list = [\\nkeras.callbacks.EarlyStopping(\\nmonitor='acc',\\npatience=1,\\n),\\nkeras.callbacks.ModelCheckpoint(\\nfilepath='my_model.h5',\\nmonitor='val_loss',\\nsave_best_only=True,\\n)\\n]\\nmodel.compile(optimizer='rmsprop',\\nloss='binary_crossentropy',\\nmetrics=['acc'])\\nmodel.fit(x, y,\\nepochs=10,\\nbatch_size=32,\\ncallbacks=callbacks_list,\\nvalidation_data=(x_val, y_val))\\nTHE REDUCELRONPLATEAU CALLBACK\\nYou can use this callback to reduce the learning rate when the validation loss has\\nstopped improving. Reducing or increasing the learning rate in case of a loss plateau is\\nis an effective strategy to get out of local minima during training. The following exam-\\nple uses the ReduceLROnPlateau callback:\\n \\n \\nCallbacks are passed to the model via the \\ncallbacks argument in fit, which takes a list of \\ncallbacks. You can pass any number of callbacks.\\nInterrupts training when \\nimprovement stops\\nMonitors the model’s \\nvalidation accuracy\\nInterrupts training when \\naccuracy has stopped \\nimproving for more than one \\nepoch (that is, two epochs)\\nSaves the current weights after every epoch\\nPath to the destination model file\\nThese two arguments mean you won’t overwrite the \\nmodel file unless val_loss has improved, which allows \\nyou to keep the best model seen during training.\\nYou monitor accuracy, so it should \\nbe part of the model’s metrics.\\nNote that because the callback will \\nmonitor validation loss and \\nvalidation accuracy, you need to pass \\nvalidation_data to the call to fit. \\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 263, 'page_label': '264'}, page_content=\"251Inspecting and monitoring deep-learning models using Keras callbacks and TensorBoard\\ncallbacks_list = [\\nkeras.callbacks.ReduceLROnPlateau(\\nmonitor='val_loss'\\nfactor=0.1,\\npatience=10,\\n)\\n]\\nmodel.fit(x, y,\\nepochs=10,\\nbatch_size=32,\\ncallbacks=callbacks_list,\\nvalidation_data=(x_val, y_val))\\nWRITING YOUR OWN CALLBACK\\nIf you need to take a specific action during  training that isn’t covered by one of the\\nbuilt-in callbacks, you can write your own callback. Callbacks are implemented by sub-\\nclassing the class keras.callbacks.Callback. You can then implement any number\\nof the following transparently named meth ods, which are called at various points\\nduring training:\\non_epoch_begin\\non_epoch_end\\non_batch_begin\\non_batch_end\\non_train_begin\\non_train_end\\nThese methods all are called with a logs argument, which is a dictionary containing\\ninformation about the previous batch, epoch,  or training run: training and validation\\nmetrics, and so on. Additionally, the callback has access to the following attributes:\\n\\uf0a1 self.model—The model instance from which the callback is being called\\n\\uf0a1 self.validation_data—The value of what was passed to fit as validation data\\nHere’s a simple example of a custom callback that saves to disk (as Numpy arrays) the\\nactivations of every layer of the model at the end of every epoch, computed on the\\nfirst sample of the validation set:\\nimport keras\\nimport numpy as np\\nclass ActivationLogger(keras.callbacks.Callback):\\ndef set_model(self, model):\\nself.model = model\\nlayer_outputs = [layer.output for layer in model.layers]\\nself.activations_model = keras.models.Model(model.input,\\nlayer_outputs)\\ndef on_epoch_end(self, epoch, logs=None):\\nif self.validation_data is None:\\nraise RuntimeError('Requires validation_data.')\\nMonitors the model’s \\nvalidation loss\\nDivides the learning rate by 10 when triggered\\nThe callback is triggered after the validation \\nloss has stopped improving for 10 epochs.\\nBecause the callback will \\nmonitor the validation loss, you \\nneed to pass validation_data to \\nthe call to fit. \\nCalled at the start of every epoch\\nCalled at the end of every epoch\\nCalled right before processing each batch\\nCalled right after processing each batch\\nCalled at the start of training\\nCalled at the end of training\\nCalled by the parent model \\nbefore training, to inform \\nthe callback of what model \\nwill be calling it\\nModel instance\\nthat returns the\\nactivations of\\nevery layer\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 264, 'page_label': '265'}, page_content=\"252 CHAPTER 7 Advanced deep-learning best practices\\nvalidation_sample = self.validation_data[0][0:1]\\nactivations = self.activations_model.predict(validation_sample)\\nf = open('activations_at_epoch_' + str(epoch) + '.npz', 'w')\\nnp.savez(f, activations)\\nf.close()\\nThis is all you need to know about callbacks—the rest is technical details, which you\\ncan easily look up. Now you’re equipped to  perform any sort of logging or prepro-\\ngrammed intervention on a Keras model during training. \\n7.2.2 Introduction to TensorBoard: \\nthe TensorFlow visualization framework\\nTo do good research or develop good models, you need rich, frequent feedback about\\nwhat’s going on inside your models during your experiments. That’s the point of run-\\nning experiments: to get information about how well a model performs—as much\\ninformation as possible. Making progress is an iterative process, or loop: you start with\\nan idea and express it as an experiment, attempting to validate or invalidate your idea.\\nYou run this experiment and process the info rmation it generates. This inspires your\\nnext idea. The more iterations  of this loop you’re able to run, the more refined and\\npowerful your ideas become. Keras helps you go from idea to experiment in the least\\npossible time, and fast GPUs can help you get from experi ment to result as quickly as\\npossible. But what about processing the ex periment results? That’s where Tensor-\\nBoard comes in.\\nThis section introduces TensorBoard, a br owser-based visualizat ion tool that comes\\npackaged with TensorFlow. Note that it’s only available for Keras models when you’re\\nusing Keras with the TensorFlow backend.\\n The key purpose of TensorBoard is to he lp you visually monitor everything that\\ngoes on inside your model during training . If you’re monitori ng more information\\nthan just the model’s final loss, you can de velop a clearer vision of what the model\\ndoes and doesn’t do, and you can make progress more quickly. TensorBoard gives you\\naccess to several neat features, all in your browser:\\nObtains the first input sample \\nof the validation data Saves arrays to disk\\nIdea\\nVisualization\\nframework:\\nTensorBoard\\nDeep-learning\\nframework:\\nKeras\\nInfrastructure\\nResults Experiment\\nFigure 7.9 The loop of progress\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 265, 'page_label': '266'}, page_content=\"253Inspecting and monitoring deep-learning models using Keras callbacks and TensorBoard\\n\\uf0a1 Visually monitoring metrics during training\\n\\uf0a1 Visualizing your model architecture\\n\\uf0a1 Visualizing histograms of activations and gradients\\n\\uf0a1 Exploring embeddings in 3D\\nLet’s demonstrate these features on a simple example. You’ll train a 1D convnet on\\nthe IMDB sentiment-analysis task.\\n The model is similar to the one you saw in the last section of chapter 6. You’ll con-\\nsider only the top 2,000 words in the IMDB vocabulary, to make visualizing word\\nembeddings more tractable.\\nimport keras\\nfrom keras import layers\\nfrom keras.datasets import imdb\\nfrom keras.preprocessing import sequence\\nmax_features = 2000\\nmax_len = 500\\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\\nx_train = sequence.pad_sequences(x_train, maxlen=max_len)\\nx_test = sequence.pad_sequences(x_test, maxlen=max_len)\\nmodel = keras.models.Sequential()\\nmodel.add(layers.Embedding(max_features, 128,\\ninput_length=max_len,\\nname='embed'))\\nmodel.add(layers.Conv1D(32, 7, activation='relu'))\\nmodel.add(layers.MaxPooling1D(5))\\nmodel.add(layers.Conv1D(32, 7, activation='relu'))\\nmodel.add(layers.GlobalMaxPooling1D())\\nmodel.add(layers.Dense(1))\\nmodel.summary()\\nmodel.compile(optimizer='rmsprop',\\nloss='binary_crossentropy',\\nmetrics=['acc'])\\nBefore you start using TensorBoard, you need  to create a directory where you’ll store\\nthe log files it generates.\\n$ mkdir my_log_dir\\nLet’s launch the training with a TensorBoard callback instance. This callback will write\\nlog events to disk at the specified location.\\nListing 7.7 Text-classification model to use with TensorBoard\\nListing 7.8 Creating a directory for TensorBoard log files\\nNumber of words to \\nconsider as features\\nCuts off texts after this number \\nof words (among max_features \\nmost common words)\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 266, 'page_label': '267'}, page_content=\"254 CHAPTER 7 Advanced deep-learning best practices\\n \\ncallbacks = [\\nkeras.callbacks.TensorBoard(\\nlog_dir='my_log_dir',\\nhistogram_freq=1,\\nembeddings_freq=1,\\n)\\n]\\nhistory = model.fit(x_train, y_train,\\nepochs=20,\\nbatch_size=128,\\nvalidation_split=0.2,\\ncallbacks=callbacks)\\nAt this point, you can launch the Te nsorBoard server fr om the command line,\\ninstructing it to read the logs the callback is currently writing. The tensorboard utility\\nshould have been automatically installed on your machine the moment you installed\\nTensorFlow (for example, via pip):\\n$ tensorboard --logdir=my_log_dir\\nYou can then browse to http://localhost:6 006 and look at your model training (see\\nfigure 7.10). In addition to live graphs of  the training and validation metrics, you get\\naccess to the Histograms tab, where you can find pretty visualizations of histograms of\\nactivation values taken by your layers (see figure 7.11).\\nListing 7.9 Training the model with a TensorBoard callback\\nLog files will be written \\nat this location.\\nRecords activation histograms every 1 epoch\\nRecords embedding \\ndata every 1 epoch\\nFigure 7.10 TensorBoard: metrics monitoring\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 267, 'page_label': '268'}, page_content='255Inspecting and monitoring deep-learning models using Keras callbacks and TensorBoard\\nThe Embeddings tab gives you a way to in spect the embedding locations and spatial\\nrelationships of the 10,000 words in the in put vocabulary, as learned by the initial\\nEmbedding layer. Because the embedding space is 128-dimensional, TensorBoard auto-\\nmatically reduces it to 2D or 3D using a dimensionality-reduction algorithm of your\\nchoice: either principal component analysis (PCA) or t-distributed stochastic neighbor\\nembedding (t-SNE). In figure 7.12, in the point cloud, you can clearly see two clusters:\\nwords with a positive connotation and words with a negative connotation. The visual-\\nization makes it immediately obvious that embeddings trained jointly with a specific\\nobjective result in mo dels that are completely specific  to the underlying task—that’s\\nthe reason using pretrained generic word embeddings is rarely a good idea.\\nFigure 7.11 TensorBoard: activation histograms\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 268, 'page_label': '269'}, page_content='256 CHAPTER 7 Advanced deep-learning best practices\\n \\nThe Graphs tab shows an interactive visualization of the graph of low-level TensorFlow\\noperations underlying your Keras model (see figure 7.13). As you can see, there’s a lot\\nmore going on than you would expect. Th e model you just built may look simple\\nwhen defined in Keras—a small stack of basic layers—but under the hood, you need\\nto construct a fairly complex graph structure to make it work. A lot of it is related to\\nthe gradient-descent process.  This complexity differential between what you see and\\nwhat you’re manipulating is the key motivation for using Keras as your way of building\\nmodels, instead of working with raw Tensor Flow to define ever ything from scratch.\\nKeras makes your workflow dramatically simpler.\\nFigure 7.12 TensorBoard: interactive 3D word-embedding visualization\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 269, 'page_label': '270'}, page_content=\"257Inspecting and monitoring deep-learning models using Keras callbacks and TensorBoard\\n \\nNote that Keras also provides  another, cleaner way to plot models as graphs of layers\\nrather than graphs of Tensor Flow operations: the utility keras.utils.plot_model.\\nUsing it requires that you’ve installed the Python pydot and pydot-ng libraries as well\\nas the graphviz library. Let’s take a quick look:\\nfrom keras.utils import plot_model\\nplot_model(model, to_file='model.png')\\nThis creates the PNG image shown in figure 7.14.\\nFigure 7.13 TensorBoard: TensorFlow graph visualization\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 270, 'page_label': '271'}, page_content=\"258 CHAPTER 7 Advanced deep-learning best practices\\n \\nYou also have the option of displaying sh ape information in the graph of layers. This\\nexample visualizes model topology using plot_model and the show_shapes option\\n(see figure 7.15):\\nfrom keras.utils import plot_model\\nplot_model(model, show_shapes=True, to_file='model.png')\\nFigure 7.14 A model plot as a graph of layers, \\ngenerated with plot_model\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 271, 'page_label': '272'}, page_content='259Inspecting and monitoring deep-learning models using Keras callbacks and TensorBoard\\n7.2.3 Wrapping up\\n\\uf0a1 Keras callbacks provide a simple way to  monitor models during training and\\nautomatically take action based on the state of the model.\\n\\uf0a1 When you’re using TensorFlow, TensorBo ard is a great way to visualize model\\nactivity in your browser. You can use it in Keras models via the TensorBoard call-\\nback. \\nFigure 7.15 A model plot with shape information\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 272, 'page_label': '273'}, page_content='260 CHAPTER 7 Advanced deep-learning best practices\\n7.3 Getting the most out of your models\\nTrying out architectures blindly works well  enough if you just need something that\\nworks okay. In this section, we’ll go beyond “works okay” to “works great and wins\\nmachine-learning competitions” by offering  you a quick guide to a set of must-know\\ntechniques for building state-of-the-art deep-learning models.\\n7.3.1 Advanced architecture patterns\\nWe covered one important design pattern in  detail in the previous section: residual\\nconnections. There are two more design patt erns you should know about: normaliza-\\ntion and depthwise separable convolution.  These patterns are especially relevant\\nwhen you’re building high-performing de ep convnets, but they’re commonly found\\nin many other types of architectures as well.\\nBATCH NORMALIZATION\\nNormalization is a broad category of methods that  seek to make different samples seen\\nby a machine-learning model more similar to each other, which helps the model learn\\nand generalize well to new data. The most common form of data normalization is one\\nyou’ve seen several times in this book alread y: centering the data on 0 by subtracting\\nthe mean from the data, and giving the data a unit standard deviation by dividing the\\ndata by its standard deviatio n. In effect, this makes the assumption that the data fol-\\nlows a normal (or Gaussian) distribution an d makes sure this distribution is centered\\nand scaled to unit variance:\\nnormalized_data = (data - np.mean(data, axis=...)) / np.std(data, axis=...)\\nPrevious examples normalized data before feeding it into models. But data normaliza-\\ntion should be a concern after every transf ormation operated by the network: even if\\nthe data entering a Dense or Conv2D network has a 0 mean and unit variance, there’s\\nno reason to expect a priori that this will be the case for the data coming out.\\n Batch normalization is a type of layer ( BatchNormalization in Keras) introduced\\nin 2015 by Ioffe and Szegedy; 7 it can adaptively normalize data even as the mean and\\nvariance change over time during training. It works by internally maintaining an expo-\\nnential moving average of the batch-wise me an and variance of the data seen during\\ntraining. The main effect of batch normalizat ion is that it helps with gradient propa-\\ngation—much like residual connections—and  thus allows for deeper networks. Some\\nvery deep networks can only be trained if they include multiple BatchNormalization\\nlayers. For instance, BatchNormalization is used liberally in many of the advanced\\nconvnet architectures that come packaged with Keras, such as ResNet50, Inception\\nV3, and Xception.\\n \\n7 Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing\\nInternal Covariate Shift,” Proceedings of the 32nd  International Conference on Machine Learning  (2015),\\nhttps:/ /arxiv.org/abs/1502.03167.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 273, 'page_label': '274'}, page_content='261Getting the most out of your models\\n The BatchNormalization layer is typically used after a convolutional or densely\\nconnected layer:\\nconv_model.add(layers.Conv2D(32, 3, activation=\\'relu\\'))\\nconv_model.add(layers.BatchNormalization())\\ndense_model.add(layers.Dense(32, activation=\\'relu\\'))\\ndense_model.add(layers.BatchNormalization())\\nThe BatchNormalization layer takes an axis argument, which specifies the feature\\naxis that should be normalized. This argument defaults to -1, the last axis in the input\\ntensor. This is the correct value when using Dense layers, Conv1D layers, RNN layers,\\nand Conv2D layers with data_format set to \"channels_last\". But in the niche use case\\nof Conv2D layers with data_format set to \"channels_first\", the features axis is axis 1;\\nthe axis argument in BatchNormalization should accordingly be set to 1.\\nDEPTHWISE SEPARABLE CONVOLUTION\\nWhat if I told you that there’s a layer you can use as a drop-in replacement for Conv2D\\nthat will make your model lighter (fewer  trainable weight parameters) and faster\\n(fewer floating-point operations) and cause it to perform a few percentage points bet-\\nter on its task? That is precisely what the depthwise separable convolution  layer does\\n(SeparableConv2D). This layer performs a spatial convolution on each channel of its\\ninput, independently, before mixing output  channels via a pointwise convolution (a\\n1 × 1 convolution), as shown in  figure 7.16. This is equiva lent to separating the learn-\\ning of spatial features and the learning of  channel-wise features, which makes a lot of\\nsense if you assume that spatial locations in the input are highly correlated, but differ-\\nent channels are fairly independent. It re quires significantly fewer parameters and\\ninvolves fewer computations, thus resulting in smaller, sp eedier models. And because\\nit’s a more representationally efficient way to perform convolution,  it tends to learn\\nbetter representations using less data, resulting in better-performing models.\\nAfter a Conv layer\\nAfter a Dense layer\\nBatch renormalization\\nA recent improvement over regular batch normalization is batch renormalization, intro-\\nduced by Ioffe in 2017.a It offers clears benefits over batch normalization, at no appar-\\nent cost. At the time of writing, it’s too early to tell whether it will supplant batch\\nnormalization—but I think it’s likely. Even more recently, Klambauer et al. introduced\\nself-normalizing neural networks,b which manage to keep data normalized after going\\nthrough any Dense layer by using a specific activation function (selu) and a specific ini-\\ntializer (lecun_normal). This scheme, although highly interesting, is limited to densely\\nconnected networks for now, and its usefulness hasn’t yet been broadly replicated. \\na Sergey Ioffe, “Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-\\nNormalized Models” (2017), https://arxiv.org/abs/1702.03275.\\nb Günter Klambauer et al., “Self-Normalizing Neural Networks,” Conference on Neural Informa-\\ntion Processing Systems (2017), https://arxiv.org/abs/1706.02515.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 274, 'page_label': '275'}, page_content=\"262 CHAPTER 7 Advanced deep-learning best practices\\n \\nThese advantages become especially impo rtant when you’re training small models\\nfrom scratch on limited data. For instance , here’s how you can build a lightweight,\\ndepthwise separable convnet for an image-classification task (softmax categorical clas-\\nsification) on a small dataset:\\nfrom keras.models import Sequential, Model\\nfrom keras import layers\\nheight = 64\\nwidth = 64\\nchannels = 3\\nnum_classes = 10\\nmodel = Sequential()\\nmodel.add(layers.SeparableConv2D(32, 3,\\nactivation='relu',\\ninput_shape=(height, width, channels,)))\\nmodel.add(layers.SeparableConv2D(64, 3, activation='relu'))\\nmodel.add(layers.MaxPooling2D(2))\\nmodel.add(layers.SeparableConv2D(64, 3, activation='relu'))\\nmodel.add(layers.SeparableConv2D(128, 3, activation='relu'))\\nmodel.add(layers.MaxPooling2D(2))\\nmodel.add(layers.SeparableConv2D(64, 3, activation='relu'))\\nmodel.add(layers.SeparableConv2D(128, 3, activation='relu'))\\nmodel.add(layers.GlobalAveragePooling2D())\\nmodel.add(layers.Dense(32, activation='relu'))\\nmodel.add(layers.Dense(num_classes, activation='softmax'))\\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\\nWhen it comes to larger-scale models, dept hwise separable convolutions are the basis\\nof the Xception architectu re, a high-performing convne t that comes packaged with\\nKeras. You can read more about the theo retical grounding for depthwise separable\\n1 × 1 conv\\n(pointwise conv)\\nDepthwise convolution: \\nindependent spatial \\nconvs per channel\\nConcatenate\\nSplit channels\\n3 × 3 conv3 × 3 conv3 × 3 conv3 × 3 conv\\nFigure 7.16 Depthwise separable \\nconvolution: a depthwise convolution \\nfollowed by a pointwise convolution\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 275, 'page_label': '276'}, page_content='263Getting the most out of your models\\nconvolutions and Xception in my paper “X ception: Deep Learning with Depthwise\\nSeparable Convolutions.”8\\n7.3.2 Hyperparameter optimization\\nWhen building a deep-learning model, you have to make many seemingly arbitrary\\ndecisions: How many layers should you stac k? How many units or filters should go in\\neach layer? Should you use relu as activation, or a different function? Should you use\\nBatchNormalization after a given layer? How much dropout should you use? And so\\non. These architecture-level parameters are called hyperparameters to distinguish them\\nfrom the parameters of a model, which are trained via backpropagation.\\n In practice, experienced machine-learni ng engineers and researchers build intu-\\nition over time as to what  works and what doesn’t when it comes to these choices—\\nthey develop hyperparameter-tuning skills. Bu t there are no formal rules. If you want\\nto get to the very limit of what can be achieved on a given task, you can’t be content\\nwith arbitrary choices made by a fallible human. Your initial decisions are almost\\nalways suboptimal, even if you have good intuition. You can refine your choices by\\ntweaking them by hand and retraining the model repeatedly—that’s what machine-\\nlearning engineers and resear chers spend most of their ti me doing. But it shouldn’t\\nbe your job as a human to fiddle with hyperp arameters all day—that is better left to a\\nmachine.\\n Thus you need to explore the space of po ssible decisions automatically, systemati-\\ncally, in a principled way. You need to search the architecture space and find the best-\\nperforming ones empirically. That’s what the field of automatic hyperparameter opti-\\nmization is about: it’s an entire field of research, and an important one.\\n The process of optimizing hyperparameters typically looks like this:\\n1 Choose a set of hyperparameters (automatically).\\n2 Build the corresponding model.\\n3 Fit it to your training data, and meas ure the final performance on the valida-\\ntion data.\\n4 Choose the next set of hyperparameters to try (automatically).\\n5 Repeat.\\n6 Eventually, measure performance on your test data.\\nThe key to this process is the algorithm that uses this history of validation perfor-\\nmance, given various sets of hyperparameters, to choose the next set of hyperparame-\\nters to evaluate. Many different techniqu es are possible: Bayesian optimization,\\ngenetic algorithms, simple random search, and so on.\\n Training the weights of a model is relatively easy: you compute a loss function on a\\nmini-batch of data and then use the Back propagation algorithm to move the weights\\n8 See note 5 above.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 276, 'page_label': '277'}, page_content='264 CHAPTER 7 Advanced deep-learning best practices\\nin the right direction. Updating hyperpar ameters, on the other hand, is extremely\\nchallenging. Consider the following:\\n\\uf0a1 Computing the feedback signal (does th is set of hyperparameters lead to a\\nhigh-performing model on this task?) ca n be extremely expensive: it requires\\ncreating and training a new model from scratch on your dataset.\\n\\uf0a1 The hyperparameter space is typically made  of discrete decisions and thus isn’t\\ncontinuous or differentiable. Hence, yo u typically can’t do gradient descent in\\nhyperparameter space. Inst ead, you must rely on gradient-free optimization\\ntechniques, which naturally are far less efficient than gradient descent.\\nBecause these challenges are difficult and th e field is still young, we currently only\\nhave access to very limited tools to optimi ze models. Often, it turns out that random\\nsearch (choosing hyperparameters to evaluate at random, repeatedly) is the best solu-\\ntion, despite being the most naive one. But one tool I have found reliably better than\\nrandom search is Hyperopt ( https:/ /github.com/hyperopt/hyperopt), a Python\\nlibrary for hyperparameter optimization that internally uses trees of Parzen estimators\\nto predict sets of hyperparameters that ar e likely to work well. Another library called\\nHyperas ( https:/ /github.com/maxpumperla/hyperas) integrates Hyperopt for use\\nwith Keras models. Do check it out.\\nNOTE One important issue to keep in mi nd when doing automatic hyperpa-\\nrameter optimization at scale is validation-set overfitting. Because you’re\\nupdating hyperparameters based on a signal that is computed using your vali-\\ndation data, you’re effectively training them on the validation data, and thus\\nthey will quickly overfit to the validation data. Always keep this in mind.\\nOverall, hyperparameter optimization is a powerful technique that is an absolute\\nrequirement to get to state-of-the-art mode ls on any task or to win machine-learning\\ncompetitions. Think about it: once upon a time, people handcrafted the features that\\nwent into shallow machine-learning mode ls. That was very much suboptimal. Now,\\ndeep learning automates the task of hier archical feature engineering—features are\\nlearned using a feedback signal, not hand-t uned, and that’s the way it should be. In\\nthe same way, you shouldn’t handcraft your model architectures; you should optimize\\nthem in a principled way. At the time of writing, the field of automatic hyperparame-\\nter optimization is very young and immature, as deep learning was some years ago, but\\nI expect it to boom in the next few years. \\n7.3.3 Model ensembling\\nAnother powerful technique for obtaining the best possible results on a task is model\\nensembling. Ensembling consists of pooling togeth er the predictions of a set of differ-\\nent models, to produce better  predictions. If you look at machine-learning competi-\\ntions, in particular on Kaggle, you’ll see that the winners use very large ensembles of\\nmodels that inevitably beat any single model, no matter how good.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 277, 'page_label': '278'}, page_content='265Getting the most out of your models\\n Ensembling relies on the assumption th at different good models trained inde-\\npendently are likely to be good for different reasons: each model looks at slightly differ-\\nent aspects of the data to make its predictions, getting part of the “truth” but not all of\\nit. You may be familiar with the ancient pa rable of the blind men and the elephant: a\\ngroup of blind men come across an elephant  for the first time and try to understand\\nwhat the elephant is by touching it. Each  man touches a different part of the ele-\\nphant’s body—just one part, su ch as the trunk or a leg. Then the men describe to\\neach other what an elephant is: “It’s like a snake,” “Like a pillar or a tree,” and so on.\\nThe blind men are essentially machine-learning models trying to understand the man-\\nifold of the training data, each from its own perspective, using its own assumptions\\n(provided by the unique architecture of the model and the unique random weight ini-\\ntialization). Each of them gets part of the truth of the data, but not the whole truth. By\\npooling their perspectives together, you can get a far more accurate description of the\\ndata. The elephant is a combination of part s: not any single blind man gets it quite\\nright, but, interviewed together, they can tell a fairly accurate story.\\n Let’s use classification as an example. The easiest way to pool the predictions of a set\\nof classifiers (to ensemble the classifiers) is to average their predictions at inference time:\\npreds_a = model_a.predict(x_val)\\npreds_b = model_b.predict(x_val)\\npreds_c = model_c.predict(x_val)\\npreds_d = model_d.predict(x_val)\\nfinal_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d)\\nThis will work only if the classifiers are more or less equally good. If one of them is sig-\\nnificantly worse than the others, the final predictions may not be as good as the best\\nclassifier of the group.\\n A smarter way to ensemble classifiers is to do a weighted average, where the\\nweights are learned on the validation data—t ypically, the better classifiers are given a\\nhigher weight, and the worse classifiers are given a lower weight. To search for a good\\nset of ensembling weights, you can use random  search or a simple optimization algo-\\nrithm such as Nelder-Mead:\\npreds_a = model_a.predict(x_val)\\npreds_b = model_b.predict(x_val)\\npreds_c = model_c.predict(x_val)\\npreds_d = model_d.predict(x_val)\\nfinal_preds = 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c + 0.15 * preds_d\\nThere are many possible variants: you can do an average of an exponential of the pre-\\ndictions, for instance. In general, a simple  weighted average with weights optimized\\non the validation data provides a very strong baseline.\\n The key to making ensembling work is the diversity of the set of classifiers. Diversity\\nis strength. If all the blind men only touc hed the elephant’s trunk, they would agree\\nUse four different models to compute initial predictions.\\nThis new prediction array\\nshould be more accurate\\nthan any of the initial ones.\\nThese weights (0.5, 0.25,\\n0.1, 0.15) are assumed to\\nbe learned empirically.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 278, 'page_label': '279'}, page_content='266 CHAPTER 7 Advanced deep-learning best practices\\nthat elephants are like snakes, and they would forever stay ignorant of the truth of the\\nelephant. Diversity is what makes ensembling  work. In machine-lear ning terms, if all\\nof your models are biased in the same way,  then your ensemble will retain this same\\nbias. If your models are biased in different ways , the biases will cancel each other out,\\nand the ensemble will be more robust and more accurate.\\n For this reason, you should ensemble models that are as good as possible while being\\nas different as possible . This typically means using very  different architectures or even\\ndifferent brands of machin e-learning approaches. One thing that is largely not worth\\ndoing is ensembling the same  network trained several times independently, from dif-\\nferent random initializations. If the only di fference between your models is their ran-\\ndom initialization and the order in which they were exposed to the training data, then\\nyour ensemble will be low-diversity and will  provide only a tiny improvement over any\\nsingle model.\\n One thing I have found to work well in practice—but that doesn’t generalize to\\nevery problem domain—is the use of an ensemble of tree-based methods (such as ran-\\ndom forests or gr adient-boosted trees) and deep ne ural networks. In 2014, partner\\nAndrei Kolev and I took fourth place in the Higgs Boson decay detection challenge\\non Kaggle (www.kaggle.com/ c/higgs-boson) using an ensemble of various tree mod-\\nels and deep neural networks . Remarkably, one of the mode ls in the ensemble origi-\\nnated from a different method than the others (it was a regularized greedy forest) and\\nhad a significantly worse score than the othe rs. Unsurprisingly, it was assigned a small\\nweight in the ensemble. But to our surpri se, it turned out to improve the overall\\nensemble by a large factor, because it was so  different from every other model: it pro-\\nvided information that the other models di dn’t have access to. That’s precisely the\\npoint of ensembling. It’s not so much abou t how good your best model is; it’s about\\nthe diversity of your set of candidate models.\\n In recent times, one style of basic ensemb le that has been very successful in prac-\\ntice is the wide and deep category of models, blending deep learning with shallow learn-\\ning. Such models consist of jointly training  a deep neural network with a large linear\\nmodel. The joint training of a family of  diverse models is yet another option to\\nachieve model ensembling. \\n7.3.4 Wrapping up\\n\\uf0a1 When building high-performing deep convnets, you’ll need to use residual con-\\nnections, batch normalization, and dept hwise separable convolutions. In the\\nfuture, it’s likely that depthwise separabl e convolutions will completely replace\\nregular convolutions, whether for 1D, 2D, or 3D applications, due to their\\nhigher representational efficiency.\\n\\uf0a1 Building deep networks requires making many small hyperparameter and\\narchitecture choices, which together define how good your model will be.\\nRather than basing these choices on in tuition or random chance, it’s better to\\nsystematically search hyperparameter sp ace to find optimal choices. At this\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 279, 'page_label': '280'}, page_content='267Getting the most out of your models\\ntime, the process is expensive, and the t ools to do it aren’t very good. But the\\nHyperopt and Hyperas libraries may be ab le to help you. When doing hyperpa-\\nrameter optimization, be mindful of validation-set overfitting!\\n\\uf0a1 Winning machine-learning competitions or otherwise obtaining the best possi-\\nble results on a task can only be done  with large ensembles of models. Ensem-\\nbling via a well-optimized weighted average is usually good enough. Remember:\\ndiversity is strength. It’s largely pointl ess to ensemble very similar models; the\\nbest ensembles are sets of models that are as dissimilar as possible (while having\\nas much predictive power as possible, naturally). \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 280, 'page_label': '281'}, page_content='268 CHAPTER 7 Advanced deep-learning best practices\\nChapter summary\\n\\uf0a1 In this chapter, you learned the following:\\n– How to build models as arbitrary gr aphs of layers, reuse layers (layer\\nweight sharing), and use models as Python functions (model templating).\\n– You can use Keras callbacks to monitor your models during training and\\ntake action based on model state.\\n– TensorBoard allows you to visualize metrics, activation histograms, and\\neven embedding spaces.\\n– What batch normalization, depthwis e separable convolution, and resid-\\nual connections are.\\n– Why you should use hyperparameter optimization and model ensembling.\\n\\uf0a1 With these new tools, you’re better eq uipped to use deep learning in the\\nreal world and start building highly competitive deep-learning models.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 281, 'page_label': '282'}, page_content='Generative deep learning\\nThe potential of artificial intelligence  to emulate human thought processes goes\\nbeyond passive tasks such as object recogn ition and mostly reactive tasks such as\\ndriving a car. It extends well into creative activities. When I first made the claim that\\nin a not-so-distant future, most of the cultural content that we consume will be cre-\\nated with substantial help from AIs, I was met with utter di sbelief, even from long-\\ntime machine-learning practi tioners. That was in 2014. Fast-forward three years,\\nand the disbelief has receded—at an incr edible speed. In th e summer of 2015, we\\nwere entertained by Google’s DeepDream algorithm turning an image into a psy-\\nchedelic mess of dog eyes and pareidolic artifacts; in 2016, we used the Prisma appli-\\ncation to turn photos into paintings of various styles. In the summer of 2016, an\\nexperimental short movie, Sunspring, was directed using a script written by a Long\\nShort-Term Memory ( LSTM) algorithm—complete with dialogue. Maybe you’ve\\nrecently listened to music that was tentatively generated by a neural network.\\nThis chapter covers\\n\\uf0a1 Text generation with LSTM\\n\\uf0a1 Implementing DeepDream\\n\\uf0a1 Performing neural style transfer\\n\\uf0a1 Variational autoencoders\\n\\uf0a1 Understanding generative adversarial networks'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 282, 'page_label': '283'}, page_content='270 CHAPTER 8 Generative deep learning\\n Granted, the artistic pr oductions we’ve seen from AI so far have been fairly low\\nquality. AI isn’t anywhere close to rivaling human screenwriters, painters, and compos-\\ners. But replacing humans was always beside  the point: artificial intelligence isn’t\\nabout replacing our own intelligence with something else, it’s about bringing into our\\nlives and work more intelligence—intelligence of a di fferent kind. In many fields, but\\nespecially in creative ones, AI will be used by humans as a tool to augment their own\\ncapabilities: more augmented intelligence than artificial intelligence.\\n A large part of artistic creation consists of simple pattern recognition and technical\\nskill. And that’s precisely the part of the pr ocess that many find less attractive or even\\ndispensable. That’s where AI comes in. Our perceptual modalities, our language, and\\nour artwork all have statistical structure. Learning this structure is what deep-learning\\nalgorithms excel at. Machine-learning  models can learn the statistical latent space  of\\nimages, music, and stories, and they can then sample from this space, creating new art-\\nworks with characteristics similar to those the model has seen in its training data. Nat-\\nurally, such sampling is hardly an act of artistic creation in itself. It’s a mere\\nmathematical operation: the algorithm has no grounding in human life, human emo-\\ntions, or our experience of the world; instead, it learns from an experience that has lit-\\ntle in common with ours. It’s only our in terpretation, as human spectators, that will\\ngive meaning to what the model generates. But in the hands of a skilled artist, algo-\\nrithmic generation can be steered to be come meaningful—and beautiful. Latent\\nspace sampling can become a brush that em powers the artist, augments our creative\\naffordances, and expands the space of what we can imagine. What’s more, it can make\\nartistic creation more accessible by elimin ating the need for tec hnical skill and prac-\\ntice—setting up a new medium of pure expression, factoring art apart from craft.\\n Iannis Xenakis, a visionary pioneer of el ectronic and algorith mic music, beauti-\\nfully expressed this same idea in the 1960s, in the context of the application of auto-\\nmation technology to music composition:\\n1\\nFreed from tedious calculations, the composer is able to devote himself to the general\\nproblems that the new musical form poses and to explore the nooks and crannies of this\\nform while modifying the values of the in put data. For example,  he may test all\\ninstrumental combinations from soloists to chamber orchestras, to large orchestras. With\\nthe aid of electronic computers the composer becomes a sort of pilot: he presses the buttons,\\nintroduces coordinates, and supervises the controls of a cosmic vessel sailing in the space\\nof sound, across sonic constellations and ga laxies that he could formerly glimpse only as\\na distant dream.\\nIn this chapter, we’ll explore from variou s angles the potential of deep learning to\\naugment artistic creation. We’ll review sequ ence data generation (which can be used\\nto generate text or music), DeepDream, and image generation using both variational\\nautoencoders and generative adversarial ne tworks. We’ll get your computer to dream\\nup content never seen before; and maybe we’ll get you to dream, too, about the fan-\\ntastic possibilities that lie at the intersection of technology and art. Let’s get started.\\n1 Iannis Xenakis, “Musiques formelles: nouveaux principes formels de composition musicale,” special issue of La\\nRevue musicale, nos. 253 -254 (1963).\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 283, 'page_label': '284'}, page_content='271Text generation with LSTM\\n8.1 Text generation with LSTM\\nIn this section, we’ll explor e how recurrent neural networ ks can be used to generate\\nsequence data. We’ll use text generation as an example, but the exact same tech-\\nniques can be generalized to any kind of sequence data: you could apply it to\\nsequences of musical notes in order to gene rate new music, to timeseries of brush-\\nstroke data (for example, re corded while an artist pain ts on an iPad) to generate\\npaintings stroke by stroke, and so on.\\n Sequence data generation is in no way limited to artistic content generation. It\\nhas been successfully applied to speech synthesis and to dialogue generation for chat-\\nbots. The Smart Reply fe ature that Google released in 2016, capable of automatically\\ngenerating a selection of quick replies to em ails or text messages,  is powered by simi-\\nlar techniques.\\n8.1.1 A brief history of generative recurrent networks\\nIn late 2014, few people had ever seen the initials LSTM, even in the machine-learning\\ncommunity. Successful applications of sequence data generation with recurrent net-\\nworks only began to appear in the mainstre am in 2016. But these techniques have a\\nfairly long history, starting with the development of the LSTM algorithm in 1997.2 This\\nnew algorithm was used early on to generate text character by character.\\n In 2002, Douglas Eck, then at Schm idhuber’s lab in Switzerland, applied LSTM to\\nmusic generation for the first time, with pr omising results. Eck is now a researcher at\\nGoogle Brain, and in 2016 he started a new research group there, called Magenta,\\nfocused on applying modern deep-learnin g techniques to produce engaging music.\\nSometimes, good ideas take 15 years to get started.\\n In the late 2000s and early 2010s, Alex Graves did important pioneering work on\\nusing recurrent networks for sequence data  generation. In particular, his 2013 work\\non applying recurrent mixtur e density networks to generate human-like handwriting\\nusing timeseries of pen positions is seen by some as a turning point. 3 This specific\\napplication of neural networks at that spec ific moment in time captured for me the\\nnotion of machines that dream  and was a significant insp iration around the time I\\nstarted developing Keras. Graves left a similar commented-out remark hidden in a\\n2013 LaTeX file uploaded to the preprint se rver arXiv: “generating sequential data is\\nthe closest computers get to dreaming.” Several years later, we take a lot of these devel-\\nopments for granted; but at the time, it was difficult to watch Graves’s demonstrations\\nand not walk away awe-inspired by the possibilities.\\n Since then, recurrent neural networks have been successfully used for music gener-\\nation, dialogue generation, image generation, speech synthesis, and molecule design.\\nThey were even used to produce a movie script that was then cast with live actors. \\n2 Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” Neural Computation 9, no. 8 (1997).\\n3 Alex Graves, “Generating Sequences With Recurrent Neural Networks,” arXiv (2013), https:/ /arxiv.org/\\nabs/1308.0850.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 284, 'page_label': '285'}, page_content='272 CHAPTER 8 Generative deep learning\\n8.1.2 How do you generate sequence data?\\nThe universal way to generate sequence data in deep learning is to train a network (usu-\\nally an RNN or a convnet) to predict the next to ken or next few tokens in a sequence,\\nusing the previous tokens as input. For instance, given the input “the cat is on the ma,”\\nthe network is trained to predict the target t, the next character. As usual when working\\nwith text data, tokens are typically words or characters, and any network that can model\\nthe probability of the next token give n the previous ones is called a language model. A\\nlanguage model captures the latent space of language: its statistical structure.\\n Once you have such a trained language model, you can sample from it (generate\\nnew sequences): you feed it an initial string of text (called conditioning data), ask it to\\ngenerate the next character or the next word (you can even generate several tokens at\\nonce), add the generated output back to the input data, and repeat the process many\\ntimes (see figure 8.1). This loop allows yo u to generate sequences of arbitrary length\\nthat reflect the structure of  the data on which the mode l was trained: sequences that\\nlook almost like human-written sentences. In th e example we present in this section,\\nyou’ll take a LSTM layer, feed it strings of N characters extracted from a text corpus,\\nand train it to predict character N + 1. The output of the model will be a softmax over\\nall possible characters: a probability dist ribution for the next character. This LSTM is\\ncalled a character-level neural language model. \\n8.1.3 The importance of the sampling strategy\\nWhen generating text, the way you choose the next character is crucially important. A\\nnaive approach is greedy sampling, consisting of always choosing the most likely next\\ncharacter. But such an approach results in  repetitive, predictable strings that don’t\\nlook like coherent language. A more intere sting approach makes slightly more sur-\\nprising choices: it introduces randomness in  the sampling proce ss, by sampling from\\nthe probability distribution for the next character. This is called stochastic sampling\\n(recall that stochasticity is what we call randomness in this field). In such a setup, if e has\\na probability 0.3 of being the next characte r, according to the model, you’ll choose it\\nLanguage\\nmodel\\nInitial text\\nProbability\\ndistribution for the\\nnext characterInitial text\\nSampled next\\ncharacter\\nSampling\\nstrategy aThe cat sat on the m\\nLanguage\\nmodel\\n...\\nSampling\\nstrategy tThe cat sat on the ma\\nFigure 8.1 The process of character-by-charact er text generation using a language model\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 285, 'page_label': '286'}, page_content='273Text generation with LSTM\\n30% of the time. Note that greedy sampling ca n be also cast as sampling from a prob-\\nability distribution: one where a certain character has probability 1 and all others have\\nprobability 0.\\n Sampling probabilistically from the softma x output of the mode l is neat: it allows\\neven unlikely characters to be sampled some of the time, generating more interesting-\\nlooking sentences and sometimes showing creativity by comi ng up with new, realistic-\\nsounding words that didn’t occur in the tr aining data. But there’ s one issue with this\\nstrategy: it doesn’t offer a way to control the amount of randomness in the sampling process.\\n Why would you want more or less rand omness? Consider an extreme case: pure\\nrandom sampling, where you draw the next character from a uniform probability dis-\\ntribution, and every characte r is equally likely. This scheme has maximum random-\\nness; in other words, this probability distri bution has maximum entropy. Naturally, it\\nwon’t produce anything interesting. At th e other extreme, greedy sampling doesn’t\\nproduce anything interesting, either, an d has no randomness: the corresponding\\nprobability distribution has minimum entropy. Sampling from the “real” probability\\ndistribution—the distribution that is output by the model’s softmax function—consti-\\ntutes an intermediate point between thes e two extremes. But there are many other\\ni nte rm e di a te  po int s  o f hi ghe r o r l ow er  ent r op y  tha t  y o u m a y  wa nt  t o e xp lo re .  L e s s\\nentropy will give the generated sequences a more predictable structure (and thus they\\nwill potentially be more realistic looking) , whereas more entropy will result in more\\nsurprising and creative sequences. When sampling from generative models, it’s always\\ngood to explore different amounts of randomness in the generation process. Because\\nwe—humans—are the ultimate judges of how interesting the generated data is, inter-\\nestingness is highly subjective, and there’ s no telling in advance where the point of\\noptimal entropy lies.\\n In order to control the amount of stochasticity in the sampling process, we’ll intro-\\nduce a parameter called the softmax temperature that characterizes the entropy of the\\nprobability distribution used for sampling: it characterizes how su rprising or predict-\\nable the choice of the next character will be. Given a temperature value, a new proba-\\nbility distribution is comp uted from the original one (the softmax output of the\\nmodel) by reweighting it in the following way.\\nimport numpy as np\\ndef reweight_distribution(original_distribution, temperature=0.5):\\ndistribution = np.log(original_distribution) / temperature\\ndistribution = np.exp(distribution)\\nreturn distribution / np.sum(distribution)\\nListing 8.1 Reweighting a probability distribution to a different temperature\\noriginal_distribution is a 1D Numpy array \\nof probability values that must sum to 1. \\ntemperature is a factor quantifying the \\nentropy of the output distribution.\\nReturns a reweighted version of \\nthe original distribution. The sum \\nof the distribution may no longer \\nbe 1, so you divide it by its sum to \\nobtain the new distribution.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 286, 'page_label': '287'}, page_content=\"274 CHAPTER 8 Generative deep learning\\nHigher temperatures result in sampling distributions of higher entropy that will generate more\\nsurprising and unstructured generated data, whereas a lower temperature will result in less ran-\\ndomness and much more predictable generated data (see figure 8.2). \\n8.1.4 Implementing character- level LSTM text generation\\nLet’s put these ideas into practice in a Keras implementation. The first thing you need\\nis a lot of text data that you can use to learn a language model. You can use any suffi-\\nciently large text file or set of text files—Wikipedia, The Lord of the Rings, and so on. In\\nthis example, you’ll use some of the writings of Nietzsche, the late-nineteenth century\\nGerman philosopher (translated into Englis h). The language model you’ll learn will\\nthus be specifically a model of Nietzsche’s writing style and topics of choice, rather\\nthan a more generic model of the English language.\\nPREPARING THE DATA\\nLet’s start by downloading the corpus and converting it to lowercase.\\nimport keras\\nimport numpy as np\\npath = keras.utils.get_file(\\n'nietzsche.txt',\\norigin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\\ntext = open(path).read().lower()\\nprint('Corpus length:', len(text))\\nListing 8.2 Downloading and parsing the initial text file\\ntemperature = 0.01 temperature = 0.2 temperature = 0.4\\ntemperature = 0.6\\nDiscrete elements (characters)\\nProbability of sampling element\\ntemperature = 0.8 temperature = 1.0\\nFigure 8.2 Different reweightings of one probability distribution. Low temperature = more \\ndeterministic, high temperature = more random.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 287, 'page_label': '288'}, page_content=\"275Text generation with LSTM\\nNext, you’ll extract partially overlapping sequences of length maxlen, one-hot encode\\nthem, and pack them in a 3D Numpy array x of shape (sequences, maxlen,\\nunique_characters). Simultaneously, you’ll prepare an array y containing the corre-\\nsponding targets: the one-hot-encoded char acters that come after each extracted\\nsequence.\\nmaxlen = 60\\nstep = 3\\nsentences = []\\nnext_chars = []\\nfor i in range(0, len(text) - maxlen, step):\\nsentences.append(text[i: i + maxlen])\\nnext_chars.append(text[i + maxlen])\\nprint('Number of sequences:', len(sentences))\\nchars = sorted(list(set(text)))\\nprint('Unique characters:', len(chars))\\nchar_indices = dict((char, chars.index(char)) for char in chars)\\nprint('Vectorization...')\\nx = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\\ny = np.zeros((len(sentences), len(chars)), dtype=np.bool)\\nfor i, sentence in enumerate(sentences):\\nfor t, char in enumerate(sentence):\\nx[i, t, char_indices[char]] = 1\\ny[i, char_indices[next_chars[i]]] = 1\\nBUILDING THE NETWORK\\nThis network is a single LSTM layer followed by a Dense classifier and softmax over all\\npossible characters. But note that recurrent neural networks aren’t the only way to do\\nsequence data generation; 1D convnets also have proven extremely successful at this\\ntask in recent times.\\nfrom keras import layers\\nmodel = keras.models.Sequential()\\nmodel.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\\nmodel.add(layers.Dense(len(chars), activation='softmax'))\\nListing 8.3 Vectorizing sequences of characters\\nListing 8.4 Single-layer LSTM model for next-character prediction\\nYou’ll extract sequences \\nof 60 characters.\\nYou’ll sample a new sequence \\nevery three characters.\\nHolds the extracted sequences\\nHolds the targets (the \\nfollow-up characters)\\nList of unique characters \\nin the corpus\\nOne-hot encodes \\nthe characters \\ninto binary arrays\\nDictionary that maps\\nunique characters to their\\nindex in the list “chars”\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 288, 'page_label': '289'}, page_content='276 CHAPTER 8 Generative deep learning\\nBecause your targets are one-hot encoded, you’ll use categorical_crossentropy as\\nthe loss to train the model. \\noptimizer = keras.optimizers.RMSprop(lr=0.01)\\nmodel.compile(loss=\\'categorical_crossentropy\\', optimizer=optimizer)\\nTRAINING THE LANGUAGE MODEL AND SAMPLING FROM IT\\nGiven a trained model and a seed text snippet, you can generate new text by doing the\\nfollowing repeatedly:\\n1 Draw from the model a probability distribution for the next character, given the\\ngenerated text available so far.\\n2 Reweight the distribution to a certain temperature.\\n3 Sample the next character at random according to the reweighted distribution.\\n4 Add the new character at the end of the available text.\\nThis is the code you use to reweight the or iginal probability dist ribution coming out\\nof the model and draw a character index from it (the sampling function).\\ndef sample(preds, temperature=1.0):\\npreds = np.asarray(preds).astype(\\'float64\\')\\npreds = np.log(preds) / temperature\\nexp_preds = np.exp(preds)\\npreds = exp_preds / np.sum(exp_preds)\\nprobas = np.random.multinomial(1, preds, 1)\\nreturn np.argmax(probas)\\nFinally, the following loop repeatedly trai ns and generates text. You begin generating\\ntext using a range of different temperatures after every epoch. This allows you to see\\nhow the generated text evolves as the model begins to converge, as well as the impact\\nof temperature in the sampling strategy.\\nimport random\\nimport sys\\nfor epoch in range(1, 60):\\nprint(\\'epoch\\', epoch)\\nmodel.fit(x, y, batch_size=128, epochs=1)\\nstart_index = random.randint(0, len(text) - maxlen - 1)\\ngenerated_text = text[start_index: start_index + maxlen]\\nprint(\\'--- Generating with seed: \"\\' + generated_text + \\'\"\\')\\nfor temperature in [0.2, 0.5, 1.0, 1.2]:\\nprint(\\'------ temperature:\\', temperature)\\nsys.stdout.write(generated_text)\\nListing 8.5 Model compilation configuration\\nListing 8.6 Function to sample the next character given the model’s predictions\\nListing 8.7 Text-generation loop\\nTrains the model for 60 epochs\\nFits the model for one iteration \\non the data \\nSelects a text \\nseed at \\nrandom\\nTries a range of different \\nsampling temperatures\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 289, 'page_label': '290'}, page_content='277Text generation with LSTM\\nfor i in range(400):\\nsampled = np.zeros((1, maxlen, len(chars)))\\nfor t, char in enumerate(generated_text):\\nsampled[0, t, char_indices[char]] = 1.\\npreds = model.predict(sampled, verbose=0)[0]\\nnext_index = sample(preds, temperature)\\nnext_char = chars[next_index]\\ngenerated_text += next_char\\ngenerated_text = generated_text[1:]\\nsys.stdout.write(next_char)\\nHere, we used the random seed text “new faculty, and the jubilation reached its cli-\\nmax when kant.” Here’s what you get at epoch 20, long before the model has fully\\nconverged, with temperature=0.2:\\nnew faculty, and the jubilation reached its climax when kant and such a man\\nin the same time the spirit of the surely and the such the such\\nas a man is the sunligh and subject the present to the superiority of the\\nspecial pain the most man and strange the subjection of the\\nspecial conscience the special and nature and such men the subjection of the\\nspecial men, the most surely the subjection of the special\\nintellect of the subjection of the same things and\\nHere’s the result with temperature=0.5:\\nnew faculty, and the jubilation reached its climax when kant in the eterned\\nand such man as it\\'s also become himself the condition of the\\nexperience of off the basis the superiory and the special morty of the\\nstrength, in the langus, as which the same time life and \"even who\\ndiscless the mankind, with a subject and fact all you have to be the stand\\nand lave no comes a troveration of the man and surely the\\nconscience the superiority, and when one must be w\\nAnd here’s what you get with temperature=1.0:\\nnew faculty, and the jubilation reached its climax when kant, as a\\nperiliting of manner to all definites and transpects it it so\\nhicable and ont him artiar resull\\ntoo such as if ever the proping to makes as cnecience. to been juden,\\nall every could coldiciousnike hother aw passife, the plies like\\nwhich might thiod was account, indifferent germin, that everythery\\ncertain destrution, intellect into the deteriorablen origin of moralian,\\nand a lessority o\\nAt epoch 60, the model has most ly converged, and the text starts to look significantly\\nmore coherent. Here’s the result with temperature=0.2:\\ncheerfulness, friendliness and kindness of a heart are the sense of the\\nspirit is a man with the sense of the sense of the world of the\\nself-end and self-concerning the subjection of the strengthorixes--the\\nGenerates 400\\ncharacters,\\nstarting from\\nthe seed text\\nOne-hot encodes \\nthe characters \\ngenerated so far\\nSamples \\nthe next \\ncharacter\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 290, 'page_label': '291'}, page_content='278 CHAPTER 8 Generative deep learning\\nsubjection of the subjection of the subjection of the\\nself-concerning the feelings in the superiority in the subjection of the\\nsubjection of the spirit isn\\'t to be a man of the sense of the\\nsubjection and said to the strength of the sense of the\\nHere’s temperature=0.5:\\ncheerfulness, friendliness and kindness of a heart are the part of the soul\\nwho have been the art of the philosophers, and which the one\\nwon\\'t say, which is it the higher the and with religion of the frences.\\nthe life of the spirit among the most continuess of the\\nstrengther of the sense the conscience of men of precisely before enough\\npresumption, and can mankind, and something the conceptions, the\\nsubjection of the sense and suffering and the\\nAnd here’s temperature=1.0:\\ncheerfulness, friendliness and kindness of a heart are spiritual by the\\nciuture for the\\nentalled is, he astraged, or errors to our you idstood--and it needs,\\nto think by spars to whole the amvives of the newoatly, prefectly\\nraals! it was\\nname, for example but voludd atu-especity\"--or rank onee, or even all\\n\"solett increessic of the world and\\nimplussional tragedy experience, transf, or insiderar,--must hast\\nif desires of the strubction is be stronges\\nAs you can see, a low temperature value results in extremely repetitive and predictable\\ntext, but local structure is highly re alistic: in particular, all words (a word being a local\\npattern of characters) are real English wo rds. With higher te mperatures, the gener-\\nated text becomes more inte resting, surprising, even cr eative; it sometimes invents\\ncompletely new words that sound somewhat plausible (such as eterned and troveration).\\nWith a high temperature, th e local structure starts to break down, and most words\\nlook like semi-random strings of characters. Without a doubt, 0.5 is the most interest-\\ning temperature for text generation in this  specific setup. Al ways experiment with\\nmultiple sampling strategies! A clever balance between learned structure and random-\\nness is what makes generation interesting.\\n Note that by training a bigger model, longer, on more data, you can achieve gen-\\nerated samples that look mu ch more coherent and realistic than this one. But, of\\ncourse, don’t expect to ever generate an y meaningful text, other than by random\\nchance: all you’re doing is sampling data from a statistical model of which characters\\ncome after which characters. Language is  a communication channel, and there’s a\\ndistinction between what communications are about and the statistical structure of\\nthe messages in which communi cations are encoded. To evidence this distinction,\\nhere’s a thought experiment: what if human language did a better job of compressing\\ncommunications, much like computers do with most digital communications?\\nLanguage would be no less meaningful, but it would lack any intrinsic statistical struc-\\nture, thus making it impossible to learn a language model as you just did. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 291, 'page_label': '292'}, page_content='279Text generation with LSTM\\n8.1.5 Wrapping up\\n\\uf0a1 You can generate discrete sequence data by training a model to predict the next\\ntokens(s), given previous tokens.\\n\\uf0a1 In the case of text, such a model is called a language model. It can be based on\\neither words or characters.\\n\\uf0a1 Sampling the next token requires balance between adhering to what the model\\njudges likely, and introducing randomness.\\n\\uf0a1 One way to handle this is the notion of softmax temperature. Always experi-\\nment with different temperatures to find the right one. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 292, 'page_label': '293'}, page_content='280 CHAPTER 8 Generative deep learning\\n8.2 DeepDream\\nDeepDream is an artistic image-modification te chnique that uses the representations\\nlearned by convolutional neural networks. It  was first released by Google in the sum-\\nmer of 2015, as an implementation written using the Caffe deep-learning library (this\\nwas several months before the firs t public release of TensorFlow). 4 It quickly became\\nan internet sensation thanks to the trippy pictures it could generate (see, for example,\\nfigure 8.3), full of algorithmic pareidolia  artifacts, bird feathers, and dog eyes—a\\nbyproduct of the fact that the DeepDream convnet was trained on ImageNet, where\\ndog breeds and bird species are vastly overrepresented.\\nThe DeepDream algorithm is almost identical to the convnet filter-visualization tech-\\nnique introduced in chapter 5, consisting of running a convnet in reverse: doing gra-\\ndient ascent on the input to the convnet in  order to maximize the activation of a\\nspecific filter in an upper layer of the convnet. DeepDream uses this same idea, with a\\nfew simple differences:\\n\\uf0a1 With DeepDream, you try to maximize the activation of entire layers rather\\nthan that of a specific fi lter, thus mixing together visualizations of large num-\\nbers of features at once.\\n4 Alexander Mordvintsev, Christopher Olah, and Mike Tyka, “DeepDream: A Code Example for Visualizing\\nNeural Networks,” Google Research Blog, July 1, 2015, http:/ /mng.bz/xXlM.\\nFigure 8.3 Example of a DeepDream output image\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 293, 'page_label': '294'}, page_content=\"281DeepDream\\n\\uf0a1 You start not from blank, slightly nois y input, but rather from an existing\\nimage—thus the resulting effects latch on to preexisting visual patterns, distort-\\ning elements of the image in a somewhat artistic fashion.\\n\\uf0a1 The input images are processed at different scales (called octaves), which\\nimproves the quality of the visualizations.\\nLet’s make some DeepDreams.\\n8.2.1 Implementing DeepDream in Keras\\nYou’ll start from a convnet pretrained on ImageNet. In Keras, many such convnets are\\navailable: VGG16, VGG19, Xception, ResNet50, and so on. You can implement Deep-\\nDream with any of them, but your convnet of choice will naturally affect your visualiza-\\ntions, because different convnet architectures result in different learned features. The\\nconvnet used in the original DeepDream release was an Inception model, and in prac-\\ntice Inception is known to produce nice-looking DeepDreams, so you’ll use the Incep-\\ntion V3 model that comes with Keras.\\nfrom keras.applications import inception_v3\\nfrom keras import backend as K\\nK.set_learning_phase(0)\\nmodel = inception_v3.InceptionV3(weights='imagenet',\\ninclude_top=False)\\nNext, you’ll compute the loss: the quantity you’ll seek to maximize during the gradient-ascent\\nprocess. In chapter 5, for filter visualization, you tried to maximize the value of a specific filter\\nin a specific layer. Here, you’ll simultaneously maximize the activation of all filters in a number\\nof layers. Specifically, you’ll maximize a weighted sum of the L2 norm of the activations of a set\\nof high-level layers. The exact set of layers you choose (as well as their contribution to the final\\nloss) has a major influence on the visuals you’ll be able to produce, so you want to make these\\nparameters easily configurable. Lower layers result in geometric patterns, whereas higher layers\\nresult in visuals in which you can recognize some classes from ImageNet (for example, birds or\\ndogs). You’ll start from a somewhat arbitrary configuration involving four layers—but you’ll\\ndefinitely want to explore many different configurations later.\\nlayer_contributions = {\\n'mixed2': 0.2,\\n'mixed3': 3.,\\n'mixed4': 2.,\\n'mixed5': 1.5,\\n}\\nListing 8.8 Loading the pretrained Inception V3 model\\nListing 8.9 Setting up the DeepDream configuration\\nYou won’t be training the model, so \\nthis command disables all training-\\nspecific operations.\\nBuilds the Inception V3 network, \\nwithout its convolutional base. \\nThe model will be loaded with \\npretrained ImageNet weights.\\nDictionary mapping layer names to a coefficient quantifying \\nhow much the layer’s activation contributes to the loss \\nyou’ll seek to maximize. Note that the layer names are \\nhardcoded in the built-in Inception V3 application. You can \\nlist all layer names using model.summary().\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 294, 'page_label': '295'}, page_content=\"282 CHAPTER 8 Generative deep learning\\nNow, let’s define a tensor th at contains the loss: the weighted sum of the L2 norm of\\nthe activations of the layers in listing 8.9.\\nlayer_dict = dict([(layer.name, layer) for layer in model.layers])\\nloss = K.variable(0.)\\nfor layer_name in layer_contributions:\\ncoeff = layer_contributions[layer_name]\\nactivation = layer_dict[layer_name].output\\nscaling = K.prod(K.cast(K.shape(activation), 'float32'))\\nloss += coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling\\nNext, you can set up the gradient-ascent process.\\ndream = model.input\\ngrads = K.gradients(loss, dream)[0]\\ngrads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\\noutputs = [loss, grads]\\nfetch_loss_and_grads = K.function([dream], outputs)\\ndef eval_loss_and_grads(x):\\nouts = fetch_loss_and_grads([x])\\nloss_value = outs[0]\\ngrad_values = outs[1]\\nreturn loss_value, grad_values\\ndef gradient_ascent(x, iterations, step, max_loss=None):\\nfor i in range(iterations):\\nloss_value, grad_values = eval_loss_and_grads(x)\\nif max_loss is not None and loss_value > max_loss:\\nbreak\\nprint('...Loss value at', i, ':', loss_value)\\nx += step * grad_values\\nreturn x\\nFinally: the actual DeepDream algorith m. First, you define a list of scales (also called\\noctaves) at which to process the images. Each su ccessive scale is larger than the previ-\\nous one by a factor of 1.4 (it’s 40% larger):  you start by processi ng a small image and\\nthen increasingly scale it up (see figure 8.4).\\nListing 8.10 Defining the loss to be maximized\\nListing 8.11 Gradient-ascent process\\nCreates a dictionary that maps \\nlayer names to layer instances\\nYou’ll define the loss by adding \\nlayer contributions to this \\nscalar variable.\\nRetrieves the layer’s output Adds the L2 norm of the features of a layer\\nto the loss. You avoid border artifacts by\\nonly involving nonborder pixels in the loss.\\nThis tensor holds the \\ngenerated image: the dream. Computes the gradients of the \\ndream with regard to the loss\\nNormalizes the gradients  \\n(important trick)\\nSets up a Keras function \\nto retrieve the value of \\nthe loss and gradients, \\ngiven an input image\\nThis function runs \\ngradient ascent for a \\nnumber of iterations.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 295, 'page_label': '296'}, page_content=\"283DeepDream\\n \\nFor each successive scale, from the smallest to the largest, you run gradient ascent to\\nmaximize the loss you previously defined, at that scale. After each gradient ascent run,\\nyou upscale the resulting image by 40%.\\n To avoid losing a lot of image detail af ter each successive sc ale-up (resulting in\\nincreasingly blurry or pixelated images), yo u can use a simple trick: after each scale-\\nup, you’ll reinject the lost details back in to the image, which is possible because you\\nknow what the original image should look like at the larger scale. Given a small image\\nsize S and a larger image size L, you can compute the difference between the original\\nimage resized to size L and the original resized to size S —this difference quantifies the\\ndetails lost when going from S to L.\\nimport numpy as np\\nstep = 0.01\\nnum_octave = 3\\noctave_scale = 1.4\\niterations = 20\\nmax_loss = 10.\\nbase_image_path = '...'\\nimg = preprocess_image(base_image_path)\\nListing 8.12 Running gradient ascent over different successive scales\\nDream\\nOctave 1\\nOctave 2\\nOctave 3\\nUpscale\\nDetail\\nreinjection\\nDetail\\nreinjection\\nDream Upscale Dream\\nFigure 8.4 The DeepDream process: successive scales of  spatial processing (octaves) and detail reinjection \\nupon upscaling\\nPlaying with these hyperparameters \\nwill let you achieve new effects.\\nNumber of scales at which to run \\ngradient ascent\\nSize ratio between scales\\nNumber of ascent steps to \\nrun at each scale\\nIf the loss grows larger than 10, you’ll interrupt \\nthe gradient-ascent process to avoid ugly artifacts.\\nFill this with the path to the image you want to use.\\nGradient ascent step size\\nLoads the base image into a Numpy \\narray (function is defined in listing 8.13)\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 296, 'page_label': '297'}, page_content=\"284 CHAPTER 8 Generative deep learning\\noriginal_shape = img.shape[1:3]\\nsuccessive_shapes = [original_shape]\\nfor i in range(1, num_octave):\\nshape = tuple([int(dim / (octave_scale ** i))\\n   for dim in original_shape])\\nsuccessive_shapes.append(shape)\\nsuccessive_shapes = successive_shapes[::-1]\\noriginal_img = np.copy(img)\\nshrunk_original_img = resize_img(img, successive_shapes[0])\\nfor shape in successive_shapes:\\nprint('Processing image shape', shape)\\nimg = resize_img(img, shape)\\nimg = gradient_ascent(img,\\niterations=iterations,\\nstep=step,\\nmax_loss=max_loss)\\nupscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\\nsame_size_original = resize_img(original_img, shape)\\nlost_detail = same_size_original - upscaled_shrunk_original_img\\nimg += lost_detail\\nshrunk_original_img = resize_img(original_img, shape)\\nsave_img(img, fname='dream_at_scale_' + str(shape) + '.png')\\nsave_img(img, fname='final_dream.png')Note that this code uses the following straightforward auxiliary Numpy functions,\\nwhich all do as their names suggest. They require that you have SciPy installed.\\nimport scipy\\nfrom keras.preprocessing import image\\ndef resize_img(img, size):\\nimg = np.copy(img)\\nfactors = (1,\\nfloat(size[0]) / img.shape[1],\\nfloat(size[1]) / img.shape[2],\\n1)\\nreturn scipy.ndimage.zoom(img, factors, order=1)\\ndef save_img(img, fname):\\npil_img = deprocess_image(np.copy(img))\\nscipy.misc.imsave(fname, pil_img)\\ndef preprocess_image(image_path):\\nimg = image.load_img(image_path)\\nimg = image.img_to_array(img)\\nListing 8.13 Auxiliary functions\\nPrepares a list of shape \\ntuples defining the different \\nscales at which to run \\ngradient ascent\\nReverses the list of \\nshapes so they’re in \\nincreasing order\\nResizes the Numpy\\narray of the image\\nto the smallest scale\\nScales up\\nthe\\ndream\\nimage\\nRuns gradient\\nascent, altering\\nthe dream\\nScales up the smaller\\nversion of the original\\nimage: it will be pixellated.\\nComputes the high-quality version \\nof the original image at this size The difference between the two is the\\ndetail that was lost when scaling up.\\nReinjects lost detail into the dream\\nUtil function to open, resize, and \\nformat pictures into tensors \\nthat Inception V3 can process\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 297, 'page_label': '298'}, page_content=\"285DeepDream\\nimg = np.expand_dims(img, axis=0)\\nimg = inception_v3.preprocess_input(img)\\nreturn img\\ndef deprocess_image(x):\\nif K.image_data_format() == 'channels_first':\\nx = x.reshape((3, x.shape[2], x.shape[3]))\\nx = x.transpose((1, 2, 0))\\nelse:\\nx = x.reshape((x.shape[1], x.shape[2], 3))\\nx/ =2 .\\nx+ =0 . 5\\nx *= 255.\\nx = np.clip(x, 0, 255).astype('uint8')\\nreturn x\\nNOTE Because the original Inception V3 network was trained to recognize\\nconcepts in images of size 299 × 299, and given that the process involves scal-\\ning the images down by a reasonable factor, the DeepDream implementation\\nproduces much better results on imag es that are somewhere between 300 ×\\n300 and 400 × 400. Regardless, you can run the same code on images of any\\nsize and any ratio.\\nStarting from a photograph taken in the small hills between San Francisco Bay and\\nthe Google campus, we obtained the DeepDream shown in figure 8.5.\\nWe strongly suggest that yo u explore what you can do by adjusting which layers you\\nuse in your loss. Layers that are lower in  the network contain more-local, less-abstract\\nrepresentations and lead to dream patterns that look more geometric. Layers that are\\nhigher up lead to more-recognizable vi sual patterns based on the most common\\nobjects found in ImageNet, such as dog ey es, bird feathers, and so on. You can use\\nUtil function to convert a \\ntensor into a valid image\\nUndoes preprocessing that \\nwas performed by \\ninception_v3.preprocess_\\ninput\\nFigure 8.5 Running the DeepDream code on an example image\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 298, 'page_label': '299'}, page_content='286 CHAPTER 8 Generative deep learning\\nrandom generation of the parameters in the layer_contributions dictionary to\\nquickly explore many different layer combinations. Figure 8.6 shows a range of results\\nobtained using different layer configuratio ns, from an image of a delicious home-\\nmade pastry. \\n8.2.2 Wrapping up\\n\\uf0a1 DeepDream consists of running a convnet in reverse to generate inputs based\\non the representations learned by the network.\\n\\uf0a1 The results produced are fun and somewh at similar to the visual artifacts\\ninduced in humans by the disruption of the visual cortex via psychedelics.\\n\\uf0a1 Note that the process isn’t specific to im age models or even to convnets. It can\\nbe done for speech, music, and more. \\nFigure 8.6 Trying a range of DeepDream configurations on an example image\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 299, 'page_label': '300'}, page_content='287Neural style transfer\\n8.3 Neural style transfer\\nIn addition to DeepDream, another major development in deep-learning-driven\\nimage modification is neural style transfer, introduced by Leon Gatys et al. in the sum-\\nmer of 2015. 5 The neural style transfer algori thm has undergone many refinements\\nand spawned many variations since its origin al introduction, and it has made its way\\ninto many smartphone photo apps. For simplicity, this section focuses on the formula-\\ntion described in the original paper.\\n Neural style transfer consis ts of applying the style of a reference image to a target\\nimage while conserving the content of the target image. Figure 8.7 shows an example.\\nIn this context, style essentially means textures, colors, and visual patterns in the image, at\\nvarious spatial scales; and the content is the higher-level macrostructure of the image.\\nFor instance, blue-and-yellow circular brushstrokes are considered to be the style in fig-\\nure 8.7 (using Starry Night by Vincent Van Gogh), and the buildings in the Tübingen\\nphotograph are considered to be the content.\\n The idea of style transfer, which is tightly related to that of texture generation, has\\nhad a long history in the image-processi ng community prior to the development of\\nneural style transfer in 2015. But as it tu rns out, the deep-learning-based implementa-\\ntions of style transfer offer results unparalleled by what had been previously achieved\\nwith classical computer-vision techniques, and they triggered an amazing renaissance\\nin creative applications of computer vision.\\n The key notion behind implementing style transfer is the same idea that’s central\\nto all deep-learning algorithms: you define a loss function to specify what you want to\\nachieve, and you minimize this loss. You kn ow what you want to achieve: conserving\\nthe content of the original image while adop ting the style of the reference image. If\\nwe were able to mathematically define content and style, then an appropriate loss func-\\ntion to minimize would be the following:\\nloss = distance(style(reference_image) - style(generated_image)) +\\ndistance(content(original_image) - content(generated_image))\\n5 Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, “A Neural Algorithm of Artistic Style,” arXiv (2015),\\nhttps:/ /arxiv.org/abs/1508.06576.\\nContent target Style reference Combination image\\nFigure 8.7 A style transfer example\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 300, 'page_label': '301'}, page_content='288 CHAPTER 8 Generative deep learning\\nHere, distance is a norm function such as the L2 norm, content is a function that\\ntakes an image and computes a representation of its content, and style is a function\\nthat takes an image and computes a repres entation of its style. Minimizing this\\nloss causes style(generated_image) to be close to style(reference_image), and\\ncontent(generated_image) is close to content(generated_image), thus achieving\\nstyle transfer as we defined it.\\n A fundamental observation made by Gatys et al. was that deep convolutional neu-\\nral networks offer a way to mathematically define the style and content functions.\\nLet’s see how.\\n8.3.1 The content loss\\nAs you already know, activations from earlier layers in a network contain local informa-\\ntion about the image, whereas activations from higher layers contain increasingly global,\\nabstract information. Formulated in a different way, the activations of the different lay-\\ners of a convnet provide a decomposition of the contents of an image over different spa-\\ntial scales. Therefore, you’d expect the content of an image, which is more global and\\nabstract, to be captured by the representations of the upper layers in a convnet.\\n A good candidate for content loss is thus  the L2 norm between the activations of\\nan upper layer in a pretrained convnet, computed over the target image, and the acti-\\nvations of the same layer computed over the generated image. This guarantees that, as\\nseen from the upper layer, the generated image will look similar to the original target\\nimage. Assuming that what the upper layers  of a convnet see is really the content of\\ntheir input images, then this works as a way to preserve image content. \\n8.3.2 The style loss\\nThe content loss only uses a single upper la yer, but the style loss as defined by Gatys\\net al. uses multiple layers of a convnet: yo u try to capture the appearance of the style-\\nreference image at all spatial scales extracte d by the convnet, not just a single scale.\\nFor the style loss, Gatys et al. use the Gram matrix  of a layer’s activations: the inner\\nproduct of the feature maps of a given layer. This inner product can be understood as\\nrepresenting a map of the correlations between the layer’s features. These feature cor-\\nrelations capture the statistics of the patterns of a particular spatial scale, which empir-\\nically correspond to the appearance of the textures found at this scale.\\n Hence, the style loss aims to preserve similar internal correlations within the activa-\\ntions of different layers, across the style- reference image and the generated image. In\\nturn, this guarantees that the textures foun d at different spatial scales look similar\\nacross the style-reference image and the generated image.\\n In short, you can use a pretrained convnet to define a loss that will do the following:\\n\\uf0a1 Preserve content by maintaining similar high-level layer activations between the\\ntarget content image and the generated image. The convnet should “see” both\\nthe target image and the generated image as containing the same things.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 301, 'page_label': '302'}, page_content=\"289Neural style transfer\\n\\uf0a1 Preserve style by maintaining similar correlations within activations for both low-\\nlevel layers and high-level layers. Feature correlations capture textures: the gen-\\nerated image and the style-reference im age should share the same textures at\\ndifferent spatial scales.\\nNow, let’s look at a Keras implementation of the original 2015 neural style transfer\\nalgorithm. As you’ll see, it shares many similarities with the DeepDream implementa-\\ntion developed in the previous section. \\n8.3.3 Neural style transfer in Keras\\nNeural style transfer can be implemented using any pretrained convnet. Here, you’ll\\nuse the VGG19 network used by Gatys et al. VGG19 is a simple variant of the VGG16 net-\\nwork introduced in chapter 5, with three more convolutional layers.\\n This is the general process:\\n1 Set up a network that computes VGG19 layer activations for the style-reference\\nimage, the target image, and the generated image at the same time.\\n2 Use the layer activations computed over these three images to define the loss\\nfunction described earlier, which you’ll  minimize in order to achieve style\\ntransfer.\\n3 Set up a gradient-descent process to minimize this loss function.\\nLet’s start by defining the paths to the st yle-reference image and the target image. To\\nmake sure that the processed images are a similar size (widel y different sizes make\\nstyle transfer more difficult), you’ll later resize them all to a shared height of 400 px.\\nfrom keras.preprocessing.image import load_img, img_to_array\\ntarget_image_path = 'img/portrait.jpg'\\nstyle_reference_image_path = 'img/transfer_style_reference.jpg'\\nwidth, height = load_img(target_image_path).size\\nimg_height = 400\\nimg_width = int(width * img_height / height)\\nYou need some auxiliary functions for loading, preprocessing, and postprocessing the\\nimages that go in and out of the VGG19 convnet.\\nimport numpy as np\\nfrom keras.applications import vgg19\\ndef preprocess_image(image_path):\\nimg = load_img(image_path, target_size=(img_height, img_width))\\nimg = img_to_array(img)\\nimg = np.expand_dims(img, axis=0)\\nimg = vgg19.preprocess_input(img)\\nreturn img\\nListing 8.14 Defining initial variables\\nListing 8.15 Auxiliary functions\\nPath to the image you \\nwant to transform\\nPath to the \\nstyle image\\nDimensions of the \\ngenerated picture\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 302, 'page_label': '303'}, page_content=\"290 CHAPTER 8 Generative deep learning\\ndef deprocess_image(x):\\nx[:, :, 0] += 103.939\\nx[:, :, 1] += 116.779\\nx[:, :, 2] += 123.68\\nx = x[:, :, ::-1]\\nx = np.clip(x, 0, 255).astype('uint8')\\nreturn x\\nLet’s set up the VGG19 network. It takes as input a batch of three images: the style-\\nreference image, the target image, and a pl aceholder that will contain the generated\\nimage. A placeholder is a symbolic tensor, the values of which are provided externally\\nvia Numpy arrays. The style-reference and ta rget image are static and thus defined\\nusing K.constant, whereas the values contained in the placeholder of the generated\\nimage will change over time.\\nfrom keras import backend as K\\ntarget_image = K.constant(preprocess_image(target_image_path))\\nstyle_reference_image = K.constant(preprocess_image(style_reference_image_path))\\ncombination_image = K.placeholder((1, img_height, img_width, 3))\\ninput_tensor = K.concatenate([target_image,\\nstyle_reference_image,\\ncombination_image], axis=0)\\nmodel = vgg19.VGG19(input_tensor=input_tensor,\\nweights='imagenet',\\ninclude_top=False)\\nprint('Model loaded.')\\nLet’s define the content loss, which will make sure the top layer of the VGG19 convnet\\nhas a similar view of the target image and the generated image.\\ndef content_loss(base, combination):\\nreturn K.sum(K.square(combination - base))\\nNext is the style loss. It uses an auxiliary function to compute the Gram matrix of an\\ninput matrix: a map of the correlations found in the original feature matrix.\\ndef gram_matrix(x):\\nfeatures = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\\ngram = K.dot(features, K.transpose(features))\\nreturn gram\\nListing 8.16 Loading the pretrained VGG19 network and applying it to the three images\\nListing 8.17 Content loss\\nListing 8.18 Style loss\\nZero-centering by removing the mean pixel value \\nfrom ImageNet. This reverses a transformation \\ndone by vgg19.preprocess_input.\\nConverts images from 'BGR' to 'RGB'. \\nThis is also part of the reversal of \\nvgg19.preprocess_input.\\nPlaceholder that will contain\\nthe generated image\\nCombines the three \\nimages in a single batch\\nBuilds the VGG19 network with \\nthe batch of three images as \\ninput. The model will be loaded \\nwith pretrained ImageNet weights.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 303, 'page_label': '304'}, page_content=\"291Neural style transfer\\ndef style_loss(style, combination):\\nS = gram_matrix(style)\\nC = gram_matrix(combination)\\nchannels = 3\\nsize = img_height * img_width\\nreturn K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\\nTo these two loss components, you add a third: the total variation loss, which operates\\non the pixels of the generated combination image. It encourages spatial continuity in\\nthe generated image, thus avoiding overly pi xelated results. You can interpret it as a\\nregularization loss.\\ndef total_variation_loss(x):\\na = K.square(\\nx[:, :img_height - 1, :img_width - 1, :] -\\n        x[:, 1:, :img_width - 1, :])\\nb = K.square(\\nx[:, :img_height - 1, :img_width - 1, :] -\\n       x[:, :img_height - 1, 1:, :])\\nreturn K.sum(K.pow(a + b, 1.25))\\nThe loss that you minimize is a weighted average of these three losses. To compute the\\ncontent loss, you use only one upper layer—the block5_conv2 layer—whereas for the\\nstyle loss, you use a list of layers than sp ans both low-level and high-level layers. You\\nadd the total variation loss at the end.\\n Depending on the style-reference image and content image you’re using, you’ll\\nlikely want to tune the content_weight coefficient (the contribution of the content\\nloss to the total loss). A higher content_weight means the target content will be more\\nrecognizable in the generated image.\\noutputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\\ncontent_layer = 'block5_conv2'\\nstyle_layers = ['block1_conv1',\\n'block2_conv1',\\n'block3_conv1',\\n'block4_conv1',\\n'block5_conv1']\\ntotal_variation_weight = 1e-4\\nstyle_weight = 1.\\ncontent_weight = 0.025\\nListing 8.19 Total variation loss\\nListing 8.20 Defining the final loss that you’ll minimize\\nDictionary that maps layer \\nnames to activation tensors\\nLayer used for content loss\\nLayers used for style loss\\nWeights in the weighted average \\nof the loss components\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 304, 'page_label': '305'}, page_content=\"292 CHAPTER 8 Generative deep learning\\nloss = K.variable(0.)\\nlayer_features = outputs_dict[content_layer]\\ntarget_image_features = layer_features[0, :, :, :]\\ncombination_features = layer_features[2, :, :, :]\\nloss += content_weight * content_loss(target_image_features,\\ncombination_features)\\nfor layer_name in style_layers:\\nlayer_features = outputs_dict[layer_name]\\nstyle_reference_features = layer_features[1, :, :, :]\\ncombination_features = layer_features[2, :, :, :]\\nsl = style_loss(style_reference_features, combination_features)\\nloss += (style_weight / len(style_layers)) * sl\\nloss += total_variation_weight * total_variation_loss(combination_image)\\nFinally, you’ll set up the gradient-descent pr ocess. In the original  Gatys et al. paper,\\noptimization is performed using the L-BFGS algorithm, so that’s what you’ll use here.\\nThis is a key difference from the DeepDream example in section 8.2. The L-BFGS algo-\\nrithm comes packaged with SciPy, but there are two slight limitations with the SciPy\\nimplementation:\\n\\uf0a1 It requires that you pass the value of th e loss function and the value of the gra-\\ndients as two separate functions.\\n\\uf0a1 It can only be applied to flat vectors, whereas you have a 3D image array.\\nIt would be inefficient to compute the valu e of the loss function and the value of the\\ngradients independently, because doing so would lead to a lot of redundant computa-\\ntion between the two; the process would be  almost twice as sl ow as computing them\\njointly. To bypass this, you’ll set up a Python class named Evaluator that computes\\nboth the loss value and the gradients value at  once, returns the loss value when called\\nthe first time, and caches the gradients for the next call.\\ngrads = K.gradients(loss, combination_image)[0]\\nfetch_loss_and_grads = K.function([combination_image], [loss, grads])\\nclass Evaluator(object):\\ndef __init__(self):\\nself.loss_value = None\\nself.grads_values = None\\ndef loss(self, x):\\nassert self.loss_value is None\\nx = x.reshape((1, img_height, img_width, 3))\\nouts = fetch_loss_and_grads([x])\\nListing 8.21 Setting up the gradient-descent process\\nAdds\\nthe\\ncontent\\nloss\\nYou’ll define the loss by \\nadding all components to \\nthis scalar variable.\\nAdds a style loss \\ncomponent for \\neach target layer\\nAdds the\\ntotal\\nvariation\\nloss\\nFunction to fetch\\nthe values of\\nthe current loss\\nand the current\\ngradients\\nThis class wraps fetch_loss_and_grads\\nin a way that lets you retrieve the losses and\\ngradients via two separate method calls, which is\\nrequired by the SciPy optimizer you'll use.\\nGets the\\ngradients\\nof the\\ngenerated\\nimage with\\nregard to\\nthe loss\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 305, 'page_label': '306'}, page_content=\"293Neural style transfer\\nloss_value = outs[0]\\ngrad_values = outs[1].flatten().astype('float64')\\nself.loss_value = loss_value\\nself.grad_values = grad_values\\nreturn self.loss_value\\ndef grads(self, x):\\nassert self.loss_value is not None\\ngrad_values = np.copy(self.grad_values)\\nself.loss_value = None\\nself.grad_values = None\\nreturn grad_values\\nevaluator = Evaluator()\\nFinally, you can run the gradient-ascent process using SciPy’s L-BFGS algorithm, saving\\nthe current generated image at each iteratio n of the algorithm (here, a single itera-\\ntion represents 20 steps of gradient ascent).\\nfrom scipy.optimize import fmin_l_bfgs_b\\nfrom scipy.misc import imsave\\nimport time\\nresult_prefix = 'my_result'\\niterations = 20\\nx = preprocess_image(target_image_path)\\nx = x.flatten()\\nfor i in range(iterations):\\nprint('Start of iteration', i)\\nstart_time = time.time()\\nx, min_val, info = fmin_l_bfgs_b(evaluator.loss,\\n                                     x,\\n        fprime=evaluator.grads,\\n                                     maxfun=20)\\nprint('Current loss value:', min_val)\\nimg = x.copy().reshape((img_height, img_width, 3))\\nimg = deprocess_image(img)\\nfname = result_prefix + '_at_iteration_%d.png' % i\\nimsave(fname, img)\\nprint('Image saved as', fname)\\nend_time = time.time()\\nprint('Iteration %d completed in %ds' % (i, end_time - start_time))\\nF i g u r e  8 . 8  s h o w s  w h a t  y o u  g e t .  K e e p  i n  m i n d  t h a t  w h a t  t h i s  t e c h n i q u e  a c h i e v e s  i s\\nmerely a form of image retexturing, or te xture transfer. It works best with style-\\nreference images that are strongly textured  and highly self-similar, and with content\\ntargets that don’t require high levels of detail in order to be recognizable. It typically\\ncan’t achieve fairly abstract feats such as  transferring the style of one portrait to\\nanother. The algorithm is closer to classical sign al processing than to AI, so don’t\\nexpect it to work like magic!\\nListing 8.22 Style-transfer loop\\nThis is the initial state: \\nthe target image.\\nYou flatten the image because \\nscipy.optimize.fmin_l_bfgs_b \\ncan only process flat vectors.\\nRuns L-BFGS optimization \\nover the pixels of the \\ngenerated image to \\nminimize the neural style \\nloss. Note that you have \\nto pass the function that \\ncomputes the loss and the \\nfunction that computes \\nthe gradients as two \\nseparate arguments.\\nSaves the current \\ngenerated image.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 306, 'page_label': '307'}, page_content='294 CHAPTER 8 Generative deep learning\\n \\nFigure 8.8 Some example results\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 307, 'page_label': '308'}, page_content='295Neural style transfer\\nAdditionally, note that running this style-tr ansfer algorithm is slow. But the transfor-\\nmation operated by the setup is simple enough that it ca n be learned by a small, fast\\nfeedforward convnet as well—as long as you have appropriate training data available.\\nFast style transfer can thus be achieved by  first spending a lot of compute cycles to\\ngenerate input-output training examples for a fixed style-reference image, using the\\nmethod outlined here, and then training a simple convnet to learn this style-specific\\ntransformation. Once that’s done, stylizing a given image is instantaneous: it’s just a\\nforward pass of this small convnet. \\n8.3.4 Wrapping up\\n\\uf0a1 Style transfer consists of creating a new image that preserves the contents of a\\ntarget image while also capturing the style of a reference image.\\n\\uf0a1 Content can be captured by the high-level activations of a convnet.\\n\\uf0a1 Style can be captured by the internal corr elations of the activations of different\\nlayers of a convnet.\\n\\uf0a1 Hence, deep learning allows  style transfer to be form ulated as an optimization\\nprocess using a loss defined with a pretrained convnet.\\n\\uf0a1 Starting from this basic idea, many variants and refinements are possible. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 308, 'page_label': '309'}, page_content='296 CHAPTER 8 Generative deep learning\\n8.4 Generating images with variational autoencoders\\nSampling from a latent space of images to cr eate entirely new images or edit existing\\nones is currently the most popular and successful application of creative AI. In this sec-\\ntion and the next, we’ll review some high-l evel concepts pertaining to image genera-\\ntion, alongside implementations details rela tive to the two main techniques in this\\ndomain: variational autoencoders (VAEs) and generative adversarial networks  (GANs). The\\ntechniques we present here aren’t specific to images—you could develop latent spaces\\nof sound, music, or  even text, using GANs and VAEs—but in practice, the most inter-\\nesting results have been obtained with pictures, and that’s what we focus on here.\\n8.4.1 Sampling from latent spaces of images\\nThe key idea of image generation is to develop a low-dimensional latent space of repre-\\nsentations (which naturally is a vector sp ace) where any point can be mapped to a\\nrealistic-looking image. The module capable of realizing this mapping, taking as input\\na latent point and outputting an image (a grid of pixels), is called a generator (in the\\ncase of GANs) or a decoder (in the case of VAEs). Once such a latent space has been\\ndeveloped, you can sample points from it, either deliberately or at random, and, by\\nmapping them to image space, generate images that have never been seen before (see\\nfigure 8.9).\\nGANs and VAEs are two different strategies for lear ning such latent spaces of image\\nrepresentations, each with  its own characteristics. VAEs are great for learning latent\\nspaces that are well structured , where specific directions encode a meaningful axis of\\nvariation in the data. GANs generate images that can potentially be highly realistic, but\\nthe latent space they come from may not have as much structure and continuity.\\nGenerator / Decoder\\nTraining data\\nLatent space\\nof images\\n(a vector space)\\nVector from the \\nlatent space\\nArtificial\\nimage\\nLearning\\nprocess?\\nFigure 8.9 Learning a latent vector space of images, and using it to sample new images\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 309, 'page_label': '310'}, page_content='297Generating images with variational autoencoders\\n  \\n8.4.2 Concept vectors for image editing\\nWe already hinted at the idea of a concept vector when we covered word embeddings in\\nchapter 6. The idea is still the same: give n a latent space of representations, or an\\nembedding space, certain directions in the space may encode interesting axes of vari-\\nation in the original data. In a latent space of images of faces, for instance, there may\\nbe a smile vectors, such that if latent point z is the embedded representation of a cer-\\ntain face, then latent point z + s  is the embedded representation of the same face,\\nsmiling. Once you’ve identified such a vect or, it then becomes possible to edit images\\nby projecting them into the latent space, moving their representation in a meaningful\\nw a y ,  a n d  t h e n  d e c o d i n g  t h e m  b a c k  t o  i mage space. There are concept vectors for\\nessentially any independent dimension of variation in image space—in the case of\\nfaces, you may discover vectors for adding sunglasses to a face, removing glasses, turn-\\ning a male face into as female face, and so on. Figure 8.11 is an example of a smile vec-\\ntor, a concept vector discovered by Tom Wh ite from the Victoria University School of\\nDesign in New Zealand, using VAEs trained on a dataset of faces of celebrities (the\\nCelebA dataset).\\nFigure 8.10 A continuous space of faces generated by Tom White using VAEs\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 310, 'page_label': '311'}, page_content='298 CHAPTER 8 Generative deep learning\\n  \\n8.4.3 Variational autoencoders\\nVariational autoencoders, simultaneously  discovered by Kingma and Welling in\\nDecember 2013 6 and Rezende, Mohamed, and Wierstra in January 2014, 7 are a kind\\nof generative model that’s especially appropriate for the task of image editing via con-\\ncept vectors. They’re a modern take on autoencoders—a type of network that aims to\\nencode an input to a low-dimensional late nt space and then decode it back—that\\nmixes ideas from deep learning with Bayesian inference.\\n A classical image autoencoder takes an imag e, maps it to a latent vector space via\\nan encoder module, and then decodes it back to an output with the same dimensions\\nas the original image, via a decoder modu le (see figure 8.12). It’s then trained by\\nusing as target data the same images  as the input images, meaning the autoencoder\\nlearns to reconstruct the original inputs. By imposing various constraints on the code\\n(the output of the encoder), you can get the autoencoder to learn more-or-less inter-\\nesting latent representations of the data. Most commonly, you’ll constrain the code to\\nbe low-dimensional and sparse (mostly zeros), in which case the encoder acts as a way\\nto compress the input data into fewer bits of information.\\n6 Diederik P. Kingma and Max Welling, “Auto- Encoding Variational Bayes, arXiv (2013), https:/ /arxiv.org/\\nabs/1312.6114.\\n7 Danilo Jimenez Rezende, Shakir Mohamed, and Daan  Wierstra, “Stochastic Backpropagation and Approxi-\\nmate Inference in Deep Generative Models,” arXiv (2014), https:/ /arxiv.org/abs/1401.4082.\\nFigure 8.11 The smile vector\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 311, 'page_label': '312'}, page_content=\"299Generating images with variational autoencoders\\n \\nIn practice, such classical au toencoders don’t lead to particularly useful or nicely\\nstructured latent spaces. They’re not much good at compression, either. For these rea-\\nsons, they have largely fallen out of fashion. VAEs, however, augm ent autoencoders\\nwith a little bit of statistical magic that fo rces them to learn continuous, highly struc-\\ntured latent spaces. They have turned out to be a powerful tool for image generation.\\n A VAE, instead of compressing its input image into a fixed code in the latent space,\\nturns the image into the parameters of a st atistical distribution: a mean and a vari-\\nance. Essentially, this means you’re assuming the input image has been generated by a\\nstatistical process, and that the randomne ss of this process should be taken into\\naccounting during encoding and decoding. The VAE then uses the mean and variance\\nparameters to randomly sample one element of the distribution, and decodes that ele-\\nment back to the original input (see figure  8.13). The stochasticity of this process\\nimproves robustness and forces the latent space to encode meaningful representa-\\ntions everywhere: every point sampled in the latent space is decoded to a valid output.\\n \\n \\nFigure 8.12 An autoencoder: mapping an input x to a compressed representation \\nand then decoding it back as x'\\nOriginal\\ninput x\\nCompressed\\nrepresentation\\nReconstructed\\ninput x\\nEncoder Decoder\\nInput image\\nReconstructed\\nimage\\nDistribution over latent\\nspace defined by z_mean\\nand z_log_var\\nPoint randomly\\nsampled from\\nthe distribution\\nEncoder\\nDecoder\\nFigure 8.13 A VAE maps an image to two vectors, z_mean and z_log_sigma, which define \\na probability distribution over the latent space, used to sample a latent point to decode.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 312, 'page_label': '313'}, page_content='300 CHAPTER 8 Generative deep learning\\n In technical terms, here’s how a VAE works:\\n1 An encoder module turns the input samples input_img into two parameters in\\na latent space of representations, z_mean and z_log_variance.\\n2 You randomly sample a point z from the latent normal distribution that’s\\nassumed to generate the input image, via z = z_mean + exp(z_log_variance) *\\nepsilon, where epsilon is a random tensor of small values.\\n3 A decoder module maps this point in the latent space back to the original input\\nimage.\\nBecause epsilon is random, the process ensures that every point that’s close to the latent loca-\\ntion where you encoded input_img (z-mean) can be decoded to something similar to\\ninput_img, thus forcing the latent space to be continuously meaningful. Any two close points\\nin the latent space will decode to highly similar images. Continuity, combined with the low\\ndimensionality of the latent space, forces every direction in the latent space to encode a mean-\\ningful axis of variation of the data, making the latent space very structured and thus highly suit-\\nable to manipulation via concept vectors.\\n The parameters of a VAE are trained via two loss functions: a reconstruction loss that\\nforces the decoded samples to match the initial inputs, and a regularization loss  that\\nhelps learn well-formed latent spaces and reduce overfitting to the training data. Let’s\\nquickly go over a Keras implementation of a VAE. Schematically, it looks like this:\\nz_mean, z_log_variance = encoder(input_img)\\nz = z_mean + exp(z_log_variance) * epsilon\\nreconstructed_img = decoder(z)\\nmodel = Model(input_img, reconstructed_img)\\nYou can then train the model using the reconstruction loss and the regularization loss.\\n The following listing shows the encoder network you’ll use, mapping images to the\\nparameters of a probability distribution over  the latent space. It ’s a simple convnet\\nthat maps the input image x to two vectors, z_mean and z_log_var.\\nimport keras\\nfrom keras import layers\\nfrom keras import backend as K\\nfrom keras.models import Model\\nimport numpy as np\\nimg_shape = (28, 28, 1)\\nbatch_size = 16\\nlatent_dim = 2\\ninput_img = keras.Input(shape=img_shape)\\nListing 8.23 VAE encoder network\\nEncodes the input into a \\nmean and variance parameter\\nDraws a latent point using \\na small random epsilonDecodes\\nz back to\\nan image Instantiates the autoencoder \\nmodel, which maps an input image \\nto its reconstruction\\nDimensionality of the \\nlatent space: a 2D plane\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 313, 'page_label': '314'}, page_content=\"301Generating images with variational autoencoders\\nx = layers.Conv2D(32, 3,\\npadding='same', activation='relu')(input_img)\\nx = layers.Conv2D(64, 3,\\npadding='same', activation='relu',\\nstrides=(2, 2))(x)\\nx = layers.Conv2D(64, 3,\\npadding='same', activation='relu')(x)\\nx = layers.Conv2D(64, 3,\\npadding='same', activation='relu')(x)\\nshape_before_flattening = K.int_shape(x)\\nx = layers.Flatten()(x)\\nx = layers.Dense(32, activation='relu')(x)\\nz_mean = layers.Dense(latent_dim)(x)\\nz_log_var = layers.Dense(latent_dim)(x)\\nNext is the code for using z_mean and z_log_var, the parameters of the statistical dis-\\ntribution assumed to  have produced input_img, to generate a latent space point z.\\nHere, you wrap some arbitrary code (built on top of Keras backend primitives) into a\\nLambda layer. In Keras, everything needs to be a layer, so code that isn’t part of a built-\\nin layer should be wrapped in a Lambda (or in a custom layer).\\ndef sampling(args):\\nz_mean, z_log_var = args\\nepsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\\nmean=0., stddev=1.)\\nreturn z_mean + K.exp(z_log_var) * epsilon\\nz = layers.Lambda(sampling)([z_mean, z_log_var])\\nThe following listing shows the decoder implementation. You reshape the vector z to\\nthe dimensions of an image and then use a few convolution layers to obtain a final\\nimage output that has the same dimensions as the original input_img.\\ndecoder_input = layers.Input(K.int_shape(z)[1:])\\nx = layers.Dense(np.prod(shape_before_flattening[1:]),\\nactivation='relu')(decoder_input)\\nx = layers.Reshape(shape_before_flattening[1:])(x)\\nx = layers.Conv2DTranspose(32, 3,\\npadding='same',\\n                           activation='relu',\\nstrides=(2, 2))(x)\\nx = layers.Conv2D(1, 3,\\npadding='same',\\n                  activation='sigmoid')(x)\\nListing 8.24 Latent-space-sampling function\\nListing 8.25 VAE decoder network, mapping latent space points to images\\nThe input image ends up \\nbeing encoded into these \\ntwo parameters.\\nInput where you’ll feed z\\nUpsamples the input\\nUses a Conv2DTranspose \\nlayer and Conv2D layer to \\ndecode z into a feature map \\nthe same size as the \\noriginal image input\\nReshapes z into a feature map of the same shape as the feature \\nmap just before the last Flatten layer in the encoder model\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 314, 'page_label': '315'}, page_content=\"302 CHAPTER 8 Generative deep learning\\ndecoder = Model(decoder_input, x)\\nz_decoded = decoder(z)\\nThe dual loss of a VAE doesn’t fit the traditional expectation of a sample-wise function\\nof the form loss(input, target). Thus, you’ll set up the loss by writing a custom\\nlayer that internally uses the built-in add_loss layer method to create an arbitrary loss.\\nclass CustomVariationalLayer(keras.layers.Layer):\\ndef vae_loss(self, x, z_decoded):\\nx = K.flatten(x)\\nz_decoded = K.flatten(z_decoded)\\nxent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\\nkl_loss = -5e-4 * K.mean(\\n1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\\nreturn K.mean(xent_loss + kl_loss)\\ndef call(self, inputs):\\nx = inputs[0]\\nz_decoded = inputs[1]\\nloss = self.vae_loss(x, z_decoded)\\nself.add_loss(loss, inputs=inputs)\\nreturn x\\ny = CustomVariationalLayer()([input_img, z_decoded])\\nFinally, you’re ready to instantiate and train the model. Because the loss is taken care\\nof in the custom layer, you don’t specify an external loss at compile time (loss=None),\\nwhich in turn means you won’t pass target data during training (as you can see, you\\nonly pass x_train to the model in fit).\\nfrom keras.datasets import mnist\\nvae = Model(input_img, y)\\nvae.compile(optimizer='rmsprop', loss=None)\\nvae.summary()\\n(x_train, _), (x_test, y_test) = mnist.load_data()\\nx_train = x_train.astype('float32') / 255.\\nx_train = x_train.reshape(x_train.shape + (1,))\\nx_test = x_test.astype('float32') / 255.\\nx_test = x_test.reshape(x_test.shape + (1,))\\nvae.fit(x=x_train, y=None,\\nshuffle=True,\\nepochs=10,\\nbatch_size=batch_size,\\nvalidation_data=(x_test, None))\\nListing 8.26 Custom layer used to compute the VAE loss\\nListing 8.27 Training the VAE\\nInstantiates the decoder model, \\nwhich turns “decoder_input” \\ninto the decoded imageApplies it to z to \\nrecover the decoded z\\nYou implement custom layers \\nby writing a call method.You don't use\\nthis output,\\nbut the layer\\nmust return\\nsomething.\\nCalls the custom layer on \\nthe input and the \\ndecoded output to obtain \\nthe final model output\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 315, 'page_label': '316'}, page_content=\"303Generating images with variational autoencoders\\nOnce such a model is trained—on MNIST, in this case—you can use the decoder net-\\nwork to turn arbitrary latent space vectors into images.\\nimport matplotlib.pyplot as plt\\nfrom scipy.stats import norm\\nn=1 5\\ndigit_size = 28\\nfigure = np.zeros((digit_size * n, digit_size * n))\\ngrid_x = norm.ppf(np.linspace(0.05, 0.95, n))\\ngrid_y = norm.ppf(np.linspace(0.05, 0.95, n))\\nfor i, yi in enumerate(grid_x):\\nfor j, xi in enumerate(grid_y):\\nz_sample = np.array([[xi, yi]])\\nz_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\\nx_decoded = decoder.predict(z_sample, batch_size=batch_size)\\ndigit = x_decoded[0].reshape(digit_size, digit_size)\\nfigure[i * digit_size: (i + 1) * digit_size,\\nj * digit_size: (j + 1) * digit_size] = digit\\nplt.figure(figsize=(10, 10))\\nplt.imshow(figure, cmap='Greys_r')\\nplt.show()\\n \\nThe grid of sampled digits (see fig-\\nure 8.14) shows a completely contin-\\nuous distribution of the different\\ndigit classes, with one digit morphing\\ninto another as you follow a path\\nthrough latent space. Specific direc-\\ntions in this space have a meaning:\\nfor example, there’s a direction for\\n“four-ness,” “one-ness,” and so on.\\n In the next section, we’ll cover in\\ndetail the other major tool for gener-\\nating artificial images: generative\\nadversarial networks (\\nGANs). \\n \\n \\n \\nListing 8.28 Sampling a grid of points from the 2D latent space and decoding them to images\\nYou’ll display a grid of 15 × 15 \\ndigits (255 digits total).\\nTransforms linearly spaced \\ncoordinates using the SciPy ppf \\nfunction to produce values of the \\nlatent variable z (because the prior \\nof the latent space is Gaussian)\\nRepeats z multiple times to\\nform a complete batch\\nReshapes the first digit in\\nthe batch from 28 × 28 × 1\\nto 28 × 28\\nDecodes the batch\\ninto digit images\\nFigure 8.14 Grid of digits decoded from the latent \\nspace\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 316, 'page_label': '317'}, page_content='304 CHAPTER 8 Generative deep learning\\n8.4.4 Wrapping up\\n\\uf0a1 Image generation with deep learning is done by learning latent spaces that cap-\\nture statistical information about a da taset of images. By sampling and decod-\\ning points from the latent space, you can generate never-before-seen images.\\nThere are two major tools to do this: VAEs and GANs.\\n\\uf0a1 VAEs result in highly structured, continuous latent representations. For this rea-\\nson, they work well for doing all sorts of image editing in latent space: face\\nswapping, turning a frowning face into a smiling face, and so on. They also work\\nnicely for doing latent-space-based animations, such as animating a walk along a\\ncross section of the latent space, showing a starting image slowly morphing into\\ndifferent images in a continuous way.\\n\\uf0a1 GANs enable the generation of realistic single-frame images but may not induce\\nlatent spaces with solid structure and high continuity.\\nMost successful practical applications I have seen with images rely on VAEs, but GANs\\nare extremely popular in the world of ac ademic research—at least, circa 2016–2017.\\nYou’ll find out how they work and how to implement one in the next section.\\nTIP To play further with image generation, I suggest working with the Large-\\nscale Celeb Faces Attributes (CelebA) da taset. It’s a free-to-download image\\ndataset containing more than 200,000 celebrity portraits. It’s great for experi-\\nmenting with concept vectors in particular—it definitely beats MNIST. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 317, 'page_label': '318'}, page_content='305Introduction to generative adversarial networks\\n8.5 Introduction to generative adversarial networks\\nGenerative adversarial networks (GANs), introduced in 2014 by Goodfellow et al.,8 are\\nan alternative to VAEs for learning latent spaces of images. They enable the generation\\nof fairly realistic synthetic images by forcing the generated images to be statistically\\nalmost indistinguishable from real ones.\\n An intuitive way to understand GANs is to imagine a forger trying to create a fake\\nPicasso painting. At first, the forger is pr etty bad at the task. He mixes some of his\\nfakes with authentic Picassos and shows them all to an art dealer. The art dealer makes\\nan authenticity assessment for each painting and gives the forger feedback about what\\nmakes a Picasso look like a Picasso. The forger goes back to his studio to prepare some\\nnew fakes. As times goes on, the forger be comes increasingly competent at imitating\\nthe style of Picasso, and the art dealer beco mes increasingly expert  at spotting fakes.\\nIn the end, they have on their hands some excellent fake Picassos.\\n That’s what a GAN is: a forger network and an expe rt network, each being trained\\nto best the other. As such, a GAN is made of two parts:\\n\\uf0a1 Generator network —Takes as input a random vect or (a random point in the\\nlatent space), and decodes it into a synthetic image\\n\\uf0a1 Discriminator network (or adversary) —Takes as input an image (real or synthetic),\\nand predicts whether the image came fr om the training set or was created by\\nthe generator network.\\nThe generator network is trained to be able  to fool the discriminator network, and\\nthus it evolves toward generating increasingly realistic images as training goes on: arti-\\nficial images that look indistinguishable from real ones, to the extent that it’s impossi-\\nble for the discriminator network to tell th e two apart (see figu re 8.15). Meanwhile,\\nthe discriminator is constantly adapting to the gradually improving capabilities of the\\ngenerator, setting a high bar of realism for the generated images. Once training is\\nover, the generator is capable of turning an y point in its input space into a believable\\nimage. Unlike VAEs, this latent space has fewer ex plicit guarantees of meaningful\\nstructure; in particular, it isn’t continuous.\\n8 Ian Goodfellow et al., “Generative Adversarial Networks,” arXiv (2014), https:/ /arxiv.org/abs/1406.2661.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 318, 'page_label': '319'}, page_content='306 CHAPTER 8 Generative deep learning\\n \\nRemarkably, a GAN is a system where the optimization minimum isn’t fixed, unlike in\\nany other training setup you’ve encountered in this book. Normally, gradient descent\\nconsists of rolling down hills in a static loss landscape. But with a GAN, every step\\ntaken down the hill changes the entire land scape a little. It’s a dynamic system where\\nthe optimization process is seeking not a minimum, but an equi librium between two\\nforces. For this reason, GANs are notoriously difficult to train—getting a GAN to work\\nrequires lots of careful tuning of the model architecture and training parameters.\\nGenerator (decoder)\\nDiscriminator “Real,” “Fake”\\nRandom vector\\nfrom the \\nlatent space\\nGenerated\\n(decoded)\\nimage\\nMix of real \\nand fake images\\nTraining\\nfeedback\\nFigure 8.15 A generator transforms random latent  vectors into images, and a discriminator \\nseeks to tell real images from generated ones. The generator is trained to fool the discriminator.\\nFigure 8.16 Latent space dwellers. Images generated by Mike Tyka using \\na multistaged GAN trained on a dataset of faces (www.miketyka.com).\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 319, 'page_label': '320'}, page_content='307Introduction to generative adversarial networks\\n8.5.1 A schematic GAN implementation\\nIn this section, we’ll explain how to implement a GAN in Keras, in its barest form—\\nbecause GANs are advanced, diving d eeply into the technical details would be out of\\nscope for this book. The specific implementation is a deep convolutional GAN (DCGAN):\\na GAN where the generator and discriminator are deep convnets. In particular, it uses\\na Conv2DTranspose layer for image upsampling in the generator.\\n You’ll train the GAN o n  i m a g e s  f r o m  CIFAR10, a dataset of 50,000 32 × 32 RGB\\nimages belonging to 10 classes (5,000 images per class). To make things easier, you’ll\\nonly use images belonging to the class “frog.”\\n Schematically, the GAN looks like this:\\n1 A generator network maps vectors of shape (latent_dim,) to images of shape\\n(32, 32, 3).\\n2 A discriminator network maps images of shape (32, 32, 3) to a binary score\\nestimating the probability that the image is real.\\n3 A gan network chains the generator and the discriminator together: gan(x) =\\ndiscriminator(generator(x)). Thus this gan network maps latent space vec-\\ntors to the discriminator’ s assessment of the realism of these latent vectors as\\ndecoded by the generator.\\n4 You train the discriminator using examples  of real and fake images along with\\n“real”/“fake” labels, just as you train any regular image-classification model.\\n5 To train the generator, you use the gr adients of the generator’s weights with\\nregard to the loss of the gan model. This means, at every step, you move the\\nweights of the generator in a direction that makes the discriminator more likely\\nto classify as “real” the images decoded by the generator. In other words, you\\ntrain the generator to fool the discriminator. \\n8.5.2 A bag of tricks\\nThe process of training GANs and tuning GAN implementations is notoriously diffi-\\ncult. There are a number of known tricks yo u should keep in mi nd. Like most things\\nin deep learning, it’s more  alchemy than science: thes e tricks are heuristics, not\\ntheory-backed guidelines. They’re supported by a level of intuitive understanding of\\nthe phenomenon at hand, and they’re known to work well empirically, although not\\nnecessarily in every context.\\n Here are a few of the tricks used in the implementation of the GAN generator and\\ndiscriminator in this section. It isn’t an exhaustive list of GAN-related tips; you’ll find\\nmany more across the GAN literature:\\n\\uf0a1 We use tanh as the last activation in the generator, instead of sigmoid, which is\\nmore commonly found in other types of models.\\n\\uf0a1 We sample points from the latent space using a normal distribution (Gaussian dis-\\ntribution), not a uniform distribution.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 320, 'page_label': '321'}, page_content='308 CHAPTER 8 Generative deep learning\\n\\uf0a1 Stochasticity is good to induce robustness. Because GAN training results in a\\ndynamic equilibrium, GANs are likely to get stuck in all sorts of ways. Introduc-\\ning randomness during training helps prevent this. We introduce randomness\\nin two ways: by using dropout in the discriminator and by adding random noise\\nto the labels for the discriminator.\\n\\uf0a1 Sparse gradients can hinder GAN training. In deep learni ng, sparsity is often a\\ndesirable property, but not in GANs. Two things can induce gradient sparsity:\\nmax pooling operations and ReLU activations. Instead of  max pooling, we rec-\\nommend using strided convolutions for downsampling, and we recommend\\nusing a LeakyReLU layer instead of a ReLU activation. It’s similar to ReLU, but it\\nrelaxes sparsity constraints by allowing small negative activation values.\\n\\uf0a1 In generated images, it’s common to see checkerboard artifacts caused by\\nunequal coverage of the pixel space in the generator (see figure 8.17). To fix\\nthis, we use a kernel size that’s divisibl e by the stride size whenever we use a\\nstrided Conv2DTranpose or Conv2D in both the generator and the discriminator. \\n8.5.3 The generator\\nFirst, let’s develop a generator model that turns a vector (from the latent space—\\nd u r i n g  t r a i n i n g  i t  w i l l  b e  s a m p l e d  a t  r andom) into a candidate image. One of the\\nmany issues that commonly arise with GANs is that the generator gets stuck with gener-\\nated images that look like noise. A possible solution is to use dropout on both the dis-\\ncriminator and the generator.\\nimport keras\\nfrom keras import layers\\nimport numpy as np\\nlatent_dim = 32\\nheight = 32\\nwidth = 32\\nchannels = 3\\nListing 8.29 GAN generator network\\nFigure 8.17 Checkerboard artifacts caused by mismatching strides and kernel \\nsizes, resulting in unequal pixel-space coverage: one of the many gotchas of GANs\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 321, 'page_label': '322'}, page_content=\"309Introduction to generative adversarial networks\\ngenerator_input = keras.Input(shape=(latent_dim,))\\nx = layers.Dense(128 * 16 * 16)(generator_input)\\nx = layers.LeakyReLU()(x)\\nx = layers.Reshape((16, 16, 128))(x)\\nx = layers.Conv2D(256, 5, padding='same')(x)\\nx = layers.LeakyReLU()(x)\\nx = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\\nx = layers.LeakyReLU()(x)\\nx = layers.Conv2D(256, 5, padding='same')(x)\\nx = layers.LeakyReLU()(x)\\nx = layers.Conv2D(256, 5, padding='same')(x)\\nx = layers.LeakyReLU()(x)\\nx = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\\ngenerator = keras.models.Model(generator_input, x)\\ngenerator.summary()\\n8.5.4 The discriminator\\nNext, you’ll develop a discriminator model that takes as input a candidate image\\n(real or synthetic) and classifi es it into one of two classes: “generated image” or “real\\nimage that comes from the training set.”\\ndiscriminator_input = layers.Input(shape=(height, width, channels))\\nx = layers.Conv2D(128, 3)(discriminator_input)\\nx = layers.LeakyReLU()(x)\\nx = layers.Conv2D(128, 4, strides=2)(x)\\nx = layers.LeakyReLU()(x)\\nx = layers.Conv2D(128, 4, strides=2)(x)\\nx = layers.LeakyReLU()(x)\\nx = layers.Conv2D(128, 4, strides=2)(x)\\nx = layers.LeakyReLU()(x)\\nx = layers.Flatten()(x)\\nx = layers.Dropout(0.4)(x)\\nx = layers.Dense(1, activation='sigmoid')(x)\\ndiscriminator = keras.models.Model(discriminator_input, x)\\ndiscriminator.summary()\\ndiscriminator_optimizer = keras.optimizers.RMSprop(\\n    lr=0.0008,\\n    clipvalue=1.0,\\n    decay=1e-8)\\ndiscriminator.compile(optimizer=discriminator_optimizer,\\nloss='binary_crossentropy')\\nListing 8.30 The GAN discriminator network\\nTransforms the input into \\na 16 × 16 128-channel \\nfeature map\\nUpsamples \\nto 32 × 32\\nInstantiates the generator model, which maps the input \\nof shape (latent_dim,) into an image of shape (32, 32, 3)\\nProduces a 32 × 32 1-channel feature\\nmap  (shape of a CIFAR10 image)\\nOne dropout layer: \\nan important trick!\\nClassification layer\\nInstantiates the discrim-\\ninator model, which turns \\na (32, 32, 3) input into a \\nbinary classifi-cation \\ndecision (fake/real)\\nUses gradient clipping (by \\nvalue) in the optimizer\\nTo stabilize training, \\nuses learning-rate decay\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 322, 'page_label': '323'}, page_content=\"310 CHAPTER 8 Generative deep learning\\n8.5.5 The adversarial network\\nFinally, you’ll set up the GAN, which chains the generator and the discriminator.\\nWhen trained, this model will move the generator in a direction that improves its abil-\\nity to fool the discriminator. This model tu rns latent-space points into a classification\\ndecision—“fake” or “real”—and it’s meant to  be trained with labels that are always\\n“these are real images.” So, training \\ngan will update the weights of generator in a way\\nthat makes discriminator more likely to predict “real” when looking at fake images.\\nIt’s very important to note that you set th e discriminator to be frozen during training\\n(non-trainable): its weights wo n’t be updated when training gan. If the discriminator\\nweights could be updated during this proces s, then you’d be training the discrimina-\\ntor to always predict “real,” which isn’t what you want!\\ndiscriminator.trainable = False\\ngan_input = keras.Input(shape=(latent_dim,))\\ngan_output = discriminator(generator(gan_input))\\ngan = keras.models.Model(gan_input, gan_output)\\ngan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\\ngan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')\\n8.5.6 How to train your DCGAN\\nNow you can begin training. To recapitulate, this is what the training loop looks like\\nschematically. For each epoch, you do the following:\\n1 Draw random points in the latent space (random noise).\\n2 Generate images with generator using this random noise.\\n3 Mix the generated images with real ones.\\n4 Train discriminator using these mixed images, with corresponding targets:\\neither “real” (for the real images) or “fake” (for the generated images).\\n5 Draw new random points in the latent space.\\n6 Train gan using these random vectors, with targets that all sa y “these are real\\nimages.” This updates the weights of th e generator (only, because the discrimi-\\nnator is frozen inside gan) to move them toward getting the discriminator to\\npredict “these are real images” for gene rated images: this trains the generator\\nto fool the discriminator.\\nLet’s implement it.\\nimport os\\nfrom keras.preprocessing import image\\n(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\\nListing 8.31 Adversarial network\\nListing 8.32 Implementing GAN training\\nSets discriminator weights to \\nnon-trainable (this will only \\napply to the gan model) \\nLoads CIFAR10 data\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 323, 'page_label': '324'}, page_content=\"311Introduction to generative adversarial networks\\nx_train = x_train[y_train.flatten() == 6]\\nx_train = x_train.reshape(\\n(x_train.shape[0],) +\\n(height, width, channels)).astype('float32') / 255.\\niterations = 10000\\nbatch_size = 20\\nsave_dir = 'your_dir'\\nstart = 0\\nfor step in range(iterations):\\nrandom_latent_vectors = np.random.normal(size=(batch_size,\\nlatent_dim))\\ngenerated_images = generator.predict(random_latent_vectors)\\nstop = start + batch_size\\nreal_images = x_train[start: stop]\\ncombined_images = np.concatenate([generated_images, real_images])\\nlabels = np.concatenate([np.ones((batch_size, 1)),\\nnp.zeros((batch_size, 1))])\\nlabels += 0.05 * np.random.random(labels.shape)\\nd_loss = discriminator.train_on_batch(combined_images, labels)\\nrandom_latent_vectors = np.random.normal(size=(batch_size,\\nlatent_dim))\\nmisleading_targets = np.zeros((batch_size, 1))\\na_loss = gan.train_on_batch(random_latent_vectors,\\nmisleading_targets)\\nstart += batch_size\\nif start > len(x_train) - batch_size:\\nstart = 0\\nif step % 100 == 0:\\ngan.save_weights('gan.h5')\\nprint('discriminator loss:', d_loss)\\nprint('adversarial loss:', a_loss)\\nimg = image.array_to_img(generated_images[0] * 255., scale=False)\\nimg.save(os.path.join(save_dir,\\n                      'generated_frog' + str(step) + '.png'))\\nimg = image.array_to_img(real_images[0] * 255., scale=False)\\nimg.save(os.path.join(save_dir,\\n                      'real_frog' + str(step) + '.png'))\\nSelects frog images (class 6)\\nNormalizes data\\nSpecifies where you want \\nto save generated images\\nSamples random \\npoints in the \\nlatent space\\nDecodes\\nthem to\\nfake\\nimages\\nCombines them\\nwith real images\\nAssembles labels, discrim-\\ninating real from fake images\\nAdds random \\nnoise to the \\nlabels—an \\nimportant trick!\\nTrains the\\ndiscriminator\\nSamples random \\npoints in the \\nlatent spaceAssembles\\nlabels that\\nsay “these\\nare all real\\nimages”\\n (it’s a lie!)\\nTrains the generator (via the \\ngan model, where the discrim-\\ninator weights are frozen)\\nOccasionally saves and \\nplots (every 100 steps)\\nSaves model weights\\nPrints metrics\\nSaves one\\ngenerated image\\nSaves one real image\\nfor comparison\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 324, 'page_label': '325'}, page_content='312 CHAPTER 8 Generative deep learning\\nWhen training, you may see the adversarial loss begin to increase considerably, while\\nthe discriminative loss tends to zero—the discriminator may end up dominating the\\ngenerator. If that’s the case, try reducing the discriminator learning rate, and increase\\nthe dropout rate of the discriminator. \\n8.5.7 Wrapping up\\n\\uf0a1 A GAN consists of a generator network coupled with a discriminator network.\\nThe discriminator is trained to differenciate between the output of the generator\\nand real images from a training dataset, and the generator is trained to fool the\\ndiscriminator. Remarkably, the generator nevers sees images from the training\\nset directly; the information it has about the data comes from the discriminator.\\n\\uf0a1 GANs are difficult to train, because training a GAN is a dynamic process rather\\nthan a simple gradient de scent process with a fixed loss landscape. Getting a\\nGAN to train correctly requires using a nu mber of heuristic tricks, as well as\\nextensive tuning.\\n\\uf0a1 GANs can potentially produce highly realistic images. But unlike VAEs, the\\nlatent space they learn doesn’t have a neat continuous structure and thus may\\nnot be suited for certain practical applications, such as image editing via latent-\\nspace concept vectors.\\nFigure 8.18 Play the discriminator: in each row, two images were dreamed up by the GAN, \\nand one image comes from the training set. Can you tell them apart? (Answers: the real \\nimages in each column are middle, top, bottom, middle.)\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 325, 'page_label': '326'}, page_content='313Introduction to generative adversarial networks\\nChapter summary\\n\\uf0a1 With creative applications of deep  learning, deep ne tworks go beyond\\nannotating existing content and star t generating their own. You learned\\nthe following:\\n– How to generate sequence data, one timestep at a time. This is applicable\\nto text generation and also to note-by-note music generation or any other\\ntype of timeseries data.\\n– How DeepDream works: by maximizing convnet layer activations through\\ngradient ascent in input space.\\n– How to perform style transfer, where a content image and a style image are\\ncombined to produce interesting-looking results.\\n–W h a t  GANs and VAEs are, how they can be used to dream up new images,\\nand how latent-space concept vectors can be used for image editing.\\n\\uf0a1 These few techniques cover only the ba sics of this fast-expanding field.\\nThere’s a lot more to discover out there—generative deep learning is\\ndeserving of an entire book of its own. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 326, 'page_label': '327'}, page_content='Conclusions\\nYou’ve almost reache d the end of this book. This last chapter will summarize and\\nreview core concepts while also expanding your horizons beyond the relatively basic\\nnotions you’ve learned so far. Understanding deep learning and AI is a journey, and\\nfinishing this book is merely the first step on it. I want to make sure you realize this\\nand are properly equipped to take the next steps of this journey on your own.\\n We’ll start with a bird’s-eye view of what you should take away from this book.\\nThis should refresh your memory regarding some of the concepts you’ve learned.\\nNext, we’ll present an overview of some ke y limitations of deep learning. To use a\\ntool appropriately, you should not only understand what it can do but also be aware\\nof what it can’t do. Finally, I’ll offer some speculative thoughts about the future evo-\\nlution of the fields of deep learning, machine learning, and AI. This should be\\nespecially interesting to you if you’d li ke to get into fundamental research. The\\nchapter ends with a short list of resource s and strategies for learning further about\\nAI and staying up to date with new advances.\\nThis chapter covers\\n\\uf0a1 Important takeaways from this book\\n\\uf0a1 The limitations of deep learning\\n\\uf0a1 The future of deep learning, machine learning, \\nand AI\\n\\uf0a1 Resources for learning further and working in \\nthe field'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 327, 'page_label': '328'}, page_content='315Key concepts in review\\n9.1 Key concepts in review\\nThis section briefly synthesizes the key take aways from this book. If you ever need a\\nquick refresher to help you recall what you’ve learned, you can read these few pages.\\n9.1.1 Various approaches to AI\\nFirst of all, deep learning isn’t synonymous with AI or even with machine learning.\\nArtificial intelligence  is an ancient, broad field that can generally be defined as “all\\nattempts to automate cognitive proce sses”—in other words, the automation of\\nthought. This can range from the very basic, such as an Excel spreadsheet, to the very\\nadvanced, like a humanoid robot that can walk and talk.\\n Machine learning is a specific subfield of AI that aims at auto matically developing\\nprograms (called models) purely from exposure to traini ng data. This process of turn-\\ning data into a program is called learning. Although machine learning has been\\naround for a long time, it only started to take off in the 1990s.\\n Deep learning is one of many branches of machine learning, where the models are\\nlong chains of geometric functions, applied one after the other. These operations are\\nstructured into modules called layers: deep-learning models are typically stacks of lay-\\ners—or, more generally, graphs of laye rs. These layers are parameterized by weights,\\nwhich are the parameters learned during training. The knowledge of a model is stored\\nin its weights, and the process of learning consists of finding good values for these\\nweights.\\n Even though deep learning is just one among many approaches to machine learn-\\ning, it isn’t on an equal footing with the others. Deep learning is a breakout success.\\nHere’s why. \\n9.1.2 What makes deep learning special \\nwithin the field of machine learning\\nIn the span of only a few years, deep learning has achieved  tremendous break-\\nthroughs across a wide range of tasks th at have been histor ically perceived as\\nextremely difficult for comput ers, especially in the area of machine perception:\\nextracting useful information from images , videos, sound, and more. Given sufficient\\ntraining data (in particular, training data appropriately labeled by humans), it’s possi-\\nble to extract from perceptual data almost anythi ng that a human could extract.\\nHence, it’s sometimes said that deep learning has solved perception, although that’s true\\nonly for a fairly narrow definition of perception.\\n Due to its unprecedented technical succ esses, deep learning has singlehandedly\\nbrought about the third and by far the largest AI summer: a period of intense interest,\\ninvestment, and hype in the field of AI. As this book is being written, we’re in the middle\\nof it. Whether this period will end in the ne ar future, and what happens after it ends,\\nare topics of debate. One thing is certain: in stark contrast with previous AI summers,\\ndeep learning has provided enormous business value to a number of large technology\\ncompanies, enabling human-le vel speech recognition, sma rt assistants, human-level\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 328, 'page_label': '329'}, page_content='316 CHAPTER 9 Conclusions\\nimage classification, vastly improved mach ine translation, and more. The hype may\\n(and likely will) recede, but the sustained economic and technological impact of deep\\nlearning will remain. In that sense, deep learning could be analogous to the internet:\\nit may be overly hyped up for a few years, but in the longer term it will still be a major\\nrevolution that will transform our economy and our lives.\\n I’m particularly optimistic about deep learning because even if we were to make no\\nfurther technological progre ss in the next decade, deploying existing algorithms to\\nevery applicable problem would be a game changer for most industries. Deep learn-\\ning is nothing short of a revolution, and progress is currently happening at an incred-\\nibly fast rate, due to an exponential investment in resources and headcount. From\\nwhere I stand, the future looks bright, although short-term expectations are somewhat\\noveroptimistic; deploying deep learning to the full extent of its potential will take well\\nover a decade. \\n9.1.3 How to think about deep learning\\nThe most surprising thing abou t deep learning is how simp le it is. Ten years ago, no\\none expected that we would achieve such  amazing results on machine-perception\\nproblems by using simple parametric models trained with gradient descent. Now, it\\nturns out that all you need is sufficiently large parametric models  trained with gradi-\\nent descent on sufficiently many examples. As Feynman once said about the universe,\\n“It’s not complicated, it’s just a lot of it.”\\n1\\n In deep learning, everything is a vector: everything is a point in a geometric space.\\nModel inputs (text, images, and so on) and targets are first vectorized: turned into an\\ninitial input vector space and target vector space. Each layer in a deep-learning model\\noperates one simple geometric transformati on on the data that goes through it.\\nTogether, the chain of layers in the mode l forms one complex geometric transforma-\\ntion, broken down into a series of simple ones. This complex transformation attempts\\nto map the input space to the target space, one point at a time. This transformation is\\nparameterized by the weights of the layers, which are iteratively updated based on how\\nwell the model is currently performing. A key characteristic of this geometric transfor-\\nmation is that it must be differentiable, which is required in order for us to be able to\\nlearn its parameters via gradient descent. Intuitively, this means the geometric morph-\\ning from inputs to outputs must be smooth and continuous—a significant constraint.\\n The entire process of applying this complex geometric transformation to the input\\ndata can be visualized in 3D by imagining a person trying to uncrumple a paper ball:\\nthe crumpled paper ball is th e manifold of the input data that the model starts with.\\nEach movement operated by the person on the paper ball is similar to a simple geo-\\nmetric transformation operated by one layer. The full uncrumpling gesture sequence\\nis the complex transformation of the enti re model. Deep-learning models are mathe-\\nmatical machines for uncrumpling complicated manifolds of high-dimensional data.\\n1 Richard Feynman, interview, The World from Another Point of View, Yorkshire Television, 1972.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 329, 'page_label': '330'}, page_content='317Key concepts in review\\n That’s the magic of deep learning: turn ing meaning into vectors, into geometric\\nspaces, and then incrementally learning complex geometric transformations that map\\none space to another. All you need are spaces of sufficiently high dimensionality in\\norder to capture the full scope of the relationships found in the original data.\\n The whole thing hinges on a single core idea: that meaning is derived from the pairwise\\nrelationship between things  (between words in a language , between pixels in an image,\\nand so on) and that these relationships can be captured by a distance function. But note that\\nwhether the brain implements meaning via geometric spaces is an entirely separate\\nquestion. Vector spaces are efficient to work with from a co mputational standpoint,\\nbut different data structures for intelligence can easily be envisioned—in particular,\\ngraphs. Neural networks initially emerged fr om the idea of using graphs as a way to\\nencode meaning, which is why they’re named neural networks; the surrounding field of\\nr e s e a r c h  u s e d  t o  b e  c a l l e d  connectionism. Nowadays the name neural network  exists\\npurely for historical reasons—it’s an extr emely misleading name because they’re nei-\\nther neural nor networks. In particular, ne ural networks have hardly anything to do\\nwith the brain. A more appropriate name would have been layered representations learn-\\ning or hierarchical representations learning , or maybe even deep differentiable models  or\\nchained geometric transforms , to emphasize the fact that continuous geometric space\\nmanipulation is at their core. \\n9.1.4 Key enabling technologies\\nThe technological revolution that’s currently unfolding didn’t start with any single\\nbreakthrough invention. Rather, like any ot her revolution, it’s the product of a vast\\naccumulation of enabling fact ors—slowly at first, and then suddenly. In the case of\\ndeep learning, we can point out the following key factors:\\n\\uf0a1 Incremental algorithmic innovations, fi rst spread over two decades (starting\\nwith backpropagation) and then happe ning increasingly faster as more\\nresearch effort was poured into deep learning after 2012.\\n\\uf0a1 The availability of large amounts of perc eptual data, which is a requirement in\\norder to realize that sufficiently large mo dels trained on sufficiently large data\\nare all we need. This is in turn a byproduct of the rise of the consumer internet\\nand Moore’s law applied to storage media.\\n\\uf0a1 The availability of fast, highly parall el computation hardwa re at a low price,\\nespecially the GPUs produced by NVIDIA—first gaming GPUs and then chips\\ndesigned from the ground up for deep learning. Early on, NVIDIA CEO Jensen\\nHuang took note of the deep-learning boom and decided to bet the company’s\\nfuture on it.\\n\\uf0a1 A complex stack of software layers that makes this computational power available\\nto humans: the CUDA language, frameworks like TensorFlow that do automatic\\ndifferentiation, and Keras, which makes deep learning accessible to most people.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 330, 'page_label': '331'}, page_content='318 CHAPTER 9 Conclusions\\nIn the future, deep learning will not only be used by specialists—researchers, graduate\\nstudents, and engineers with an  academic profile—but will also be a tool in the tool-\\nbox of every developer, much like web te chnology today. Ever yone needs to build\\nintelligent apps: just as every business toda y needs a website, every product will need\\nto intelligently make sense of user-generat ed data. Bringing a bout this future will\\nrequire us to build tools that make deep le arning radically easy to use and accessible\\nto anyone with basic coding abilities. Keras is the first major step in that direction. \\n9.1.5 The universal machine-learning workflow\\nHaving access to an extremely powerful tool  for creating models that map any input\\nspace to any target space is great, but the difficult part of the machine-learning work-\\nflow is often everything that comes before  designing and training such models (and,\\nfor production models, what comes afte r, as well). Understanding the problem\\ndomain so as to be able to determine what to attempt to predict, given what data, and\\nhow to measure success, is a prerequisite  for any successful a pplication of machine\\nlearning, and it isn’t something that adva nced tools like Keras and TensorFlow can\\nhelp you with. As a reminder, here’s a quick summary of the typical machine-learning\\nworkflow as described in chapter 4:\\n1 Define the problem: What data is availa ble, and what are you trying to predict?\\nWill you need to collect more data or hire people to manually label a dataset?\\n2 Identify a way to reliably measure succe ss on your goal. For simple tasks, this\\nmay be prediction accuracy, but in many cases it will require sophisticated\\ndomain-specific metrics.\\n3 Prepare the validation process that you’ll use to evaluate your models. In partic-\\nular, you should define a training set, a validation set, and a test set. The valida-\\ntion- and test-set labels shouldn’t leak into the training data: for instance, with\\ntemporal prediction, the validation and test data should be  posterior to the\\ntraining data.\\n4 Vectorize the data by turning it into ve ctors and preprocessing  it in a way that\\nmakes it more easily approachable by a neural network (normalization, and so\\non).\\n5 Develop a first model that beats a triv ial common-sense base line, thus demon-\\nstrating that machine learning can work  on your problem. This may not always\\nbe the case!\\n6 Gradually refine your mode l architecture by tuning  hyperparameters and add-\\ning regularization. Make changes based on performance on the validation data\\nonly, not the test data or the training data. Remember that you should get your\\nmodel to overfit (thus identifying a model capacity level that’s greater than you\\nneed) and only then begin to add regularization or downsize your model.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 331, 'page_label': '332'}, page_content='319Key concepts in review\\n7 Be aware of validation-set overfitting when turning hyperparameters: the fact\\nthat your hyperparameters may end up being overspecialized to the validation\\nset. Avoiding this is the purpose of having a separate test set!\\n9.1.6 Key network architectures\\nThe three families of network architectures that you should be familiar with are densely\\nconnected networks, convolutional networks, and recurrent networks. Each type of network is\\nmeant for a specific input modality: a ne twork architecture (dense, convolutional,\\nrecurrent) encodes assumptions about the structure of the data: a hypothesis space within\\nwhich the search for a good model will proceed. Whether a given architecture will\\nwork on a given problem depends entirely on the match between the structure of the\\ndata and the assumptions of the network architecture.\\n These different network ty pes can easily be combined to achieve larger multi-\\nmodal networks, much as you combine LEGO bricks. In a way, deep-learning layers are\\nLEGO bricks for information processing. He re’s a quick overvi ew of the mapping\\nbetween input modalities and appropriate network architectures:\\n\\uf0a1 Vector data—Densely connected network (Dense layers).\\n\\uf0a1 Image data—2D convnets.\\n\\uf0a1 Sound data (for example, waveform)—Either 1D convnets (preferred) or RNNs.\\n\\uf0a1 Text data—Either 1D convnets (preferred) or RNNs.\\n\\uf0a1 Timeseries data—Either RNNs (preferred) or 1D convnets.\\n\\uf0a1 Other types of sequence data —Either RNNs or 1D convnets. Prefer RNNs if data\\nordering is strongly meaningful (for example, for timeseries, but not for text).\\n\\uf0a1 Video data —Either 3D convnets (if you need to capture motion effects) or a\\ncombination of a frame-level 2D convnet for feature extraction followed by\\neither an RNN or a 1D convnet to process the resulting sequences.\\n\\uf0a1 Volumetric data—3D convnets.\\nNow, let’s quickly review the specificities of each network architecture.\\nDENSELY CONNECTED NETWORKS\\nA densely connected ne twork is a stack of Dense layers, meant to process vector data\\n(batches of vectors). Such networks assume no specific structure in the input features:\\nthey’re called densely connected because the units of a \\nDense layer are connected to every\\nother unit. The layer attempts to map relationships between any two input features; this\\nis unlike a 2D convolution layer, for instance, which only looks at local relationships.\\n Densely connected networks are most commonly used for categorical data (for\\nexample, where the input features are lists of  attributes), such as the Boston Housing\\nPrice dataset used in chapter 3. They’re also used as the final classification or regres-\\nsion stage of most networks. For instance, the convnets covered in chapter 5 typically\\nend with one or two Dense layers, and so do the recurrent networks in chapter 6.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 332, 'page_label': '333'}, page_content=\"320 CHAPTER 9 Conclusions\\n Remember: to perform binary classification, end your stack of layers with a Dense\\nlayer with a single unit and a sigmoid activation, and use binary_crossentropy as the\\nloss. Your targets should be either 0 or 1:\\nfrom keras import models\\nfrom keras import layers\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(32, activation='relu', input_shape=(num_input_features,)))\\nmodel.add(layers.Dense(32, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy')\\nTo perform single-label categorical classification (where each sample has exactly one class,\\nno more), end your stack of layers with a Dense layer with a number of units equal to the\\nnumber of classes, and a softmax activation. If your targets are one-hot encoded, use\\ncategorical_crossentropy as the loss; if they’re integers, use sparse_categorical_\\ncrossentropy:\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(32, activation='relu', input_shape=(num_input_features,)))\\nmodel.add(layers.Dense(32, activation='relu'))\\nmodel.add(layers.Dense(num_classes, activation='softmax'))\\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\\nTo perform multilabel categori cal classification  (where each sample  can have several\\nclasses), end your stack of layers with a Dense layer with a number of units equal to the\\nnumber of classes and a sigmoid activation, and use binary_crossentropy as the loss.\\nYour targets should be k-hot encoded:\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(32, activation='relu', input_shape=(num_input_features,)))\\nmodel.add(layers.Dense(32, activation='relu'))\\nmodel.add(layers.Dense(num_classes, activation='sigmoid'))\\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy')\\nTo perform regression toward a vector of continuous values, end your stack of layers\\nwith a Dense layer with a number of units equal to  the number of values you’re trying\\nto predict (often a single one, such as the price of a house), and no activation. Several\\nlosses can be used for re gression, most commonly mean_squared_error ( MSE) and\\nmean_absolute_error (MAE):\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(32, activation='relu', input_shape=(num_input_features,)))\\nmodel.add(layers.Dense(32, activation='relu'))\\nmodel.add(layers.Dense(num_values))\\nmodel.compile(optimizer='rmsprop', loss='mse')Licensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 333, 'page_label': '334'}, page_content=\"321Key concepts in review\\nCONVNETS\\nConvolution layers look at spatially local patterns by applying the same geometric\\ntransformation to different spatial locations ( patches) in an input tensor. This results\\nin representations that are translation invariant, making convolution layers highly data\\nefficient and modular. This idea is appl icable to spaces of  any dimensionality: 1D\\n(sequences), 2D (images), 3D (volumes), and so on. You can use the Conv1D layer to\\nprocess sequences (especially text—it doesn’t work as well on timeseries, which often\\ndon’t follow the translation- invariance assumption), the Conv2D layer to process\\nimages, and the Conv3D layers to process volumes.\\n Convnets, or convolutional networks, consist of stacks of convolution and max-pooling\\nlayers. The pooling layers let you spatially downsample the data, which is required to\\nkeep feature maps to a reason able size as the number of features grows, and to allow\\nsubsequent convolution layers to “see” a greater spatial extent of the inputs. Convnets\\nare often ended with either a Flatten operation or a global pooling layer, turning spa-\\ntial feature maps into vectors, followed by Dense layers to achieve classification or\\nregression.\\n Note that it’s highly likely that regular convolutions will soon be mostly (or com-\\npletely) replaced by an equivalent but fast er and representational ly efficient alterna-\\ntive: the depthwise separable convolution  (SeparableConv2D layer). This is true for 3D,\\n2D, and 1D inputs. When you’re building a new network from scratch, using depth-\\nwise separable convolutions is definitely the way to go. The SeparableConv2D layer\\ncan be used as a drop-in replacement for Conv2D, resulting in a smaller, faster network\\nthat also performs better on its task.\\n Here’s a typical image-classification ne twork (categorical classification, in this\\ncase):\\nmodel = models.Sequential()\\nmodel.add(layers.SeparableConv2D(32, 3, activation='relu',\\n        input_shape=(height, width, channels)))\\nmodel.add(layers.SeparableConv2D(64, 3, activation='relu'))\\nmodel.add(layers.MaxPooling2D(2))\\nmodel.add(layers.SeparableConv2D(64, 3, activation='relu'))\\nmodel.add(layers.SeparableConv2D(128, 3, activation='relu'))\\nmodel.add(layers.MaxPooling2D(2))\\nmodel.add(layers.SeparableConv2D(64, 3, activation='relu'))\\nmodel.add(layers.SeparableConv2D(128, 3, activation='relu'))\\nmodel.add(layers.GlobalAveragePooling2D())\\nmodel.add(layers.Dense(32, activation='relu'))\\nmodel.add(layers.Dense(num_classes, activation='softmax'))\\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\\nRNNS\\nRecurrent neural networks (RNNs) work by processing sequences of inputs one timestep at\\na time and maintaining a state throughout (a state is typically a vector or set of vectors:\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 334, 'page_label': '335'}, page_content=\"322 CHAPTER 9 Conclusions\\na point in a geometric space of states). They should be used preferentially over 1D conv-\\nnets in the case of sequences where pattern s of interest aren’t invariant by temporal\\ntranslation (for instance, timeseries data where the recent past is more important than\\nthe distant past).\\n Three RNN layers are available in Keras: SimpleRNN, GRU, and LSTM. For most prac-\\ntical purposes, you should use either GRU or LSTM. LSTM is the more powerful of the\\ntwo but is also more expensive; you can think of GRU as a simpler, cheaper alternative\\nto it.\\n In order to stack multiple RNN layers on top of each other, each layer prior to the\\nlast layer in the stack should return the fu ll sequence of its ou tputs (each input time-\\nstep will correspond to an output timestep); if you aren’t stacking any further RNN lay-\\ners, then it’s common to return only th e last output, which contains information\\nabout the entire sequence.\\n Following is a single RNN layer for binary classification of vector sequences:\\nmodel = models.Sequential()\\nmodel.add(layers.LSTM(32, input_shape=(num_timesteps, num_features)))\\nmodel.add(layers.Dense(num_classes, activation='sigmoid'))\\n50model.compile(optimizer='rmsprop', loss='binary_crossentropy')\\nAnd this is a stacked RNN layer for binary classification of vector sequences: \\nmodel = models.Sequential()\\nmodel.add(layers.LSTM(32, return_sequences=True,\\ninput_shape=(num_timesteps, num_features)))\\nmodel.add(layers.LSTM(32, return_sequences=True))\\nmodel.add(layers.LSTM(32))\\nmodel.add(layers.Dense(num_classes, activation='sigmoid'))\\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy')\\n9.1.7 The space of possibilities\\nWhat will you build with deep learning? Remember, building deep-learning models is\\nlike playing with LEGO bricks: layers can be plugged together to map essentially any-\\nthing to anything, given that you have appropriate training data available and that the\\nmapping is achievable via a continuous ge ometric transformation of reasonable com-\\nplexity. The space of possibilities is infi nite. This section offers a few examples to\\ninspire you to think beyond the basic classifi cation and regression tasks that have tra-\\nditionally been the bread and butter of machine learning.\\n I’ve sorted my suggested applications by input and ou tput modalities. Note that\\nquite a few of them stretch the limits of what is possible—although a model could be\\ntrained on all of these tasks, in some cases such a model probably wouldn’t generalize\\nfar from its training data. Sections 9.2 and 9.3 will address how these limitations could\\nbe lifted in the future.\\n \\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 335, 'page_label': '336'}, page_content='323Key concepts in review\\n\\uf0a1 Mapping vector data to vector data\\n– Predictive healthcare —Mapping patient medical records to predictions of\\npatient outcomes\\n– Behavioral targeting —Mapping a set of website attributes with data on how\\nlong a user will spend on the website\\n– Product quality control—Mapping a set of attributes relative to an instance of a\\nmanufactured product with the probabilit y that the product will fail by next\\nyear\\n\\uf0a1 Mapping image data to vector data\\n– Doctor assistant —Mapping slides of medical im ages with a prediction about\\nthe presence of a tumor\\n– Self-driving vehicle —Mapping car dash-cam vide o frames to steering wheel\\nangle commands\\n– Board game AI—Mapping Go and chess boards to the next player move\\n– Diet helper—Mapping pictures of a dish to its calorie count\\n– Age prediction—Mapping selfies to the age of the person\\n\\uf0a1 Mapping timeseries data to vector data\\n– Weather prediction —Mapping timeseries of weather data in a grid of locations\\nof weather data the following week at a specific location\\n– Brain-computer interfaces —Mapping timeseries of magnetoencephalogram\\n(MEG) data to computer commands\\n– Behavioral targeting —Mapping timeseries of user interactions on a website to\\nthe probability that a user will buy something\\n\\uf0a1 Mapping text to text\\n– Smart reply—Mapping emails to possible one-line replies\\n– Answering questions—Mapping general-knowledge questions to answers\\n– Summarization—Mapping a long article to a short summary of the article\\n\\uf0a1 Mapping images to text\\n– Captioning—Mapping images to short captio ns describing the contents of\\nthe images\\n\\uf0a1 Mapping text to images\\n– Conditioned image generation —Mapping a short text description to images\\nmatching the description\\n– Logo generation/selection —Mapping the name and description of a company\\nto the company’s logo\\n\\uf0a1 Mapping images to images\\n– Super-resolution—Mapping downsized images to higher-resolution versions of\\nthe same images\\n– Visual depth sensing —Mapping images of indoor environments to maps of\\ndepth predictions\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 336, 'page_label': '337'}, page_content='324 CHAPTER 9 Conclusions\\n\\uf0a1 Mapping images and text to text\\n– Visual QA—Mapping images and natural-la nguage questions about the con-\\ntents of images to natural-language answers\\n\\uf0a1 Mapping video and text to text\\n– Video QA—Mapping short videos and natura l-language questions about the\\ncontents of videos to natural-language answers\\nAlmost anything is possible—but not quite anything. Let’s see in the next section what\\nwe can’t do with deep learning. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 337, 'page_label': '338'}, page_content='325The limitations of deep learning\\n9.2 The limitations of deep learning\\nThe space of applications that can be im plemented with deep learning is nearly\\ninfinite. And yet, many applications are completely out of reach for current deep-\\nlearning techniques—even gi ven vast amounts of human- annotated data. Say, for\\ninstance, that you could assemble a data set of hundreds of thousands—even mil-\\nlions—of English-language descriptions of the features of a software product, written\\nby a product manager, as well as the corr esponding source code developed by a team\\nof engineers to meet these requirements. Even with this data, you could not train a\\ndeep-learning model to read a product description and generate the appropriate\\ncodebase. That’s just one example among ma ny. In general, anything that requires\\nreasoning—like programming or applying the scientific method—long-term plan-\\nning, and algorithmic data manipulation is out of reach for deep-learning models, no\\nmatter how much data you throw at them. Even learning a sorting algorithm with a\\ndeep neural network is tremendously difficult.\\n This is because a deep-learning model is just a chain of simple, co ntinuous geometric\\ntransformations mapping one vector space into another. All it can do is map one data\\nmanifold \\nX into another manifold Y, assuming the existence of a learnable continuous\\ntransform from X to Y. A deep-learning model can be interpreted as a kind of pro-\\ngram; but, inversely, most programs can’t be expressed as deep-learning models —for most\\ntasks, either there exists no corresponding deep-neural network that solves the task or,\\neven if one exists, it may not be learnable: the corresponding geometric transform may\\nbe far too complex, or there may not be appropriate data available to learn it.\\n Scaling up current deep-learning techniqu e s  b y  s t a c k i n g  m o r e  l a y e r s  a n d  u s i n g\\nmore training data can only superficially pa lliate some of these issues. It won’t solve\\nthe more fundamental problems that deep-learning models are limited in what they\\ncan represent and that most of the programs you may wish to learn can’t be expressed\\nas a continuous geometric morphing of a data manifold.\\n9.2.1 The risk of anthropomorphizing machine-learning models\\nOne real risk with contemporary AI is misinterpreting what deep-learning models do\\nand overestimating their abilities. A fundamental feature of humans is our theory of\\nmind: our tendency to project intentions, beliefs, and knowledge on the things around\\nus. Drawing a smiley face on a rock suddenly makes it “happy”—in our minds. Applied\\nto deep learning, this means that, for inst ance, when we’re able to somewhat success-\\nfully train a model to generate captions to describe pictures, we’re led to believe that\\nthe model “understands” the contents of the pictures and the captions it generates.\\nThen we’re surprised when any slight departure from the sort of images present in the\\ntraining data causes the model to generate completely absurd captions (see figure 9.1).\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 338, 'page_label': '339'}, page_content='326 CHAPTER 9 Conclusions\\n \\nIn particular, this is highlighted by adversarial examples , which are samples fed to a\\ndeep-learning network that are designed to  trick the model into misclassifying them.\\nYou’re already aware that, for instance, it’s possible to do  gradient ascent in input\\nspace to generate inputs that maximize the activation of some convnet filter—this is\\nthe basis of the filter-visualization techniqu e introduced in chapter 5, as well as the\\nDeepDream algorithm in chapter 8. Simila rly, through gradie nt ascent, you can\\nslightly modify an image in order to maximize the class prediction for a given class. By\\ntaking a picture of a panda and adding to it a gibbon gradient, we can get a neural\\nnetwork to classify the panda as a gibbon (see figure 9.2). This evidences both the brit-\\ntleness of these models and the deep diff erence between their input-to-output map-\\nping and our human perception.\\nThe boy is holding a baseball bat.\\nFigure 9.1 Failure of an image-captioning \\nsystem based on deep learning\\nGibbon\\nclass gradient\\nPanda\\nPanda\\nf(x)\\nAdversarial example\\nGibbon!\\nf(x)\\nFigure 9.2 An adversarial example: imperceptible changes in an image can \\nupend a model’s classification of the image.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 339, 'page_label': '340'}, page_content='327The limitations of deep learning\\nIn short, deep-learning models don’t have any understanding of their input—at least,\\nnot in a human sense. Our own understand ing of images, sounds, and language is\\ngrounded in our sensorimotor experience as humans. Machine-learning models have\\nno access to such experiences and thus ca n’t understand their inputs in a human-\\nrelatable way. By annotating large numbers of training examples to feed into our mod-\\nels, we get them to learn a geometric transform that maps data to human concepts on\\na specific set of examples, but this mapping is a simplistic sketch of the original model\\nin our minds—the one developed from our experience as embodied agents. It’s like a\\ndim image in a mirror (see figure 9.3).\\nAs a machine-learning practitioner, always be  mindful of this, and never fall into the\\ntrap of believing that neural networks un derstand the task they perform—they don’t,\\nat least not in a way that would make sense to us. They were trained on a different, far\\nnarrower task than the one we wanted to teach them: that of mapping training inputs\\nto training targets, point by point. Show them anything that deviates from their train-\\ning data, and they will break in absurd ways. \\n9.2.2 Local generalization vs. extreme generalization\\nThere are fundamental differences betwee n the straightforward geometric morphing\\nfrom input to output that deep-learning models do, and the way humans think and\\nlearn. It isn’t only the fact that humans  learn by themselves from embodied experi-\\nence instead of being presented with explicit training examples. In addition to the dif-\\nferent learning processes, th ere’s a basic difference in the nature of the underlying\\nrepresentations.\\n Humans are capable of far more than mapping immediate stimuli to immediate\\nresponses, as a deep network, or maybe an insect, would. We maintain complex, abstract\\nmodels of our current situation, of ourselves,  and of other people, and can use these\\nmodels to anticipate different possible futures and perform long-term planning. We\\ncan merge together known concepts to represent something we’ve never experienced\\nReal world\\nEmbodied\\nhuman experience\\nAbstract concepts \\nin human mind\\nLabeled data\\nexemplifying\\nthese concepts\\nMay not always \\ntransfer well to \\nthe real world\\nDoesn’t match the\\nhuman mental model \\nit came from\\nMatches the\\ntraining data\\nMachine-learning\\nmodel\\nf(x)\\nFigure 9.3 Current machine-learning models: like a dim image in a mirror\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 340, 'page_label': '341'}, page_content='328 CHAPTER 9 Conclusions\\nbefore—like picturing a horse wearing jeans,  for instance, or imagining what we’d do\\nif we won the lottery. This ability to handle hypotheticals, to expand our mental model\\nspace far beyond what we can experience directly—to perform abstraction and reason-\\ning—is arguably the defining characteristic of human cognition. I call it extreme general-\\nization: an ability to adapt to novel, never-before-experienced situations using little data\\nor even no new data at all.\\n This stands in sharp contrast with what deep nets do, which I call local generalization\\n(see figure 9.4). The mapping from inputs to outputs performed by a deep net quickly\\nstops making sense if new inputs differ even slightly from what the net saw at training\\ntime. Consider, for instance, the problem of learning the appropriate launch parame-\\nters to get a rocket to land on the moon. If you used a deep net for this task and trained\\nit using supervised learning or reinforcement learning, you’d have to feed it thousands\\nor even millions of launch trials: you’d need to expose it to a dense sampling of the input\\nspace, in order for it to learn a reliable mapping from input space to output space. In\\ncontrast, as humans we can use our power of abstraction to come up with physical mod-\\nels—rocket science—and derive an exact solution that will land the rocket on the moon\\nin one or a few trials. Similarly, if you developed a deep net controlling a human body,\\nand you wanted it to learn to safely navigate  a city without getting hit by cars, the net\\nwould have to die many thousands of times in various situations until it could infer that\\ncars are dangerous, and develop appropriate avoidance behaviors. Dropped into a new\\ncity, the net would have to relearn most of what it knows. On the other hand, humans\\nare able to learn safe behaviors without having to die even once—again, thanks to our\\npower of abstract modeling of hypothetical situations.\\nIn short, despite our progress on machin e perception, we’re still far from human-\\nlevel AI. Our models can only perform local generalization, adapting to new situa-\\ntions that must be similar to past da ta, whereas human cognition is capable of\\nThe same set of\\ndata points\\nor experience\\nLocal generalization:\\ngeneralization power of\\npattern recognition\\nExtreme generalization:\\ngeneralization power\\nachieved via abstraction\\nand reasoning\\nFigure 9.4 Local generalization \\nvs. extreme generalization\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 341, 'page_label': '342'}, page_content='329The limitations of deep learning\\nextreme generalization, quickly adapting to  radically novel situations and planning\\nfor long-term future situations. \\n9.2.3 Wrapping up\\nHere’s what you should  remember: the only real succe ss of deep learning so far has\\nbeen the ability to map spac e X to space Y using a continuous geometric transform,\\ngiven large amounts of human-annotated data . Doing this well is a game-changer for\\nessentially every industry, but it’s still a long way from human-level AI.\\n To lift some of the limitation s we have discussed and create AI that can compete\\nwith human brains, we need to move away  from straightforward input-to-output map-\\npings and on to reasoning and abstraction. A likely appropriate substrate for abstract\\nmodeling of various situations  and concepts is that of computer programs. We said\\npreviously that machine-learning models can be defined as learnable programs ; cur-\\nrently we can only learn programs that belo ng to a narrow and specific subset of all\\npossible programs. But what if we could learn any program, in a modular and reusable\\nway? Let’s see in the next section what the road ahead may look like. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 342, 'page_label': '343'}, page_content='330 CHAPTER 9 Conclusions\\n9.3 The future of deep learning\\nThis is a more speculative section aimed at opening horizons for people who want to\\njoin a research program or begin doing independent research. Given what we know of\\nhow deep nets work, their limitations, and the current state of the research landscape,\\ncan we predict where things are headed in the medium term? Following are some\\npurely personal thoughts. Note that I don’t have a crystal ball, so a lot of what I antici-\\npate may fail to become reality. I’m shar ing these predictions not because I expect\\nthem to be proven completely right in th e future, but because they’re interesting and\\nactionable in the present.\\n At a high level, these are the main directions in which I see promise:\\n\\uf0a1 Models closer to general-purpose computer programs , built on top of far richer primi-\\ntives than the current differentiable layers. This is how we’ll get to reasoning and\\nabstraction, the lack of which is the fundamental weakness of current models.\\n\\uf0a1 New forms of learning that make the previous point possible , allowing models to move\\naway from differentiable transforms.\\n\\uf0a1 Models that require less involvement from human engineers. It shouldn’t be your job to\\ntune knobs endlessly.\\n\\uf0a1 Greater, systematic reuse of previously learned features and architectures , such as meta-\\nlearning systems using reusable and modular program subroutines.\\nAdditionally, note that these considerations aren’t specific to the sort of supervised\\nlearning that has been the bread and butter of deep learning so far—rather, they’re\\napplicable to any form of machine learni ng, including unsupervised, self-supervised,\\nand reinforcement learning. It isn’t fundamentally important where your labels come\\nfrom or what your training loop looks like; these different branches of machine learn-\\ning are different facets of the same construct. Let’s dive in.\\n9.3.1 Models as programs\\nAs noted in the previous section, a nece ssary transformational development that we\\ncan expect in the field of machine learning is a move away from models that perform\\npurely pattern recognition and can only achieve local generalization, toward models capa-\\nble of abstraction and reasoning that can achieve extreme generalization. Current AI pro-\\ngrams that are capable of basic forms of  reasoning are all hardcoded by human\\nprogrammers: for instance, software that relies on search algorithms, graph manipula-\\ntion, and formal logic. In DeepMind’s AlphaGo, for example, most of the intelligence\\non display is designed and hardcoded by expert programmers (such as Monte Carlo\\nTree Search); learning from data happens only in specialized submodules (value net-\\nworks and policy networks). But in the futu re, such AI systems may be fully learned,\\nwith no human involvement.\\n What path could make this happen? Consider a well-known type of network: RNNs.\\nIt’s important to note that RNNs have slightly fewer limitations than feedforward net-\\nworks. That’s because RNNs are a bit more than mere  geometric transformations:\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 343, 'page_label': '344'}, page_content='331The future of deep learning\\nthey’re geometric transformations repeatedly applied inside a for loop. The temporal for\\nloop is itself hardcoded by  human developers: it’s a bu ilt-in assumption of the net-\\nwork. Naturally, RNNs are still extremely limited in what they can represent, primarily\\nbecause each step they perform is a differentiable geometric transformation, and they\\ncarry information from step to step via points in a continuous geometric space (state\\nvectors). Now imagine a neural  network that’s augmented in a similar way with pro-\\ngramming primitives—but inst ead of a single hardcoded for l o o p  w i t h  h a r d c o d e d\\ngeometric memory, the network includes a large set of  programming primitives that\\nthe model is free to manipulate to ex pand its processing function, such as if\\nbranches, while statements, variable creation, di sk storage for long-term memory,\\nsorting operators, advanced data structures (such as lis ts, graphs, and hash tables),\\nand many more. The space of programs that such a network could represent would be\\nfar broader than what can be represented with current deep-learning models, and\\nsome of these programs could achieve superior generalization power.\\n We’ll move away from having, on one hand, hardcoded algorithmic intelligence\\n(handcrafted software) and, on the other hand, learned geometric intelligence (deep\\nlearning). Instead, we’ll have a blend of formal algorithmic modules that provide rea-\\nsoning and abstraction capabilities, and geometric modules that provide informal\\nintuition and pattern-recognition capabiliti es. The entire system will be learned with\\nlittle or no human involvement.\\n A related subfield of \\nAI that I think may be about to take off in a big way is program\\nsynthesis, in particular neural program synthesi s. Program synthesis consists of auto-\\nmatically generating simple programs by us ing a search algorithm (possibly genetic\\nsearch, as in genetic programming) to ex plore a large space of possible programs.\\nThe search stops when a program is found that matches the required specifications,\\noften provided as a set of input-output pair s. This is highly reminiscent of machine\\nlearning: given training data provided as input-output pairs, we find a program that\\nmatches inputs to outputs and can generalize to new inputs. The difference is that\\ninstead of learning parameter values in a hardcoded program (a neural network), we\\ngenerate source code via a discrete search process.\\n I definitely expect this subfield to see a wave of renewed interest in the next few\\nyears. In particular, I expect the emerge nce of a crossover su bfield between deep\\nlearning and program synthesis, where inst ead of generating programs in a general-\\npurpose language, we’ll generate neural ne tworks (geometric data-processing flows)\\naugmented with a rich set of al gorithmic primitives, such as \\nfor loops and many oth-\\ners (see figure 9.5). This sh ould be far more tractable an d useful than directly gener-\\nating source code, and it wi ll dramatically expand the sc ope of problems that can be\\nsolved with machine learning—the space of programs that we can generate automati-\\ncally, given appropriate training data. Contemporary RNNs can be seen as a prehis-\\ntoric ancestor of such hybrid algorithmic-geometric models.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 344, 'page_label': '345'}, page_content='332 CHAPTER 9 Conclusions\\n  \\n9.3.2 Beyond backpropagation and differentiable layers\\nIf machine-learning models become more like programs, then they will mostly no lon-\\nger be differentiable—these programs will still use continuous geometric layers as sub-\\nroutines, which will be differentiable, but the model as a whole won’t be. As a result,\\nusing backpropagation to adjust weight values in a fixed, hardcoded network can’t be\\nthe method of choice for training models in the future—at least, it can’t be the entire\\nstory. We need to figure out how to trai n non-differentiable syst ems efficiently. Cur-\\nrent approaches include genetic algorithms,  evolution strategies, certain reinforce-\\nment-learning methods, and alternatin g direction method of multipliers (\\nADMM).\\nNaturally, gradient descent isn’t going an ywhere; gradient information will always be\\nuseful for optimizing differentiable parametric functions. But our models will become\\nincreasingly more ambitious than mere diff erentiable parametric functions, and thus\\ntheir automatic development (the learning in machine learning) will require more than\\nbackpropagation.\\n In addition, backpropagation is end to end, which is a great thing for learning\\ngood chained transformations but is comput ationally inefficient because it doesn’t\\nfully take advantage of the modularity of  deep networks. To make something more\\nefficient, there’s one universal recipe: introduce modularity and hierarchy. So we can\\nmake backpropagation more efficient by introducing decoupled training modules\\nwith a synchronization mechanism between them, organized in a hierarchical fashion.\\nThis strategy is somewhat reflected in DeepMind’s recent work on synthetic gradients.\\nI expect more along these lines in the near future. I can imagine a future where mod-\\nels that are globally non-differentiable (but feature differentiable parts) are trained—\\ngrown—using an efficient search process th at doesn’t use gradients, whereas the dif-\\nferentiable parts are trained even faster by taking advantage of gradients using a more\\nefficient version of backpropagation. \\n9.3.3 Automated machine learning\\nIn the future, model architectures will be learned rather than be handcrafted by engi-\\nneer-artisans. Learning architectures goes ha nd in hand with the use of richer sets of\\nprimitives and program-like machine-learning models.\\nModular task-level program \\nlearned on the fly to solve\\na specific task Data and\\nfeedback\\nActions\\nGeometric\\nsubroutine\\nAlgorithmic\\nsubroutine\\nGeometric\\nsubroutine\\nAlgorithmic\\nsubroutine\\nTask #002456 Figure 9.5 A learned program relying \\non both geometric primitives (pattern \\nrecognition, intuition) and algorithmic \\nprimitives (reasoning, search, \\nmemory)\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 345, 'page_label': '346'}, page_content='333The future of deep learning\\n Currently, most of the job of a deep-l earning engineer consists of munging data\\nwith Python scripts and then tuning the architecture and hype rparameters of a deep\\nnetwork at length to get a working model—or even to get a state-of-the-art model, if\\nthe engineer is that ambitious. Needless to say, that isn’t an optimal setup. But AI can\\nhelp. Unfortunately, the data-munging part  is tough to automate, because it often\\nrequires domain knowledge as well as a cl ear, high-level understanding of what the\\nengineer wants to achieve. Hyperparameter tuning, however, is a simple search proce-\\ndure; and in that case we know what the engineer wants to achieve: it’s defined by the\\nloss function of the network being tuned. It’s already common practice to set up basic\\nAutoML systems that take care of most model knob tuning. I even set up my own, years\\nago, to win Kaggle competitions.\\n At the most basic level, such a system wo uld tune the number of layers in a stack,\\ntheir order, and the number of units or fi lters in each layer. This is commonly done\\nwith libraries such as  Hyperopt, which we discussed in  chapter 7. But we can also be\\nfar more ambitious and attempt to learn an  appropriate architecture from scratch,\\nwith as few constraints as possible: for inst ance, via reinforcement learning or genetic\\nalgorithms.\\n Another important AutoML direction involv es learning model architecture jointly\\nwith model weights. Because training a ne w model from scratch every time we try a\\nslightly different architecture is tremendo usly inefficient, a truly powerful AutoML\\nsystem would evolve architectures at the same time the featur es of the model were\\nbeing tuned via backpropagation on the tr aining data. Such approaches are begin-\\nning to emerge as I write these lines.\\n When this starts to happen, the jobs of machine-learning engineers won’t disap-\\npear—rather, engineers will move up the valu e-creation chain. They will begin to put\\nmuch more effort into crafting complex loss functions that truly reflect business goals\\nand understanding how their models impact the digital ecosystems in which they’re\\ndeployed (for example, the users who consume the model’s predictions and generate\\nthe model’s training data)— problems that only the larg est companies can afford to\\nconsider at present. \\n9.3.4 Lifelong learning and modular subroutine reuse\\nIf models become more complex and are built on top of richer algorithmic primitives,\\nthen this increased complexity will requir e higher reuse between tasks, rather than\\ntraining a new model from scratch every ti me we have a new task or a new dataset.\\nMany datasets don’t contain enough inform ation for us to develop a new, complex\\nmodel from scratch, and it will be necessary to use information from previously\\nencountered datasets (much as you don’t learn English from scratch every time you\\nopen a new book—that would be impossible). Training models from scratch on every\\nnew task is also inefficient due to the large overlap between the current tasks and pre-\\nviously encountered tasks.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 346, 'page_label': '347'}, page_content='334 CHAPTER 9 Conclusions\\n A remarkable observation has been made re peatedly in recent years: training the\\nsame model to do several loosely connected ta sks at the same time  results in a model\\nthat’s better at each task . For instance, training the sa me neural machine-translation\\nmodel to perform both English-to-German translation and French-to-Italian transla-\\ntion will result in a model that’s better at  each language pair. Similarly, training an\\nimage-classification model jo intly with an image-segmen tation model, sharing the\\nsame convolutional base, results in a model th at’s better at both tasks. This is fairly\\nintuitive: there’s always some information overlap between seemingly disconnected\\ntasks, and a joint model has access to a greater amount of in formation about each\\nindividual task than a model trained on that specific task only.\\n Currently, when it comes to model reuse across tasks, we use pretrained weights for\\nmodels that perform common fu nctions, such as visual feature extraction. You saw\\nthis in action in chapter 5. In the future, I expect a generalized version of this to be\\ncommonplace: we’ll use not only previously learned features (submodel weights) but\\nalso model architectures and training procedures. As mo dels become more like pro-\\ngrams, we’ll begin to reuse program subroutines like the functions and classes found in\\nhuman programming languages.\\n Think of the process of software development today: once an engineer solves a spe-\\ncific problem ( HTTP queries in Python, for instance), they package it as an abstract,\\nreusable library. Engineers wh o face a similar problem in the future will be able to\\nsearch for existing libraries, download one, and use it in their own project. In a similar\\nway, in the future, metalearning systems will be able to assemble new programs by sift-\\ning through a global library of high-level reusable blocks. When the system finds itself\\ndeveloping similar program su broutines for several differe nt tasks, it can come up\\nwith an abstract, reusable version of the subroutine and store it in the global library\\n(see figure 9.6). Such a process will implement abstraction: a necessary component for\\nachieving extreme generalization. A subroutine that’s useful across different tasks and\\ndomains can be said to abstract some aspect of problem solving. This definition of\\nabstraction is similar to the notion of ab straction in software engineering. These sub-\\nroutines can be either geometric (deep-learning modules with pretrained representa-\\ntions) or algorithmic (closer to the libra ries that contemporary software engineers\\nmanipulate).\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 347, 'page_label': '348'}, page_content='335The future of deep learning\\n  \\n9.3.5 The long-term vision\\nIn short, here’s my long-term vision for machine learning:\\n\\uf0a1 Models will be more like programs and will have capabilities that go far beyond\\nthe continuous geometric transformation s of the input data we currently work\\nwith. These programs will arguably be much closer to the abstract mental mod-\\nels that humans maintain about their surroundings and themselves, and they\\nwill be capable of stronger generalization due to their rich algorithmic nature.\\n\\uf0a1 In particular, models will blend algorithmic modules providing formal reasoning,\\nsearch, and abstraction capabilities with geometric modules  providing informal\\nintuition and pattern-recognition capabilities. AlphaGo (a system that required\\na lot of manual software engineering and human-made design decisions) pro-\\nvides an early example of what such a blend of symbolic and geometric AI could\\nlook like.\\n\\uf0a1 Such models will be grown automatically rather than hardcoded by human engi-\\nneers, using modular parts stored in a global library of reusable subroutines—a\\nlibrary evolved by learning  high-performing models on thousands of previous\\ntasks and datasets. As frequent problem- solving patterns are identified by the\\nmeta-learning system, they will be turned into reusable subroutines—much like\\nfunctions and classes in software engineering—and added to the global library.\\nThis will achieve abstraction.\\n\\uf0a1 This global library and associated model-gr owing system will be able to achieve\\nsome form of human-like extreme generalization: given a new task or situation,\\nModular task-level program \\nlearned on the fly to solve\\na specific task Data and\\nfeedback\\nActions\\nGeometric\\nsubroutine\\nAlgorithmic\\nsubroutine\\nGeometric\\nsubroutine\\nAlgorithmic\\nsubroutine\\nGlobal library of\\nabstract subroutines\\nGeometric\\nsubroutine\\nAlgorithmic\\nsubroutine\\nAlgorithmic\\nsubroutine\\nGeometric\\nsubroutine\\nAlgorithmic\\nsubroutine\\nAlgorithmic\\nsubroutine\\nGeometric\\nsubroutine\\nAlgorithmic\\nsubroutine\\nAlgorithmic\\nsubroutine\\nPerpetual meta-learner\\ncapable of quickly growing\\na task-level model\\nacross a variety of tasks\\nPush\\nreusable\\nsubroutines\\nData and\\nfeedback\\nDesign\\nchoices\\nFetch\\nrelevant\\nsubroutines\\nTask #002456\\nTask #002455\\nTask #002454\\nTask #002453\\nFigure 9.6 A meta-learner capable of quickly developi ng task-specific models using reusable primitives \\n(both algorithmic and geometric), thus achieving extreme generalization\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 348, 'page_label': '349'}, page_content='336 CHAPTER 9 Conclusions\\nthe system will be able to assemble a new working model appropriate for the\\ntask using very little data, thanks to rich program-like primitives that generalize\\nwell, and extensive experience  with similar tasks. In the same way, humans can\\nquickly learn to play a complex new vide o game if they have experience with\\nmany previous games, because the models  derived from this previous experi-\\nence are abstract and prog ram-like, rather than a basic mapping between stim-\\nuli and action.\\n\\uf0a1 As such, this perpetually learning model-growing system can be interpreted as\\nan artificial general intelligence  (\\nAGI). But don’t expect an y singularitarian robot\\napocalypse to ensue: that’s pure fantasy, coming from a long series of profound\\nmisunderstandings of both  intelligence and technology. Such a critique, how-\\never, doesn’t belong in this book. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 349, 'page_label': '350'}, page_content='337Staying up to date in a fast-moving field\\n9.4 Staying up to date in a fast-moving field\\nAs final parting words, I want to give you some pointers about how to keep learning\\nand updating your knowledge and skills after you’ve turned the last page of this book.\\nThe field of modern deep learning, as we know it today, is only a few years old, despite\\na long, slow prehistory stre tching back decades. With an exponential increase in\\nfinancial resources and research headcount since 2013, the field as a whole is now\\nmoving at a frenetic pace. What you’ve le arned in this book won’t stay relevant for-\\never, and it isn’t all you’ll need for the rest of your career.\\n Fortunately, there are plenty of free online resources that you can use to stay up to\\ndate and expand your horizons. Here are a few.\\n9.4.1 Practice on real-world problems using Kaggle\\nOne effective way to acquire real-world expe rience is to try your hand at machine-\\nlearning competitions on Kaggle ( https:/ /kaggle.com). The only real way to learn is\\nthrough practice and actual coding—that’s the philosophy of this book, and Kaggle\\ncompetitions are the natural continuation of  this. On Kaggle, you’ll find an array of\\nconstantly renewed data-science competitio ns, many of which involve deep learning,\\nprepared by companies intere sted in obtaining novel solutions to some of their most\\nchallenging machine-learning problems. Fairly large monetary prizes are offered to\\ntop entrants.\\n Most competitions are won using either the XGBoost library (for shallow machine\\nlearning) or Keras (for deep le arning). So you’ll fit right in! By participating in a few\\ncompetitions, maybe as part of a team, you’ll  become more familiar with the practical\\nside of some of the advanced best practices described in this book, especially hyperpa-\\nrameter tuning, avoiding validation-set overfitting, and model ensembling. \\n9.4.2 Read about the latest  developments on arXiv\\nDeep-learning research, in contrast with some other scientific fields, takes places com-\\npletely in the open. Papers are made publicly  and freely accessible as soon as they’re\\nfinalized, and a lot of related software is open source. arXiv ( https:/ /arxiv.org)—pro-\\nnounced “archive” (the X stands for the Greek chi)—is an open-access preprint server\\nfor physics, mathematics, and computer sc ience research papers. It has become the\\nde facto way to stay up to date on the bl eeding edge of machine learning and deep\\nlearning. The large majority of deep-learning researchers upload any paper they write\\nto arXiv shortly after completion. This allows  them to plant a flag and claim a specific\\nfinding without waiting for a conference ac ceptance (which takes months), which is\\nnecessary given the fast pace of research and the intense competition in the field. It\\nalso allows the field to move extremely fast: all new findings are immediately available\\nfor all to see and to build on.\\n An important downside is that the sheer quantity of new papers posted every day\\non arXiv makes it impossible to even skim th em all; and the fact that they aren’t peer\\nreviewed makes it difficult to identify thos e that are both important and high quality.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 350, 'page_label': '351'}, page_content='338 CHAPTER 9 Conclusions\\nIt’s difficult, and becoming in creasingly more so, to find the signal in the noise. Cur-\\nrently, there isn’t a good solution to this problem. But some tools can help: an auxil-\\niary website called arXiv Sanity Preserver ( http:/ /arxiv-sanity.com) serves as a\\nrecommendation engine for new papers and can help you keep track of new develop-\\nments within a specific narr ow vertical of deep learni ng. Additionally, you can use\\nGoogle Scholar ( https:/ /scholar.google.com) to keep track of publications by your\\nfavorite authors. \\n9.4.3 Explore the Keras ecosystem\\nWith about 200,000 users as of November 2017 and growing fast, Keras has a large\\necosystem of tutorials, guides, and related open source projects:\\n\\uf0a1 Your main reference for working with  Keras is the online documentation at\\nhttps:/ /keras.io. The Keras source code can be found at https:/ /github.com/\\nfchollet/keras.\\n\\uf0a1 You can ask for help and join deep-lea rning discussions on the Keras Slack\\nchannel: https://kerasteam.slack.com.\\n\\uf0a1 The Keras blog, https:/ /blog.keras.io, offers Keras tutorials and other articles\\nrelated to deep learning.\\n\\uf0a1 You can follow me on Twitter: @fchollet. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 351, 'page_label': '352'}, page_content='339Final words\\n9.5 Final words\\nThis is the end of Deep Learning with Python! I hope you’ve learned a thing or two about\\nmachine learning, deep learning, Keras, and maybe even cognition in general. Learn-\\ning is a lifelong journey, especially in the field of AI, where we have far more unknowns\\non our hands than certitudes. So please go on learning, questioning, and researching.\\nNever stop. Because even give n the progress made so far, most of the fundamental\\nquestions in AI remain unanswered. Many haven’t even been properly asked yet.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 352, 'page_label': '353'}, page_content='appendix A\\nInstalling Keras and its\\ndependencies on Ubuntu\\nThe process of setting up a deep-learning workstation is fairly involved and consists\\nof the following steps, which this appendix will cover in detail:\\n1 Install the Python scientific suite— Numpy and SciPy—and make sure you\\nhave a Basic Linear Algebra Subprogram ( BLAS) library installed so your\\nmodels run fast on CPU.\\n2 Install two extras packages that come in handy when using Keras: HDF5 (for\\nsaving large neural-network files) an d Graphviz (for visualizing neural-\\nnetwork architectures).\\n3 Make sure your GPU can run deep-learning code, by installing CUDA drivers\\nand cuDNN.\\n4 Install a backend for Keras: TensorFlow, CNTK, or Theano.\\n5 Install Keras.\\nIt may seem like a daunting process. In fact, the only difficult part is setting up GPU\\nsupport—otherwise, the entire process can be done with a few commands and\\ntakes only a couple of minutes.\\n We’ll assume you have a fresh installation of Ubuntu, with an NVIDIA GPU avail-\\nable. Before you start, make sure you have pip installed and that your package man-\\nager is up to date:\\n$ sudo apt-get update\\n$ sudo apt-get upgrade\\n$ sudo apt-get install python-pip python-dev'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 353, 'page_label': '354'}, page_content='341Installing the Python scientific suite\\nA.1 Installing the Python scientific suite\\nIf you use a Mac, we recommend that you in stall the Python scientific suite via Ana-\\nconda, which you can get at www.continuu m.io/downloads. Note that this won’t\\ninclude HDF5 and Graphviz, which you have to install manually. Following are the\\nsteps for a manual installation of the Python scientific suite on Ubuntu:\\n1 Install a BLAS library (Open BLAS, in this case), to ensure that you can run fast\\ntensor operations on your CPU:\\n$ sudo apt-get install build-essential cmake git unzip \\\\\\npkg-config libopenblas-dev liblapack-dev\\n2 Install the Python scientific suite: Nump y, SciPy and Matplotlib. This is neces-\\nsary in order to perform any kind of ma chine learning or scientific computing\\nin Python, regardless of whether you’re doing deep learning:\\n$ sudo apt-get install python-numpy python-scipy python- matplotlib\\n ➥ python-yaml\\n3 Install HDF5. This library, originally developed by NASA, stores large files of\\nnumeric data in an efficient binary format . It will allow you to save your Keras\\nmodels to disk quickly and efficiently:\\n$ sudo apt-get install libhdf5-serial-dev python-h5py\\n4 Install Graphviz and pydot-ng, two packag es that will let you visualize Keras\\nmodels. They aren’t necessary to run Ke ras, so you could skip this step and\\ninstall these packages when you need them. Here are the commands:\\n$ sudo apt-get install graphviz\\n$ sudo pip install pydot-ng\\n5 Install additional packages that are used in some of our code examples:\\n$ sudo apt-get install python-opencv\\nPython 2 vs. Python 3\\nBy default, Ubuntu uses Python 2 when it installs Python packages such as python-\\npip. If you wish to use Python 3 instead, you should use the python3 prefix instead\\nof python. For instance:\\n$ sudo apt-get install python3-pip python3-dev\\nWhen you’re installing packages using pip, keep in mind that by default, it targets\\nPython 2. To target Python 3, you should use pip3:\\n$ sudo pip3 install tensorflow-gpu'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 354, 'page_label': '355'}, page_content='342 APPENDIX A Installing Keras and its dependencies on Ubuntu\\nA.2 Setting up GPU support\\nUsing a GPU isn’t strictly necessary, but it’s strongly recommended. All the code exam-\\nples found in this book can be run on a laptop CPU, but you may sometimes have to wait\\nfor several hours for a model to train,  instead of mere minutes on a good GPU. If you\\ndon’t have a modern NVIDIA GPU, you can skip this step and go directly to section A.3.\\n To use your NVIDIA GPU for deep learning, you need to install two things:\\n\\uf0a1 CUDA—A set of drivers for your GPU that allows it to run a low-level program-\\nming language for parallel computing.\\n\\uf0a1 cuDNN—A library of highly optimized primitives for deep learning. When using\\ncuDNN and running on a GPU, you can typically increase the training speed of\\nyour models by 50% to 100%.\\nTensorFlow depends on particular versions of CUDA and the cu DNN library. At the\\ntime of writing, it uses CUDA version 8 and cu DNN version 6. Please consult the\\nTensorFlow website for detail ed instructions ab out which versions are currently rec-\\nommended: www.tensorflow.org/install/install_linux.\\n Follow these steps:\\n1 Download CUDA. For Ubuntu (and other Linux flavors), NVIDIA provides a\\nready-to-use package that you can download from https:/ /developer\\n.nvidia.com/cuda-downloads:\\n$ wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/\\n➥ x86_64/cuda-repo-ubuntu1604_9.0.176-1_amd64.deb\\n2 Install CUDA. The easiest way to do so is to use Ubuntu’s apt on this package.\\nThis will allow you to easily install updates via apt as they become available:\\n$ sudo dpkg -i cuda-repo-ubuntu1604_9.0.176-1_amd64.deb\\n$ sudo apt-key adv --fetch-keys\\n➥ http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/\\n➥ x86_64/7fa2af80.pub\\n$ sudo apt-get update\\n$ sudo apt-get install cuda-8-0\\n3 Install cuDNN:\\na Register for a free NVIDIA developer account (unfortunately, this is necessary\\nin order to gain access to the cu DNN download), and download cu DNN at\\nhttps:/ /developer.NVIDIA.com/cudnn (select the version of cuDNN compati-\\nble with TensorFlow). Like CUDA, NVIDIA provides packages for different\\nLinux flavors—we’ll use the version for Ubuntu 16.04. Note that if you’re\\nworking with an EC2 install, you won’t be able to download the cu DNN\\narchive directly to your instance; instea d, download it to your local machine\\nand then upload it to your EC2 instance (via scp).\\nb Install cuDNN:\\n$ sudo dpkg -i dpkg -i libcudnn6*.deb'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 355, 'page_label': '356'}, page_content='343Installing Keras\\n4 Install TensorFlow:\\na TensorFlow with or without GPU support can be installed from PyPI using\\nPip. Here’s the command without GPU support:\\n$ sudo pip install tensorflow\\nb Here’s the command to install TensorFlow with GPU support:\\n$ sudo pip install tensorflow-gpu\\nA.3 Installing Theano (optional)\\nBecause you’ve already installed TensorFlow, you don’t have to install Theano in order\\nto run Keras code. But it ca n sometimes be useful to switch back and forth from\\nTensorFlow to Theano when building Keras models.\\n Theano can also be installed from PyPI:\\n$ sudo pip install theano\\nIf you’re using a GPU, then you should configure Theano to use your GPU. You can\\ncreate a Theano configuration file with this command:\\nnano ~/.theanorc\\nThen, fill in the file with the following configuration:\\n[global]\\nfloatX = float32\\ndevice = gpu0\\n[nvcc]\\nfastmath = True\\nA.4 Installing Keras\\nYou can install Keras from PyPI:\\n$ sudo pip install keras\\nAlternatively, you can install Keras from GitHub. Doing so will allow you to access the\\nkeras/examples folder, which contains many example scripts for you to learn from:\\n$ git clone https://github.com/fchollet/keras\\n$ cd keras\\n$ sudo python setup.py install\\nYou can now try to run a Keras script, such as this MNIST example:\\npython examples/mnist_cnn.py\\nNote that running this example to completion  may take a few minutes, so feel free to\\nforce-quit it (Ctrl-C) once you’ve verified that it’s working normally.\\n After you’ve run Keras at least once, th e Keras configuration file can be found at\\n~/.keras/keras.json. You can edit it to select the backend that Keras runs on: tensorflow,\\ntheano, or cntk. Your configuration file should like this:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 356, 'page_label': '357'}, page_content='344 APPENDIX A Installing Keras and its dependencies on Ubuntu\\n{\\n\"image_data_format\": \"channels_last\",\\n\"epsilon\": 1e-07,\\n\"floatx\": \"float32\",\\n\"backend\": \"tensorflow\"\\n}\\nWhile the Keras script examples/mnist_cnn.py is running, you can monitor GPU utili-\\nzation in a different shell window:\\n$ watch -n 5 NVIDIA-smi -a --display=utilization\\nYou’re all set! Congratulations—you can now begin building deep-learning applications.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 357, 'page_label': '358'}, page_content='appendix B\\nRunning Jupyter notebooks\\non an EC2 GPU instance\\nThis appendix provides a step-by-step guide to running deep-learning Jupyter note-\\nbooks on an AWS GPU instance and editing the notebooks from anywhere in your\\nbrowser. This is the perfect setup for deep-learning research if you don’t have a\\nGPU on your local machine. The original (and up-to-date) version of this guide can\\nbe found at https:/ /blog.keras.io.\\nB.1 What are Jupyter notebooks? \\nWhy run Jupyter notebooks on AWS GPUs?\\nA Jupyter notebook is a web app that allows you to write and annotate Python code\\ninteractively. It’s a great way to experi ment, do research, and share what you’re\\nworking on.\\n Many deep-learning applications are very computationally intensive and can take\\nhours or even days when running on a laptop’s CPU cores. Running on a GPU can\\nspeed up training and inference by a considerable factor (often 5 to 10 times, when\\ngoing from a modern CPU to a single modern GPU). But you may not have access to\\na GPU on your local machine. Running Jupyter notebooks on AWS gives you the same\\nexperience as running on your local machine, while allowing you to use one or sev-\\neral GPUs on AWS. And you only pay for what you use, which can compare favorably\\nto investing in your own GPU(s) if you use deep learning only occasionally.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 358, 'page_label': '359'}, page_content='346 APPENDIX B Running Jupyter notebooks on an EC2 GPU instance\\nB.2 Why would you not want to use Jupyter \\non AWS for deep learning?\\nAWS GPU instances can quickly become expe nsive. The one we suggest using costs\\n$0.90 per hour. This is fine for occasional use; but if you’re going to run experiments\\nfor several hours per day every day, then you’re better off building your own deep-\\nlearning machine with a TITAN X or GTX 1080 Ti.\\n In summary, use the Jupyter-on-EC2 setup if you don’t have access to a local GPU or\\ni f  y o u  d o n ’ t  w a n t  t o  d e a l  w i t h  i n s t alling Keras dependencies, in particular GPU\\ndrivers. If you have a access to a local GPU, we recommend running your models\\nlocally, instead. In that case, use the installation guide in appendix A.\\nNOTE You’ll need an active AWS account. Some familiarity with AWS EC2 will\\nhelp, but it isn’t mandatory. \\nB.3 Setting up an AWS GPU instance\\nThe following setup process will take 5 to 10 minutes:\\n1 Navigate to the EC2 control panel at https:/ /console.aws.amazon.com/ec2/v2,\\nand click the Launch Instance link (see figure B.1).\\n2 Select AWS Marketplace (see figure B.2), and search for “deep learning” in the\\nsearch box. Scroll down until you find the AMI named Deep Learning AMI\\nUbuntu Version (see figure B.3); select it.\\nFigure B.1 The EC2 \\ncontrol panel\\nFigure B.2 The EC2 AMI Marketplace'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 359, 'page_label': '360'}, page_content='347Setting up an AWS GPU instance\\n3 Select the p2.xlarge instance (see figure B.4). This instance type provides access\\nto a single GPU and costs $0.90 per hour of usage (as of March 2017).\\n4 You can keep the default configuration for the steps Config ure Instance, Add\\nStorage, and Add Tags, but you’ll customize the Configure Security Group step.\\nCreate a custom TCP rule to allow port 8888 (see figure B.5): this rule can be\\nallowed either for your current public IP (such as that of your laptop) or for any\\nIP (such as 0.0.0.0/0) if the former isn’ t possible. Note that if you allow port\\n8888 for any IP, then literally anyone will be able to listen to that port on your\\ninstance (which is where you’ll run IPython notebooks). You’ll add password\\nprotection to the notebooks to mitigate the risk of random strangers modifying\\nthem, but that may be pretty weak protection. If at all possible, you should con-\\nsider restricting access to a specific IP. But if your IP address changes constantly,\\nthen that isn’t a practical choice. If yo u’re going to leave access open to any IP,\\nthen remember not to leave sensitive data on the instance.\\nFigure B.3 The EC2 Deep Learning AMI\\nFigure B.4 The p2.xlarge instance'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 360, 'page_label': '361'}, page_content='348 APPENDIX B Running Jupyter notebooks on an EC2 GPU instance\\nNOTE At the end of the launch process, you’ll be asked if you want to create\\nnew connection keys or if you want to reuse existing keys. If you’ve never used\\nEC2 before, create new keys and download them.\\n5 To connect to your instance, select it on the EC2 control panel, click the Con-\\nnect button, and follow the instructions (see figure B.6). Note that it may take a\\nfew minutes for the instance to boot up. If you can’t connect at first, wait a bit\\nand try again.\\n6 Once you’re logged in to the instance via SSH, create an ssl directory at the root\\nof the instance, and cd to it (not mandatory, but cleaner):\\n$ mkdir ssl\\n$ cd ssl\\nFigure B.5 Configure a new security group.\\nFigure B.6 Connection instructions'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 361, 'page_label': '362'}, page_content='349Setting up an AWS GPU instance\\n7 Create a new SSL certificate using Open SSL, and create cert.key and cert.pem\\nfiles in the current ssl directory:\\n$ openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout \"cert.key\" -out\\n➥ \"cert.pem\" -batch\\n2.3.1 Configuring Jupyter\\nBefore you use Jupyter, you need to touch up its default configuration. Follow these\\nsteps:\\n1 Generate a new Jupyter config file (still on the remote instance):\\n$ jupyter notebook --generate-config\\n2 Optionally, you can generate a Jupyter password for your notebooks. Because\\nyour instance may be configur ed to be accessible from any IP (depending on\\nthe choice you made when configuring the security group), it’s better to restrict\\naccess to Jupyter via a password. To generate a password, open an IPython shell\\n(ipython command) and run the following:\\nfrom IPython.lib import passwd\\npasswd()\\nexit\\n3 The passwd() command will ask you to enter and verify a password. After you\\ndo, it will display a hash of your password. Copy that hash—you’ll need it soon.\\nIt looks something like this:\\nsha1:b592a9cf2ec6:b99edb2fd3d0727e336185a0b0eab561aa533a43\\nNote that this is a hash of the word password, which isn’t a password you should\\nbe using.\\n4 Use vi (or your favorite available text editor) to edit the Jupyter config file:\\n$ vi ~/.jupyter/jupyter_notebook_config.py\\n5 The config file is a Python file with al l lines commented out.  Insert the follow-\\ning lines of Python code at the beginning of the file:\\nc = get_config()\\nc.NotebookApp.certfile = u\\'/home/ubuntu/ssl/cert.pem\\'\\nc.NotebookApp.keyfile = u\\'/home/ubuntu/ssl/cert.key\\'\\nc.IPKernelApp.pylab = \\'inline\\'\\nc.NotebookApp.ip = \\'*\\'\\nc.NotebookApp.open_browser = False\\nc.NotebookApp.password =\\n➥ \\'sha1:b592a9cf2ec6:b99edb2fd3d0727e336185a0b0eab561aa533a43\\'\\nPath to the private key you \\ngenerated for the certificate\\nPath to the certificate \\nyou generated\\nServes the\\nnotebooks locally\\nInline figure when\\nusing MatplotlibGets the config object\\nDon’t open a browser window by \\ndefault when using notebooks.\\nPassword hash you\\ngenerated earlier'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 362, 'page_label': '363'}, page_content='350 APPENDIX B Running Jupyter notebooks on an EC2 GPU instance\\nNOTE In case you aren’t accustomed to using vi, remember that you need to\\npress I to begin inserting content. When you’re finished, press Esc, enter :wq,\\nand press Enter to quit vi and save your changes (:wq stands for write-quit). \\nB.4 Installing Keras\\nYou’re almost ready to start using Jupyter. But first, you need to update Keras. A ver-\\nsion of Keras is preinstalled on the AMI, but it may not necessarily be up to date. On\\nthe remote instance, run this command:\\n$ sudo pip install keras --upgrade\\nBecause you’ll probably use Python 3 (the notebooks provided with this book use\\nPython 3), you should also update Keras using pip3:\\n$ sudo pip3 install keras --upgrade\\nIf there’s an existing Keras configuration file on the inst ance (there shouldn’t be, but\\nthe AMI may have changed since I wrote this), yo u should delete it, just in case. Keras\\nwill re-create a standard configuration file when it’s launched for the first time.\\n If the following code snippet returns an error saying that the file doesn’t exist, you\\ncan ignore it:\\n$ rm -f ~/.keras/keras.json\\nB.5 Setting up local port forwarding\\nIn a shell on your local machine  (not the remote instance), st art forwarding your local\\nport 443 (the HTTPS port) to port 8888 of the remote instance:\\n$ sudo ssh -i awsKeys.pem -L local_port:local_machine:remote_port remote_machine\\nIn my case, it would look like the following:\\n$ sudo ssh -i awsKeys.pem -L\\n➥ 443:127.0.0.1:8888 ubuntu@ec2-54-147-126-214.compute-1.amazonaws.com\\nB.6 Using Jupyter from  your local browser\\nOn the remote instance, clone the GitHub repository containing the Jupyter note-\\nbooks associated with this book:\\n$ git clone https://github.com/fchollet/deep-learning-with-python-notebooks.git\\ncd deep-learning-with-python-notebooks\\nStart Jupyter Notebook by running this command, still on the remote instance:\\n$ jupyter notebook\\nThen, in your local browser, navigate to the local address you’re forwarding to the\\nremote notebook process ( https:/ /127.0.0.1). Be sure you use HTTPS in the address,\\nor you’ll get an SSL error.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 363, 'page_label': '364'}, page_content='351Using Jupyter from your local browser\\n You should see the safety wa rning shown in figure B.7. This warning is due to the\\nfact that the SSL certificate you generated isn’t veri fied by a trusted authority (obvi-\\nously—you generated your own). Click Advanced, and proceed to navigate.\\nYou should be prompted to enter your Jupyter password. You’ll then arrive at the Jupy-\\nter dashboard (see figure B.8).\\n \\n \\n \\nChoose New > Notebook to get started (see fig-\\nure B.9). You can use the Python version of your\\nchoice. All set!\\nFigure B.7 A safety warning you can ignore\\nFigure B.8 The Jupyter dashboard\\nFigure B.9 Create a new notebook.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 364, 'page_label': '365'}, page_content='Symbols\\n* operator 40\\n+ operator 99\\nNumerics\\n0D tensors. See scalars\\n1D convolutions 225–227\\n1D pooling, for sequence \\ndata 226\\n1D tensors. See vectors\\n2D tensors. See matrices\\n3D embeddings 253\\nA\\nactivation 160\\nactivation functions 22\\nad targeting 12\\nadd_loss method 302\\nADMM (alternating direction \\nmethod of \\nmultipliers) 332\\nadversarial networks 310\\nSee also generative deep \\nlearning; generative \\nadversarial networks\\nadversary network 305\\naffine transformations 72\\nAmazon Web Services. See AWS\\nAMD 20\\nAnalytical Engine 4\\nannotations 94, 96\\napplication program interfaces. \\nSee functional APIs\\narchitecture of networks\\n319–322\\nconvnets 321\\ndensely connected networks\\n319–320\\nrecurrent neural networks\\n321–322\\narchitecture patterns of \\nmodels 260–263\\nbatch normalization\\n260–261\\ndepthwise separable \\nconvolution 261–263\\nresidual connections\\n235–236, 244–246\\narrow of time 100\\nartificial intelligence 3–13, \\n270\\nexpectations for 13\\nhistory of 12–13\\narXiv preprint server 271, \\n337–338\\nassembling datasets 111–112\\naugmented intelligence 270\\naugmenting data 138–142\\nfeature extraction with\\n149–152\\nfeature extraction without\\n147–149\\nautoencoders. See VAEs (varia-\\ntional autoencoders)\\nAutoML systems 333\\nautonomous driving 12\\nAWS (Amazon Web Services)\\nGPU instances\\nrunning Jupyter on 350\\nsetting up 346–350\\nusing Jupyter on 346\\nB\\nBabbage, Charles 4\\nbackend engine, Keras 62\\nbackpropagation algorithm\\n11, 51–52, 246\\nbackward pass 49\\nbag-of-2-grams 181\\nbag-of-3-grams 181\\nbag-of-words 181\\nBaidu 22\\nbatch axis 35\\nbatch normalization 22\\nBatchNormalization layer 260\\nbatch_size 211\\nBengio, Yoshua 17, 188, 202\\nbidirectional layers 207\\nbinary classifications 68–77, \\n96\\nbinary crossentropy 60, 72\\nblack boxes 160\\nBLAS (Basic Linear Algebra \\nSubprograms) 39, 340\\nborder effects 125–126\\nbroadcasting operations\\n39–40\\nbrowsers, local, using Jupyter \\nfrom 350–351\\nC\\ncallbacks, writing 251–252\\nCAM (class activation map)\\n172\\ncategorical encoding 79\\ncategorical_crossentropy \\nfunction 53, 80, 83\\nindex'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 365, 'page_label': '366'}, page_content='354 INDEX\\nCERN 17\\nchannels axis 123\\nchannels-first convention 36\\nchannels-last convention 36\\ncharacter-level neural language \\nmodel 272\\nCiresan, Dan 17, 20\\nclass activation, visualizing heat-\\nmaps of 172–176\\nclasses 27, 96\\nclassification 60\\ncloud, running jobs in 66\\nclustering 94\\nCNNs. See convnets (convolu-\\ntional neural networks)\\nCNTK (Microsoft Cognitive \\nToolkit) 62\\ncompilation step 29\\nconcept vectors, for editing \\nimages 297–298\\nconditioning data 272\\nConnect button, EC2 control \\npanel 348\\nconnections, residual 244–246\\ncontent loss 288\\nConv1D layer 226\\nConv2D layer 120, 122, 124\\nconvnets (convolutional neural \\nnetworks) 321\\n1D 226–227\\ncombining with recurrent \\nneural networks\\n228–231\\noverview 120–129\\nconvolution operations\\n122–127\\nmax-pooling operations\\n127–129\\nprocessing sequences with\\n225–231\\ntraining on small datasets\\n130–142\\nbuilding networks\\n133–135\\ndata preprocessing\\n135–138\\nrelevance for small-data \\nproblems 130–131\\nusing data augmentation\\n138–142\\nusing pretrained convnets\\n143–159\\nfeature extraction\\n143–152\\nfine-tuning 152–158\\nvisualizing convnet learning\\n160–176\\nconvnet filters 167–172\\nheatmaps of class \\nactivation 172–176\\nintermediate activations\\n160–166\\nconvnets filters 160\\nconvolution base 143–144\\nconvolution operations\\n122–127\\nborder effects 125–126\\nconvolution strides 127\\npaddling 125–126\\nconvolution strides 127\\nconvolutions\\n1D 225–226\\ndepthwise separable\\n261–263\\nCortes, Corinna 15\\ncrossentropy 73\\nCUDA drivers 20, 340, 342\\ncuDNN library 62, 340, 342\\ncurvature 48\\nD\\ndata\\naugmenting 138–142\\nfeature extraction with\\n149–152\\nfeature extraction without\\n147–149\\nbatches of 34–35\\ngenerating sequence data\\n272\\nheterogeneous 101\\nhomogenous 101\\nlearning representations \\nfrom 6–8\\nmissing 102\\npreparing 112–113\\nfor character-level LSTM \\ntext generation 274\\nfor recurrent neural \\nnetworks 210–212\\npreprocessing 101–103,\\n135–138\\nredundancy 100\\nrepresentations for neural \\nnetworks 31–37\\n3D tensors 32\\ndata batches 34–35\\nexamples of data tensors\\n35\\nhigher-dimensional \\ntensors 32\\nimage data 36–37\\nkey attributes of tensors\\n32–33\\nmanipulating tensors in \\nNumpy 34\\nmatrices (2D tensors)\\n31–32\\nscalars (0D tensors) 31\\nsequence data 35–36\\ntimeseries data 35–36\\nvector data 35\\nvectors (1D tensors) 31\\nvideo data 37\\nshuffling 98, 100\\nsplitting 98\\ntokenizing 189–190\\ntransformations 10\\ntransforming 6\\nvectorization 101\\ndata augmentation 130\\ndata distillation 28\\ndata points 27, 220\\ndata representativeness 100\\ndata tensors, examples of 35\\ndatasets, assembling 111–112\\nDCGANs (deep convolutional \\nGANs)\\noverview 307\\ntraining 310–312\\ndecision boundaries 15\\ndecision trees 16–17\\ndeep convnets 266\\ndeep learning 3, 6–13\\naccomplishments of 11–12\\nachievements of 315–316\\ndemocratization of 23\\nenabling technologies\\n317–318\\nfuture of 23–24, 330–336\\nautomated machine \\nlearning 332–333\\nlifelong learning 333–335\\nlong-term vision 335–336\\nmodels as programs\\n330–332\\nmodular subroutine reuse\\n333–335\\ngeometric interpretation of\\n44–45\\nhardware and 20–21\\ninvestment in 22–23\\nlimitations of 325–329\\nlocal generalization vs. \\nextreme generalization\\n327–329'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 366, 'page_label': '367'}, page_content='355INDEX\\ndeep learning, limitations of \\n(continued)\\nrisk of anthropomorphiz-\\ning machine-learning \\nmodels 325–327\\noverview 9–11, 316–317\\npossible uses of 322–324\\nreasons for interest in 20–24\\nSee also generative deep \\nlearning\\nDeep Learning AMI, EC2 66\\nDeepDream technique\\n280–286\\noverview 280\\nimplementing in Keras\\n281–286\\nDeepMind, Google 22, 95\\nDense layers 28, 38, 53, 69–70, \\n122, 187, 213, 321\\ndense sampling 328\\ndensely connected networks\\n319–320\\ndepthwise separable \\nconvolution 321\\nderivatives, defined 47–48\\ndeveloper account, NVIDIA\\n342\\ndigital assistants 12\\ndimension 31\\ndimensionality 31, 94\\ndirected acyclic graphs of \\nlayers 242–246\\ninception modules 242–244\\nresidual connections\\n244–246\\ndiscriminator networks\\noverview 305\\nimplementing 307–309\\ndistance function 288\\ndot operations 40–42\\ndot product 38\\ndownloading\\nGloVe word embeddings 190\\nraw text 189\\nDropout layer 140\\ndtype attribute 32–33\\nE\\nEarlyStopping callbacks 250\\nEck, Douglas 271\\nediting images, concept vectors \\nfor 297–298\\nEigen 62\\nelement-wise operations 38–39\\nembedding layers, learning \\nword embeddings \\nwith 185–187\\nengineering features 101–103\\nensembling models 264–266\\nepochs 53, 74, 76, 82\\nepsilon 300\\nevaluating models 192–195\\nevaluation protocols, choosing\\n100, 112\\nexpert systems 4\\nextreme generalization, local \\ngeneralization vs.\\n327–329\\nextreme inception 244\\nF\\nfeature engineering 16, 18\\nfeature learning 101–103\\nfeature maps 123, 129\\nfeatures\\nengineering 101–103\\nextracting 143–152\\nwith data augmentation\\n149–152\\nwithout data augmentation\\n147–149\\nfeatures axis 35\\nfeedforward networks 196, 202\\nFeynman, Richard 316\\nfill_mode 139\\nfilter visualizations 172\\nfilters\\noverview 124\\nconvnets, visualizing 167–172\\nfine-tuning 152–158\\nfit method 29\\nfit_generator method 136\\nFlatten layer 133\\nFlickr 21\\nfloat32 29, 101, 173\\nfor loop 38, 197, 331\\nforward pass 46\\nfreezing layers 150\\nfully connected layers 58\\nfunctional APIs, Keras 234–248\\ndirected acyclic graphs of \\nlayers 242–246\\nlayer weight sharing 246–247\\nmodels as layers 247–248\\nmulti-input models 238–240\\nmulti-output models 240–242\\nG\\nGal, Yarin 216\\nGANs (generative adversarial \\nnetworks) 296, 305\\ngated recurrent unit layers.\\nSee GRU layers\\nGatys, Leon 287\\nGaussian distribution 307\\ngeneralization 104, 327–329\\ngenerative deep learning\\ngenerating images with varia-\\ntional autoencoders\\n296–304\\nconcept vectors for image \\nediting 297–298\\nsampling from latent \\nspaces of images\\n296–297\\ngenerating text with LSTM\\n271–279\\ngenerating sequence data\\n272\\nhistory of generative recur-\\nrent networks 271\\nimplementing character-\\nlevel LSTM text \\ngeneration 274–279\\nsampling strategy 272–274\\ngenerative adversarial \\nnetworks 305–313\\nadversarial networks 310\\ndiscriminator networks\\n307–309\\ngenerator networks\\n307–308\\nschematic implementa-\\ntion of 307\\ntraining DCGANs 310–312\\nneural style transfer 287–295\\ncontent loss 288\\nin Keras 289–295\\nstyle loss 288–289\\ngenerative deep learning, \\nDeepDream 280–286\\ngenerative recurrent networks, \\nhistory of 271\\ngenerator function 211, 230\\ngenerator networks, \\nimplementing 307–308\\ngeometric interpretation\\nof deep learning 44–45\\nof tensor operations 43–44\\ngeometric space 316'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 367, 'page_label': '368'}, page_content='356 INDEX\\nGloVe (Global Vectors for Word \\nRepresentation)\\ndownloading word \\nembeddings 190\\nloading embeddings in \\nmodels 191\\nGoodfellow, Ian 305\\nGPUs (graphics processing \\nunits)\\ninstalling on AWS 350\\ninstances, on AWS 346–350\\noverview 20\\nselecting 66–67\\nsupport, setting up on \\nUbuntu 342–343\\ngradient boosting machines\\n16–17\\ngradient descent 167\\ngradient propagation 22\\ngradient-based optimization\\n46–52\\nbackpropagation algorithm\\n51–52\\nderivatives, defined 47–48\\ngradients 48\\nstochastic gradient descent\\n48–51\\ngradients 48\\nGram matrix 288\\ngraphs, directed acyclic of \\nlayers 242–246\\nGraphviz 257, 340–341\\nGraves, Alex 271\\ngreedy sampling 272\\nground-truth 96\\nGRU (gated recurrent unit) \\nlayers 202–204, 215\\nH\\nhandwriting transcription 11\\nhardware 20–21\\nhash collisions 183\\nHDF5 340\\nheatmaps\\nof class activation, visualizing\\n172–176\\noverview 160\\nheight_shift range 139\\nheterogeneous data 101\\nhidden layers 77\\nhidden unit 70\\nhierarchical representation \\nlearning 8\\nHinton, Geoffrey 17, 109\\nHochreiter, Sepp 202\\nhold-out validation 98–99\\nhomogenous data 101\\nhorizontal_flip 139\\nHSV (hue-saturation-value) \\nformat 6\\nHyperas library 264\\nHyperopt 264\\nhyperparameters\\noptimizing 263–264\\noverview 98\\ntuning 114–115\\nhyperplanes 15\\nhypothesis space 59, 72, 319\\nI\\nIDSIA 17\\nILSVRC (ImageNet Large Scale \\nVisual Recognition \\nChallenge) 21\\nimage classification 11\\nimage data 36–37, 319\\nimage segmentation 94\\nimage-classification task 262\\nImageDataGenerator class 135, \\n139, 147\\nImageNet class 17, 145, 281\\nimages\\nediting concept vectors for\\n297–298\\nflipping 139\\ngenerating with variational \\nautoencoders 296–304\\nconcept vectors for image \\nediting 297–298\\noverview 296\\nsampling from latent spaces \\nof 296–297\\ninception blocks 59\\nInception modules 235,\\n242–244, 281\\ninclude_top argument 145\\ninformation bottlenecks 80, 84\\ninformation distillation \\npipeline 166\\ninformation leaks 97\\ninitial state 196\\ninput data 6–7, 58, 95\\ninput_shape argument 145\\ninput_tensor 237\\ninstalling\\nCUDA 342\\ncuDNN 342\\nKeras 343–344, 350\\nOpenBLAS 341\\nOpenCV 341\\nPython scientific suite on \\nUbuntu 341\\nTensorFLow 343\\nTheano on Ubuntu 343\\nIntel 22\\nintermediate activations, \\nvisualizing 160–166\\ninvestments in deep learning\\n22–23\\nipython command 349\\nJ\\njoint feature learning 18\\nJupyter notebooks 65\\nconfiguring 349–350\\noverview 345\\nrunning on AWS GPU \\ninstances\\ninstalling Keras 350\\nsetting up AWS GPU \\ninstances 346–350\\nsetting up local port \\nforwarding 350\\nusing from local browsers\\n350–351\\nusing on AWS 346\\nK\\nK80, NVIDIA 21\\nKaggle platform\\noverview 16, 19, 266\\npractice on real-world prob-\\nlems using 337\\nKeras API 234–248\\ndirected acyclic graphs of \\nlayers 242–246\\nexploring 338\\nfunctional APIs 236–238\\nimplementing DeepDream \\nin 281–286\\ninstalling 343–344, 350\\nlayer weight sharing 246–247\\nmodels as layers 247–248\\nmulti-input models 238–240\\nmulti-output models\\n240–242\\nneural style transfer in\\n289–295\\nrecurrent layers in 198–202\\nusing callbacks 249–259'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 368, 'page_label': '369'}, page_content='357INDEX\\nKeras framework 61–64\\nCNTK 62\\ndeveloping with 62–64\\nrunning 66\\nTensorFlow 62\\nTheano 62\\nKeras library 27\\nkeras.applications module 145\\nkeras.callbacks module 249, \\n251\\nkeras.preprocessing.image 135\\nkernel methods 15–16\\nkernel trick 15\\nK-fold validation, iterated with \\nshuffling 99–100\\nKingma, Diederik P. 298\\nKrizhevsky, Alex 20\\nL\\nL1 regularization 107\\nL2 regularization 107\\nlabel 27, 96\\nLambda layer 301\\nlanguage models\\nsampling from 276–278\\ntraining 276–278\\nlast-layer activation 113\\nlatent spaces\\nof images, sampling from\\n296–297\\noverview 270\\nlayer compatibility 59\\nlayered representations \\nlearning 8\\nlayers\\ndifferentiable 332\\ndirected acyclic graphs of\\n242–246\\ninception modules\\n242–244\\nresidual connections\\n244–246\\nfreezing 150\\nmodels as 247–248\\noverview 8, 58–59\\nrecurrent\\nin Keras 198–202\\nstacking 217–219\\nunfreezing 154\\nweight sharing 246–247\\nlayer-wise pretraining 22\\nLeakyReLU layer 308\\nLeCun, Yann 15, 17\\nLeNet network 15\\nLHC (Large Hadron Collider)\\n17\\nlifelong learning 333–335\\nlinear transformations 72\\nlocal generalization, extreme \\ngeneralization vs.\\n327–329\\nlocal port forwarding, setting \\nup 350\\nlogistic regression algorithm\\n85\\nlogreg (logistic regression) 14\\nlogs argument 251\\nlookback parameter 230\\nlookback timesteps 210\\nloss function 10, 29, 58, 60, 113\\nloss plateau 250\\nloss value 96\\nLovelace, Ada 5\\nLSTM (long short-term \\nmemory) 58, 202–204\\ngenerating text with 271–279\\ngenerating sequence data\\n272\\nhistory of generative recur-\\nrent networks 271\\nimplementing character-\\nlevel text generation\\n274–279\\nsampling strategy 272–274\\noverview 20, 269\\nM\\nmachine learning\\nautomated 332–333\\nbasic approaches 213–215\\nbranches of 94–96\\nreinforcement learning\\n95–96\\nself-supervised learning\\n94–95\\nsupervised learning 94\\nunsupervised learning 94\\ndata preprocessing 101–103\\ndeep learning vs. 17–18\\nevaluating models of 97–100\\nchoosing evaluation \\nprotocols 100\\ntest sets 97–100\\ntraining sets 97–100\\nvalidation sets 97–100\\nfeature engineering 101–103\\nfeature learning 101–103\\nhistory of 14–19\\ndecision trees 16–17\\ngradient boosting \\nmachines 16–17\\nkernel methods 15–16\\nneural networks 14–15, 17\\nprobabilistic modeling 14\\nrandom forests 16–17\\nlearning representations \\nfrom data 6–8\\nmodels, risk of anthropo-\\nmorphizing 325–327\\noverfitting and underfitting\\n104–110\\nadding dropout 109–110\\nadding weight \\nregularization 107–108\\nreducing network size\\n104–107\\nworkflow of 111–115,\\n318–319\\nassembling datasets\\n111–112\\nchoosing evaluation \\nprotocol 112\\nchoosing measure of \\nsuccess 112\\ndefining problems\\n111–112\\ndeveloping models\\n113–114\\npreparing data 112–113\\nregularizing models\\n114–115\\ntuning hyperparameters\\n114–115\\nSee also non-machine learn-\\ning\\nMAE (mean absolute error)\\n87, 91, 212, 320\\nMatplotlib library 33, 74, 349\\nmatrices (2D tensors) 31–32\\nmaximum operation 40\\nmax-pooling operations\\n127–129\\nMaxPooling1D layer 226, 231\\nMaxPooling2D layer 120, 122, \\n127\\nmean_squared_error 73\\nmemorization capacity 104\\nmetrics 29\\nmetrics, logging 249\\nMicrosoft Cognitive Toolkit.\\nSee CNTK\\nMikolov, Tomas 188'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 369, 'page_label': '370'}, page_content='358 INDEX\\nmini-batch 96\\nmini-batch SGD (mini-batch \\nstochastic gradient \\ndescent) 49\\nMinsky, Marvin 12\\nMNIST dataset 27, 68\\nmodel checkpointing 249\\nModel class 162\\nmodel depth 8\\nmodel plot 258\\nmodel.fit() function 249\\nmodel.fit_generator() function\\n249\\nModelCheckpoint callbacks\\n250\\nmodels\\narchitecture patterns\\n260–263\\nbatch normalization\\n260–261\\ndepthwise separable \\nconvolution 261–263\\nresidual connections\\n235–236, 244–246\\nas layers 247–248\\nas programs 330–332\\ndefining 191\\ndeveloping\\nachieving statistical power\\n113–114\\ndetermining capacity 114\\nensembling 264–266\\nevaluating 192–195\\nhyperparameter optimization\\n263–264\\nlanguage\\nsampling from 276–278\\ntraining 276–278\\nloading GloVe embeddings \\nin 191\\nmachine learning, risk of \\nanthropomorphizing 3\\n25–327\\nmulti-input 238–240\\nmulti-output 240–242\\nregularizing 114–115\\ntraining 192–195\\nusing Keras callbacks\\n249–259\\nusing TensorBoard 249–259\\nmodular subroutines, reusing\\n333–335\\nmodules, inception 242–244\\nmomentum 50\\nMoore’s law 21, 317\\nMSE (mean squared error) 77, \\n87, 91, 241, 320\\nmulticlass classifications 78–84\\nmultihead networks 59\\nmulti-input models 238–240\\nmultilabel classification 78, 96, \\n320\\nmultimodal inputs 234\\nmulti-output models 240–242\\nN\\nN classes 84\\nNaive Bayes algorithm 14\\nnaive_add 39\\nNational Institute of Standards \\nand Technology.\\nSee NIST\\nndim attribute 31\\nNervana Systems 22\\nneural layers 22\\nneural networks\\nanatomy of 58–60\\nlayers 58–59\\nloss functions 60\\nmodels 59–60\\noptimizers 60\\nbinary classifications 68–77\\nbreakthroughs in 17\\ndata preprocessing for\\n101–102\\nhandling missing values\\n102\\nvalue normalization\\n101–102\\nvectorization 101\\ndata representations for\\n31–37\\n3D tensors 32\\ndata batches 34–35\\nexamples of data tensors\\n35\\nhigher-dimensional \\ntensors 32\\nimage data 36–37\\nkey attributes of tensors\\n32–33\\nmanipulating tensors in \\nNumpy 34\\nmatrices (2D tensors)\\n31–32\\nscalars (0D tensors) 31\\nsequence data 35–36\\ntimeseries data 35–36\\nvector data 35\\nvectors (1D tensors) 31\\nvideo data 37\\ngradient-based optimization\\n46–52\\nbackpropagation \\nalgorithm 51–52\\nderivatives 47–48\\ngradients 48\\nstochastic gradient \\ndescent 48–51\\nKeras 61–64\\nCNTK 62\\ndeveloping with 62–64\\nTensorFlow 62\\nTheano 62\\nmulticlass classifications\\n78–84\\nregression 85–91\\nsetting up workstations\\n65–67\\nGPUs for deep learning\\n66–67\\nJupyter notebooks 65\\nrunning jobs in cloud 66\\nrunning Keras 66\\ntensor operations 38–45\\nbroadcasting 39–40\\ndot 40–42\\nelement-wise 38–39\\ngeometric interpretation \\nof 43–44\\ngeometric interpretation \\nof deep learning 44–45\\nreshaping 42–43\\nneural style transfer 287–295\\ncontent loss 288\\nin Keras 289–295\\nstyle loss 288–289\\nN-grams 180\\nNIST (National Institute of \\nStandards and \\nTechnology) 27\\nnon-linearity function 72\\nnon-machine learning, \\nbaselines 212–213\\nnonstationary problems 111\\nnormalizing batches 260–261\\nnormalizing values 101–102\\nNumpy arrays 28, 31\\nNumpy library, manipulating \\ntensors in 34\\nNumpy matrix 31\\nNumpy tensors 53\\nNVIDIA 20, 66'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 370, 'page_label': '371'}, page_content='359INDEX\\nO\\nobject detection 94\\nobjective function 10, 60\\nOccam’s razor principle 107\\noctaves 281–282\\none-hot encoding\\nof characters 181–183\\nof words 181–183\\noverview 79, 84, 101\\nonline documentation, Keras\\n338\\noptimization 22, 50, 104, 113, \\n263–264\\noptimizer argument 11, 29, 58, \\n73\\noptimizers 60\\noutput\\nclasses 77\\noverview 95\\ntensor 237\\noverfitting\\nadding dropout 109–110\\nadding weight regularization\\n107–108\\nreducing network size\\n104–107\\nusing recurrent dropout to \\nfight 216–217\\nP\\npadding 125–126\\nparameterized layers 10\\nparameters\\nadjusting 249\\noverview 97\\npartitions 99\\npasswd() command 349\\nPCA (principal component \\nanalysis) 255\\nPichai, Sundar 22\\npip 350\\nplot_model 258\\nplotting code 156\\npointwise convolutions 243\\npooling 1D, for sequence data\\n226\\npredict method 76, 83, 147\\nprediction error 95–96\\npredictions 83\\npreparing data 112–113\\npreprocessing\\ndata 101–103, 135–138\\nfor neural networks\\n101–102\\noverview 135–138\\nembeddings 190–191\\npretrained convnets 143–159\\nfeature extraction 143–152\\nwith data augmentation\\n149–152\\nwithout data augmentation\\n147–149\\nfine-tuning 152–158\\nwith small datasets 159\\npretrained networks 130, 143\\npretrained word embeddings\\n184\\nprobabilistic modeling 14\\nprobability distribution 80\\nproblems, defining 111–112\\nprocessing sequences with \\nconvnets 225–231\\n1D convolution for sequence \\ndata 225–226\\n1D pooling for sequence data\\n226\\ncombining with recurrent \\nneural networks to pro-\\ncess long sequences\\n228–231\\nimplementing 1D convnets\\n226–227\\nprogram subroutines 334\\nprogram synthesis 331\\nPyCharm 65\\npydot library 257\\npydot-ng 341\\nPython\\ninstalling scientific suite on \\nUbuntu 341\\noverview 19\\npython-pip package 341\\nQ\\nquestion-answering model 238\\nR\\nrandom forests 16–17\\nrandomly shuffle data 100\\nrandomness 272\\nrank 31\\nrecurrent dropout 207, 216\\nrecurrent layers, bidirectional\\n207\\nrecurrent neural networks\\n196–224, 319, 321–322\\nbasic machine-learning \\napproach 213–215\\nbidirectional 219–222\\ncombining with \\nconvnets 228–231\\nfirst recurrent baseline\\n215–216\\ngenerative, history of 271\\nGRU layers 202–204\\nLSTM layers 202–204\\nnon-machine-learning \\nbaselines 212–213\\npreparing data for 210–212\\nrecurrent layers in Keras\\n198–202\\nstacking recurrent layers\\n217–219\\nusing recurrent dropout to \\nfight overfitting\\n216–217\\nReduceLROnPlateau callbacks\\n250–251\\nregression 60, 85–91, 320\\nregularization loss function\\n300\\nregularizing models 114–115\\nreinforcement learning 95–96\\nrelu (rectified linear unit) 71\\nrepresentations\\nextracting 28\\noverview 6\\nreshaping tensors 42–43\\nresidual connections 235\\nresponse map 124\\nreturn_sequences argument\\n198\\nreusability 23\\nreverse-mode differentiation\\n52\\nRGB (red-green-blue) format 6\\nRMSProp optimizer 53, 73, 77, \\n135, 155, 222\\nRNN (recurrent neural \\nnetwork) 196\\nrotation_range 139\\nS\\nsamples axis 34\\nsamples dimension 34\\nsampling\\nfrom language models\\n276–278'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 371, 'page_label': '372'}, page_content='360 INDEX\\nsampling (continued)\\nfrom latent spaces of images\\n296–297\\nstrategies 272–274\\nSanity Preserver, arXiv 338\\nscalar regression 86, 96\\nscalar tensor 31\\nscalars (0D tensors) 31\\nschematic implementation, of \\nGAN 307\\nSchmidhuber, Jürgen 202\\nScikit-Learn 63\\nSciPy 284, 341\\nself-supervised learning 94–95\\nselu function 261\\nSeparableConv2D layer 261, \\n321\\nseparation hyperplane 15\\nsequence data\\ngenerating 272\\noverview 35–36\\nsequence generation 94\\nsequence prediction 60\\nsequences, processing with \\nconvnets 225–231\\n1D convolution for sequence \\ndata 225–226\\n1D pooling for sequence \\ndata 226\\ncombining with recurrent \\nneural networks\\n228–231\\nimplementing 1D convnets\\n226–227\\nSequential class 63, 248\\nSequential model 150, 234\\nSGD (stochastic gradient \\ndescent) 48–51, 60\\nshallow learning 8\\nshared LSTM 247\\nshear_range 139\\nshow_shapes option 258\\nshuffling, iterated K-fold valida-\\ntion with 100\\nSiamese LSTM model 247\\nsigmoid function 71, 86, 320\\nSimonyan, Karen 143\\nSimpleRNN layer 198, 322\\nsingle-label\\ncategorical classification 320\\nmulticlass classification 78\\nsliding windows 124\\nSmart Reply feature, Google\\n271\\nsmile vector 297\\nsoftmax 28, 80, 84, 273, 320\\nsound data 319\\nsparse_categorical_crossentropy\\n83–84\\nspatially hierarchical patterns\\n123\\nspeech recognition 11\\nssl directory 348\\nstacking recurrent layers\\n217–219\\nstatistical power, developing \\nmodels with 113–114\\nsteps_per_epoch 136\\nstochastic gradient descent.\\nSee SGD\\nstochastic sampling 272\\nstochasticity 272, 308\\nstrided convolutions 127\\nstrides 125\\nstyle function 288\\nstyle loss 288–289\\nsubroutines, reusing modular\\n333–335\\nsupervised learning 94\\nSVM (support vector machine)\\n15\\nsymbolic AI 4, 12\\nsymbolic differentiation 52\\nsyntax tree prediction 94\\nSzegedy, Christian 235\\nT\\ntanh activation 77\\ntarget 95\\ntemporal leak 100\\ntemporally supervised learning\\n95\\nTensorBoard applications 233, \\n249–259\\nTensorFlow visualization \\nframework 252–258\\ntensors\\nhigher-dimensional 32\\nkey attributes of 32–33\\nmanipulating in Numpy 34\\noperations of 38–45\\nbroadcasting 39–40\\ndot 40–42\\nelement-wise 38–39\\ngeometric interpretation \\nof 43–44\\ngeometric interpretation \\nof deep learning 44–45\\nreshaping 42–43\\nreshaping 42\\nslicing 34\\nSee also data tensors\\ntest sets 97–100\\nhold-out validation 98–99\\niterated K-fold validation \\nwith shuffling 100\\nK-fold validation 99\\ntext data 180–195, 319\\ndownloading raw text\\n188–195\\none-hot encoding of words \\nand characters\\n181–183\\nword embeddings 184–195\\ndefining models 191\\ndownloading GloVe word \\nembeddings 190\\nlearning with embedding \\nlayers 185–187\\nloading GloVe embeddings \\nin models 191\\npreprocessing 190–191\\npretrained 188\\ntokenizing data 189–190\\ntraining and evaluating \\nmodels 192–195\\ntext, generating with LSTM\\n271–279\\ngenerating sequence data\\n272\\nhistory of generative recur-\\nrent networks 271\\nimplementing character-level \\ntext generation 274–279\\nsampling strategy 272–274\\ntext-to-speech conversion 12\\nTheano\\ninstalling on Ubuntu 343\\noverview 23, 62\\ntimeseries data 35–36, 319\\ntimesteps 210\\nTITAN X, NVIDIA 21\\ntoken embedding 180\\ntokenizing data, word \\nembeddings 189–190\\ntotal variation loss 291\\nTPU (tensor processing unit)\\n21\\ntrainable attribute 150\\ntraining\\nconvnets on small datasets\\n130–142\\nbuilding networks\\n133–135'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 372, 'page_label': '373'}, page_content='361INDEX\\ntraining, convnets on small \\ndatasets (continued)\\ndata preprocessing\\n135–138\\ndownloading data\\n131–133\\nrelevance for small-data \\nproblems 130–131\\nusing data augmentation\\n138–142\\ninterrupting 249\\nlanguage models 276–278\\nmodels 192–195\\ntraining loop 11, 46\\ntraining sets 97–100\\nhold-out validation 98–99\\niterated K-fold validation \\nwith shuffling 100\\nK-fold validation 99\\ntrain_labels variable 27, 68\\ntranslation-invariant patterns\\n123, 321\\ntransposition 43\\nTuring test 5\\nTuring, Alan 5\\ntwo-branch networks 59\\nTyka, Mike 280, 306\\nU\\nUbuntu\\ninstalling Keras on 343–344\\ninstalling Python scientific \\nsuite on 341\\ninstalling Theano on 343\\nsetting up GPU support\\n342–343\\nunderfitting 104–110\\nadding dropout 109–110\\nadding weight regularization\\n107–108\\nreducing network size\\n104–107\\nunfreezing layers 154\\nUnix workstation 65\\nunsupervised learning 94\\nV\\nVAEs (variational autoencod-\\ners), generating images \\nwith 296–304\\nconcept vectors for image \\nediting 297–298\\nsampling from latent spaces \\nof images 296–297\\nvalidation scores 100\\nvalidation sets 97–100\\nhold-out validation 98–99\\niterated K-fold validation \\nwith shuffling 100\\nK-fold validation 99\\noverfitting 97\\noverview 73\\nvalidation_data argument 74, \\n137\\nvalidation_steps argument 137\\nvalues\\nhandling missing 102\\nnormalizing 101–102\\nvanishing gradient problem\\n202\\nVapnik, Vladimir 15\\nvector data 35, 319\\nvector regression 96\\nvectorization 101\\nvectorized data 69\\nvectorized implementations 38\\nvectorizing text 180\\nvectors (1D tensors) 31\\nversatility 23\\nvi 350\\nvideo data 37, 319\\nvisual concepts 160\\nvisualizing\\nconvnet filters 167–172\\nconvnet learning 160–176\\nheatmaps of class activation\\n172–176\\nintermediate activations\\n160–166\\nvolumetric data 319\\nW\\nweight decay 107\\nweight regularization, \\nadding 107–108\\nweight sharing of layers\\n246–247\\nweight-initialization schemes\\n22\\nweights argument, VGG16 58, \\n145\\nweights, layers 10\\nWelling, Max 298\\nwidth_shift range 139\\nword embeddings 184–195\\ndefining models 191\\ndownloading GloVe word \\nembeddings 190\\nevaluating models 192–195\\nlearning embedding layers\\n185–187\\nloading GloVe embeddings \\nin models 191\\npreprocessing embeddings\\n190–191\\ntokenizing data 189–190\\ntraining models 192–195\\nusing pretrained word \\nembeddings 188\\nword vectors 184\\nWord2vec algorithm 188\\nword-embedding space 185\\nword_index 69\\nworkflow of machine learning\\n111–115, 318–319\\nassembling datasets 111–112\\nchoosing evaluation protocol\\n112\\nchoosing measure of success\\n112\\ndefining problems 111–112\\ndeveloping models 113–114\\npreparing data 112–113\\nregularizing models 114–115\\ntuning hyperparameters\\n114–115\\nworkflows 18\\nworkstations, setting up 65–67\\nJupyter notebooks 65\\nrunning jobs in cloud 66\\nrunning Keras 66\\nselecting GPUs 66–67\\nwriting callbacks 251–252\\nX\\nXception 244, 248\\nXGBoost library 19, 337\\nY\\nyield operator 136\\nZ\\nZisserman, Andrew 143\\nzoom_range 139')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Deep Learning with Python - François Chollet - Manning (2018).pdf\")\n",
    "pdf_docs = loader.load()\n",
    "pdf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ef4eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f38a3ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 0, 'page_label': '1'}, page_content='Deep Learning\\nwith Python\\nFRANÇOIS CHOLLET\\nMANNING\\nSHELTER ISLAND'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 1, 'page_label': '2'}, page_content='ISBN 9781617294433\\nPrinted in the United States of America\\n©2018 by Manning Publications Co.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 2, 'page_label': '3'}, page_content='brief contents\\nPART 1F UNDAMENTALS OF DEEP LEARNING .................................. 1\\n1 ■ What is deep learning? 3\\n2 ■ Before we begin: the mathematical building blocks of neural \\nnetworks 25\\n3 ■ Getting started with neural networks 56\\n4 ■ Fundamentals of machine learning 93\\nPART 2D EEP LEARNING IN PRACTICE ........................................ 117\\n5 ■ Deep learning for computer vision 119\\n6 ■ Deep learning for text and sequences 178\\n7 ■ Advanced deep-learning best practices 233'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 2, 'page_label': '3'}, page_content='7 ■ Advanced deep-learning best practices 233\\n8 ■ Generative deep learning 269\\n9 ■ Conclusions 314'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 3, 'page_label': '4'}, page_content='contents\\npreface xiii\\nacknowledgments xv\\nabout this book xvi\\nabout the author xx\\nabout the cover xxi\\nPART 1F UNDAMENTALS OF DEEP LEARNING ...................1\\n1 What is deep learning? 3\\n1.1 Artificial intelligence, machine learning, \\nand deep learning 4\\nArtificial intelligence 4 ■ Machine learning 4 ■ Learning \\nrepresentations from data 6 ■ The “deep” in deep learning 8\\nUnderstanding how deep learning works, in three figures 9\\nWhat deep learning has achieved so far 11 ■ Don’t believe'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 3, 'page_label': '4'}, page_content='the short-term hype 12 ■ The promise of AI 13\\n1.2 Before deep learning: a brief history of machine \\nlearning 14\\nProbabilistic modeling 14 ■ Early neural networks 14\\nKernel methods 15 ■ Decision trees, random forests, \\nand gradient boosting machines 16 ■ Back to neural \\nnetworks 17 ■ What makes deep learning different 17\\nThe modern machine-learning landscape 18'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 4, 'page_label': '5'}, page_content='1.3 Why deep learning? Why now? 20\\nHardware 20 ■ Data 21 ■ Algorithms 21 ■ A new \\nwave of investment 22 ■ The democratization of deep \\nlearning 23 ■ Will it last? 23\\n2 Before we begin: the mathematical building blocks of \\nneural networks 25\\n2.1 A first look at a neural network 27\\n2.2 Data representations for neural networks 31\\nScalars (0D tensors) 31 ■ Vectors (1D tensors) 31\\nMatrices (2D tensors) 31 ■ 3D tensors and higher-\\ndimensional tensors 32 ■ Key attributes 32'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 4, 'page_label': '5'}, page_content='dimensional tensors 32 ■ Key attributes 32\\nManipulating tensors in Numpy 34 ■ The notion \\nof data batches 34 ■ Real-world examples of data \\ntensors 35 ■ Vector data 35 ■ Timeseries data or \\nsequence data 35 ■ Image data 36 ■ Video data 37\\n2.3 The gears of neural ne tworks: tensor operations 38\\nElement-wise operations 38 ■ Broadcasting 39 ■ Tensor \\ndot 40 ■ Tensor reshaping 42 ■ Geometric interpretation \\nof tensor operations 43 ■ A geometric interpretation of deep \\nlearning 44'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 4, 'page_label': '5'}, page_content='learning 44\\n2.4 The engine of neural networks: gradient-based \\noptimization 46\\nWhat’s a derivative? 47 ■ Derivative of a tensor operation: \\nthe gradient 48 ■ Stochastic gradient descent 48\\nChaining derivatives: the Backpropagation algorithm 51\\n2.5 Looking back at our first example 53\\n2.6 Chapter summary 55\\n3 Getting started with neural networks 56\\n3.1 Anatomy of a neural network 58\\nLayers: the building blocks of deep learning 58 ■ Models:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 4, 'page_label': '5'}, page_content='networks of layers 59 ■ Loss functions and optimizers: keys \\nto configuring the learning process 60\\n3.2 Introduction to Keras 61\\nKeras, TensorFlow, Theano, and CNTK 62 ■ Developing \\nwith Keras: a quick overview 62\\n3.3 Setting up a deep-learning workstation 65\\nJupyter notebooks: the preferred way to run deep-learning \\nexperiments 65 ■ Getting Keras running: two options 66')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This splitter tries to split text logically (paragraphs → sentences → characters)\n",
    "# chunk_size = max characters per chunk\n",
    "# chunk_overlap = shared characters between chunks to preserve context\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "final_docs = text_splitter.split_documents(pdf_docs)\n",
    "final_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dd36f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1765"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8c540d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'cosmics.txt'}, page_content='Cosmics: Their Presence in the Galaxy and Their Relation with the Sound “Om”\\n\\nThe universe is vast, mysterious, and filled with forces that go far beyond what human senses can directly perceive. From ancient civilizations to modern astrophysics, humans have tried to understand the nature of the cosmos and its underlying energy. The term cosmics broadly refers to cosmic entities, energies, radiations, and phenomena that exist throughout the galaxy. These cosmics play a crucial role in shaping stars, planets, life, and even human consciousness. Interestingly, ancient spiritual traditions, especially from India, describe the universe as originating from a primordial sound known as Om. This essay explores the presence of cosmics in the galaxy and their deep symbolic and scientific connection with the sound Om.\\n\\nUnderstanding Cosmics and Cosmic Presence\\n\\nCosmics can be understood as universal forces and energies present throughout space. In scientific terms, this includes cosmic radiation, electromagnetic waves, dark matter, dark energy, gravitational fields, and subatomic particles traveling across the universe. Cosmic rays, for example, are high-energy particles that originate from supernova explosions, black holes, and distant galaxies. These particles constantly interact with Earth and all living beings, even though we are not consciously aware of them.\\n\\nThe galaxy is not empty space; it is a dynamic system filled with motion, vibration, and energy. Stars are born from clouds of cosmic dust, galaxies collide and merge, and black holes bend space and time. All these processes show that the cosmos is alive with activity. Cosmics act as the invisible framework that maintains balance and order in the universe. Without these forces, galaxies would not form, stars would not shine, and life would not exist.\\n\\nThe Universe as Vibration and Energy\\n\\nModern physics tells us that everything in the universe is made of energy. According to quantum mechanics, even solid matter is composed of vibrating particles. At the most fundamental level, reality behaves like waves rather than fixed objects. This idea strongly aligns with ancient philosophies that described the universe as sound or vibration.\\n\\nAstrophysical observations show that space itself vibrates. Planets produce electromagnetic frequencies, stars emit harmonic oscillations, and even black holes generate gravitational waves. These vibrations form a cosmic symphony, where each celestial object contributes its own frequency. Cosmics, therefore, are not just particles or forces; they are expressions of universal vibration.\\n\\nThe Concept of Om in Ancient Wisdom\\n\\nIn ancient Indian philosophy, Om (also written as AUM) is considered the primordial sound of the universe. It is believed to be the first vibration from which all creation emerged. The Mandukya Upanishad describes Om as the essence of past, present, and future, and as the sound that connects the physical, mental, and cosmic realms.\\n\\nOm is not just a spoken syllable; it is a vibrational frequency. Chanting Om produces a resonance that affects the body, mind, and environment. The sound begins deep in the abdomen, moves through the chest, and fades at the head, symbolizing the journey from material existence to higher consciousness. This flow mirrors the expansion of the universe from a single point to infinite space.\\n\\nScientific Parallels to the Sound Om\\n\\nWhile Om originates from spiritual traditions, its concept has surprising parallels in modern science. The Big Bang theory suggests that the universe began from a singular event, releasing immense energy and creating space, time, and matter. This event can be metaphorically compared to a cosmic sound or vibration, similar to Om.\\n\\nNASA has recorded sounds from space by converting electromagnetic waves into audible frequencies. These recordings reveal deep, humming, rhythmic sounds produced by planets, stars, and interstellar plasma. For example, the low-frequency vibrations emitted by the sun and the resonant waves of Saturn resemble continuous cosmic hums. Many researchers describe the universe as having a background “cosmic noise,” which can be symbolically related to the eternal sound Om.\\n\\nCosmics and Consciousness\\n\\nAnother fascinating connection between cosmics and Om lies in consciousness. Human brains operate through electrical signals and frequencies. Studies show that meditation and Om chanting can synchronize brain waves, especially alpha and theta waves, which are associated with calmness, focus, and creativity. This suggests that human consciousness may resonate with universal cosmic frequencies.\\n\\nSome scientists and philosophers believe that consciousness itself may be a fundamental property of the universe, influenced by cosmic energy. If cosmics permeate all space and matter, then human awareness may be indirectly shaped by these universal forces. Om, in this context, becomes a bridge between individual consciousness and cosmic consciousness.\\n\\nSymbolic Unity of Science and Spirituality\\n\\nThe relationship between cosmics and Om highlights a deeper unity between science and spirituality. Science explains how the universe functions through equations and observations, while spirituality explores why it exists and how humans connect to it. Both perspectives point toward a universe that is interconnected, dynamic, and vibrational in nature.\\n\\nCosmics represent the external expression of universal energy, while Om represents its internal, experiential form. One is observed through telescopes and detectors; the other is felt through meditation and awareness. Together, they suggest that the galaxy is not just a physical space but a living system of energy and meaning.\\n\\nConclusion\\n\\nThe presence of cosmics throughout the galaxy reveals that the universe is filled with powerful energies and constant motion. From cosmic rays to gravitational waves, these forces shape the structure of galaxies and influence life itself. Ancient wisdom, through the concept of Om, described this same universe as a manifestation of primordial vibration. Though expressed differently, both science and spirituality point toward the same truth: the cosmos is fundamentally vibrational.\\n\\nOm symbolizes the eternal sound of existence, while cosmics represent its measurable form in space. Understanding their relationship helps bridge the gap between modern astrophysics and ancient philosophy. In this unity, humanity finds not only knowledge of the galaxy but also a deeper connection to the universe and to itself.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Text File Loader (Recursive Splitter)\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"cosmics.txt\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88184546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cosmics: Their Presence in the Galaxy and Their Relation with the Sound “Om”\\n\\nThe universe is vast, mysterious, and filled with forces that go far beyond what human senses can directly perceive. From ancient civilizations to modern astrophysics, humans have tried to understand the nature of the cosmos and its underlying energy. The term cosmics broadly refers to cosmic entities, energies, radiations, and phenomena that exist throughout the galaxy. These cosmics play a crucial role in shaping stars, planets, life, and even human consciousness. Interestingly, ancient spiritual traditions, especially from India, describe the universe as originating from a primordial sound known as Om. This essay explores the presence of cosmics in the galaxy and their deep symbolic and scientific connection with the sound Om.\\n\\nUnderstanding Cosmics and Cosmic Presence\\n\\nCosmics can be understood as universal forces and energies present throughout space. In scientific terms, this includes cosmic radiation, electromagnetic waves, dark matter, dark energy, gravitational fields, and subatomic particles traveling across the universe. Cosmic rays, for example, are high-energy particles that originate from supernova explosions, black holes, and distant galaxies. These particles constantly interact with Earth and all living beings, even though we are not consciously aware of them.\\n\\nThe galaxy is not empty space; it is a dynamic system filled with motion, vibration, and energy. Stars are born from clouds of cosmic dust, galaxies collide and merge, and black holes bend space and time. All these processes show that the cosmos is alive with activity. Cosmics act as the invisible framework that maintains balance and order in the universe. Without these forces, galaxies would not form, stars would not shine, and life would not exist.\\n\\nThe Universe as Vibration and Energy\\n\\nModern physics tells us that everything in the universe is made of energy. According to quantum mechanics, even solid matter is composed of vibrating particles. At the most fundamental level, reality behaves like waves rather than fixed objects. This idea strongly aligns with ancient philosophies that described the universe as sound or vibration.\\n\\nAstrophysical observations show that space itself vibrates. Planets produce electromagnetic frequencies, stars emit harmonic oscillations, and even black holes generate gravitational waves. These vibrations form a cosmic symphony, where each celestial object contributes its own frequency. Cosmics, therefore, are not just particles or forces; they are expressions of universal vibration.\\n\\nThe Concept of Om in Ancient Wisdom\\n\\nIn ancient Indian philosophy, Om (also written as AUM) is considered the primordial sound of the universe. It is believed to be the first vibration from which all creation emerged. The Mandukya Upanishad describes Om as the essence of past, present, and future, and as the sound that connects the physical, mental, and cosmic realms.\\n\\nOm is not just a spoken syllable; it is a vibrational frequency. Chanting Om produces a resonance that affects the body, mind, and environment. The sound begins deep in the abdomen, moves through the chest, and fades at the head, symbolizing the journey from material existence to higher consciousness. This flow mirrors the expansion of the universe from a single point to infinite space.\\n\\nScientific Parallels to the Sound Om\\n\\nWhile Om originates from spiritual traditions, its concept has surprising parallels in modern science. The Big Bang theory suggests that the universe began from a singular event, releasing immense energy and creating space, time, and matter. This event can be metaphorically compared to a cosmic sound or vibration, similar to Om.\\n\\nNASA has recorded sounds from space by converting electromagnetic waves into audible frequencies. These recordings reveal deep, humming, rhythmic sounds produced by planets, stars, and interstellar plasma. For example, the low-frequency vibrations emitted by the sun and the resonant waves of Saturn resemble continuous cosmic hums. Many researchers describe the universe as having a background “cosmic noise,” which can be symbolically related to the eternal sound Om.\\n\\nCosmics and Consciousness\\n\\nAnother fascinating connection between cosmics and Om lies in consciousness. Human brains operate through electrical signals and frequencies. Studies show that meditation and Om chanting can synchronize brain waves, especially alpha and theta waves, which are associated with calmness, focus, and creativity. This suggests that human consciousness may resonate with universal cosmic frequencies.\\n\\nSome scientists and philosophers believe that consciousness itself may be a fundamental property of the universe, influenced by cosmic energy. If cosmics permeate all space and matter, then human awareness may be indirectly shaped by these universal forces. Om, in this context, becomes a bridge between individual consciousness and cosmic consciousness.\\n\\nSymbolic Unity of Science and Spirituality\\n\\nThe relationship between cosmics and Om highlights a deeper unity between science and spirituality. Science explains how the universe functions through equations and observations, while spirituality explores why it exists and how humans connect to it. Both perspectives point toward a universe that is interconnected, dynamic, and vibrational in nature.\\n\\nCosmics represent the external expression of universal energy, while Om represents its internal, experiential form. One is observed through telescopes and detectors; the other is felt through meditation and awareness. Together, they suggest that the galaxy is not just a physical space but a living system of energy and meaning.\\n\\nConclusion\\n\\nThe presence of cosmics throughout the galaxy reveals that the universe is filled with powerful energies and constant motion. From cosmic rays to gravitational waves, these forces shape the structure of galaxies and influence life itself. Ancient wisdom, through the concept of Om, described this same universe as a manifestation of primordial vibration. Though expressed differently, both science and spirituality point toward the same truth: the cosmos is fundamentally vibrational.\\n\\nOm symbolizes the eternal sound of existence, while cosmics represent its measurable form in space. Understanding their relationship helps bridge the gap between modern astrophysics and ancient philosophy. In this unity, humanity finds not only knowledge of the galaxy but also a deeper connection to the universe and to itself.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the text file directly\n",
    "speech = \"\"\n",
    "with open(\"cosmics.txt\", \"r\") as file:\n",
    "    speech = file.read()\n",
    "\n",
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6414eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "text = text_splitter.split_text(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef9a2d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosmics: Their Presence in the Galaxy and Their Relation with the Sound “Om”\n",
      "The universe is vast, mysterious, and filled with forces that go far beyond what human senses can\n"
     ]
    }
   ],
   "source": [
    "print(text[0])\n",
    "print(text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4023f7e7",
   "metadata": {},
   "source": [
    "### **2. Using Character-Character Text Splitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71d43961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'cosmics.txt'}, page_content='Cosmics: Their Presence in the Galaxy and Their Relation with the Sound “Om”\\n\\nThe universe is vast, mysterious, and filled with forces that go far beyond what human senses can directly perceive. From ancient civilizations to modern astrophysics, humans have tried to understand the nature of the cosmos and its underlying energy. The term cosmics broadly refers to cosmic entities, energies, radiations, and phenomena that exist throughout the galaxy. These cosmics play a crucial role in shaping stars, planets, life, and even human consciousness. Interestingly, ancient spiritual traditions, especially from India, describe the universe as originating from a primordial sound known as Om. This essay explores the presence of cosmics in the galaxy and their deep symbolic and scientific connection with the sound Om.\\n\\nUnderstanding Cosmics and Cosmic Presence\\n\\nCosmics can be understood as universal forces and energies present throughout space. In scientific terms, this includes cosmic radiation, electromagnetic waves, dark matter, dark energy, gravitational fields, and subatomic particles traveling across the universe. Cosmic rays, for example, are high-energy particles that originate from supernova explosions, black holes, and distant galaxies. These particles constantly interact with Earth and all living beings, even though we are not consciously aware of them.\\n\\nThe galaxy is not empty space; it is a dynamic system filled with motion, vibration, and energy. Stars are born from clouds of cosmic dust, galaxies collide and merge, and black holes bend space and time. All these processes show that the cosmos is alive with activity. Cosmics act as the invisible framework that maintains balance and order in the universe. Without these forces, galaxies would not form, stars would not shine, and life would not exist.\\n\\nThe Universe as Vibration and Energy\\n\\nModern physics tells us that everything in the universe is made of energy. According to quantum mechanics, even solid matter is composed of vibrating particles. At the most fundamental level, reality behaves like waves rather than fixed objects. This idea strongly aligns with ancient philosophies that described the universe as sound or vibration.\\n\\nAstrophysical observations show that space itself vibrates. Planets produce electromagnetic frequencies, stars emit harmonic oscillations, and even black holes generate gravitational waves. These vibrations form a cosmic symphony, where each celestial object contributes its own frequency. Cosmics, therefore, are not just particles or forces; they are expressions of universal vibration.\\n\\nThe Concept of Om in Ancient Wisdom\\n\\nIn ancient Indian philosophy, Om (also written as AUM) is considered the primordial sound of the universe. It is believed to be the first vibration from which all creation emerged. The Mandukya Upanishad describes Om as the essence of past, present, and future, and as the sound that connects the physical, mental, and cosmic realms.\\n\\nOm is not just a spoken syllable; it is a vibrational frequency. Chanting Om produces a resonance that affects the body, mind, and environment. The sound begins deep in the abdomen, moves through the chest, and fades at the head, symbolizing the journey from material existence to higher consciousness. This flow mirrors the expansion of the universe from a single point to infinite space.\\n\\nScientific Parallels to the Sound Om\\n\\nWhile Om originates from spiritual traditions, its concept has surprising parallels in modern science. The Big Bang theory suggests that the universe began from a singular event, releasing immense energy and creating space, time, and matter. This event can be metaphorically compared to a cosmic sound or vibration, similar to Om.\\n\\nNASA has recorded sounds from space by converting electromagnetic waves into audible frequencies. These recordings reveal deep, humming, rhythmic sounds produced by planets, stars, and interstellar plasma. For example, the low-frequency vibrations emitted by the sun and the resonant waves of Saturn resemble continuous cosmic hums. Many researchers describe the universe as having a background “cosmic noise,” which can be symbolically related to the eternal sound Om.\\n\\nCosmics and Consciousness\\n\\nAnother fascinating connection between cosmics and Om lies in consciousness. Human brains operate through electrical signals and frequencies. Studies show that meditation and Om chanting can synchronize brain waves, especially alpha and theta waves, which are associated with calmness, focus, and creativity. This suggests that human consciousness may resonate with universal cosmic frequencies.\\n\\nSome scientists and philosophers believe that consciousness itself may be a fundamental property of the universe, influenced by cosmic energy. If cosmics permeate all space and matter, then human awareness may be indirectly shaped by these universal forces. Om, in this context, becomes a bridge between individual consciousness and cosmic consciousness.\\n\\nSymbolic Unity of Science and Spirituality\\n\\nThe relationship between cosmics and Om highlights a deeper unity between science and spirituality. Science explains how the universe functions through equations and observations, while spirituality explores why it exists and how humans connect to it. Both perspectives point toward a universe that is interconnected, dynamic, and vibrational in nature.\\n\\nCosmics represent the external expression of universal energy, while Om represents its internal, experiential form. One is observed through telescopes and detectors; the other is felt through meditation and awareness. Together, they suggest that the galaxy is not just a physical space but a living system of energy and meaning.\\n\\nConclusion\\n\\nThe presence of cosmics throughout the galaxy reveals that the universe is filled with powerful energies and constant motion. From cosmic rays to gravitational waves, these forces shape the structure of galaxies and influence life itself. Ancient wisdom, through the concept of Om, described this same universe as a manifestation of primordial vibration. Though expressed differently, both science and spirituality point toward the same truth: the cosmos is fundamentally vibrational.\\n\\nOm symbolizes the eternal sound of existence, while cosmics represent its measurable form in space. Understanding their relationship helps bridge the gap between modern astrophysics and ancient philosophy. In this unity, humanity finds not only knowledge of the galaxy but also a deeper connection to the universe and to itself.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"cosmics.txt\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24c5e615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 739, which is longer than the specified 100\n",
      "Created a chunk of size 512, which is longer than the specified 100\n",
      "Created a chunk of size 454, which is longer than the specified 100\n",
      "Created a chunk of size 348, which is longer than the specified 100\n",
      "Created a chunk of size 388, which is longer than the specified 100\n",
      "Created a chunk of size 333, which is longer than the specified 100\n",
      "Created a chunk of size 389, which is longer than the specified 100\n",
      "Created a chunk of size 330, which is longer than the specified 100\n",
      "Created a chunk of size 469, which is longer than the specified 100\n",
      "Created a chunk of size 395, which is longer than the specified 100\n",
      "Created a chunk of size 354, which is longer than the specified 100\n",
      "Created a chunk of size 353, which is longer than the specified 100\n",
      "Created a chunk of size 328, which is longer than the specified 100\n",
      "Created a chunk of size 484, which is longer than the specified 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'cosmics.txt'}, page_content='Cosmics: Their Presence in the Galaxy and Their Relation with the Sound “Om”'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='The universe is vast, mysterious, and filled with forces that go far beyond what human senses can directly perceive. From ancient civilizations to modern astrophysics, humans have tried to understand the nature of the cosmos and its underlying energy. The term cosmics broadly refers to cosmic entities, energies, radiations, and phenomena that exist throughout the galaxy. These cosmics play a crucial role in shaping stars, planets, life, and even human consciousness. Interestingly, ancient spiritual traditions, especially from India, describe the universe as originating from a primordial sound known as Om. This essay explores the presence of cosmics in the galaxy and their deep symbolic and scientific connection with the sound Om.'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='Understanding Cosmics and Cosmic Presence'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='Cosmics can be understood as universal forces and energies present throughout space. In scientific terms, this includes cosmic radiation, electromagnetic waves, dark matter, dark energy, gravitational fields, and subatomic particles traveling across the universe. Cosmic rays, for example, are high-energy particles that originate from supernova explosions, black holes, and distant galaxies. These particles constantly interact with Earth and all living beings, even though we are not consciously aware of them.'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='The galaxy is not empty space; it is a dynamic system filled with motion, vibration, and energy. Stars are born from clouds of cosmic dust, galaxies collide and merge, and black holes bend space and time. All these processes show that the cosmos is alive with activity. Cosmics act as the invisible framework that maintains balance and order in the universe. Without these forces, galaxies would not form, stars would not shine, and life would not exist.'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='The Universe as Vibration and Energy'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='Modern physics tells us that everything in the universe is made of energy. According to quantum mechanics, even solid matter is composed of vibrating particles. At the most fundamental level, reality behaves like waves rather than fixed objects. This idea strongly aligns with ancient philosophies that described the universe as sound or vibration.'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='Astrophysical observations show that space itself vibrates. Planets produce electromagnetic frequencies, stars emit harmonic oscillations, and even black holes generate gravitational waves. These vibrations form a cosmic symphony, where each celestial object contributes its own frequency. Cosmics, therefore, are not just particles or forces; they are expressions of universal vibration.'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='The Concept of Om in Ancient Wisdom'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='In ancient Indian philosophy, Om (also written as AUM) is considered the primordial sound of the universe. It is believed to be the first vibration from which all creation emerged. The Mandukya Upanishad describes Om as the essence of past, present, and future, and as the sound that connects the physical, mental, and cosmic realms.'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='Om is not just a spoken syllable; it is a vibrational frequency. Chanting Om produces a resonance that affects the body, mind, and environment. The sound begins deep in the abdomen, moves through the chest, and fades at the head, symbolizing the journey from material existence to higher consciousness. This flow mirrors the expansion of the universe from a single point to infinite space.'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='Scientific Parallels to the Sound Om'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='While Om originates from spiritual traditions, its concept has surprising parallels in modern science. The Big Bang theory suggests that the universe began from a singular event, releasing immense energy and creating space, time, and matter. This event can be metaphorically compared to a cosmic sound or vibration, similar to Om.'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='NASA has recorded sounds from space by converting electromagnetic waves into audible frequencies. These recordings reveal deep, humming, rhythmic sounds produced by planets, stars, and interstellar plasma. For example, the low-frequency vibrations emitted by the sun and the resonant waves of Saturn resemble continuous cosmic hums. Many researchers describe the universe as having a background “cosmic noise,” which can be symbolically related to the eternal sound Om.'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='Cosmics and Consciousness'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='Another fascinating connection between cosmics and Om lies in consciousness. Human brains operate through electrical signals and frequencies. Studies show that meditation and Om chanting can synchronize brain waves, especially alpha and theta waves, which are associated with calmness, focus, and creativity. This suggests that human consciousness may resonate with universal cosmic frequencies.'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='Some scientists and philosophers believe that consciousness itself may be a fundamental property of the universe, influenced by cosmic energy. If cosmics permeate all space and matter, then human awareness may be indirectly shaped by these universal forces. Om, in this context, becomes a bridge between individual consciousness and cosmic consciousness.'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='Symbolic Unity of Science and Spirituality'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='The relationship between cosmics and Om highlights a deeper unity between science and spirituality. Science explains how the universe functions through equations and observations, while spirituality explores why it exists and how humans connect to it. Both perspectives point toward a universe that is interconnected, dynamic, and vibrational in nature.'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='Cosmics represent the external expression of universal energy, while Om represents its internal, experiential form. One is observed through telescopes and detectors; the other is felt through meditation and awareness. Together, they suggest that the galaxy is not just a physical space but a living system of energy and meaning.'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='Conclusion'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='The presence of cosmics throughout the galaxy reveals that the universe is filled with powerful energies and constant motion. From cosmic rays to gravitational waves, these forces shape the structure of galaxies and influence life itself. Ancient wisdom, through the concept of Om, described this same universe as a manifestation of primordial vibration. Though expressed differently, both science and spirituality point toward the same truth: the cosmos is fundamentally vibrational.'),\n",
       " Document(metadata={'source': 'cosmics.txt'}, page_content='Om symbolizes the eternal sound of existence, while cosmics represent its measurable form in space. Understanding their relationship helps bridge the gap between modern astrophysics and ancient philosophy. In this unity, humanity finds not only knowledge of the galaxy but also a deeper connection to the universe and to itself.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "# This splitter splits text strictly based on a separator\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=100, chunk_overlap=20)\n",
    "char_docs = text_splitter.split_documents(docs)\n",
    "char_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "318490a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 739, which is longer than the specified 100\n",
      "Created a chunk of size 512, which is longer than the specified 100\n",
      "Created a chunk of size 454, which is longer than the specified 100\n",
      "Created a chunk of size 348, which is longer than the specified 100\n",
      "Created a chunk of size 388, which is longer than the specified 100\n",
      "Created a chunk of size 333, which is longer than the specified 100\n",
      "Created a chunk of size 389, which is longer than the specified 100\n",
      "Created a chunk of size 330, which is longer than the specified 100\n",
      "Created a chunk of size 469, which is longer than the specified 100\n",
      "Created a chunk of size 395, which is longer than the specified 100\n",
      "Created a chunk of size 354, which is longer than the specified 100\n",
      "Created a chunk of size 353, which is longer than the specified 100\n",
      "Created a chunk of size 328, which is longer than the specified 100\n",
      "Created a chunk of size 484, which is longer than the specified 100\n"
     ]
    }
   ],
   "source": [
    "# Read raw text again\n",
    "speech = \"\"\n",
    "with open(\"cosmics.txt\", \"r\") as file:\n",
    "    speech = file.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "text = text_splitter.create_documents([speech])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2472514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Cosmics: Their Presence in the Galaxy and Their Relation with the Sound “Om”'\n",
      "page_content='The universe is vast, mysterious, and filled with forces that go far beyond what human senses can directly perceive. From ancient civilizations to modern astrophysics, humans have tried to understand the nature of the cosmos and its underlying energy. The term cosmics broadly refers to cosmic entities, energies, radiations, and phenomena that exist throughout the galaxy. These cosmics play a crucial role in shaping stars, planets, life, and even human consciousness. Interestingly, ancient spiritual traditions, especially from India, describe the universe as originating from a primordial sound known as Om. This essay explores the presence of cosmics in the galaxy and their deep symbolic and scientific connection with the sound Om.'\n"
     ]
    }
   ],
   "source": [
    "print(text[0])\n",
    "print(text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aefe27",
   "metadata": {},
   "source": [
    "**1. RecursiveCharacterTextSplitter**\n",
    "- Tries to split text intelligently\n",
    "- Follows a hierarchy of separators ==> Paragraphs → Sentences → Words → Characters\n",
    "- Preserves semantic meaning\n",
    "- Avoids breaking sentences randomly\n",
    "\n",
    "\n",
    "**2. CharacterTextSplitter**\n",
    "- Splits text strictly based on a given separator\n",
    "- Does NOT try to understand structure\n",
    "- Simple and fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927573c",
   "metadata": {},
   "source": [
    "### **3. HTML Header Text Splitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ac48930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "# Sample HTML \n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample HTML Document</title>\n",
    "    <h1>Main Header</h1>\n",
    "    <h2>Sub Header 1</h2>\n",
    "    <h3>Sub Sub Header 1</h3>\n",
    "    <h2>Sub Header 2</h2>\n",
    "</head>\n",
    "<body>\n",
    "    <p>This is a sample paragraph in the HTML document.</p>\n",
    "    <p>Another paragraph follows here.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c1077e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is a sample paragraph in the HTML document.  \\nAnother paragraph follows here.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define which HTML headers to split on\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_headers_splits = html_splitter.split_text(html_string)\n",
    "html_headers_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf29bbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'file.html', 'title': 'Introduction to Artificial Intelligence'}, page_content='\\n\\n\\nIntroduction to Artificial Intelligence\\n\\n\\n\\nArtificial Intelligence\\n\\n        Artificial Intelligence (AI) is a branch of computer science that focuses on\\n        creating intelligent machines capable of performing tasks that normally\\n        require human intelligence.\\n    \\n\\n        These tasks include learning, reasoning, problem-solving, perception,\\n        and understanding natural language.\\n    \\n\\nHistory of Artificial Intelligence\\n\\n        The concept of artificial intelligence dates back to ancient times, but\\n        the formal field of AI was founded in 1956 at the Dartmouth Conference.\\n    \\nEarly Development\\n\\n        During the 1950s and 1960s, researchers believed that human-level intelligence\\n        could be achieved within a few decades.\\n    \\nAI Winter\\n\\n        Due to limited computing power and unrealistic expectations, AI research\\n        faced multiple periods of reduced funding known as AI winters.\\n    \\n\\nTypes of Artificial Intelligence\\n\\n        AI systems can be classified based on their capabilities and functionalities.\\n    \\nNarrow AI\\n\\n        Narrow AI is designed to perform a specific task, such as facial recognition\\n        or voice assistants like Siri and Alexa.\\n    \\nGeneral AI\\n\\n        General AI refers to systems that possess human-like intelligence and can\\n        perform any intellectual task that a human can do.\\n    \\nSuper AI\\n\\n        Super AI surpasses human intelligence in all aspects, including creativity,\\n        decision-making, and emotional understanding.\\n    \\n\\nApplications of Artificial Intelligence\\n\\n        AI is widely used across multiple industries to improve efficiency and\\n        decision-making.\\n    \\n\\nHealthcare: Disease diagnosis and drug discovery\\nFinance: Fraud detection and algorithmic trading\\nEducation: Personalized learning systems\\nTransportation: Self-driving cars\\n\\n\\nAdvantages and Challenges\\nAdvantages\\n\\n        AI increases productivity, reduces human error, and enables automation\\n        of repetitive tasks.\\n    \\nChallenges\\n\\n        Ethical concerns, data privacy, job displacement, and bias in AI models\\n        are major challenges that need to be addressed.\\n    \\n\\nConclusion\\n\\n        Artificial Intelligence is transforming the way humans interact with\\n        technology. With responsible development, AI has the potential to greatly\\n        benefit society.\\n    \\n\\n\\n')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import BSHTMLLoader to load local HTML files\n",
    "# pip install lxml\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "loader = BSHTMLLoader(\"file.html\")\n",
    "html_docs = loader.load()\n",
    "html_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fae4247e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Introduction to Artificial Intelligence\\n\\n\\n\\nArtificial Intelligence\\n\\n        Artificial Intelligence (AI) is a branch of computer science that focuses on\\n        creating intelligent machines capable of performing tasks that normally\\n        require human intelligence.\\n    \\n\\n        These tasks include learning, reasoning, problem-solving, perception,\\n        and understanding natural language.\\n    \\n\\nHistory of Artificial Intelligence\\n\\n        The concept of artificial intelligence dates back to ancient times, but\\n        the formal field of AI was founded in 1956 at the Dartmouth Conference.\\n    \\nEarly Development\\n\\n        During the 1950s and 1960s, researchers believed that human-level intelligence\\n        could be achieved within a few decades.\\n    \\nAI Winter\\n\\n        Due to limited computing power and unrealistic expectations, AI research\\n        faced multiple periods of reduced funding known as AI winters.\\n    \\n\\nTypes of Artificial Intelligence\\n\\n        AI systems can be classified based on their capabilities and functionalities.\\n    \\nNarrow AI\\n\\n        Narrow AI is designed to perform a specific task, such as facial recognition\\n        or voice assistants like Siri and Alexa.\\n    \\nGeneral AI\\n\\n        General AI refers to systems that possess human-like intelligence and can\\n        perform any intellectual task that a human can do.\\n    \\nSuper AI\\n\\n        Super AI surpasses human intelligence in all aspects, including creativity,\\n        decision-making, and emotional understanding.\\n    \\n\\nApplications of Artificial Intelligence\\n\\n        AI is widely used across multiple industries to improve efficiency and\\n        decision-making.\\n    \\n\\nHealthcare: Disease diagnosis and drug discovery\\nFinance: Fraud detection and algorithmic trading\\nEducation: Personalized learning systems\\nTransportation: Self-driving cars\\n\\n\\nAdvantages and Challenges\\nAdvantages\\n\\n        AI increases productivity, reduces human error, and enables automation\\n        of repetitive tasks.\\n    \\nChallenges\\n\\n        Ethical concerns, data privacy, job displacement, and bias in AI models\\n        are major challenges that need to be addressed.\\n    \\n\\nConclusion\\n\\n        Artificial Intelligence is transforming the way humans interact with\\n        technology. With responsible development, AI has the potential to greatly\\n        benefit society.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_splits = html_splitter.split_text(html_docs[0].page_content)\n",
    "html_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6db6d5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='End container NOTE: Script required for drop-down button to work (mirrors).  \\nEnd header wrapper End content End footer  \\nEnd header  \\nEnd navigation End search  \\nStanford Encyclopedia of Philosophy  \\nMenu  \\nBrowse  \\nTable of Contents  \\nWhat\\'s New  \\nRandom Entry  \\nChronological  \\nArchives  \\nAbout  \\nEditorial Information  \\nAbout the SEP  \\nEditorial Board  \\nHow to Cite the SEP  \\nSpecial Characters  \\nAdvanced Tools  \\nContact  \\nSupport SEP  \\nSupport the SEP  \\nPDFs for SEP Friends  \\nMake a Donation  \\nSEPIA for Libraries  \\nBegin article sidebar End article sidebar NOTE: Article content must have two wrapper divs: id=\"article\" and id=\"article-content\" End article NOTE: article banner is outside of the id=\"article\" div. End article-banner  \\nEntry Navigation  \\nEntry Contents  \\nBibliography  \\nAcademic Tools  \\nFriends PDF Preview  \\nAuthor and Citation Info  \\nBack to Top  \\nEnd article-content  \\nBEGIN ARTICLE HTML #aueditable DO NOT MODIFY THIS LINE AND BELOW END ARTICLE HTML  \\nDO NOT MODIFY THIS LINE AND ABOVE'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities'}, page_content='Moral Responsibility and the Principle of Alternative Possibilities'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities'}, page_content='First published Thu Jul 9, 2020  \\nSuppose you harm, offend, or otherwise wrong another person.\\nConfronted with the possibility of sanction, you might say any of the\\nfollowing in an attempt avoid blame: “I couldn’t help\\nit.” “Someone made me do it.” “I had no\\nchoice.” “It was unavoidable.” “There was no\\nother option.” There’s a natural reading of such defenses\\non which they appeal to the principle at the center of this entry, the\\n“Principle of Alternative Possibilities” (cp. Frankfurt\\n1969):  \\n: a\\nperson is morally responsible for what she has done only if she could\\nhave done otherwise.  \\nPrinciple of Alternative Possibilities (PAP)  \\nAlthough its precise form and interpretation have varied, this\\nprinciple has enjoyed broad support in the history of philosophy. PAP\\nwas a standard—even if not universal—presupposition of\\nGreek, medieval, and early modern thought (Irwin 1999: 225; Pasnau\\n2003: 226; Rowe 1987: 43). And until about fifty years ago, it was\\nusually taken for granted by both sides in debates on whether moral\\nresponsibility is compatible with determinism.  \\nNo doubt the principle’s appeal can in part be traced to\\nordinary moral practice. One day at the cafeteria, Kurt steals\\nJohn’s lunch. Under normal circumstances, we hold Kurt\\nresponsible for his act. But now add that he had to act as he did.\\nSuppose, for example, that Kurt was coerced by a bully to steal\\nJohn’s lunch; or he is suffering from a neurological disorder\\ncompelling him to act; or he was brainwashed. These are some of the\\nmany ways in which his alternatives can be closed off. But however\\nthis happens, once the alternatives are gone—once Kurt must act\\nas he does—blaming him no longer seems appropriate.  \\nPAP presents one requirement for moral responsibility. There are\\nothers. For example, we often excuse those who act out of ignorance.\\nSuppose it turns out that, contrary to initial reports, Kurt\\nreasonably mistook John’s lunch for his own; it was this\\ninnocent mistake that resulted in the “theft”. Here too we\\nshould be much less inclined to blame Kurt, for in the relevant sense,\\nhe didn’t know what he was doing. Such examples suggest that in\\naddition to PAP, there is an epistemic condition for responsibility.\\nAnd there are still other candidate requirements. But while some of\\nthem are closely related to the ability to do otherwise, for the most\\npart they will, like the epistemic condition, be set to one side in\\nthis entry: the focus will be on PAP.  \\nEntry Contents Entry Contents  \\n1. Background  \\n1.1 The Importance of PAP  \\n1.2 What is Moral Responsibility?  \\n1.3 What is the Ability to Do Otherwise?  \\n1.4 A Note on Freedom  \\n2. Arguments for PAP  \\n2.1 Is the Principle Self-Evident?  \\n2.2 Ordinary Judgments about Responsibility  \\n2.3 The Nature of Agency  \\n2.4 Moral Arguments  \\n3. Arguments against PAP  \\n3.1 The Irrelevance of Determinism  \\n3.2 Compelled but Willing Action  \\n3.3 Volitional Necessity  \\n3.4 Inevitability without Causation  \\n3.4.1 Divine Foreknowledge  \\n3.4.2 Locke’s Content Prisoner  \\n3.4.3 Frankfurt-Style Cases  \\n4. Objections to Frankfurt-Style Cases  \\n4.1 The Cases are Too Unusual  \\n4.2 Frankfurt Agents Could Have Done Otherwise  \\n4.2.1 General Abilities  \\n4.2.2 Flickers of Freedom  \\n4.3 A Dilemma for Frankfurt-Style Cases  \\n4.3.1 The Dilemma Stated  \\n4.3.2 Responses to the Dilemma  \\n5. Beyond PAP  \\n5.1 Compatibilism Re-energized?  \\n5.2 Incompatibilism without PAP  \\n5.3 Replacement Principles  \\n5.3.1 Responsibility for Actions  \\n5.3.2 Responsibility for Consequences  \\n5.3.3 Responsibility for Omissions  \\nBibliography  \\nAcademic Tools  \\nOther Internet Resources  \\nRelated Entries'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '1. Background'}, page_content='1. Background'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '1. Background'}, page_content='Like most important philosophical claims, PAP is more complex than it\\nfirst appears. It’s useful for now to suppress many of these\\ncomplexities. But a few of them should be mentioned at the outset, if\\nonly to make some simplifying assumptions and bracket certain\\ncontroversies. We begin, however, with some reasons to think this\\ntopic is worth the effort.'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '1. Background', 'Header 3': '1.1 The Importance of PAP'}, page_content='1.1 The Importance of PAP'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '1. Background', 'Header 3': '1.1 The Importance of PAP'}, page_content='We are both moral and embodied agents, persons and organisms. We move\\nin the space of reasons, answerable to rules of morality and\\nrationality, as well as a space of causes, governed by the laws of\\nnature. How are these two aspects of our lives linked? This question\\nis too big to answer here. But PAP, if true, is one piece of the\\npuzzle. Our status as persons is enabled by a kind of power we\\nexercise in the world of causes, namely, the power to choose among\\nalternative courses of action. In this way, our principle links a\\nnormative feature of our lives to a causal feature.  \\nBut with this abstract promise comes a concrete threat. The principle\\ntells us that when alternatives are blocked, when only one course of\\naction is available, you are not responsible for what you do. As noted\\nearlier, there are some extreme conditions, such as coercion, that can\\npreclude responsibility by blocking alternatives. But suppose it turns\\nout that even under standard conditions, no one has alternatives.\\nGiven PAP, no one is morally responsible for anything. Universal\\nalternative-blockers have frequently appeared in the philosophical\\nliterature. They include bivalence, divine foreknowledge, divine\\nsovereignty, mechanism, psychological determinism, and causal\\ndeterminism. If, due to any of these, none of us can act otherwise\\nthan we actually act, then PAP will join in to produce a disturbing\\nconclusion: we are never responsible for what we do. A less global but\\nequally worrisome threat is that our principle will combine with some\\nthesis entailing the widespread reduction of abilities, even if not\\ntheir elimination. For example, situational, genetic, or\\nsocio-economic influences, while not universally blocking\\nalternatives, could so severely limit them in particular cases that,\\nwith PAP in the background, moral praise and blame are no longer\\nappropriate. (On the threat of “situationism”, see Nelkin\\n2005; McKenna & Warmke 2017.)  \\nAnother reason PAP is important is that it intersects with the\\nphilosophy and practice of law. Even if moral and legal responsibility\\ndo not coincide, there is a region of overlap (Duff 2009; Brink &\\nNelkin 2013) where PAP and its attendant literature can inform legal\\nreasoning, especially when it aims at the just punishment of\\nwrongdoers. A necessary condition of criminal responsibility is that\\nthe conduct included a voluntary act (Sinnott-Armstrong 2012), one\\nthat the accused had a “reasonable opportunity to avoid”\\n(Kelly 2017). If Mark was sleepwalking or hypnotized when he broke\\ninto Ken’s house, this could be excused, both morally and\\nlegally, on the grounds that in his condition, Mark could not have\\ndone otherwise. If, due to delusion, an accused murderer could not\\navoid forming the intention to kill his victim—and acting\\naccordingly—this also could be exculpating.  \\nThat said, standards applied by the law may not match what in moral\\ncontexts we think of as avoidable action. Imagine that a mild\\nprovocation, one that a reasonable person could ignore, compels an\\nemotionally disturbed man to murder. Criminal law evaluates his\\nabilities according to the reasonable person standard—thereby\\nconvicting him of murder rather than manslaughter—even if moral\\nreasoning would take into account the emotional state severely\\nrestricting his alternatives (Kelly 2017). The relation between PAP\\nand the law is further complicated by the fact that criminal law does\\nnot seem especially concerned with causal determinism (Morse 2013), a\\nthesis widely thought to limit us to exactly one future. That said, if\\nthe irrelevance of determinism in the law reflects some degree of\\nindifference toward alternatives in assigning responsibility, this\\ncould be part of a case against PAP in moral matters as well (see also ).  \\n§3.1'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '1. Background', 'Header 3': '1.2 What is Moral Responsibility?'}, page_content='1.2 What is Moral Responsibility?'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '1. Background', 'Header 3': '1.2 What is Moral Responsibility?'}, page_content='Start with what a person is morally responsible . As\\nformulated above, PAP is about responsibility for what a person\\n“has done”. For most of this entry this will be\\ninterpreted to mean her actions (see entry on ),\\n here construed broadly to include both overt actions, such as walking\\nto the store or shooting a gun, and mental actions, such as choosing\\nto go to law school or calculating a move in chess. In addition to\\nbeing responsible for actions, a person can be responsible for the\\nconsequences of her actions. By putting salt in Sean’s tea,\\nMeghan is responsible, not just for ruining Sean’s tea (what she\\ndid), but for his tea’s being ruined (the consequence of what\\nshe did). A politician’s opponents may claim she is responsible\\nfor an economic downturn, a result of her decisions and policies.\\nFinally, a person can be responsible for failures to act, sometimes\\nknown as refrainings or omissions, and the consequences of such. Luke\\nthe indifferent lifeguard is accountable for not saving the drowning\\nswimmer, and thus for the death as well. PAP has a different cast\\ndepending on whether responsibility for actions, consequences, or\\nomissions is at issue. The focus for most of this entry is on actions,\\nbut responsibility for consequences and omissions will appear again in .  \\nfor  \\naction  \\n§5.3  \\nWhat is it to be morally responsible? This entry adopts a broadly\\n“Strawsonian” view in the tradition of P.F.\\nStrawson’s “Freedom and Resentment” (P. F. Strawson\\n1962; cp. Fischer & Ravizza 1998, 6–8). To be morally\\nresponsible is to be the proper object of the “reactive\\nattitudes,” such as respect, praise, forgiveness, blame,\\nindignation, and the like. If Marija is responsible for adopting a\\nstray cat, it’s proper to praise her for this act. If it turns\\nout Meghan is not responsible for ruining Sean’s tea, then this\\nmeans it’s not appropriate, for Sean or anyone else, to blame or\\nresent her for what she did. There are other theories of moral\\nresponsibility (see entry on ).\\n What our principle looks like under these alternative accounts will\\nremain open, though one can expect that problems similar to those\\nconsidered below will arise on any but the most radically revisionary\\nways of thinking of responsibility.  \\nmoral responsibility  \\nOne final note on responsibility: In the literature one sometimes\\nfinds the notion of or moral\\nresponsibility (Klein 1990; G. Strawson 1994; Kane 1996). It’s\\nnot always clear how this is related to what PAP calls, simply, moral\\nresponsibility (M. Bernstein 2005; Boxer 2013). On one reading,\\nultimacy is an additional condition on ordinary responsibility to be\\nlisted alongside epistemic and other requirements alluded to earlier.\\nOn the other hand, if talk of ultimacy is intended to signal a higher\\ngrade of responsibility, there will be a corresponding version of our\\nprinciple to be evaluated on its own terms:  \\ntrue  \\nultimate  \\n: a person is ultimately morally\\nresponsible for what she has done only if she could have done\\notherwise.  \\nPAP-ultimate  \\nThis enhanced version, if it’s distinct from PAP, will not\\nappear in what follows. But the discussion below could be relevant to\\nPAP-ultimate. In particular, it’s likely that PAP entails\\nPAP-ultimate, so that arguments considered below for the former\\n ( )\\n could be brought in favor of the latter. What’s less clear is\\nwhether PAP-ultimate entails PAP. If it doesn’t, then objections\\nto the latter\\n ( )\\n needn’t damage the former.  \\n§2  \\n§3'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '1. Background', 'Header 3': '1.3 What is the Ability to Do Otherwise?'}, page_content='1.3 What is the Ability to Do Otherwise?'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '1. Background', 'Header 3': '1.3 What is the Ability to Do Otherwise?'}, page_content='When a person, , does something, what is it for her to have\\nthe ability to do otherwise? For help we might look to alternative\\nlocutions: do otherwise; has the to do otherwise; this is ; the world in which does otherwise is ; doing so is ; this is to . But without further elaboration, these\\naren’t much more than synonyms for what’s to be explained.\\nNo theory of ability is adopted here, and for the most part we will\\nrely on judgments about ability in particular cases. But a few general\\nissues are worth flagging. (See also and the entry on .)  \\nS  \\nS  \\ncan  \\nS  \\npower  \\npossible for  \\nS  \\nS  \\naccessible\\nto  \\nS  \\nup to  \\nS  \\nopen  \\nS  \\n§4.2.1  \\nabilities  \\n’s being able to do otherwise is distinct from its\\nmerely being possible that do otherwise. Once Lisa the\\nskydiver jumps out her plane, it is (logically) possible that she\\nfloat in midair rather than fall. But it’s not in her power to\\ndo so: the world in which Lisa floats is not accessible to her. And if\\nKurt is brainwashed to steal John’s lunch, then while it’s\\npossible that he refrain, he nevertheless can’t. Something must\\nbe added to ’s possibly doing something else to ensure\\nit’s within ’s power to do so.  \\nS  \\nS  \\nS  \\nS  \\nAs Hume (1748 [1999: 158]) and others have argued, adding that it to as if there are alternatives doesn’t\\nseem to help here (Chisholm 1967; Ginet 1990, 90–1). While the\\nphenomenology of having alternatives—and more generally, of\\nagency—is an important topic in its own right (Nagel 1986: ch.\\n7; Horgan, Tienson, & Graham 2003), it does not secure the ability\\nin question. Fred feels he can overcome the temptation to smoke, then\\ndecides to smoke anyway. But it turns out that he could not have\\nresisted his craving. Nor is seeming to have alternatives necessary\\nfor having them. Fred’s sister Fanny feels she is too weak to\\novercome the temptation at noon to smoke, and in fact she does smoke\\nat that time. But she was wrong about her own capacities: with just a\\nbit of effort, she would have been able to resist.  \\nseems  \\nS  \\nWhat these agents can or cannot do is relative to a set of facts, not\\nall of which need be conscious. In the cases of Fred and Fanny, the\\nfacts concern, among other things, the strengths of their cravings and\\ntheir respective degrees of will-power. These are held fixed to\\nevaluate their abilities. But what other facts are salient? There is\\nno consensus on this issue, though it does seem that which facts are\\nheld fixed can vary depending on context (Lewis 1976; Horgan 1979;\\nUnger 1984). Such context-sensitivity is relevant to the proper\\napplication of PAP. Keyne is confronted by a “your money or your\\nlife” mugger, and hands over his wallet. According to PAP,\\nwhether Keyne is responsible for doing so turns, in part, on whether\\nhe could have refused. So could he have? Considering the example in\\none sort of context, we will say Yes: his arm was under his control\\nand he was fully aware of what noncompliance involved and how to\\nachieve it. But in another sort of context we might say No: Keyne was\\nrational and valued his life far more than the money in his wallet, as\\nwould any reasonable person such circumstances; fix this cognitive\\nbackground, and the mugger’s demand left him no other option. So\\nin handing over his wallet, has Keyne met PAP’s requirement on\\nresponsibility or not? There seems to be no clear answer to this\\nquestion. Complicating matters still further is that ability—and\\nin particular, the ability to do otherwise—varies along a\\ncontinuum. The motive power of a threat, mental disorder, or hypnotic\\nsuggestion varies from mild to overwhelming. Where along this\\ncontinuum does its influence becomes so strong as to preclude\\nalternatives? There is no non-arbitrary line to draw here—nor,\\napparently, has science settled on one (Graham 2013: 186, citing Morse\\n2011).  \\nThese points can make it difficult to test or apply PAP in particular\\ncases. However, they do not by themselves entail that PAP is false, so\\nlong as judgments of moral responsibility show a similar\\ncontext-sensitivity and gradation. (On degrees of responsibility, see\\nZimmerman 1988: ch. 3; Coates & Swenson 2013; Nelkin 2016.)'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '1. Background', 'Header 3': '1.4 A Note on Freedom'}, page_content='1.4 A Note on Freedom'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '1. Background', 'Header 3': '1.4 A Note on Freedom'}, page_content='A final preliminary point concerns acting . A principle\\nclosely related to PAP is the following (Audi 1974; van Inwagen 1983:\\n161; Widerker & McKenna 2003: 2):  \\nfreely  \\na person is morally responsible\\nfor what she has done only if she did it freely.  \\nFreedom Condition:  \\nThis principle raises many of the issues that will come up below. But\\nit’s stronger than PAP if, as is plausible, freedom requires\\nmore than alternatives (see entry on ).\\n Being the source of one’s action may be one such requirement;\\nhaving properly functioning faculties could be another; agent-causing\\none’s action may be yet another. Investigating the Freedom\\nCondition would thus bring in additional topics that cannot be\\nexplored here. So the discussion to follow will skirt the problems of\\nfree action (and free will) and instead focus on PAP’s weaker\\nrequirement. But at least this much is worth noting: if PAP is false,\\nthen so is the Freedom Condition, assuming that one must have\\nalternatives to act freely (see also Warfield 2007; McKenna 2008:\\n353).  \\nfree will'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '2. Arguments for PAP'}, page_content='2. Arguments for PAP'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '2. Arguments for PAP'}, page_content='Much of the literature on PAP consists of arguments against it,\\nfollowed by replies to those arguments, and so on. Perhaps this\\npattern is due to the strong plausibility the\\nprinciple enjoys: it should be accepted unless there are good reasons\\nnot to. That said, there are a few positive arguments to advance in\\nfavor of PAP; it’s best to begin with those before turning to\\nobjections in .  \\nprima facie  \\n§3'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '2. Arguments for PAP', 'Header 3': '2.1 Is the Principle Self-Evident?'}, page_content='2.1 Is the Principle Self-Evident?'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '2. Arguments for PAP', 'Header 3': '2.1 Is the Principle Self-Evident?'}, page_content='One quick argument is that PAP is self-evident; alternatively, it is\\nimmediately known , intuitively obvious, or analytic.\\nThose who deny it are thereby convicted of confusion about moral\\nresponsibility, the ability to do otherwise, or both. If PAP does\\nenjoy such a high epistemic status, this explains its wide acceptance\\nin the major historical periods of philosophy, a discipline not\\nusually known for producing even approximate consensus. But in spite\\nof this agreement, the principle has turned out not to be so obvious,\\nand in fact appears to make a substantive, controversial claim about\\nresponsibility. Witness some notable dissenters, including the Stoics,\\nAugustine, Aquinas, and Hobbes (see, respectively, ;\\n Hunt 1999; Stump 1997; Martinich 2005). Moreover, PAP has come under\\nsustained attack in recent decades due in large part to the early\\nefforts of Frankfurt (1969), Fischer (1982), and Dennett (1984a,b).\\nThis is enough to show that, however plausible PAP might first appear,\\nit requires more than this quick defense.  \\na priori  \\n§3.2'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '2. Arguments for PAP', 'Header 3': '2.2 Ordinary Judgments about Responsibility'}, page_content='2.2 Ordinary Judgments about Responsibility'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '2. Arguments for PAP', 'Header 3': '2.2 Ordinary Judgments about Responsibility'}, page_content='PAP is supported by a wide range of cases in which we judge that\\nwithout alternatives, we are not responsible for acting—not\\nresponsible, it seems, we lack alternatives.  \\nbecause  \\nConsider persons with mental disorders such as kleptomania or\\nagoraphobia. When these disorders are considered severe enough to\\ncompel behavior, blame and resentment no longer seem warranted. (It is\\nthe reasoning employed in ordinary judgment that’s relevant\\nhere, but for scientific evidence against the compulsion of some\\nmental disorders, see Schroeder 2005; Vranas 2007; Pickard 2015.)\\nSimilarly, we may be less inclined to condemn a criminal when we\\ndiscover that his illegal acts were the result of a brain tumor\\n(Sinnott-Armstrong 2012), or to praise the generosity of person who is\\ncompelled to give things away (Bennett 2008). Those who, whether\\npathologically or not, act from overwhelming stress, fear, or guilt\\nare not praised or blamed for what they do. And while young children\\nmay have a limited capacity for self-control (Duckworth, Gendler,\\n& Gross 2014), among the many reasons we don’t hold them\\nmorally responsible is that we think they are often powerless to\\nresist their impulses.  \\nSimilar lessons can be drawn from cases of manipulation, in which\\nagents are influenced through physical or psychological means.\\nExamples of the latter include brainwashing, hypnosis, and duress. One\\nmight also include more high-tech examples in which the manipulation\\noccurs via direct control of the victim’s brain. (Such examples\\nare usually science-fiction, but see Krug, Salzman, & Waddell\\n2015.) The extent to which manipulation involves removing, rather than\\nmerely reducing, alternatives depends on the mechanism at work. But\\ninsofar as we take such examples to involve the blocking of\\nalternatives, we take them to rule out responsibility as well.  \\nIn reply, an opponent of PAP may concede that in all such cases, both\\nresponsibility and alternatives are diminished or absent, yet still\\ndeny that we can generalize from these to PAP, for in addition to\\nlacking alternatives, these agents are missing other important moral\\nor psychological capacities. It could be the absence of these, not the\\nlack of alternatives, that explains why these agents aren’t\\nresponsible (cp. Wallace 1994: ch. 5). Put another way: perhaps having\\nalternatives is merely correlated in ordinary circumstances with the\\ntrue requirements for responsibility, whatever those might be. A more\\nrefined test of PAP needs to look at more exotic scenarios in which\\nthese other conditions are present while alternatives are not\\n ( ;\\n see for more on “ordinary circumstances”).  \\n§3.4  \\n§4.1'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '2. Arguments for PAP', 'Header 3': '2.3 The Nature of Agency'}, page_content='2.3 The Nature of Agency'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '2. Arguments for PAP', 'Header 3': '2.3 The Nature of Agency'}, page_content='Among the goals of action theory is to distinguish what we do from\\nwhat merely happens to us (see, e.g., Davidson 1980: ch. 3). Suppose\\nthat on the best way to draw this distinction, action (agency)\\nrequires alternatives. Some event in your life, that is, doesn’t\\ncount as something you unless an alternative was within\\nyour power. For example, when you’re awakened by a noise, this\\nis not an event you could have avoided, and so it was not an action,\\nnot an exercise of your agency. Similarly, events such as stumbling,\\nsweating, and blushing aren’t things you (strictly) do: they\\nhappen to you. To this list we could add mental events such as\\nobsessive thoughts: when such thoughts are, in the relevant sense,\\nunavoidable, they belong to the sufferer only in the sense that the\\nperson is their subject, the arena in which they occur, not their\\nagent. Given this view of action, PAP follows immediately: if action\\nrequires alternatives, then , action for which one\\nis morally responsible requires them.  \\ndid  \\na fortiori  \\nThis line of defense finds a home in some versions of agent-causalism,\\non which, not just free action, but action requires agent causation (Taylor 1966; Steward 2012b; Brent 2017). On\\nthis picture, the difference between what I do and what merely happens\\nto me is that I am the (or a) cause of the former, not the latter. Now\\nadd a second, logically independent premise that agent causation\\nrequires alternatives. If someone is necessitated, either internally\\nor externally, it is the compelling factor, not the agent, that causes\\nthe resulting event, which thus is not an action. Not all agent\\ncausalists endorse both premises (Clarke 1993 and O’Connor 2000\\ndeny that action simpliciter requires agent causation; Taylor 1966 and\\nMarkosian 2012 deny that agent causation requires alternatives). But\\njoining the premises entails that action—and thus morally\\nresponsible action—requires alternatives.  \\nsimpliciter  \\nThis approach to defending PAP will have to say something about\\napparent actions that do not involve alternatives. When an addict\\ntakes a drug due to an irresistible urge to do so, it seems this is\\nsomething the addict does, even if the addict can’t resist. (For\\na reply to such examples, see Alvarez 2013.) Similarly, the view\\nentails that if causal determinism or some other universal block of\\nalternatives is in place, no one ever acts; whether this is an\\nacceptable consequence is disputed (Shabo 2011; Steward 2012a).'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '2. Arguments for PAP', 'Header 3': '2.4 Moral Arguments'}, page_content='2.4 Moral Arguments'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '2. Arguments for PAP', 'Header 3': '2.4 Moral Arguments'}, page_content='A broader concern about the previous defense is that it grounds our\\nprinciple in the metaphysics of agency rather than in the nature of\\nmoral responsibility. Morality, the thought goes, should be more\\ndirectly engaged in defense of PAP, a principle that explicitly\\nconstrains our practices of praise and blame. It’s not\\nsurprising, then, that a number of arguments supporting PAP start from\\nmoral premises.  \\nA straightforward moral argument says it’s not fair to praise or\\nblame a person for doing something that was, for her, unavoidable (for\\ndiscussion see, e.g., Glover 1970: 70–3; Watson 1996; Nelkin\\n2011: ch. 2). A responsible agent is a target of the\\nreactive attitudes, and unfairly or unjustly blaming someone, however\\nexpedient, would not be morally proper. One challenge for this defense\\nof PAP is that fairness is itself entangled with the reactive\\nattitudes (P. F. Strawson 1962; cp. Wallace 1994: 4.3). Thus, those\\nwho wonder whether praise and blame require alternatives are unlikely\\nto be persuaded by the claim that targeting by the\\nreactive attitudes requires alternatives—at least not without\\nfurther argument. Someone questioning PAP, that is, is unlikely to be\\nswayed by a moral appeal that seems only to reassert the\\nprinciple.  \\nproper  \\nfair  \\nAnother line appeals to the principle, often associated with Kant\\n(1785), that “ought” implies “can” (OIC). You\\ncannot be morally obligated to do something that’s impossible\\nfor you. While it would be good for you to eradicate world hunger, you\\nare under no obligation to do so for the simple reason that you\\ncan’t. (You may, however, be obligated to relieve hunger to the\\nextent you can.) Perhaps OIC is not, as Reid thought, “as\\nself-evident as the axioms of mathematics” (Reid 1788: IV.v),\\nbut it is nevertheless very plausible.  \\nNow return to PAP. Suppose Kristin robs a bank. A first try at the\\nargument runs as follows (here we take from doing\\nsomething to be a way doing something else; for a bit more on this,\\nsee ):  \\nrefraining  \\n§4.2.2  \\nP1.  \\nIf Kristin is blameworthy for robbing the bank, then she ought\\nnot to have done so.  \\nP2.  \\nIf Kristin ought not to have robbed the bank, then she could have\\nrefrained from doing so.  \\nC.  \\nTherefore, if Kristin is blameworthy for robbing the bank, then\\nshe could have refrained from doing so.  \\nP1 looks safe, assuming that the moral sense of blame is in play (but\\nsee Capes 2012; Haji 2019). P2 is an instance of OIC. However, the\\nconclusion, even when generalized, stops short of PAP, as it claims\\nmerely that alternatives are necessary for actions for which one is\\nmorally blameworthy; it is silent on actions for which one is morally\\npraiseworthy, and it’s harder to see how OIC could be leveraged\\nto cover such actions (Frankfurt 1983). After all, when you do\\nsomething praiseworthy, you act as you should, so OIC can’t get\\na grip on alternative courses of action. That said, if some version of\\nthe above argument were sound, this would be a major victory for\\nproponents of PAP. (Compare C. A. Campbell 1951: 451; van Inwagen\\n1983: 161; Widerker 1991; Copp 1997; critics include Yaffe 1999, 2005;\\nsee also Widerker 2003; Speak 2005.)  \\nA third moral argument for PAP is closely related to the first two. It\\nis the “What-should-he-have-done defense,” or\\n“W-defense” for short (Widerker 2003). Like the argument\\nin the previous paragraph, the W-defense is designed to apply to\\nactions for which an agent is morally blameworthy. If Trent is to be\\nblamed for loudly coughing during Maggie’s lecture, it’s\\nmorally reasonable to expect him to have refrained from coughing. But\\nthe central thought is that this expectation is morally\\nreasonable if he couldn’t help himself due to, say, a\\nbug’s flying into his mouth. “What would you have had me\\ndo?,” he might ask a peeved Maggie. Without any satisfactory\\nanswer to such a question, blame seems inappropriate.  \\nnot'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP'}, page_content='3. Arguments against PAP'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP'}, page_content='Like most of the important conceptual claims of philosophy, PAP has\\nproved to be a tempting target, especially to philosophers wanting to\\navoid its potential threat to responsibility\\n ( ).  \\n§1.1'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP', 'Header 3': '3.1 The Irrelevance of Determinism'}, page_content='3.1 The Irrelevance of Determinism'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP', 'Header 3': '3.1 The Irrelevance of Determinism'}, page_content='As noted earlier\\n ( ),\\n it looks as if PAP is supported by our ordinary practices of praise\\nand blame. In particular, those who could not avoid what they’ve\\ndone are excused. But despite these initial appearances, ordinary\\npractices might end up undermining the principle.  \\n§2.2  \\nConsider again causal determinism, a thesis often thought to rule out\\nalternatives. Determinism is a substantive empirical claim, and no\\none, certainly no ordinary person, knows whether it’s true (see\\nentry on ).\\n It appears, then, that if we presuppose PAP in ordinary life, we\\nshould for the time being withhold judgment on whether any person is\\nresponsible for her actions, cautiously awaiting a verdict from\\nscientists on the question of determinism. But, the argument goes, it\\nwould be absurd to think we have to wait for scientific progress to\\ndecide whether, say, Kevin should be grateful to Denise for picking up\\nhis mail when he was on vacation, or a victim of fraud should blame\\nthe con artist. These and similar reactions are justified however\\nthings turn out in the domain of\\nmicro-physics. Determinism’s threatened block of alternatives is\\nirrelevant to whether someone should be praised or blamed (P. F.\\nStrawson 1962; Dennett 1984b: ch. 6; Fischer 2006: 5).  \\ncausal determinism  \\nrecherché  \\nIn reply, PAP’s defenders might take a cue from contextualists\\nand say that in ordinary contexts, determinism is routinely (and\\nproperly) ignored. This would explain why micro-physics never appears\\nin ordinary thinking about responsibility, but in a way compatible\\nwith our recognizing PAP’s truth: if determinism were to become\\nsalient—as it is in philosophical and scientific\\ncontexts—its block of alternatives cause us to\\nretract our ordinary judgments (cp. Hawthorne 2001). Another option is\\nto say that we ordinarily assume—even if we don’t yet\\nknow—that determinism is false (Wiggins 1973: §8), and with\\nthis we normally take ourselves to have alternatives. Perhaps if we\\nbecame determinists, we would stop our practices of praise and blame,\\nor at least stop thinking they were justified, precisely because we\\nendorse PAP or some closely related principle. There is a growing\\nexperimental literature on whether we would attribute moral\\nresponsibility under the explicit assumption of determinism, but the\\nresults so far seem to be mixed (Sommers 2010; Nichols 2015: ch.\\n4).  \\nwould'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP', 'Header 3': '3.2 Compelled but Willing Action'}, page_content='3.2 Compelled but Willing Action'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP', 'Header 3': '3.2 Compelled but Willing Action'}, page_content='The previous section tried to undermine PAP by arguing that our\\npractices of praise and blame are neutral on whether we have\\nalternatives. A more direct line of attack—to explored in this\\nand following sections—tries to produce cases in which we\\nproperly praise or blame agents even while knowing they lack\\nalternatives.  \\nOne sort of example is from the Stoics (Inwood & Gerson 1988:\\n134). Imagine a dog happily running behind a cart to which he is\\nchained. The cart makes the dog’s running inevitable—he\\ncannot do otherwise—yet he nevertheless runs willingly. Whatever\\nexactly the Stoics meant by this example (Sauvé Meyer 1999;\\nHankinson 2014), an opponent of PAP may be inspired to argue by\\nanalogy that necessity or “fate” doesn’t preclude\\nresponsibility. Even if Kurt is fated to steal John’s lunch,\\nhe’s still responsible for doing so if he acts willingly: even\\nif fate “pulls” him along like the cart, he could be a\\nhappy and contributing participant. Similarly, imagine a drug user\\nwhose addiction compels him to take a drug, but who doesn’t mind\\nbeing addicted, and thus indulges willingly. The addict is responsible\\nfor taking the drug, even if he couldn’t do otherwise (Frankfurt\\n1971; cf. Wallace 1994: 172–5; the irresistible addiction, while\\nperhaps useful for making such conceptual points, appears not to match\\nactual cases of addiction: see Levy 2006; Pickard 2015.)  \\nThat said, the power of such examples against PAP is limited, as they\\ninvolve the sorts of compulsion usually thought to preclude\\nresponsibility (cp. Lamb 1993). At best these are examples of\\noverdetermination: responsibility-undermining factors are at work,\\neven if the agent’s own character and values play a (redundant)\\nrole in producing the action. A kind of case to be considered next,\\nhowever, may be more effective.'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP', 'Header 3': '3.3 Volitional Necessity'}, page_content='3.3 Volitional Necessity'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP', 'Header 3': '3.3 Volitional Necessity'}, page_content='Brought before Charles V and ordered to recant, Luther refused, saying\\n(according to tradition), “Here I stand. I can do no\\nother”. If we can take his word for it, Luther was compelled by\\nhis conscience. Yet we still praise him for refusing, in spite of his\\ninability to do otherwise. Indeed, his being compelled by conscience\\nmay enhance our moral admiration. Such cases of “volitional\\nnecessity” (Frankfurt 1982) are counterexamples to PAP. Sometimes when we act, alternatives are\\n“unthinkable”: if a normal person is offered money to\\ntorture someone, that person will have no option but to refuse\\n(Dennett 1984a). Again, this lack of alternatives is compatible with,\\nand even enhances, moral praise. (See also Wolf 1980; Nelkin 2011, in\\nwhich the goodness of the determined act plays an important role.)\\nSuch cases are similar to the compelled actions considered in the\\nprevious section, but there’s at least this difference: in those\\nearlier cases, what compels is something “alien” to the\\nagent: a cart, fate, an irresistible desire for a drug. But here,\\ncompulsion comes only from the agent’s own internalized system\\nof values. This might explain why praise is more forthcoming.  \\nprima facie  \\nNevertheless, a defender of PAP could question whether such agents are\\nresponsible in the sense we’re concerned with here. Reid notes\\nthat Cato was said to be “ ”. Reid replies:  \\ngood because he could not be\\notherwise  \\nthis saying, if understood literally and strictly, is not the praise\\nof Cato, but of his constitution, which was no more the work of Cato,\\nthan his existence. (Reid 1788, IV.1; quoted in Chisholm 1966)  \\nPraising Cato’s character, we conflate this with moral credit\\nand thereby hold him responsible. Carefully avoid this mistake, and\\ncases of volitional necessity no longer appear to be counterexamples\\nto PAP.  \\nBut a more common reply to such examples is to clarify our principle\\nby allowing the relevant alternatives to occur before the time of\\nacting. Insofar as Luther should be praised for his refusal,\\nit’s because the character compelling the action was itself the\\nresult of Luther’s past choices in which he could have done\\notherwise. This important qualification to PAP, sometimes called\\n“tracing”, has been standard since Aristotle\\n( iii.v; cp. John Locke 1689: II.xxi.56; C.\\nA. Campbell 1957; Lamb 1993; Kane 1996: 39–40). Only by moral\\nstruggle, choosing a certain path among the many available, did Luther\\nfinally arrive at a point in which his character and conscience\\nnecessitated his actions. In light of this, let us formulate PAP with\\nan additional clause (closely following Mele 1999: 282):  \\nNicomachean Ethics  \\n: a person is morally responsible for\\nwhat she has done at time only if (i) she could have done\\notherwise at , or (ii) even though she could not have done\\notherwise at , the psychological character on the basis of\\nwhich she acted at is itself partially a product of an\\nearlier action (or actions) of hers which was performed at a time when\\nshe could have done otherwise.  \\nPAP-historical  \\nt  \\nt  \\nt  \\nt  \\nThis less demanding version of PAP was needed all along, even for more\\nordinary cases. Tom, who becomes violent when drunk, couldn’t\\nhelp himself when he punched someone: Tom’s intoxication put him\\nat the mercy of his anger. Yet, assuming other conditions on\\nresponsibility are met, we should blame him, especially when learning\\nthat he willingly drank beforehand. It’s the prior act of\\ndrinking for which Tom had alternatives, and it’s these\\nalternatives, not those at the time of action, that satisfy our new\\ncondition.  \\nHenceforth let PAP and similar principles have such a historical\\nrider, at least implicitly (so “PAP” below can be read as\\nshort for “PAP-historical”). The principle’s\\ndefenders can then allow a wide range of cases—willing\\nincapacitation, moral saintliness, confrontation with the\\n“unthinkable”—in which a person is responsible for\\nacting without, at the time, having alternatives. To test PAP, then,\\none must go to “the source” and look at cases in which the\\nhistorical clause is not relevant to the agent’s moral\\nresponsibility. Stipulate that the actions to be considered below,\\nunless otherwise stated, are those for which, if the agent is\\nresponsible at all, she is “directly” responsible\\n(Zimmerman 1988: ch. 3). Perhaps such actions are what Kane (1996)\\ncalls “self-forming willings”, foundational choices that\\nshape the characters that later constrain, or even fix, future action\\n(cp. Klein 1990: 58).'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP', 'Header 3': '3.4 Inevitability without Causation'}, page_content='3.4 Inevitability without Causation'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP', 'Header 3': '3.4 Inevitability without Causation'}, page_content='According to Frankfurt (1983: 322),  \\n[c]onstructing counterexamples to PAP is not difficult. It is\\nnecessary only to conceive circumstances which make it inevitable that\\na person will perform some action but which do not bring it about that\\nhe performs it.  \\nFrankfurt himself proposed examples of this sort, and they have\\ninspired a massive literature, a portion of which is discussed below.\\nBut before we get to Frankfurt’s own examples, others in the\\nsame neighborhood are worth looking at.'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP', 'Header 3': '3.4 Inevitability without Causation', 'Header 4': '3.4.1 Divine Foreknowledge'}, page_content='3.4.1 Divine Foreknowledge'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP', 'Header 3': '3.4 Inevitability without Causation', 'Header 4': '3.4.1 Divine Foreknowledge'}, page_content='As noted earlier\\n ( ),\\n PAP threatens to join with divine foreknowledge to rule out moral\\nresponsibility. The reasoning starts like this:  \\n§1.1  \\nP1.  \\nIf God knew a billion years ago that Kurt would steal\\nJohn’s lunch, then Kurt could not have done otherwise.  \\nP2.  \\nIf Kurt could not have done otherwise than steal John’s\\nlunch, then he was not morally responsible for doing so.  \\nC.  \\nTherefore, if God knew a billion years ago that Kurt would steal\\nJohn’s lunch, then Kurt was not morally responsible for doing\\nso.  \\nAdding the thesis of divine foreknowledge as a premise would let us\\ncomplete the argument, but this initial stage is what matters here.\\nThe conclusion looks false. After all, God’s foreknowledge\\ndidn’t cause Kurt to act as he does. God merely saw ahead of\\ntime that Kurt would do this on his own, for his own reasons. By\\nanalogy, my covertly observing Kurt as he steals John’s lunch\\ndoes not affect his responsibility for the theft. Why then should\\nGod’s being aware of that same act (albeit ahead of time) make\\nany difference to whether he should be blamed?  \\nSuppose this case against C is convincing. The above argument is\\nvalid, so either P1 or P2 must be rejected. P1, however, is supported\\nby a venerable line of reasoning, which goes roughly as follows:\\nSupposing that God foreknew Kurt’s action, if Kurt had the power\\nto refrain from stealing John’s lunch, he would thereby have the\\npower to change what God knew a billion years ago. But Kurt has no\\nsuch power, as the past is fixed. If this argument for P1 is sound,\\nthen P2 is the culprit. But P2 is entailed by PAP, so PAP is false as\\nwell.  \\nThis case against PAP follows Frankfurt’s recipe for\\nconstructing counterexamples (Hunt 1996, 1999; but cf. Zagzebski 1991:\\nch. 6; Widerker 2000: 187–8). God’s foreknowledge makes it\\n“inevitable” that Kurt will steal John’s lunch, but\\ndoes not “bring it about” that he does so: Kurt steals on\\nhis own, without divine interference. Evaluating this proposed\\ncounterexample would lead us beyond this scope of this entry into the\\ntopic of divine foreknowledge (see entry on ).\\n But some of the moves one might make here will come up later when\\nreplies to Frankfurt’s own proposed counterexamples are\\nconsidered.  \\ndivine foreknowledge and free will'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP', 'Header 3': '3.4 Inevitability without Causation', 'Header 4': '3.4.2 Locke’s Content Prisoner'}, page_content='3.4.2 Locke’s Content Prisoner'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP', 'Header 3': '3.4 Inevitability without Causation', 'Header 4': '3.4.2 Locke’s Content Prisoner'}, page_content='In the Locke asks us to imagine that  \\nEssay  \\na man be carried, whilst fast asleep, into a room, where is a person\\nhe longs to see and speak with; and be there locked fast in, beyond\\nhis power to get out: he awakes, and is glad to find himself in so\\ndesirable company, which he stays willingly in, i.e., prefers his stay\\nto going away. (J. Locke 1689: II.xxi.10)  \\nLocke says the man remains in the room voluntarily, even if he is not\\nfree to leave. If the man is responsible for staying, this is a\\ncounterexample to PAP. Like the previous example, this one also\\nappears to follow Frankfurt’s recipe. What makes the man’s\\nstaying in the room unavoidable—namely, the room’s being\\nlocked—plays no role in his staying: he remains willingly, for\\nhis own reasons (see Zimmerman 1988: 120–6 for a similar\\nexample).  \\nWhile Locke’s example shaped later discussions of moral\\nresponsibility, it appears that—whatever Locke\\nintended—it’s not decisive against PAP. Distinguish two of\\nthe man’s actions: (A1) staying in the room, and (A2) deciding\\n(agreeing, willing) to stay in the room (Lowe 1995: 131). It’s\\nnot at all clear that he should be blamed for A1; the room is, after\\nall, locked. On the other hand, it seems he can be blamed for A2. But\\nhe could have done otherwise than A2: he could have protested or tried\\nto get out. Whatever apparent pressure the example puts on PAP derives\\nfrom conflating A1 and A2. There is no action of the man that’s\\nboth unavoidable (for him) and one for which we should hold him\\nmorally responsible.'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP', 'Header 3': '3.4 Inevitability without Causation', 'Header 4': '3.4.3 Frankfurt-Style Cases'}, page_content='3.4.3 Frankfurt-Style Cases'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '3. Arguments against PAP', 'Header 3': '3.4 Inevitability without Causation', 'Header 4': '3.4.3 Frankfurt-Style Cases'}, page_content='So far we’ve looked at two sorts of proposed counterexample to\\nPAP that follow Frankfurt’s recipe. Frankfurt’s own\\nexamples, and those they inspired, have come to be known as\\n“Frankfurt-Style Cases” (FSCs). Here is one:  \\n…let us say that a person decides to take and does take a\\ncertain drug, just in order to enjoy the euphoria he expects it to\\ninduce. Now suppose further that his taking the drug would have been\\nmade to happen in any case, by forces which were in fact inactive but\\nwhich would have come into play if he had not on his own decided and\\nacted as he did. Let us say that, unknown to himself, the person is\\naddicted to the drug and would therefore have been driven irresistibly\\nto take it if he had not freely gone about doing so. His dormant\\naddiction guarantees that he could have avoided neither deciding to\\ntake nor taking the drug, but it plays no role in bringing about his\\ndecision or his act. As the actual sequence of events develops,\\neverything happens as if he were not addicted at all. The addiction is\\nclearly irrelevant in this case to the question of whether the person\\nis morally responsible for taking the drug. (Frankfurt 1983:\\n322–3)  \\nThis person resembles the willing addict considered earlier\\n ( ),\\n except in this case the addiction remains dormant. He takes the drug\\non his own, for his own reasons. He thus seems morally responsible for\\ndeciding to take the drug and for taking it. Yet both decision and\\naction were inevitable, for the addiction would have forced him had he\\nnot chosen on his own to do so.  \\n§3.2  \\nA more well-known FSC comes in an earlier paper (Frankfurt 1969;\\nFrankfurt credits Robert Nozick’s unpublished lectures for\\nsimilar examples). In what follows, it will be useful to consider a\\nversion of Frankfurt’s example from Fischer:  \\nBlack is a nefarious neurosurgeon. In performing an operation on Jones\\nto remove a brain tumor, Black inserts a mechanism into Jones’s\\nbrain which enables Black to monitor and control Jones’s\\nactivities. Jones, meanwhile, knows nothing of this. Black exercises\\nthis control through a computer which he has programmed so that, among\\nother things, it monitors Jones’s voting behavior. If Jones\\nshows an inclination to decide to vote for [the Democrat], then the\\ncomputer, through the mechanism in Jones’s brain, intervenes to\\nassure that he actually decides to vote for [the Republican], and does\\nso vote. But if Jones decides on his own to vote for [the Republican],\\nthe computer does nothing but continue to monitor—without\\naffecting—the goings-on in Jones’s head.  \\nSuppose Jones decides to vote for [the Republican] on his own, just as\\nhe would have if Black had inserted the mechanism into\\nhis head. Then Frankfurt claims that Jones is responsible for voting\\nfor [the Republican], regardless of the fact that he could not have\\ndone otherwise. (Fischer 1982: 26; cp. Frankfurt 1969:\\n835–836)  \\nnot  \\nComplicating details will come in later sections, but for now\\nit’s worth noting two advantages of this kind of example. First,\\nunlike some of the earlier attempts to undermine PAP, there is no\\nfactor, internal to the agent or otherwise, compelling the action.\\nWhat makes an action inevitable—the dormant addiction in one\\ncase, Black and his computer in another—in no way brings about\\nthe action, which the agent does for his own reasons. This makes\\npraise or blame more natural. Second, unlike the case of Locke’s\\nprisoner, FSCs do not invite us to conflate decision and overt action,\\nfor it is the decision itself (as well as the action) that is both\\ninevitable and that for which the agent is morally responsible.'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases'}, page_content='4. Objections to Frankfurt-Style Cases'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases'}, page_content='Not everyone is impressed by Frankfurt’s proposed\\ncounterexamples and the FSCs he inspired. Some critics concede that\\nPAP is false, but insist that some neighboring principle—one\\nthat will serve many of the same purposes as PAP—is immune to\\nFrankfurt’s examples. Others attack FSCs directly, arguing that\\nthe examples fail to show even that PAP is false. The critical\\nliterature is too large to cover in detail here, but this section\\noutlines a few broad themes. (Among the many useful surveys are\\nFischer 1999, 2011; Sartorio 2017; and the pertinent chapters of\\nBeebee 2013; Griffith 2013; Timpe 2013.)'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases', 'Header 3': '4.1 The Cases are Too Unusual'}, page_content='4.1 The Cases are Too Unusual'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases', 'Header 3': '4.1 The Cases are Too Unusual'}, page_content='FSCs are philosophical thought-experiments, often involving science\\nfiction and covert figures with mysterious powers and unlikely\\nobsessions. The growing literature on FSCs continues this pattern (for\\na sampling, see Widerker & McKenna 2003). As the cases get\\nincreasingly “strange and esoteric” (Kane 2007: 168),\\nachieving “dizzying degrees of development” (Speak 2002:\\n98), we might start to wonder whether we’ve lost sight of the\\ngoal, which is to understand concepts originating, not in\\nphilosophical fantasy, but in everyday life and ordinary\\ncircumstances. However things look in the distant possible worlds of\\nFSCs, our natural practices and judgments of moral responsibility\\nrequire alternatives.  \\nThis suggestion could play out in a few ways. One is to concede that\\nFSCs have refuted PAP, but to insist that the principle—much\\nlike Newtonian theory in the domain of physics—is still good\\nenough for our ordinary practices of assigning praise and blame (Stump\\n1990). Or one might just abandon PAP as formulated but replace it with\\na principle explicitly restricted to ordinary, non-Frankfurt\\ncases:  \\nin ordinary circumstances, a person is\\nmorally responsible for what she has done only if she could have done\\notherwise.  \\nPAP-ordinary:  \\nSuch a principle appears Frankfurt-proof, yet is designed to be just\\nas useful to assigning praise and blame as the original\\n(O’Connor 2005; Glatz 2008; Perry 2010; Whittle 2016). Finally,\\none could say that this sort of restriction was part of PAP all along,\\nas the principle gets its content from—and is restricted\\nto—our ordinary practices. In that case, PAP-ordinary just is\\nthe original PAP, so FSCs were never a threat to begin with.  \\nIn reply, a defender of FSCs could note that not all such cases are so\\nfar removed from everyday life. There is nothing especially bizarre\\nabout Locke’s content prisoner (an early kind of FSC) or\\nFrankfurt’s own example of the dormant addiction. And there is\\nno fantastic technology or malevolent being in another FSC, the\\n“driver instruction vehicle” with dual controls: Sally\\nsteers her car to the right, which the instructor is happy to allow.\\nBut  \\nif Sally had shown any inclination to cause the car to go in some\\nother direction, the instructor would have intervened and caused the\\ncar to go to the right (just as it actually goes). (Fischer &\\nRavizza 1998: 32; cp. Naylor 1984)  \\nIt’s worth noting as well that if traditional theism is true,\\nand if divine foreknowledge is an FSC (Hunt 2003), then FSCs, far from\\nbeing unusual, are commonplace. That said, all of these more\\n“ordinary” FSCs still violate background assumptions\\nusually in place when persons are praised or blamed. Those favoring\\nPAP-ordinary could claim that these examples do not fall within the\\nprinciple’s scope.  \\nIn any case, there is a more fundamental reply to the general strategy\\nof this section: if FSCs, strange as some of them are, are at least\\ncoherent, they do point to a conceptual divide between moral\\nresponsibility and alternatives. Even if alternatives are needed for\\nresponsibility in ordinary circumstances, FSCs, if successful, show\\nthat this is a contingent fact. Fallback principles such as\\nPAP-ordinary reveal, at best, that alternatives are usually correlated\\nwith responsibility, and tell us little about what it is to responsible. FSCs could thereby serve an important\\ntheoretical role in directing attention away from alternatives to what\\nmakes someone responsible for their actions (Fischer & Ravizza\\n1998: 30; Ekstrom 2000: 184–5).  \\nbe'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases', 'Header 3': '4.2 Frankfurt Agents Could Have Done Otherwise'}, page_content='4.2 Frankfurt Agents Could Have Done Otherwise'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases', 'Header 3': '4.2 Frankfurt Agents Could Have Done Otherwise'}, page_content='A central feature of all FSCs is that the agent could not have done\\notherwise. Due to the presence of the (counterfactual) intervener, all\\noptions but one are closed. But some have objected that the agent in\\nfact can do otherwise, and that it’s this ability that, in\\naccordance with PAP, explains why the agent is responsible (assuming\\nthat other conditions of responsibility are met). Below are two ways\\nFrankfurt agents could turn out to have alternatives after all.'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases', 'Header 3': '4.2 Frankfurt Agents Could Have Done Otherwise', 'Header 4': '4.2.1 General Abilities'}, page_content='4.2.1 General Abilities'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases', 'Header 3': '4.2 Frankfurt Agents Could Have Done Otherwise', 'Header 4': '4.2.1 General Abilities'}, page_content='Wilderness Tim finds himself stranded in the middle of nowhere, miles\\nfrom any automobile. Can Tim drive a car? Yes and No\\n ( ).\\n He is (for now) healthy, and has not lost any of the skills needed to\\ndrive. So in that sense he can. But there are no cars available, so in\\nthat sense he cannot. Say then that Tim has the (capacity, skill, competence) to drive, but he\\ncan’t drive in a more inclusive sense, that is,\\nwhen his particular circumstances are also taken into account. This\\ndistinction between kinds of ability is a staple of action theory\\n(e.g., Austin 1956; Goldman 1970: ch. 7; D. Locke 1974; Mele 2003).\\nHere we look briefly at its relevance to FSCs.  \\n§1.3  \\ngeneral\\nability  \\nall-in  \\nAs the interveners in FSCs are merely counterfactual, they leave the\\ngeneral abilities of their agents intact. Black’s device, for\\nexample, merely monitors Jones’ brain: as things turn out,\\nbecause matters proceed as Black wishes, he does nothing to affect how\\nJones deliberates, decides, or acts. Jones thus retains all of the\\ngeneral abilities he would have had if Black had never been present.\\n(Stipulate that the device Black places in Jones does not interfere\\nwith any relevant processes in Jones’ brain: Black is a\\nsupremely talented neurosurgeon.) Assuming Jones is otherwise normal,\\nwe can suppose that among his general abilities is that he can vote\\nfor the Democrat. So while Jones cannot in a more inclusive sense\\nchoose otherwise than to vote for the Republican (that is, when the\\npresence of Black and his device are held fixed), Jones choose to vote for the Democrat in the general sense, and this is\\nability enough for responsibility, thus rescuing PAP from such\\nexamples. (Variations on this theme include J. K. Campbell 1997, 2005;\\nSmith 2003; Vihvelin 2004; Fara 2008.)  \\ncan  \\nIn light of this defense, let us for now formulate our principle\\nexplicitly in terms of general abilities, allowing that this might\\nhave been what PAP was saying all along:  \\na person is morally responsible for what\\nshe has done only if she had the general ability to do otherwise.  \\nPAP-general:  \\nThis version appears to be immune to FSCs. And it has the added virtue\\nof explaining standard cases in which agents are blameless\\n ( ),\\n so long as these can plausibly be construed as involving absent or\\ndiminished general abilities. Moreover, compatibilists will note that\\ncausal determinism is no longer the threat to moral responsibility it\\nfirst appeared to be\\n ( ;\\n but see also ).\\n After all, determinism does not rob us of our unexercised general\\nabilities. For example, even in a deterministic world, during periods\\nof silence one typically still has the to speak.\\nWhy, then, should determinism deprive anyone of the ability—that\\nis, the general ability—to do otherwise? The point is ancient\\n(Sorabji 1980: 78), but it has received new life and sophisticated\\ndefense in the hands of the “new dispositionalists” or\\n“dispositional compatibilists,” so-called because general\\nabilities can be equated with dispositions (see entry on ).  \\n§2.2  \\n§1.1  \\n§5.2  \\ncompetence  \\ncompatibilism §4.1.5  \\nOne question this defense raises is whether PAP, now construed as\\nPAP-general, is subject to counterexamples similar to FSCs (Cohen\\n& Handfield 2007; Whittle 2010). Another is whether this revised\\nprinciple reflects the kind of alternatives traditionally thought\\nneeded to explain responsibility (McKenna 1998; Kane 2002; Clarke\\n2009; Whittle 2010; Fischer 2018; cp. entry on ).\\n There’s a set of moral practices, and with it a version of PAP,\\non which responsibility requires more than just the general ability to\\ndo otherwise. It’s apparently this reading of PAP that FSCs are\\ntargeting. Even if Frankfurt agents have the relevant general\\nabilities, they seem to lack the ability to do otherwise in the sense\\ntraditionally—but wrongly, according to Frankfurt and his\\nallies—required for responsibility.  \\nabilities §5.2'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases', 'Header 3': '4.2 Frankfurt Agents Could Have Done Otherwise', 'Header 4': '4.2.2 Flickers of Freedom'}, page_content='4.2.2 Flickers of Freedom'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases', 'Header 3': '4.2 Frankfurt Agents Could Have Done Otherwise', 'Header 4': '4.2.2 Flickers of Freedom'}, page_content='The previous section assumed that agents in FSCs cannot do otherwise\\nin the more inclusive all-in sense. But a more ambitious line of\\nresponse rejects this, insisting that Frankfurt agents can do\\notherwise even when their peculiar circumstances are taken into\\naccount. Or rather, they have all-in alternative, one\\nthat, however minor, is enough satisfy PAP’s requirement on\\nmoral responsibility. These are “little alternatives”\\n(Rowe 2006) or “flickers of freedom” (Fischer 1994).  \\nan  \\nThere are several versions of this strategy. Here’s one: Black\\nremains inactive only because his device detects in Jones no\\ninclination to vote for the Democrat. But then it looks as if Jones\\nhad an alternative after all: he could have been inclined to vote for\\nthe Democrat. This alternative would not have issued in a choice, but\\nit is an alternative for all that. But this sort of flicker is not\\nespecially useful in defense of PAP. For one thing, it seems to locate\\nthe alternative at the wrong time, and regarding the wrong action (if\\nindeed an inclination can be considered an action at all). For\\nanother, there may be FSCs in which Black does not rely on any such\\nprior sign before the moment of choice\\n ( ).  \\n§4.3.2  \\nThere are, however, more promising versions of the flicker strategy:\\nearly attempts include van Inwagen 1978; Naylor 1984; Rowe 1987. Take\\nNaylor’s version to be representative. Black remains inactive\\nbecause he sees that Jones will decide on his own to do what Black\\nwishes, namely, vote for the Republican. Black would have intervened\\notherwise. But doesn’t this mean there’s something Jones\\ncould have refrained from doing, namely voting for the Republican , i.e., without Black’s intervention? This is\\nan all-in ability, and it’s because Jones had this alternative\\nopen to him that he is rightly held responsible.  \\non his own  \\nFrankfurt’s allies may concede there are such flickers of\\nfreedom, but deny that they are sufficiently “robust” to\\nground our judgments of moral responsibility (Fischer 1994:\\n140–7; 1999). Grant that Jones could do otherwise than choose to\\nvote for the Republican on his own. But this alleged alternative is\\none in which Jones doesn’t choose at all: rather, in this\\ncounterfactual scenario, it is Black who, by activating the neural\\nimplant, causes a “choice” to vote for the Republican.\\nHow, then, could an alternative in which Jones doesn’t\\nact—one in which he is merely Black’s puppet—be\\nrelevant to Jones’s responsibility in the actual scenario? Put\\nanother way: the alternative in question is merely one in which\\nsomething else happens to Jones; it is not one in which he anything. This cannot be the kind of alternative PAP\\nrequires for responsibility. The existence of flickers, it seems,\\ncannot save PAP from FSCs.  \\ndoes  \\nIn reply, some of PAP’s defenders have asked whether\\nalternatives need to be robust in this sense. Perhaps Jones’\\nalternative, however small, can ground his responsibility, not by\\nbeing an alternative action, but by indicating that Jones was not\\ndetermined to act as he did (Mele 1996; Della Rocca 1998; Ekstrom\\n2000: 190). A more ambitious response notes that even if Black\\nintervenes in the alternative scenario, Jones has there from choosing—at least he’s refrained\\nfrom the particular choosing we see in the actual scenario. If we can\\nconsider such refrainings as “doings” in some broad sense\\nof the term, then this is a sense in which Jones does otherwise in the\\nalternative scenario. “Robust” or not, this may be the\\nonly kind of alternative that flicker theorists need (cp. Alvarez\\n2009; Steward 2009; 2012a; ;\\n see also Speak 2002; Capes & Swenson 2017).  \\nrefrained  \\n§5.3.1'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases', 'Header 3': '4.3 A Dilemma for Frankfurt-Style Cases'}, page_content='4.3 A Dilemma for Frankfurt-Style Cases'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases', 'Header 3': '4.3 A Dilemma for Frankfurt-Style Cases'}, page_content='An influential criticism of FSCs is in the form of a dilemma, one that\\nbecomes clear when we look more closely at how FSCs are supposed to\\nwork. Below is one version of this dilemma, followed by two broad\\ncategories of response.'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases', 'Header 3': '4.3 A Dilemma for Frankfurt-Style Cases', 'Header 4': '4.3.1 The Dilemma Stated'}, page_content='4.3.1 The Dilemma Stated'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases', 'Header 3': '4.3 A Dilemma for Frankfurt-Style Cases', 'Header 4': '4.3.1 The Dilemma Stated'}, page_content='Although it is not always explicit in FSCs, the counterfactual\\nintervener appears to reply on some prior sign of what the agent is\\nabout to do (Blumenfeld 1971). Black’s device, for example, is\\nsensitive to Jones’ inclination: if it detects an inclination\\n(call it “Pro-D”) to choose the Democrat, Black will\\nintervene; if it detects an inclination (“Pro-R”) to\\nchoose the Republican, Black will remain idle. As matters turn out,\\nPro-R appears, Black does nothing, and Jones votes for the Republican\\non his own. The dilemma is this: either Pro-R determines (or is\\nassociated with something that determines—a qualification to be\\nleft out for simplicity) that Jones will vote for the Republican, or\\nit doesn’t. Either way, the FSC fails as a counterexample to\\nPAP.  \\nSuppose first that Pro-R determines Jones’ choice. In that case,\\nit’s no longer clear that Jones is responsible, for he’s\\nbeen locked into his choice by a prior cause. Certainly PAPs\\ndefenders—who tend to be libertarians about free will and moral\\nresponsibility—will balk at attributing responsibility to an\\nagent whose actions are causally determined. Lost, then, is the\\ninitially powerful appearance that Jones is morally responsible. On\\nthe other hand, suppose Pro-R does not determine Jones’ later\\nchoice. In that case, Jones could have chosen otherwise, for the\\nsign—the thing that stays Black’s\\nhand—leaves open that Jones will vote for the Democrat. Jones\\nthus has an alternative, and a robust one at that: this is no mere\\nflicker.  \\nonly  \\nThe upshot is that there is no way of spelling out the details of this\\nFSC in a way that preserves both the responsibility of the agent and\\nthe lack of alternatives. Yet both are need for it to be a successful\\ncounterexample. If all FSCs are structurally similar to this one, they\\nare powerless against PAP. (Versions of this “dilemma\\ndefense” include Kane 1985: 51; 1996: 142–3, 191–2;\\nWiderker 1995; Ginet 1996; Wyma 1997; cp. Ekstrom 2000: 197 for a\\nsimilar dilemma; and see Haji & McKenna 2004 for a critical\\noverview. Vihvelin 2000 presents a different dilemma for FSCs, one\\nthat nevertheless engages some of issues that will come up below; for\\nan exchange on that alternative dilemma, see Fischer 2008; Vihvelin\\n2008.)'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases', 'Header 3': '4.3 A Dilemma for Frankfurt-Style Cases', 'Header 4': '4.3.2 Responses to the Dilemma'}, page_content='4.3.2 Responses to the Dilemma'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '4. Objections to Frankfurt-Style Cases', 'Header 3': '4.3 A Dilemma for Frankfurt-Style Cases', 'Header 4': '4.3.2 Responses to the Dilemma'}, page_content='Start with the first horn and stipulate that Pro-R is a deterministic\\ncause of Jones’ subsequent choice. But to prevent the\\nlibertarian from objecting immediately, let us suppose that\\nJones’ responsibility for his choice is not clear; indeed, we\\ncan suppose—even if initially shocking to Frankfurt’s\\nallies—that he’s responsible for it.\\nNevertheless, his being blameless is not due to something, namely\\nBlack and his device, that rules out alternatives. In the actual\\nscenario, Black remains passive, merely monitoring Jones’ brain\\nwithout interfering. The central idea here is that any factor causally\\nisolated from an agent cannot be relevant to that person’s moral\\nresponsibility. So even if Jones responsible,\\nthis isn’t by his lacking alternatives; this\\nwould be enough to refute PAP. (Fischer 1999: 113; cp. Frankfurt 2003.\\nThis is just a first pass at what turns out to be a much more complex\\nargument. For further refinement and criticism, see Goetz 2005;\\nFischer 2010; Widerker & Goetz 2013; Fischer 2013; Palmer\\n2014.)  \\nnot  \\nisn’t  \\nentailed  \\nFrankfurt’s defenders more often take on the second horn, on\\nwhich Pro-R is not causally sufficient for Jones’ choice. This\\nmakes his responsibility clearer, especially for libertarians. But now\\nit seems as if he could choose otherwise.  \\nPereboom (2001: ch. 1; see also Hunt 2005) replies that even with an\\nindeterministic sign, we can construct a successful FSC in which Jones\\nlacks alternatives. For variety, switch for a moment to\\nPereboom’s “Tax Evasion,” though one could construct\\nthe Black-Jones case to make similar points. Joe is an agent with\\nlibertarian freedom who is considering whether to claim an illegal tax\\ndeduction. He’s inclined to do so, and as he deliberates, the\\nonly factor that could potentially change his mind is a strong moral\\nreason. Whether such a reason occurs to Joe is not determined (cp.\\nMele 1995: ch. 12), and even if it does occur to him, it may not\\nchange his mind. But at least a condition of his\\nchoosing not to evade taxes is the occurrence of such a reason\\n(thereby entering a “mental buffer zone”—these are\\nthus called “buffer cases”). Now add the counterfactual\\nintervener, who wants Joe to break the law:  \\nnecessary  \\n… to ensure that [Joe] chooses to evade taxes, a neuroscientist\\nnow implants a device which, were it to sense a moral reason occurring\\nwith the specified force, would electronically stimulate his brain so\\nthat he would choose to evade taxes. In actual fact, no moral reason\\noccurs to him with such force, and he chooses to evade takes while the\\ndevice remains idle (Pereboom 2001: 19).  \\nJoe seems responsible for his choice—even by the\\nlibertarian’s lights—but he could not have chosen\\notherwise, for that would have required a moral reason, one that would\\nhave triggered the intervention. In this FSC, the only overt sign is\\nin the counterfactual sequence, but one might nevertheless worry that\\nthe of this sign in the actual sequence is, contrary\\nto the constraints of the second horn, causally sufficient for\\nJoe’s deciding to take the illegal deduction. So add that at any\\nmoment leading up to the decision, a moral reason could occur to Joe.\\nAs Jones deliberates, then, there’s no point at which the\\nabsence (up to that time) of a moral reason determines his deciding to\\nevade taxes. (Critics of this buffer case include Ginet 2002; Goetz\\n2002; Widerker 2006.)  \\nabsence  \\nAnother way to avoid a causally sufficient sign of Jones’ choice\\nis simply to excise the sign itself, a feature, by the way, not always\\nexplicit in Frankfurt’s own FSCs. Suppose there is no sign or\\nanything else causally sufficient for Jones’ choice, and\\nstipulate that the causal processes leading up to it are as the\\nlibertarian sees fit. The question then is how to make Jones’\\nchoice inevitable, as a successful FSC requires. Filling in the\\ndetails, Hunt (2000; 2003) imagines that the neural processes leading\\nup to Jones’ choice proceed without any outside interference.\\nBut while these processes are indeterministic, it turns out that all\\nof the alternative neural pathways—those that might have\\nrealized a decision to vote for the Democrat—have been blocked.\\nSuch “blockage” (Fischer 1999) gives Jones no alternative\\nbut to vote for the Republican, yet he still makes the decision on his\\nown—the blockage never plays a role in his\\ndeliberations—and is thus morally responsible for it. In a\\nrelated FSC (Mele & Robb 1998), Black has set up in Jones’\\nbrain a process that will, at the time of the decision, cause Jones to\\nvote for the Republican, Jones decides at that time to\\ndo so on his own. The process set up by Black does not interfere with\\nJones’ own deliberations which are assumed, as before, to\\nsatisfy the libertarian. Nor does Black’s process look for a\\nsign of what Jones will decide. When, at the relevant time, Jones\\ndecides on his own to vote for the Republican, it seems he is\\nresponsible for the decision, even though the process Black has set up\\nensures that Jones could not have decided otherwise. (It turns out\\nthat this example too involves blockage, as the process set up by\\nBlack has also “neutralized” the neural realization of any\\nalternative decision.)  \\nunless  \\nAmong the reasons to worry about blockage cases is that they may,\\ncontrary to what’s intended, render Jones’ choice causally\\ndetermined, again putting his responsibility into question. After all,\\nthe objection goes, there seems little if any difference between\\nsaying (i) that structures and processes set up in Jones’ brain\\nblock off all alternatives other than choosing to vote for the\\nRepublican, and (ii) that Jones’ choice is causally determined\\nby those same structures and processes. (For further critical\\ndiscussion of such cases, see, e.g., O’Connor 2000: 83–84;\\nEkstrom 2002; Ginet 1996 [2001 addendum]; 2003; Kane 2003; Timpe 2013:\\nch. 6. An important response to the dilemma defense not considered\\nhere is Stump 1996; see also Goetz 1999; Stump 1999.)'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '5. Beyond PAP'}, page_content='5. Beyond PAP'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '5. Beyond PAP'}, page_content='So far not much consensus has emerged from the literature on PAP.\\nOptimists will still see progress of a sort: a principle taken for\\ngranted through much of philosophy’s history is now considered\\nmore complex than it initially appeared, worth discussing, and\\npossibly false. In any case, in this this final section, assume for a\\nmoment that for some reason or other PAP should be rejected. What\\nthen?'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '5. Beyond PAP', 'Header 3': '5.1 Compatibilism Re-energized?'}, page_content='5.1 Compatibilism Re-energized?'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '5. Beyond PAP', 'Header 3': '5.1 Compatibilism Re-energized?'}, page_content='noted that combining PAP with some universal block of\\nalternatives—such as causal determinism—rules out moral\\nresponsibility. Here is a sketch of this kind of argument:  \\n§1.1  \\nP1.  \\nIf determinism is true, then Kurt could not have done otherwise\\nthan steal John’s lunch.  \\nP2.  \\nIf Kurt could not have done otherwise than steal John’s\\nlunch, then Kurt was not morally responsible for doing so.  \\nC.  \\nTherefore, if determinism is true, then Kurt was not morally\\nresponsible for stealing John’s lunch.  \\nThe first premise is supported by the Consequence Argument (see entry\\non ):\\n Given determinism, Kurt’s theft was a necessary consequence of\\nthe distant past plus the laws of nature. Kurt has no power over\\neither of these, so he couldn’t avoid stealing John’s\\nlunch. The second premise is an instance of PAP. And the conclusion\\ncan be generalized to apply to any action: if determinism is true, no\\none is responsible for anything they do.  \\narguments for incompatibilism §5  \\nCompatibilists about determinism and moral responsibility reject this\\nconclusion, so they must reject at least one of the premises. One\\ntraditional sort of compatibilist accepts PAP (and P2), and targets P1\\nand with it the Consequence Argument. This entangles the compatibilist\\nin questions about the ability to do otherwise\\n ( and ).\\n However, if PAP is false, the compatibilist can avoid these\\ndifficulties and reject P2 as unsupported. Without PAP in the way,\\ncompatibilism receives new energy (see entry on ).  \\n§1.3  \\n§4.2.1  \\ncompatibilism §4  \\nBut this route to compatibilism isn’t straightforward. Even\\nwithout the help of PAP, determinism may still preclude responsibility\\ndue to the it blocks alternatives (Frankfurt 1969;\\nBlumenfeld 1971; Fischer 1982; Speak 2007). In FSCs, what makes the\\naction inevitable doesn’t interfere with the agent’s own\\ndeliberative processes or actions: the “intervention” in\\nthese examples is merely counterfactual. By contrast, if determinism\\nis true, its intervention is actual. After all, determinism entails\\nthat every detail of how we choose and act is fixed by factors in the\\ndistant past. As one might put it: in a deterministic world, we act as\\nwe do determinism has put us on the single track,\\nbecause we could not do otherwise. So even if FSCs show that the mere\\nblocking of alternatives does not preclude moral responsibility,\\ndeterminism’s distinctive way of blocking alternatives is still\\na threat. (In light of this point, one could reformulate the above\\nincompatibilist argument explicitly in terms of the particular way\\ndeterminism blocks alternatives. The modified P2 would not require\\nPAP, though it would need a weaker principle.)  \\nway  \\nbecause  \\nThis move shifts the burden back to compatibilists, who need to\\nexplain why determinism, in spite of the way it blocks alternatives,\\ndoesn’t undermine responsibility. A traditional compatibilist\\nreply points out that determinism—unlike compulsion and other\\nthreats to responsibility—does not disrupt the normal operation\\nof our faculties; in that sense, its “invasiveness” is\\nbenign (cp. ).\\n A more ambitious line defends this point by developing an\\nindependently plausible compatibilist account of moral responsibility.\\nHere, briefly, are two such attempts (see entry on for a detailed treatment of these and other compatibilist\\noptions).  \\n§3.1  \\ncompatibilism  \\nOne is from Frankfurt (1971) and others, who say the morally\\nresponsible agent is one who identifies with the motives that cause\\nhim to act. Consider volitional necessity\\n ( ):\\n While Luther is compelled by his conscience, he identifies with these\\nmoral motives. Similarly, even if the willing addict can’t help\\nbut take the drug, he endorses the addiction. Such cases appeared\\nearlier as attempted counterexamples to PAP. Now they are treated as\\ncases of benign necessitation, ways of blocking alternatives, that,\\nwhile operating through an agent’s own deliberative processes,\\nare compatible with moral responsibility. As long as the agent\\nendorses (or at least does not reject) these processes, they\\nneedn’t preclude responsibility. If an account along these lines\\ncan be independently defended, a compatibilist can claim to have met\\nthe above demand, as identification, and thus responsibility, is\\ncompatible with determinism.  \\n§3.3  \\nAnother influential account is from Fischer and Ravizza (1998; see\\nalso Fischer 1994). One lesson of FSCs is that the explanation of\\nmoral responsibility should be found in the actual sequence leading up\\nto an agent’s choice and action. In the Black-Jones case, what\\nhappens in the alternative sequence, in which Black intervenes, is not\\nrelevant to Jones’ responsibility. It’s not even clear\\nthat Jones anything in the alternative sequence\\n ( ).\\n His moral responsibility, then, must be grounded in what actually\\nhappens when Jones chooses on his own to vote for the Republican.\\nFischer and Ravizza argue that the relevant portion of the actual\\nsequence includes a “reasons-responsive mechanism”\\nproducing Jones’ choice. As Jones deliberates, his deliberative\\nfaculties are sensitive to reasons to vote for the Democrat as well as\\nreasons to vote for the Republican. As it turns out, the latter\\noutweigh the former, but there is still a scenario in which the\\nreasons to vote for the Democrat to outweigh those to vote for the\\nRepublican, Jones’ deliberative faculties are sensitive to this\\nfact, and he votes accordingly. In this sense, his deliberative\\nfaculties can respond to competing reasons. (This Democratic-friendly\\nscenario needn’t be accessible to Jones; we can thus preserve\\nFrankfurt’s constraints.) Supposing that, given the appropriate\\nbackground conditions, reasons-responsiveness secures moral\\nresponsibility, the compatibilist has met the earlier demand, for\\nhaving a reasons-responsive mechanism looks compatible with\\ndeterminism.  \\ndoes  \\n§4.2.2  \\nFischer and Ravizza’s account allows that control of a sort is\\nrequired for moral responsibility. One kind of control over\\none’s choices and actions is a matter of having alternatives: in\\nthis sense, the control condition is equivalent to the rejected PAP;\\nFischer and Ravizza call this “regulative control”. But\\nthere’s another kind, “guidance control,” that\\ndoesn’t require that the agent has alternatives available, and\\nit’s this that the reasons-responsiveness account attempts to\\ncapture. Jones guides his action in the relevant sense, and for this\\nreason can be responsible for what he does, even if, due to Black and\\nhis computer, Jones lacks regulative control.'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '5. Beyond PAP', 'Header 3': '5.2 Incompatibilism without PAP'}, page_content='5.2 Incompatibilism without PAP'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '5. Beyond PAP', 'Header 3': '5.2 Incompatibilism without PAP'}, page_content='In spite of compatibilist theories developed in the wake of FSCs, some\\nphilosophers have remained convinced that even if PAP is false,\\ndeterminism precludes responsibility. A compatibilist will challenge\\nsuch philosophers to explain why this is so, if not because\\ndeterminism blocks all alternatives.  \\nOne response to this challenge was in the previous section. Another\\nmodifies the Consequence Argument so that it doesn’t rely on\\nPAP. This is the “Direct Argument”, because it doesn’t go through PAP. It can be reconstructed along\\nthe following lines (a formal version is in van Inwagen 1980):  \\ndirect  \\nP1.  \\nIf determinism is true, then Kurt’s stealing John’s\\nlunch is an inevitable consequence of factors (namely, the distant\\npast and laws) for which Kurt is not morally responsible.  \\nP2.  \\nIf a person is not morally responsible for , and is an inevitable consequence of , then that person is not\\nmorally responsible for .  \\nX  \\nY  \\nX  \\nY  \\nC.  \\nTherefore, if determinism is true, then Kurt is not morally\\nresponsible for stealing John’s lunch.  \\nThe first premise P1 follows from the definition of determinism as\\nwell as some auxiliary claims about moral responsibility. P2 is the\\n“transfer of blamelessness”; whatever else might be said\\nabout this premise, it does not require PAP. And the conclusion, which\\ncan be generalized, follows from P1 and P2. But while this argument\\ndoes not appeal to PAP, it appears that FSCs can be used against P2,\\nthe transfer of blamelessness. The presence of Black, for which we can\\nhardly blame Jones, makes the choice to vote Republican inevitable,\\nbut Jones is for all that morally responsible for so-choosing (see\\nFischer’s introduction to his 1986; Fischer & Ravizza 1998:\\nch. 6; and for more discussion, Stump 2000; McKenna 2008).  \\nA related answer to the compatibilist challenge is from the\\n“source incompatibilist” or “causal history\\nincompatibilist” (Stump 1990; Klein 1990; Pereboom 1995, 2001;\\nZagzebski 2000). Under determinism, you are not the source—at\\nleast not the source\\xad—of your actions.\\nRather, the sources of your actions are found in the distant past.\\nThis point seems to undermine your responsibility, at least one\\ndesirable form of it (Mele 1996). Whatever the merits of source\\nincompatibilism, it addresses the compatibilist challenge, for it says\\nwhy determinism threatens moral responsibility—namely, by ruling\\nout sourcehood—but without appealing to its blocking of\\nalternatives, and thus without PAP. In this way, FSCs shift the\\ndispute away from alternatives to the nature and importance of\\nsourcehood. (Some source incompatibilists accept PAP or a closely\\nrelated principle—see, e.g., Kane 1996; Timpe 2013: ch.\\n9—but we are for now assuming PAP is false.)  \\nultimate'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '5. Beyond PAP', 'Header 3': '5.3 Replacement Principles'}, page_content='5.3 Replacement Principles'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '5. Beyond PAP', 'Header 3': '5.3 Replacement Principles'}, page_content='One of the results of FSCs has been a long list of principles similar\\nto PAP, but intended to improve on it. Some of these keep PAP’s\\nfocus on action, while others switch to responsibility for\\nconsequences or omissions\\n ( ).\\n This final section samples a few of these replacement principles.  \\n§1.2'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '5. Beyond PAP', 'Header 3': '5.3 Replacement Principles', 'Header 4': '5.3.1 Responsibility for Actions'}, page_content='5.3.1 Responsibility for Actions'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '5. Beyond PAP', 'Header 3': '5.3 Replacement Principles', 'Header 4': '5.3.1 Responsibility for Actions'}, page_content='This entry assumes that PAP’s proposed condition is intended to\\nbe explanatory, not merely necessary for responsibility. (Not all\\nnecessary conditions are explanatory, as any Euthyphro-style problem\\nwill show.) That is, the intent of the principle is that when a person\\nlacks alternatives, it is at least in part this\\nthat she is not morally responsible (Fischer 1994: 140; Pereboom 2001:\\n25). As formulated, however, PAP seems too weak for this purpose, as\\nit appears to allow any alternative action to meet its requirement.\\nSuppose that when Kurt stole John’s lunch, he had an\\nalternative, but it was so remote from his mind, so irrelevant to his\\ncircumstances, that he never considered it: Kurt could have sat down\\nand whistled a tune. Although this is an alternative to stealing\\nJohn’s lunch, it is not the sort that could potentially explain\\nhis responsibility. Thus finding PAP too weak, McKenna strengthens its\\nrequirements:  \\nin virtue of  \\n: a person is morally responsible for -ing only if she could have performed some alternative action such that (i) -ing was in her control; (ii) -ing is less morally bad than -ing; and (iii) it would\\nhave been reasonable by her lights for her to consider -ing as\\nan alternative to -ing. (simplified from McKenna 2003:\\n209)  \\nPAP-significant  \\nA  \\nB  \\nB  \\nB  \\nA  \\nB  \\nA  \\nCondition (i) brings to mind the earlier discussion of robustness\\n ( ),\\n but (iii) is the focus at the moment, as it shows why Kurt’s\\nsit-and-whistle alternative is not enough to ground his\\nresponsibility: it was not reasonable for him to consider this as an\\noption. McKenna rejects PAP as “too inclusive”, favoring\\ninstead a principle with these more significant constraints. Having\\nsaid that, he argues that even this improved principle falls to an\\nFSC, one that, however, leaves open plenty of alternatives to assuage libertarian concerns (see Robinson 2014 for\\ncritical discussion).  \\n§4.2.2  \\ninsignificant  \\nOne apparent lesson from PAP-significant is that, contrary to the\\nsimplifying assumption adopted at the outset of this entry, one cannot\\ncleanly separate PAP’s condition on moral responsibility from\\nepistemic requirements (cp. Mele 2010). In particular, in order for\\nalternatives to count as explanatory in the relevant sense, they must\\nbe reasonable from the point of view of the agent. Similar\\nentanglements arise when one considers PAP’s historical clause\\n ( ;\\n see, e.g., Ginet 1996; Ekstrom 2000: 211). Luther is responsible for\\nhis defiant act even if he could not have at the time done otherwise.\\nHis responsibility can be “traced” to past self-forming\\nchoices in which he could have done otherwise. If these are to help\\nexplain his responsibility when brought before Charles V, it seems as\\nif Luther must in the past have known, at least in broad outline, how\\nthose choices would shape his future actions. But if there are such\\nepistemic conditions in PAP’s historical rider, there may be\\nproblems looming, ordinary cases (not FSCs) in which a person is\\nresponsible for her action at , cannot at do\\notherwise, and yet whose responsibility cannot be traced to choices in\\nthe past with the appropriate epistemic credentials. (See Vargas 2005,\\nand for discussion, Fischer & Tognazzini 2009; Shabo 2015.)  \\n§3.3  \\nt  \\nt  \\nThere are other action-focused replacements for PAP worth considering.\\nBut because action has been in play for most of this entry, the\\nconcluding sub-sections look at responsibility for something other\\nthan action: consequences and omissions.'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '5. Beyond PAP', 'Header 3': '5.3 Replacement Principles', 'Header 4': '5.3.2 Responsibility for Consequences'}, page_content='5.3.2 Responsibility for Consequences'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '5. Beyond PAP', 'Header 3': '5.3 Replacement Principles', 'Header 4': '5.3.2 Responsibility for Consequences'}, page_content='It seems clear that sometimes we are responsible for the consequences\\nof our actions (for dissent, see Frankfurt 1983). Recall that Meghan\\nis responsible, not just for ruining Sean’s tea by putting salt\\nin it (an action), but for Sean’s tea being ruined (a\\nconsequence). Call this consequence a “state of affairs”.\\nPerhaps one can also be responsible for states of affairs that\\naren’t the consequences of action. A homeowner is responsible\\nfor the dangerous sink hole developing in her back yard, not because\\nof anything she did, but due simply to negligence. However one is to\\ntreat such cases, they are here set aside to focus on states of\\naffairs that are more clearly the consequences of actions. And not\\nsurprisingly, there is a replacement for PAP explicitly about these\\n(this principle and much of what follows is adapted from van Inwagen\\n1978; cp. Ginet 1996: 403):  \\na person is morally responsible for\\na consequence of what she has done only if she could have prevented\\nit.  \\nPAP-consequences:  \\nThis has as much initial appeal as PAP. If the result of what Meghan\\ndid would have happened anyway—if it was, for her,\\ninevitable—then it’s hard to see how she could be praised\\nor blamed for it. Indeed, the examples that support PAP\\n ( )\\n could also be adapted to support this new principle. If it’s\\ninappropriate to praise a person’s “generous”\\ndonation to a charity when it’s caused by a compulsion to give\\nthings away, it is also inappropriate to praise that person for the\\nconsequence of this act, namely the charity’s windfall. And the\\nreason is that the donor could not have prevented this state of\\naffairs from obtaining.  \\n§2.2  \\nMore relevant to present concerns, however, is that this replacement\\nmay not be vulnerable to the examples that plague PAP. Return to our\\ncentral FSC: It seems as if Jones is responsible for voting Republican\\n(an action) because he does so on his own, for his own reasons. But\\nnow consider a resulting state of affairs, say, the Republican’s\\nhaving one more vote. This would have obtained no matter what Jones\\ndid, or failed to do, on his own. Due to Black, the obtaining of the\\nstate of affairs is not (as Jones’ action is) sensitive to how\\nthings proceed with Jones’ own deliberative processes. One thus\\nloses any sense in which Jones is the author of the consequence, and\\nwith it any sense in which he is responsible. This creates some space\\nbetween PAP-Consequences and PAP, permitting the former to stand when\\nthe latter falls.  \\nHere a state of affairs is a universal: whether Jones votes for a\\nRepublican on his own or because of Black’s intervention, one\\nand the same consequence obtains. In this way, the object of moral\\nappraisal (the state of affairs) doesn’t turn on how things\\nactually go for Jones. But matters look different if the consequence\\nis instead construed as a concrete particular, something produced by\\nJones in the actual scenario. In that case, PAP-consequences looks\\nvulnerable to FSCs in just the same way PAP is. When Jones votes for\\nthe Republican, the particular consequence—that of one vote’s being cast for a Republican—seems to be\\nsomething Jones produced on his own, and is thus responsible for.\\n(Whether numerically the same event would have obtained in the\\ncounterfactual scenario is a controversial issue, one whose resolution\\ncould yield a version of the “flicker” strategy considered\\nin .)\\n Let us in any case continue to assume that a consequence is a\\n(universal) state of affairs.  \\nevent  \\n§4.2.2  \\nEven under this assumption, might this replacement fall to FSCs? The\\nabove defense suggests not, but this has not prevented philosophers\\nfrom trying, and with a variety of examples (e.g., Heinaman 1986; Rowe\\n1989; Klein 1990: ch. 2; Fischer & Ravizza 1998: ch. 4). Here is\\none thought to motivate such a project: Even if a consequence of an\\nagent’s action was inevitable, we can still make sense of her\\ncausing that state of affairs to obtain, which thus looks like the\\nresult of something she did, even if it would have obtained anyway.\\nPerhaps this is enough to make the state of affairs sensitive to the\\nactual sequence by which it was produced, grounding the agent’s\\nmoral responsibility, as an FSC requires. Rowe (1989), for example,\\nimagines you are on a train headed down Track 2, a track that leads to\\na safe stopping point. If you throw a switch, the train will go onto\\nanother track, Track 1, on which a dog is tied. Unbeknownst to you,\\nthere is someone (Peter) who is poised to throw that switch if you\\ndon’t. As it turns out, you throw the switch on your\\nown—Peter doesn’t act at all—and the train goes down\\nTrack 1, killing the dog. Rowe says you caused the dog’s death\\nand are in fact morally responsible for it. (As usual, assume other\\nstandard requirements are met.) Yet that consequence, the dog’s\\ndeath, was inevitable: there was nothing you could have done to\\nprevent it. (For a successor to PAP-consequences, one that may handle\\npotential counterexamples more easily, see Sartorio 2012.)'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '5. Beyond PAP', 'Header 3': '5.3 Replacement Principles', 'Header 4': '5.3.3 Responsibility for Omissions'}, page_content='5.3.3 Responsibility for Omissions'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': '5. Beyond PAP', 'Header 3': '5.3 Replacement Principles', 'Header 4': '5.3.3 Responsibility for Omissions'}, page_content='Sometimes a person is responsible for acting: Blame Luke\\nthe indifferent lifeguard for failing to save the drowning swimmer.\\nCredit Emily for not cheating on a test. Let us formulate one more\\nreplacement for PAP, using “omission,” broadly, for any\\nfailure to act:  \\nnot  \\n: a person is morally responsible for\\nwhat she didn’t do only if she could have done it.  \\nPAP-omissions  \\nAccording to this new principle, if the waters were too treacherous\\nfor a rescue, Luke is blameless for his failure. (He may still be\\nresponsible for failing to try, or for failing to call for help.)\\nSimilarly, Emily is responsible for not cheating only if she could\\nhave done so. If, due to a fear of being caught, she was unable to\\ncheat, or if the means to cheat were simply unavailable to her, she\\ncould not be credited for her omission, at least not on that occasion.\\nPAP-omissions is here treated as its own stand-alone principle, though\\nthere is some metaphysics behind such an assumption (Clarke 2012; S.\\nBernstein 2015). The present question is whether it can survive the\\ndemise of PAP.  \\nIt appears not, for this replacement looks vulnerable to many of the\\nsame arguments\\n ( ).\\n But to test this alleged symmetry, focus in particular on whether an\\nFSC can be constructed as a counterexample to PAP-omissions (van\\nInwagen 1978, 204–5): Bob sees a man being robbed and beaten\\noutside of his house. After considering whether to call the police,\\nBob decides not to get involved, calculating that it wouldn’t be\\nworth the trouble. For his own reasons, then, Bob refrains from\\ncalling the police. What Bob didn’t know, however, is that the\\ndue to a technology disaster, all of the phones in the city are out of\\norder and will be for hours. It seems then that Bob isn’t\\nresponsible for failing to call the police. And plausibly,\\nPAP-omissions explains why he’s off the hook: due to the\\ntelephone disaster, he could not have called the police. (He might\\nstill be responsible for failing to try to call the police, but trying\\nis something he could have done.) Given the present assumption that\\nPAP is false, what emerges from such examples is, contrary to initial\\nappearances, an important asymmetry:  \\n§3  \\nmoral responsibility for an action does not require the freedom to\\nrefrain from performing the action, whereas moral responsibility for\\nfailure to perform an action requires the freedom to perform the\\naction. (Fischer & Ravizza 1991: 262, with similar examples; they\\nlater retract the asymmetry claim)  \\nHere’s one try at explaining this asymmetry (Fischer\\n1985–86: 267–268): moral responsibility for\\nanything—action, consequence, omission—requires a certain\\nkind of control, though what kind of control is needed may vary\\ndepending on the case. Now grant that FSCs show, contra PAP, that\\nregulative control\\n ( )\\n isn’t required for responsibility for actions: guidance control\\nis enough (assuming other conditions on moral responsibility are met).\\nWhen omissions are in play, however, there is no potential object of\\nguidance control, for there is nothing to guide. For omissions, the\\nonly kind of control available to satisfy the control requirement is\\nregulative control. No wonder then that FSCs fail to refute\\nPAP-omissions, as these cases remove the only sort of control one\\ncould have over an omission.  \\n§5.1  \\nThat said, not everyone grants the claimed asymmetry. Both PAP and\\nPAP-omissions should fall together (Frankfurt 1994; Glannon 1995;\\nFischer & Ravizza 1998: ch. 5; or stand together: Swenson 2016).\\nConsider one final FSC, this time bringing the omission inside to the\\n“locus of responsibility”: a choice (or in this case, the\\nlack of one). Luke the indifferent lifeguard fails to save the\\nswimmer, and indeed fails even to try, for reasons entirely his own:\\nhe’s lazy and indifferent. But now add that unbeknownst to Luke,\\nhad he even considered saving the swimmer, he would have been\\nparalyzed by an overwhelming fear, compelling him to remain inactive,\\nnot even choosing or trying to save the swimmer. Here there is no\\noverdetermination, for the fear and the underlying pathology remain\\ndormant in the actual scenario: the only causal factors relevant to\\nthe omission are Luke’s own cognitive states and values, which\\nappear to exert a kind of responsibility-relevant control over his\\nfailure. This looks like a genuine FSC, and, if so, a counterexample\\nto PAP-omissions. (For more on the considerable complexities involved\\nwith PAP-omissions, see McIntyre 1994; Sartorio 2005; Clarke 2014;\\nFischer 2017.)'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': 'Bibliography'}, page_content='Bibliography'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': 'Bibliography'}, page_content='Alvarez, Maria, 2009, “Actions, Thought-Experiments and the\\n‘Principle of Alternate Possibilities’”, , 87(1): 61–81.\\ndoi:10.1080/00048400802215505  \\nAustralasian Journal of Philosophy  \\n–––, 2013, “Agency and Two-Way\\nPowers”, ,\\n113(1pt1): 101–121. doi:10.1111/j.1467-9264.2013.00347.x  \\nProceedings of the Aristotelian Society  \\nAristotle, , in T. Irwin (trans), , second edition, Indianapolis, IN: Hackett\\nPublishing Company, 1999.  \\nNicomachean Ethics  \\nNicomachean Ethics  \\nAudi, Robert, 1974, “Moral Responsibility, Freedom, and\\nCompulsion”, , 11(1):\\n1–14.  \\nAmerican Philosophical Quarterly  \\nAustin, J. L., 1956, “Ifs and Cans”, , 42: 107–132. Reprinted in J. L.\\nAustin, , third edition, J. O. Urmson and\\nG. J. Warnock (eds.), Oxford: Oxford University Press, 1979, pp.\\n205–232.  \\nProceedings\\nof the British Academy  \\nPhilosophical Papers  \\nBeebee, Helen, 2013, ,\\nBasingstoke, UK: Palgrave Macmillan. doi:10.1057/9781137316066  \\nFree Will: An Introduction  \\nBennett, Jonathan, 2008, “Accountability (II)”, in , Michael\\nMcKenna and Paul Russell (eds.), Burlington, VT: Ashgate Publishing\\nCompany, pp. 47–68; reprinted London: Routledge, 2016.  \\nFree Will and Reactive Attitudes: Perspectives on P.F.\\nStrawson’s “Freedom and Resentment”,  \\nBernstein, Mark, 2005, “Can We Ever Be Really, Truly,\\nUltimately, Free?”, ,\\n29(1): 1–12. doi:10.1111/j.1475-4975.2005.00102.x  \\nMidwest Studies in Philosophy  \\nBernstein, Sara, 2015, “The Metaphysics of Omissions: The\\nMetaphysics of Omissions”, , 10(3):\\n208–218. doi:10.1111/phc3.12206  \\nPhilosophy Compass  \\nBlumenfeld, David, 1971, “The Principle of Alternate\\nPossibilities”, , 68(11):\\n339–345. doi:10.2307/2024867  \\nThe Journal of Philosophy  \\nBoxer, K. E., 2013, , Oxford:\\nOxford University Press.\\ndoi:10.1093/acprof:oso/9780199695324.001.0001  \\nRethinking Responsibility  \\nBrent, Michael, 2017, “Agent Causation as a Solution to the\\nProblem of Action”, ,\\n47(5): 656–673. doi:10.1080/00455091.2017.1285643  \\nCanadian Journal of Philosophy  \\nBrink, David O. and Dana K. Nelkin, 2013, “Fairness and the\\nArchitecture of Responsibility”, in (Volume 1), David Shoemaker (ed.), Oxford:\\nOxford University Press, pp. 284–314.\\ndoi:10.1093/acprof:oso/9780199694853.003.0013  \\nOxford Studies in Agency\\nand Responsibility  \\nCampbell, C. A., 1951, “Is ‘Freewill’ a\\nPseudo-Problem?”, , 60(240): 441–465.\\ndoi:10.1093/mind/LX.240.441  \\nMind  \\n–––, 1957, “Has the Self ‘Free\\nWill’?”, in his , London:\\nGeorge Allen and Unwin, pp. 158–179.  \\nOn Selfhood and Godhood  \\nCampbell, Joseph Keim, 1997, “A Compatibilist Theory of\\nAlternative Possibilities”, ,\\n88(3): 319–330.  \\nPhilosophical Studies  \\n–––, 2005, “Compatibilist\\nAlternatives”, , 35(3):\\n387–406. doi:10.1080/00455091.2005.10716595  \\nCanadian Journal of Philosophy  \\nCapes, Justin A., 2012, “Blameworthiness without\\nWrongdoing”, , 93(3):\\n417–437. doi:10.1111/j.1468-0114.2012.01433.x  \\nPacific Philosophical Quarterly  \\nCapes, Justin A. and Philip Swenson, 2017, “Frankfurt Cases:\\nThe Fine-Grained Response Revisited”, , 174(4): 967–981.\\ndoi:10.1007/s11098-016-0726-z  \\nPhilosophical\\nStudies  \\nChisholm, Roderick M., 1966, “Freedom and Action”, in , Keith Lehrer (ed.), New York: Random\\nHouse, pp. 11–44.  \\nFreedom and Determinism  \\n–––, 1967, “He Could Have Done\\nOtherwise”, , 64(13):\\n409–417. doi:10.2307/2024211  \\nThe Journal of Philosophy  \\nClarke, Randolph, 1993, “Toward A Credible Agent-Causal\\nAccount of Free Will”, , 27(2):\\n191–203. doi:10.2307/2215755  \\nNoûs  \\n–––, 2009, “Dispositions, Abilities to\\nAct, and Free Will: The New Dispositionalism”, ,\\n118(470): 323–351. doi:10.1093/mind/fzp034  \\nMind  \\n–––, 2012, “What Is an Omission?”, , 22(1): 127–143.\\ndoi:10.1111/j.1533-6077.2012.00221.x  \\nPhilosophical Issues  \\n–––, 2014, , Oxford: Oxford University Press.\\ndoi:10.1093/acprof:oso/9780199347520.001.0001  \\nOmissions: Agency, Metaphysics,\\nand Responsibility  \\nCoates, D. Justin and Philip Swenson, 2013,\\n“Reasons-Responsiveness and Degrees of Responsibility”, , 165(2): 629–645.\\ndoi:10.1007/s11098-012-9969-5  \\nPhilosophical Studies  \\nCohen, Daniel and Toby Handfield, 2007, “Finking\\nFrankfurt”, , 135(3):\\n363–374. doi:10.1007/s11098-005-5732-5  \\nPhilosophical Studies  \\nCopp, David, 1997, “Defending the Principle of Alternate\\nPossibilities: Blameworthiness and Moral Responsibility”, , 31(4): 441–456.\\ndoi:10.1111/0029-4624.00055  \\nNoûs  \\nDavidson, Donald, 1980, ,\\nOxford: Clarendon Press.  \\nEssays on Actions and Events  \\nDella Rocca, Michael, 1998, “Frankfurt, Fischer and\\nFlickers”, , 32(1): 99–105.\\ndoi:10.1111/0029-4624.00090  \\nNoûs  \\nDennett, Daniel C., 1984a, “I Could Not Have Done\\nOtherwise-So What?”, , 81(10):\\n553–565. doi:10.5840/jphil1984811022  \\nJournal of Philosophy  \\n–––, 1984b, , Cambridge, MA: MIT Press; new edition\\n2015.  \\nElbow Room: The Varieties of\\nFree Will Worth Wanting  \\nDuckworth, Angela L., Tamar Szabó Gendler, and James J.\\nGross, 2014, “Self-Control in School-Age Children”, , 49(3): 199–217.\\ndoi:10.1080/00461520.2014.926225  \\nEducational Psychologist  \\nDuff, Antony, 2009, “Legal and Moral Responsibility”, , 4(6): 978–986.\\ndoi:10.1111/j.1747-9991.2009.00257.x  \\nPhilosophy Compass  \\nEkstrom, Laura W., 2000, , Boulder, CO: Westview Press.  \\nFree Will: A Philosophical\\nStudy  \\n–––, 2002, “Libertarianism and\\nFrankfurt-Style Cases”, in , R. Kane (ed.), Oxford: Oxford University Press, pp.\\n309–322.  \\nThe Oxford Handbook of Free\\nWill  \\nFara, Michael, 2008, “Masked Abilities and\\nCompatibilism”, , 117(468): 843–865.\\ndoi:10.1093/mind/fzn078  \\nMind  \\nFischer, John Martin, 1982, “Responsibility and\\nControl”, , 79(1): 24–40.\\ndoi:10.5840/jphil198279159  \\nJournal of Philosophy  \\n–––, 1985–6, “Responsibility and\\nFailure”, ,\\n86(1): 251–272. doi:10.1093/aristotelian/86.1.251  \\nProceedings of the Aristotelian Society  \\n––– (ed.), 1986, ,\\nIthaca, NY: Cornell University Press.  \\nMoral Responsibility  \\n–––, 1994, , Oxford: Blackwell.  \\nThe Metaphysics of Free\\nWill  \\n–––, 1999, “Recent Work on Moral\\nResponsibility”, , 110(1): 93–139.\\ndoi:10.1086/233206  \\nEthics  \\n–––, 2006, , Oxford: Oxford University Press.  \\nMy Way: Essays on Moral\\nResponsibility  \\n–––, 2008, “Freedom, Foreknowledge, and\\nFrankfurt: A Reply to Vihvelin”, , 38(3): 327–342. doi:10.1353/cjp.0.0021  \\nCanadian Journal of\\nPhilosophy  \\n–––, 2010, “The Frankfurt Cases: The Moral\\nof the Stories”, , 119(3):\\n315–336. doi:10.1215/00318108-2010-002  \\nPhilosophical Review  \\n–––, 2011, “Frankfurt-Type Examples and\\nSemiCompatibilism: New Work”, in , second edition, R. Kane (ed.), Oxford: Oxford University\\nPress, pp. 243–265.  \\nThe Oxford Handbook of Free\\nWill  \\n–––, 2013, “The Deterministic Horn of the\\nDilemma Defence: A Reply to Widerker and Goetz”, , 73(3): 489–496.\\ndoi:10.1093/analys/ant036  \\nAnalysis  \\n–––, 2017, “Responsibility and\\nOmissions”, in , Dana\\nKay Nelkin, and Samuel C. Rickless (eds.), Oxford: Oxford University\\nPress, pp. 148–61.  \\nThe Ethics and Law of Omissions  \\n–––, 2018, “The Freedom Required for Moral\\nResponsibility”, in , David O. Brink,\\nSusan Sauvé Meyer, and Christopher Shields (eds.), Oxford:\\nOxford University Press, pp. 216–33.  \\nVirtue, Happiness, Knowledge: Themes\\nfrom the Work of Gail Fine and Terence Irwin  \\nFischer, John Martin and Mark Ravizza (eds.), 1986, , Ithaca, NY: Cornell University Press.  \\nMoral\\nResponsibility  \\n–––, 1991, “Responsibility and\\nInevitability”, , 101(2): 258–278.\\ndoi:10.1086/293288  \\nEthics  \\n––– (eds.), 1993, , Ithaca, NY: Cornell University Press.  \\nPerspectives on Moral\\nResponsibility  \\n–––, 1998, , Cambridge: Cambridge University\\nPress. doi:10.1017/CBO9780511814594  \\nResponsibility and Control: A\\nTheory of Moral Responsibility  \\nFischer, John Martin and Neal A. Tognazzini, 2009, “The\\nTruth about Tracing”, , 43(3): 531–556.\\ndoi:10.1111/j.1468-0068.2009.00717.x  \\nNoûs  \\nFrankfurt, Harry G., 1969, “Alternate Possibilities and\\nMoral Responsibility”, ,\\n66(23): 829–839. Reprinted in Fischer 1986, pp. 143–52; in\\nFrankfurt 1988, pp. 1–10; and in Widerker and McKenna 2003, pp.\\n17–25. doi:10.2307/2023833  \\nThe Journal of Philosophy  \\n–––, 1971, “Freedom of the Will and the\\nConcept of a Person”, , 68(1):\\n5–20. Reprinted in Frankfurt 1988, pp. 11–25 and in\\nFischer and Ravizza 1986, pp. 65–80. doi:10.2307/2024717  \\nThe Journal of Philosophy  \\n–––, 1982, “The Importance of What We Care\\nAbout”, , 53(2): 257–272. reprinted in\\nFrankfurt 1988, pp. 80–94. doi:10.1007/BF00484902  \\nSynthese  \\n–––, 1983, “What We Are Morally\\nResponsible For”, in L.S. Cauman et al. (eds.), , Indianapolis,\\nIN: Hackett Publishing Company, pp. 321–335. Reprinted in\\nFrankfurt 1988, pp. 95–103 and in Fischer and Ravizza 1993, pp.\\n286–295.  \\nHow Many\\nQuestions? Essays in Honor of Sidney Morgenbesser  \\n–––, 1988, , Cambridge: Cambridge University\\nPress. doi:10.1017/CBO9780511818172  \\nThe Importance of What We Care\\nAbout: Philosophical Essays  \\n–––, 1994, “An Alleged Asymmetry Between\\nActions and Omissions”, , 104(3): 620–623.\\ndoi:10.1086/293633  \\nEthics  \\n–––, 2003, “Some Thoughts Concerning\\nPAP”, in Widerker and McKenna 2003, pp. 339–45.  \\nGinet, Carl, 1990, , Cambridge: Cambridge\\nUniversity Press. doi:10.1017/CBO9781139173780  \\nOn Action  \\n–––, 1996, “In Defense of the Principle of\\nAlternative Possibilities: Why I Don’t Find Frankfurt’s\\nArgument Convincing”, , 10:\\n403–417. reprinted (with a 2001 addendum) in Widerker and\\nMcKenna 2003, pp. 75–90. doi:10.2307/2216254  \\nPhilosophical Perspectives  \\n–––, 2002, “ by Derk Pereboom”, ,\\n6(3): 305–309. doi:10.1023/A:1019569214450  \\nLiving without Free\\nWill  \\nThe Journal of Ethics  \\n–––, 2003, “Libertarianism”, in , M. J. Loux and D. W.\\nZimmerman (eds.), Oxford: Oxford University Press, pp.\\n587–612.  \\nThe Oxford Handbook of Metaphysics  \\nGlannon, Walter, 1995, “Responsibility and the Principle of\\nPossible Action”, , 92(5):\\n261–274. doi:10.2307/2940918  \\nJournal of Philosophy  \\nGlatz, Richard M., 2008, “The (near) Necessity of Alternate\\nPossibilities for Moral Responsibility”, , 139(2): 257–272.\\ndoi:10.1007/s11098-007-9116-x  \\nPhilosophical\\nStudies  \\nGlover, Jonathan, 1970, , London: Routledge\\n& Kegan Paul.  \\nResponsibility  \\nGoetz, Stewart, 1999, “Stumping For Widerker”, , 16(1): 83–89.\\ndoi:10.5840/faithphil19991619  \\nFaith and Philosophy  \\n–––, 2002, “Alternative Frankfurt-Style\\nCounterexamples to the Principle of Alternative Possibilities”, , 83(2): 131–147.\\ndoi:10.1111/1468-0114.00139  \\nPacific Philosophical Quarterly  \\n–––, 2005, “Frankfurt-Style\\nCounterexamples and Begging the Question”, , 29(1): 83–105.\\ndoi:10.1111/j.1475-4975.2005.00107.x  \\nMidwest Studies\\nin Philosophy  \\nGoldman, Alvin I., 1970, ,\\nEnglewood Cliffs, NJ: Prentice Hall.  \\nA Theory of Human Action  \\nGraham, George, 2013, , second edition, London:\\nRoutledge; first edition, 2010.  \\nThe Disordered Mind: An Introduction to\\nPhilosophy of Mind and Mental Illness  \\nGriffith, Meghan, 2013, , London:\\nRoutledge.  \\nFree Will: the Basics  \\nHaji, Ishtiyaque, 2019, “A Paradox Concerning Frankfurt\\nExamples”, , 196(1): 87–103.\\ndoi:10.1007/s11229-016-1025-0  \\nSynthese  \\nHaji, Ishtiyaque and Michael McKenna, 2004, “Dialectical\\nDelicacies in the Debate About Freedom and Alternative\\nPossibilities”, , 101(6):\\n299–314. doi:10.5840/jphil2004101616  \\nJournal of Philosophy  \\nHankinson, R. J., 2014, “Efficient Causation in the Stoic\\nTradition”, in , Tad M.\\nSchmaltz (ed.), Oxford: Oxford University Press, pp. 54–82.\\ndoi:10.1093/acprof:oso/9780199782185.003.0004  \\nEfficient Causation: a History  \\nHawthorne, John, 2001, “Freedom in Context”, , 104(1): 63–79.\\ndoi:10.1023/A:1010398805497  \\nPhilosophical Studies  \\nHeinaman, Robert, 1986, “Incompatibilism without the\\nPrinciple of Alternative Possibilities”, , 64(3): 266–276.\\ndoi:10.1080/00048408612342481  \\nAustralasian\\nJournal of Philosophy  \\nHorgan, Terence, 1979, “‘Could’, Possible\\nWorlds, and Moral Responsibility”, , 17(3): 345–358.\\ndoi:10.1111/j.2041-6962.1979.tb00250.x  \\nThe Southern Journal of\\nPhilosophy  \\nHorgan, Terence, John Tienson, and Geroge Graham, 2003, “The\\nPhenomenology of First-Person Agency”, in S. Walter and H.\\nHeckmann (eds.), , Exeter: Imprint Academic, pp.\\n323–40.  \\nPhysicalism and Mental Causation: The Metaphysics\\nof Mind and Action  \\nHume, David, 1748 [1999], , T. L. Beauchamp (ed.), Oxford: Oxford University\\nPress.  \\nAn Enquiry Concerning Human\\nUnderstanding  \\nHunt, David P., 1996, “Frankfurt Counterexamples: Some\\nComments on the Widerker-Fischer Debate”, , 13(3): 395–401.\\ndoi:10.5840/faithphil199613331  \\nFaith and\\nPhilosophy  \\n–––, 1999, “On Augustine’s Way\\nOut:”, , 16(1): 3–26.\\ndoi:10.5840/faithphil19991612  \\nFaith and Philosophy  \\n–––, 2000, “Moral Responsibility and\\nUnavoidable Action”, , 97(2):\\n195–227. doi:10.1023/A:1018331202006  \\nPhilosophical Studies  \\n–––, 2003, “Freedom, Foreknowledge and\\nFrankfurt”, in Widerker and McKenna 2003,pp. 159–183.  \\n–––, 2005, “Moral Responsibility and\\nBuffered Alternatives”, ,\\n29(1): 126–145. doi:10.1111/j.1475-4975.2005.00109.x  \\nMidwest Studies in Philosophy  \\nInwood, Brad and L. P. Gerson (trans.), 1988, , Indianapolis: Hackett\\nPublishing Company.  \\nHellenistic\\nPhilosophy: Introductory Readings  \\nIrwin, Terence (ed.), 1999, , Oxford:\\nOxford University Press.  \\nClassical Philosophy  \\nKane, Robert, 1985, , Albany, NY:\\nState University of New York Press.  \\nFree Will and Values  \\n–––, 1996, , Oxford: Oxford University Press.\\ndoi:10.1093/0195126564.001.0001  \\nThe Significance of Free\\nWill  \\n–––, 2002, “Responsibility, Reactive\\nAttitudes and Free Will: Reflections on Wallace’s Theory”, , 64(3):\\n693–698. doi:10.1111/j.1933-1592.2002.tb00175.x  \\nPhilosophy and Phenomenological Research  \\n–––, 2003, “Responsibility, Indeterminism\\nand Frankfurt-Style Cases: A Reply to Mele and Robb”, in\\nWiderker and McKenna 2003, pp. 91–105.  \\n–––, 2007, “Response to Fischer, Pereboom,\\nand Vargas”, in R. Fischer, J. M., R. Kane, D. Pereboom, and M.\\nVargas, , Oxford: Blackwell\\nPublishing, pp. 166–183.  \\nFour Views on Free Will  \\nKant, Immanuel, 1785, , L. White Beck (trans), New York: McMillan Publishing\\nCompany, 2nd edition, 1985.  \\nFoundations of the Metaphysics of\\nMorals  \\nKelly, Erin, 2017, “Free Will and Criminal Law”, in\\nTimpe et al. 2017, pp. 577–589.  \\nKlein, Martha, 1990, , Oxford: Clarendon Press.  \\nDeterminism, Blameworthiness and\\nDeprivation  \\nKrug, Kristine, C. Daniel Salzman, and Scott Waddell, 2015,\\n“Understanding the Brain by Controlling Neural Activity”, , 370(1677): 20140201. doi:10.1098/rstb.2014.0201  \\nPhilosophical Transactions of the Royal Society B: Biological\\nSciences  \\nLamb, James W., 1993, “Evaluative Compatibilism and the\\nPrinciple of Alternate Possibilities”, , 90(10): 517–527.  \\nJournal of\\nPhilosophy  \\nLevy, Neil, 2006, “Autonomy and Addiction”, , 36(3): 427–447.\\ndoi:10.1353/cjp.2006.0018  \\nCanadian Journal of Philosophy  \\nLewis, David, 1976, “The Paradoxes of Time Travel”, , 13(2): 145–152.  \\nAmerican Philosophical Quarterly  \\nLocke, Don, 1974, “Natural Powers and Human\\nAbilities”, ,\\n74(1): 171–187. doi:10.1093/aristotelian/74.1.171  \\nProceedings of the Aristotelian Society  \\nLocke, John, 1689 [1996], , in K. P. Winkler (ed.), , Indianapolis: Hackett Publishing Company,\\n1996.  \\nAn Essay Concerning Human\\nUnderstanding  \\nAn Essay Concerning\\nHuman Understanding  \\nLowe, E. J., 1995, , London: Routledge.  \\nRoutledge Philosophy Guidebook to Locke on\\nHuman Understanding  \\nMcIntyre, Alison, 1994, “Compatibilists Could Have Done\\nOtherwise: Responsibility and Negative Agency”, , 103(3): 453–488.\\ndoi:10.2307/2185789  \\nPhilosophical Review  \\nMcKenna, Michael S., 1998, “Does Strong Compatibilism\\nSurvive Frankfurt Counter-Examples?”, , 91(3): 259–264. doi:10.1023/A:1004232828687  \\nPhilosophical\\nStudies  \\n–––, 2003, “Robustness, Control, and the\\nDemand for Morally Significant Alternatives: Frankfurt Examples with\\nOodles and Oodles of Alternatives”, in Widerker and McKenna\\n2003, pp. 201–217.  \\n–––, 2008, “Saying Good-Bye to the Direct\\nArgument the Right Way”, , 117(3):\\n349–383. doi:10.1215/00318108-2008-002  \\nPhilosophical Review  \\nMcKenna, Michael S. and Brandon Warmke, 2017, “Does\\nSituationism Threaten Free Will and Moral Responsibility?”, , 14(6): 698–733.\\ndoi:10.1163/17455243-46810068  \\nJournal of Moral Philosophy  \\nMarkosian, Ned, 2012, “Agent Causation as the Solution to\\nAll the Compatibilist’s Problems”, , 157(3): 383–398.\\ndoi:10.1007/s11098-010-9654-5  \\nPhilosophical\\nStudies  \\nMartinich, A. P., 2005, , London:\\nRoutledge.  \\nHobbes: A Biography  \\nMele, Alfred R., 1995, , Oxford: Oxford\\nUniversity Press. doi:10.1093/0195150430.001.0001  \\nAutonomous Agents  \\n–––, 1996, “Soft Libertarianism and\\nFrankfurt-Style Scenarios”, ,\\n24(2): 123–141. doi:10.5840/philtopics199624220  \\nPhilosophical Topics  \\n–––, 1999, “Ultimate Responsibility and\\nDumb Luck”, , 16(2):\\n274–293. doi:10.1017/S0265052500002478  \\nSocial Philosophy and Policy  \\n–––, 2003, “Agents’\\nAbilities”, , 37(3): 447–470.\\ndoi:10.1111/1468-0068.00446  \\nNoûs  \\n–––, 2010, “Moral Responsibility for\\nActions: Epistemic and Freedom Conditions”, , 13(2): 101–111.\\ndoi:10.1080/13869790903494556  \\nPhilosophical\\nExplorations  \\nMele, Alfred R. and David Robb, 1998, “Rescuing\\nFrankfurt-Style Cases”, , 107(1):\\n97–112. doi:10.2307/2998316  \\nPhilosophical Review  \\nMorse, Stephen J., 2011, “Addiction and Criminal\\nResponsibility”, in J. Poland and G. Graham (eds.), , Cambridge, MA: MIT Press, pp.\\n159–99.  \\nAddiction and Responsibility  \\n–––, 2013, “Common Criminal Law\\nCompatibilism”, in Nicole A. Vincent (ed.), , Oxford: Oxford University Press, pp.\\n27–52. doi:10.1093/acprof:oso/9780199925605.003.0002  \\nNeuroscience and\\nLegal Responsibility  \\nNagel, Thomas, 1986, , Oxford:\\nOxford University Press.  \\nThe View from Nowhere  \\nNaylor, Margery Bedford, 1984, “Frankfurt on the Principle\\nof Alternate Possibilities”, ,\\n46(2): 249–258. doi:10.1007/BF00373108  \\nPhilosophical Studies  \\nNelkin, Dana Kay, 2005, “Freedom, Responsibility and the\\nChallenge of Situationism”, , 29(1): 181–206.\\ndoi:10.1111/j.1475-4975.2005.00112.x  \\nMidwest Studies in\\nPhilosophy  \\n–––, 2011, , Oxford: Oxford University Press.\\ndoi:10.1093/acprof:oso/9780199608560.001.0001  \\nMaking Sense of Freedom and\\nResponsibility  \\n–––, 2016, “Difficulty and Degrees of\\nMoral Praiseworthiness and Blameworthiness: Difficulty and Degrees of\\nMoral Praiseworthiness and Blameworthiness”, , 50(2): 356–378. doi:10.1111/nous.12079  \\nNoûs  \\nNichols, Shaun, 2015, , Oxford: Oxford University Press.\\ndoi:10.1093/acprof:oso/9780199291847.001.0001  \\nBound: Essays on Free Will and\\nResponsibility  \\nO’Connor, Timothy, 2000, , Oxford: Oxford University Press.\\ndoi:10.1093/019515374X.001.0001  \\nPersons and Causes: The\\nMetaphysics of Free Will  \\n–––, 2005, “Freedom with a Human\\nFace”, , 29(1):\\n207–227. doi:10.1111/j.1475-4975.2005.00113.x  \\nMidwest Studies in Philosophy  \\nOtsuka, Michael, 1998, “Incompatibilism and the Avoidability\\nof Blame”, , 108(4): 685–701.\\ndoi:10.1086/233847  \\nEthics  \\nPalmer, David, 2014, “Deterministic Frankfurt Cases”, , 191(16): 3847–3864.\\ndoi:10.1007/s11229-014-0500-8  \\nSynthese  \\nPasnau, Robert, 2003, “Human Nature”, in , A. S. McGrade (ed.),\\nCambridge: Cambridge University Press, pp. 208–230.\\ndoi:10.1017/CCOL0521806038.010  \\nThe\\nCambridge Companion to Medieval Philosophy  \\nPereboom, Derk, 1995, “Determinism al Dente”, , 29(1): 21–45. doi:10.2307/2215725  \\nNoûs  \\n–––, 2001, ,\\nCambridge: Cambridge University Press.\\ndoi:10.1017/CBO9780511498824  \\nLiving without Free Will  \\nPerry, John, 2010, “Wretched Subterfuge: A Defense of the\\nCompatibilism of Freedom and Natural Causation”, , 84(2):\\n93–113.  \\nProceedings\\nand Addresses of the American Philosophical Association  \\nPickard, Hanna, 2015, “Psychopathology and the Ability to Do\\nOtherwise”, ,\\n90(1): 135–163. doi:10.1111/phpr.12025  \\nPhilosophy and Phenomenological Research  \\nReid, Thomas, 1788, ,\\nin K. Haakonssen and J. A. Harris (eds.), , University Park, PA: Pennsylvania State University\\nPress, 2010.  \\nEssays on the Active Powers of Man  \\nEssays on the Active\\nPowers of Man  \\nRobinson, Michael, 2014, “The Limits of Limited-Blockage\\nFrankfurt-Style Cases”, , 169(3):\\n429–446. doi:10.1007/s11098-013-0190-y  \\nPhilosophical Studies  \\nRowe, William L., 1987, “Two Concepts of Freedom”, , 61(1): 43–64.  \\nProceedings and Addresses of the American Philosophical\\nAssociation  \\n–––, 1989, “Causing and Being Responsible\\nfor What Is Inevitable”, , 26(2): 153–159; reprinted in Fischer and Ravizza\\n1993, pp. 310–321.  \\nAmerican Philosophical\\nQuarterly  \\n–––, 2006, “Free Will, Moral\\nResponsibility, and the Problem of ‘OOMPH’”, , 10(3): 295–313.\\ndoi:10.1007/s10892-005-5779-8  \\nThe\\nJournal of Ethics  \\nSartorio, Carolina, 2005, “A New Asymmetry Between Actions\\nAnd Omissions*”, , 39(3): 460–482.\\ndoi:10.1111/j.0029-4624.2005.00509.x  \\nNoûs  \\n–––, 2012, “Causation and Freedom”, , 109(11): 629–651.\\ndoi:10.5840/jphil20121091137  \\nJournal of Philosophy  \\n–––, 2017, “Frankfurt-Style\\nExamples”, in Timpe et al. 2017, pp. 179–190.  \\nSauvé Meyer, Susan, 1999, “Fate, Fatalism, and Agency\\nin Stoicism”, , 16(2):\\n250–273. doi:10.1017/S0265052500002466  \\nSocial Philosophy and Policy  \\nSchroeder, Timothy, 2005, “Moral Responsibility and Tourette\\nSyndrome”, ,\\n71(1): 106–123. doi:10.1111/j.1933-1592.2005.tb00432.x  \\nPhilosophy and Phenomenological Research  \\nShabo, Seth, 2011, “Agency without Avoidability: Defusing a\\nNew Threat to Frankfurt’s Counterexample Strategy”, , 41(4): 505–522.\\ndoi:10.1353/cjp.2011.0042  \\nCanadian Journal of Philosophy  \\n–––, 2015, “More Trouble with\\nTracing”, , 80(5): 987–1011.\\ndoi:10.1007/s10670-014-9693-y  \\nErkenntnis  \\nSinnott-Armstrong, Walter, 2012, “A Case Study In\\nNeuroscience and Responsibility”, , 52(1):\\n194–211.  \\nNomos  \\nSmith, Michael, 2003, “Rational Capacities, or: How to\\nDistinguish Recklessness, Weakness, and Compulsion”, in , Sarah Stroud\\nand Christine Tappolet (eds.), Oxford: Oxford University Press, pp.\\n17–38. doi:10.1093/0199257361.003.0002  \\nWeakness of Will and Practical Irrationality  \\nSommers, Tamler, 2010, “Experimental Philosophy and Free\\nWill”, , 5(2): 199–212.\\ndoi:10.1111/j.1747-9991.2009.00273.x  \\nPhilosophy Compass  \\nSorabji, Richard, 1980, , Ithaca, NY: Cornell\\nUniversity Press.  \\nNecessity, Cause, and Blame:\\nPerspectives on Aristotle’s Theory  \\nSpeak, Daniel, 2002, “Fanning the Flickers of\\nFreedom”, , 39(1):\\n91–105.  \\nAmerican Philosophical Quarterly  \\n–––, 2005, “PAPistry: Another\\nDefense”, , 29(1):\\n262–268. doi:10.1111/j.1475-4975.2005.00116.x  \\nMidwest Studies in Philosophy  \\n–––, 2007, “The Impertinence of\\nFrankfurt-Style Argument”, ,\\n57(226): 76–95. doi:10.1111/j.1467-9213.2007.470.x  \\nThe Philosophical Quarterly  \\nSteward, Helen, 2009, “Fairness, Agency and the Flicker of\\nFreedom”, , 43(1): 64–93.\\ndoi:10.1111/j.1468-0068.2008.01696.x  \\nNoûs  \\n–––, 2012a, “The Metaphysical\\nPresuppositions of Moral Responsibility”, , 16(2): 241–271. doi:10.1007/s10892-012-9127-5  \\nThe Journal of\\nEthics  \\n–––, 2012b, ,\\nOxford: Oxford University Press.\\ndoi:10.1093/acprof:oso/9780199552054.001.0001  \\nA Metaphysics for Freedom  \\nStrawson, Galen, 1994, “The Impossibility of Moral\\nResponsibility”, , 75(1–2):\\n5–24. doi:10.1007/BF00989879  \\nPhilosophical Studies  \\nStrawson, Peter Frederick, 1962, “Freedom and\\nResentment”, , 48:\\n1–25. Reprinted in his , London: Routledge, 2008, pp. 1–28; and in Fischer\\nand Ravizza 1993, pp. 45–66.  \\nProceedings of the British Academy  \\nFreedom and Resentment and Other\\nEssays  \\nStump, Eleonore, 1990, “Intellect, Will, and the Principle\\nof Alternative Possibilities”, in M. D. Beaty (ed.), , Notre Dame,\\nIN: University of Notre Dame Press, pp. 254–85; Reprinted in\\nFischer and Ravizza 1993, pp. 237–262.  \\nChristian Theism and the Problems of Philosophy  \\n–––, 1996, “Libertarian Freedom and the\\nPrinciple of Alternative Possibilities”, in J. Jordan and D.\\nHoward-Snyder (eds.), , Lanham: Rowman and Littlefield, pp.\\n73–88.  \\nFaith, Freedom, and Rationality: Philosophy\\nof Religion Today  \\n–––, 1997, “Aquinas’s Account of\\nFreedom: Intellect and Will”, , 80(4):\\n576–597. doi:10.5840/monist199780429  \\nMonist  \\n–––, 1999, “Dust, Determinism, and\\nFrankfurt: A Reply to Goetz”, ,\\n16(3): 413–422. doi:10.5840/faithphil199916337  \\nFaith and Philosophy  \\n–––, 2000, “The Direct Argument for\\nIncompatibilism”, , 61(2): 459–466. doi:10.2307/2653663  \\nPhilosophy and Phenomenological\\nResearch  \\nSwenson, Philip, 2016, “The Frankfurt Cases and\\nResponsibility for Omissions”, , 66(264): 579–595. doi:10.1093/pq/pqv127  \\nThe Philosophical\\nQuarterly  \\nTaylor, Richard, 1966, , Englewood\\nCliffs, NJ: Prentice-Hall.  \\nAction and Purpose  \\nTimpe, Kevin, 2013, , second edition, London: Bloomsbury Academic.  \\nFree Will: Sourcehood and Its\\nAlternatives  \\nTimpe, Kevin, Meghan Griffith, and Neil Levy (eds.), 2017, , London: Routledge.  \\nThe\\nRoutledge Companion to Free Will  \\nUnger, Peter, 1984, ,\\nMinneapolis: University of Minnesota Press; reprinted Oxford: Oxford\\nUniversity Press, 2002.  \\nPhilosophical Relativity  \\nVargas, Manuel, 2005, “The Trouble with Tracing”, , 29(1): 269–291.\\ndoi:10.1111/j.1475-4975.2005.00117.x  \\nMidwest Studies in Philosophy  \\nvan Inwagen, Peter, 1978, “Ability and\\nResponsibility”, , 87(2):\\n201–224. Reprinted in Fischer 1986, pp. 153–173; and in\\nvan Inwagen 1983, pp. 162–182. doi:10.2307/2184752  \\nPhilosophical Review  \\n–––, 1980, “The Incompatibility of\\nResponsibility and Determinism”, in M. Bradie and M. Brand\\n(eds.), , vol. 2,\\nBowling Green, OH: Bowling Green State University, pp.30–36;\\nreprinted in Fischer and Ravizza 1986, pp. 241–249.  \\nBowling Green Studies in Applied Philosophy  \\n–––, 1983, ,\\nOxford: Clarendon Press.  \\nAn Essay on Free Will  \\nVihvelin, Kadri, 2000, “Freedom, Foreknowledge, and the\\nPrinciple of Alternate Possibilities”, , 30(1): 1–23.\\ndoi:10.1080/00455091.2000.10717523  \\nCanadian Journal of\\nPhilosophy  \\n–––, 2004, “Free Will Demystified: A\\nDispositional Account”, , 32(1):\\n427–450. doi:10.5840/philtopics2004321/211  \\nPhilosophical Topics  \\n–––, 2008, “Foreknowledge, Frankfurt, and\\nAbility to Do Otherwise: A Reply to Fischer”, , 38(3): 343–371.\\ndoi:10.1353/cjp.0.0022  \\nCanadian\\nJournal of Philosophy  \\nVranas, Peter B. M., 2007, “I Ought, Therefore I Can”, , 136(2): 167–216.\\ndoi:10.1007/s11098-007-9071-6  \\nPhilosophical Studies  \\nWallace, R. Jay, 1994, , Cambridge, MA: Harvard University Press.  \\nResponsibility and the Moral\\nSentiments  \\nWarfield, Ted A., 2007, “Metaphysical Compatibilism’s\\nAppropriation of Frankfurt”, in (Volume 3), Dean Zimmerman (ed.), pp.\\n283–295.  \\nOxford Studies in\\nMetaphysics  \\nWatson, Gary, 1996, “Two Faces of Responsibility”, , 24(2): 227–248.\\ndoi:10.5840/philtopics199624222  \\nPhilosophical Topics  \\nWhittle, Ann, 2010, “Dispositional Abilities”, , 10(12): 23 pages.\\n [ ]  \\nPhilosopher’s Imprint  \\nWhittle 2010 available online  \\n–––, 2016, “Ceteris Paribus, I Could Have\\nDone Otherwise”, , 92(1): 73–85. doi:10.1111/phpr.12111  \\nPhilosophy and Phenomenological\\nResearch  \\nWiderker, David, 1991, “Frankfurt on ‘Ought Implies\\nCan’ and Alternative Possibilities”, ,\\n51(4): 222–224. doi:10.1093/analys/51.4.222  \\nAnalysis  \\n–––, 1995, “Libertarianism and\\nFrankfurt’s Attack on the Principle of Alternative\\nPossibilities”, , 104(2):\\n247–261. doi:10.2307/2185979  \\nPhilosophical Review  \\n–––, 2000, “Theological Fatalism and\\nFrankfurt Counterexamples to the Principle of Alternative\\nPossibilities”, , 17(2):\\n249–254. doi:10.5840/faithphil200017213  \\nFaith and Philosophy  \\n–––, 2003, “Blameworthiness and\\nFrankfurt’s Argument Against the Principle of Alternative\\nPossibilities”, in Widerker and McKenna 2003, pp.\\n53–73.  \\n–––, 2006, “Libertarianism and the\\nPhilosophical Significance of Frankfurt Scenarios”, , 103(4): 163–187.\\ndoi:10.5840/jphil2006103433  \\nJournal\\nof Philosophy  \\nWiderker, David and Stewart Goetz, 2013, “Fischer against\\nthe Dilemma Defence: The Defence Prevails”, ,\\n73(2): 283–295. doi:10.1093/analys/ant013  \\nAnalysis  \\nWiderker, David and Michael McKenna (eds.), 2003, , Burlington, VT: Ashgate Publishing\\nCompany. doi:10.4324/9781315199924  \\nMoral\\nResponsibility and Alternative Possibilities: Essays on the Importance\\nof Alternative Possibilities  \\nWiggins, David, 1973, “Towards a Reasonable\\nLibertarianism”, in T. Honderich (ed.), , London: Routledge and Kegan Paul; reprinted in his , Oxford: Oxford University Press, 3rd\\nedition, pp. 269–302.  \\nEssays on Freedom of\\nAction  \\nNeeds, Values, Truth  \\nWolf, Susan, 1980, “Asymmetrical Freedom”, , 77(3): 151–166. Reprinted in Fischer\\n1986, pp. 225–240. doi:10.2307/2025667  \\nThe\\nJournal of Philosophy  \\nWyma, Keith D., 1997, “Moral Responsibility and Leeway for\\nAction”, , 34(1):\\n57–70.  \\nAmerican Philosophical Quarterly  \\nYaffe, Gideon, 1999, “‘Ought’ Implies\\n‘Can’ and the Principle of Alternate Possibilities”, , 59(3): 218–222.\\ndoi:10.1093/analys/59.3.218  \\nAnalysis  \\n–––, 2005, “More on ‘Ought’\\nImplies ‘Can’ and the Principle of Alternate\\nPossibilities”, , 29(1):\\n307–312. doi:10.1111/j.1475-4975.2005.00119.x  \\nMidwest Studies in Philosophy  \\nZagzebski, Linda Trinkaus, 1991, , Oxford: Oxford University Press.\\ndoi:10.1093/acprof:oso/9780195107630.001.0001  \\nThe Dilemma of Freedom and\\nForeknowledge  \\n–––, 2000, “Does Libertarian Freedom\\nRequire Alternate Possibilities?”, , 14: 231–248.\\ndoi:10.1111/0029-4624.34.s14.12  \\nPhilosophical\\nPerspectives  \\nZimmerman, Michael J., 1988, , Totowa, NJ: Rowman and Littlefield.  \\nAn Essay on Moral\\nResponsibility'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': 'Academic Tools'}, page_content='Academic Tools'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': 'Academic Tools'}, page_content='.  \\nHow to cite this entry  \\nat the .  \\nPreview the PDF version of this entry  \\nFriends of the SEP Society  \\nat the Internet Philosophy Ontology Project (InPhO).  \\nLook up topics and thinkers related to this entry  \\nat , with links to its database.  \\nEnhanced bibliography for this entry  \\nPhilPapers'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': 'Other Internet Resources'}, page_content='Other Internet Resources'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': 'Other Internet Resources'}, page_content='[Please contact the author with suggestions.]'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': 'Related Entries'}, page_content='Related Entries'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': 'Related Entries'}, page_content='| | | | | | | | | | | |  \\nabilities  \\naction  \\nblame  \\ncompatibilism  \\ndeterminism: causal  \\nfatalism  \\nfree will  \\nfree will: divine foreknowledge and  \\nincompatibilism: (nondeterministic) theories of free will  \\nincompatibilism: arguments for  \\nluck: moral  \\nmoral responsibility  \\nskepticism: about moral responsibility'),\n",
       " Document(metadata={'Header 1': 'Moral Responsibility and the Principle of Alternative Possibilities', 'Header 2': 'Related Entries', 'Header 3': 'Acknowledgments'}, page_content='Acknowledgments'),\n",
       " Document(metadata={}, page_content='I am grateful to the SEP editorial staff and to Derk Pereboom for\\nhelpful advice on preparing this entry.  \\nby < >  \\nCopyright © 2020  \\nDavid Robb  \\ndarobb davidson edu  \\n@  \\n.  \\nOpen access to the SEP is made possible by a world-wide funding initiative. The Encyclopedia Now Needs Your Support Please Read How You Can Help Keep the Encyclopedia Free  \\nEnd footer menu End mirrors End site credits'),\n",
       " Document(metadata={'Header 4': 'Browse'}, page_content='Browse'),\n",
       " Document(metadata={'Header 4': 'Browse'}, page_content=\"Table of Contents  \\nWhat's New  \\nRandom Entry  \\nChronological  \\nArchives\"),\n",
       " Document(metadata={'Header 4': 'About'}, page_content='About'),\n",
       " Document(metadata={'Header 4': 'About'}, page_content='Editorial Information  \\nAbout the SEP  \\nEditorial Board  \\nHow to Cite the SEP  \\nSpecial Characters  \\nAdvanced Tools  \\nAccessibility  \\nContact'),\n",
       " Document(metadata={'Header 4': 'Support SEP'}, page_content='Support SEP'),\n",
       " Document(metadata={'Header 4': 'Support SEP'}, page_content='Support the SEP  \\nPDFs for SEP Friends  \\nMake a Donation  \\nSEPIA for Libraries'),\n",
       " Document(metadata={'Header 4': 'Mirror Sites'}, page_content='Mirror Sites'),\n",
       " Document(metadata={}, page_content=\"View this site from another server:  \\nUSA (Main Site)  \\nPhilosophy, Stanford University  \\nInfo about mirror sites  \\nThe Stanford Encyclopedia of Philosophy is by , Department of Philosophy, Stanford University  \\ncopyright © 2023  \\nThe Metaphysics Research Lab  \\nLibrary of Congress Catalog Data: ISSN 1095-5054  \\n$('.dropdown-toggle').dropdown();\")]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HTML Splitter Using URL\n",
    "url = \"https://plato.stanford.edu/entries/alternative-possibilities/\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\")\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_headers_splits = html_splitter.split_text_from_url(url)\n",
    "html_headers_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba592f23",
   "metadata": {},
   "source": [
    "### **4. Split JSON Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb924630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openapi': '3.1.0',\n",
       " 'info': {'title': 'LangSmith',\n",
       "  'description': 'The LangSmith API is used to programmatically create and manage LangSmith resources.\\n\\n## Host\\nhttps://api.smith.langchain.com\\n\\n## Authentication\\nTo authenticate with the LangSmith API, set the `X-Api-Key` header\\nto a valid [LangSmith API key](https://docs.langchain.com/langsmith/create-account-api-key#create-an-api-key).\\n\\n',\n",
       "  'version': '0.1.0'},\n",
       " 'paths': {'/api/v1/audit-logs': {'get': {'tags': ['audit-logs'],\n",
       "    'summary': 'Get Audit Logs',\n",
       "    'description': \"Retrieve audit log records for the authenticated user's organization in OCSF format.\\n\\nRequires both start_time and end_time parameters to filter logs within a date range.\\nSupports cursor-based pagination.\\n\\nReturns results in OCSF API Activity (Class UID: 6003) format,\\nwhich is compatible with security monitoring and SIEM tools.\\nReference: https://schema.ocsf.io/1.7.0/classes/api_activity\",\n",
       "    'operationId': 'get_audit_logs_api_v1_audit_logs_get',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'description': 'Number of items to return',\n",
       "       'default': 10,\n",
       "       'title': 'Limit'},\n",
       "      'description': 'Number of items to return'},\n",
       "     {'name': 'cursor',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'description': 'Cursor for pagination (base64-encoded JSON string)',\n",
       "       'title': 'Cursor'},\n",
       "      'description': 'Cursor for pagination (base64-encoded JSON string)'},\n",
       "     {'name': 'workspace_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'description': 'Filter by workspace ID',\n",
       "       'title': 'Workspace Id'},\n",
       "      'description': 'Filter by workspace ID'},\n",
       "     {'name': 'start_time',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'date-time',\n",
       "       'description': 'Start datetime (inclusive) in ISO 8601 format',\n",
       "       'title': 'Start Time'},\n",
       "      'description': 'Start datetime (inclusive) in ISO 8601 format'},\n",
       "     {'name': 'end_time',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'date-time',\n",
       "       'description': 'End datetime (inclusive) in ISO 8601 format',\n",
       "       'title': 'End Time'},\n",
       "      'description': 'End datetime (inclusive) in ISO 8601 format'}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ListAuditLogsOCSFResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/sessions/{session_id}/dashboard': {'post': {'tags': ['tracer-sessions'],\n",
       "    'summary': 'Get Tracing Project Prebuilt Dashboard',\n",
       "    'description': 'Get a prebuilt dashboard for a tracing project.',\n",
       "    'operationId': 'get_tracing_project_prebuilt_dashboard_api_v1_sessions__session_id__dashboard_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'accept',\n",
       "      'in': 'header',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Accept'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsSectionRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsSection'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/sessions/{session_id}': {'get': {'tags': ['tracer-sessions'],\n",
       "    'summary': 'Read Tracer Session',\n",
       "    'description': 'Get a specific session.',\n",
       "    'operationId': 'read_tracer_session_api_v1_sessions__session_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'include_stats',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Include Stats'}},\n",
       "     {'name': 'stats_start_time',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Stats Start Time'}},\n",
       "     {'name': 'accept',\n",
       "      'in': 'header',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Accept'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TracerSession'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['tracer-sessions'],\n",
       "    'summary': 'Update Tracer Session',\n",
       "    'description': 'Create a new session.',\n",
       "    'operationId': 'update_tracer_session_api_v1_sessions__session_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TracerSessionUpdate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TracerSessionWithoutVirtualFields'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['tracer-sessions'],\n",
       "    'summary': 'Delete Tracer Session',\n",
       "    'description': 'Delete a specific session.',\n",
       "    'operationId': 'delete_tracer_session_api_v1_sessions__session_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/sessions': {'get': {'tags': ['tracer-sessions'],\n",
       "    'summary': 'Read Tracer Sessions',\n",
       "    'description': 'Get all sessions.',\n",
       "    'operationId': 'read_tracer_sessions_api_v1_sessions_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'reference_free',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Reference Free'}},\n",
       "     {'name': 'reference_dataset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Reference Dataset'}},\n",
       "     {'name': 'id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Id'}},\n",
       "     {'name': 'name',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name'}},\n",
       "     {'name': 'name_contains',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name Contains'}},\n",
       "     {'name': 'dataset_version',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Dataset Version'}},\n",
       "     {'name': 'sort_by',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'$ref': '#/components/schemas/SessionSortableColumns',\n",
       "       'default': 'start_time'}},\n",
       "     {'name': 'sort_by_desc',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean', 'default': True, 'title': 'Sort By Desc'}},\n",
       "     {'name': 'metadata',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Metadata'}},\n",
       "     {'name': 'sort_by_feedback_key',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Sort By Feedback Key'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'tag_value_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Tag Value Id'}},\n",
       "     {'name': 'facets',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean', 'default': False, 'title': 'Facets'}},\n",
       "     {'name': 'filter',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Filter'}},\n",
       "     {'name': 'include_stats',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Include Stats'}},\n",
       "     {'name': 'use_approx_stats',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Use Approx Stats'}},\n",
       "     {'name': 'stats_start_time',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Stats Start Time'}},\n",
       "     {'name': 'accept',\n",
       "      'in': 'header',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Accept'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/TracerSession'},\n",
       "         'title': 'Response Read Tracer Sessions Api V1 Sessions Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['tracer-sessions'],\n",
       "    'summary': 'Create Tracer Session',\n",
       "    'description': 'Create a new session.',\n",
       "    'operationId': 'create_tracer_session_api_v1_sessions_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'upsert',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean', 'default': False, 'title': 'Upsert'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TracerSessionCreate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TracerSessionWithoutVirtualFields'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['tracer-sessions'],\n",
       "    'summary': 'Delete Tracer Sessions',\n",
       "    'description': 'Delete a specific session.',\n",
       "    'operationId': 'delete_tracer_sessions_api_v1_sessions_delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_ids',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'array',\n",
       "       'items': {'type': 'string', 'format': 'uuid'},\n",
       "       'title': 'Session Ids'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/sessions/{session_id}/metadata': {'get': {'tags': ['tracer-sessions'],\n",
       "    'summary': 'Read Tracer Sessions Runs Metadata',\n",
       "    'description': 'Given a session, a number K, and (optionally) a list of metadata keys, return the top K values for each key.',\n",
       "    'operationId': 'read_tracer_sessions_runs_metadata_api_v1_sessions__session_id__metadata_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'metadata_keys',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array', 'items': {'type': 'string'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Metadata Keys'}},\n",
       "     {'name': 'start_time',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Start Time'}},\n",
       "     {'name': 'k',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 1,\n",
       "       'default': 10,\n",
       "       'title': 'K'}},\n",
       "     {'name': 'root_runs_only',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Root Runs Only'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RootModel_Dict_str__list_str___'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/sessions/{session_id}/views': {'get': {'tags': ['tracer-sessions'],\n",
       "    'summary': 'Read Filter Views',\n",
       "    'description': 'Get all filter views for a session.',\n",
       "    'operationId': 'read_filter_views_api_v1_sessions__session_id__views_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'type',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'$ref': '#/components/schemas/FilterViewType'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Type'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/FilterView'},\n",
       "         'title': 'Response Read Filter Views Api V1 Sessions  Session Id  Views Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['tracer-sessions'],\n",
       "    'summary': 'Create Filter View',\n",
       "    'description': 'Create a new filter view.',\n",
       "    'operationId': 'create_filter_view_api_v1_sessions__session_id__views_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FilterViewCreate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FilterView'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/sessions/{session_id}/views/{view_id}': {'get': {'tags': ['tracer-sessions'],\n",
       "    'summary': 'Read Filter View',\n",
       "    'description': 'Get a specific filter view.',\n",
       "    'operationId': 'read_filter_view_api_v1_sessions__session_id__views__view_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'view_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'View Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FilterView'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['tracer-sessions'],\n",
       "    'summary': 'Update Filter View',\n",
       "    'description': 'Update a filter view.',\n",
       "    'operationId': 'update_filter_view_api_v1_sessions__session_id__views__view_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'view_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'View Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FilterViewUpdate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FilterView'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['tracer-sessions'],\n",
       "    'summary': 'Delete Filter View',\n",
       "    'description': 'Delete a specific filter view.',\n",
       "    'operationId': 'delete_filter_view_api_v1_sessions__session_id__views__view_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'view_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'View Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/sessions/{session_id}/insights': {'get': {'tags': ['tracer-sessions'],\n",
       "    'summary': '[Beta] Get Insights Jobs',\n",
       "    'description': 'Get all clusters for a session.',\n",
       "    'operationId': '_Beta__Get_Insights_Jobs_api_v1_sessions__session_id__insights_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/GetRunClusteringJobsResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['tracer-sessions'],\n",
       "    'summary': '[Beta] Create Insights Job',\n",
       "    'description': 'Create an insights job.',\n",
       "    'operationId': '_Beta__Create_Insights_Job_api_v1_sessions__session_id__insights_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CreateRunClusteringJobRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CreateRunClusteringJobResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/sessions/{session_id}/insights/configs': {'get': {'tags': ['tracer-sessions'],\n",
       "    'summary': '[Beta] Get Insights Job Configs',\n",
       "    'description': 'Get all insights job configs for a session.',\n",
       "    'operationId': '_Beta__Get_Insights_Job_Configs_api_v1_sessions__session_id__insights_configs_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'include_prebuilts',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Include Prebuilts'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/GetClusteringJobConfigsResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['tracer-sessions'],\n",
       "    'summary': '[Beta] Create Insights Job Config',\n",
       "    'description': 'Save an insights job config.',\n",
       "    'operationId': '_Beta__Create_Insights_Job_Config_api_v1_sessions__session_id__insights_configs_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CreateClusteringJobConfigRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CreateClusteringJobConfigResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/sessions/{session_id}/insights/configs/generate': {'post': {'tags': ['tracer-sessions'],\n",
       "    'summary': '[Beta] Auto-Generate Insights Job Config',\n",
       "    'description': 'Auto-generate an insights job config.',\n",
       "    'operationId': '_Beta__Auto_Generate_Insights_Job_Config_api_v1_sessions__session_id__insights_configs_generate_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/GenerateClusteringJobConfigRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/GenerateClusteringJobConfigResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/sessions/{session_id}/insights/configs/{config_id}': {'patch': {'tags': ['tracer-sessions'],\n",
       "    'summary': '[Beta] Update Insights Job Config',\n",
       "    'description': 'Update an insights job config.',\n",
       "    'operationId': '_Beta__Update_Insights_Job_Config_api_v1_sessions__session_id__insights_configs__config_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'config_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Config Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/UpdateClusteringJobConfigRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CreateClusteringJobConfigResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['tracer-sessions'],\n",
       "    'summary': '[Beta] Delete Insights Job Config',\n",
       "    'description': 'Delete an insights job config.',\n",
       "    'operationId': '_Beta__Delete_Insights_Job_Config_api_v1_sessions__session_id__insights_configs__config_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'config_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Config Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/DeleteClusteringJobConfigResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/sessions/{session_id}/insights/{job_id}': {'get': {'tags': ['tracer-sessions'],\n",
       "    'summary': '[Beta] Get Insights Job',\n",
       "    'description': 'Get a specific cluster job for a session.',\n",
       "    'operationId': '_Beta__Get_Insights_Job_api_v1_sessions__session_id__insights__job_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'job_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Job Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/GetRunClusteringJobResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['tracer-sessions'],\n",
       "    'summary': '[Beta] Update Insights Job',\n",
       "    'description': 'Update a session cluster job.',\n",
       "    'operationId': '_Beta__Update_Insights_Job_api_v1_sessions__session_id__insights__job_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'job_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Job Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/UpdateRunClusteringJobRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/UpdateRunClusteringJobResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['tracer-sessions'],\n",
       "    'summary': '[Beta] Delete Insights Job',\n",
       "    'description': 'Delete a session cluster job.',\n",
       "    'operationId': '_Beta__Delete_Insights_Job_api_v1_sessions__session_id__insights__job_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'job_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Job Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/DeleteRunClusteringJobResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/sessions/{session_id}/insights/{job_id}/clusters/{cluster_id}': {'get': {'tags': ['tracer-sessions'],\n",
       "    'summary': '[Beta] Get Run Cluster From Insights Job',\n",
       "    'description': 'Get a specific cluster for a session.',\n",
       "    'operationId': '_Beta__Get_Run_Cluster_from_Insights_Job_api_v1_sessions__session_id__insights__job_id__clusters__cluster_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'job_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Job Id'}},\n",
       "     {'name': 'cluster_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Cluster Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/GetRunClusterResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/sessions/{session_id}/insights/{job_id}/runs': {'get': {'tags': ['tracer-sessions'],\n",
       "    'summary': '[Beta] Get Runs From Insights Job',\n",
       "    'description': 'Get all runs for a cluster job, optionally filtered by cluster.',\n",
       "    'operationId': '_Beta__Get_Runs_from_Insights_Job_api_v1_sessions__session_id__insights__job_id__runs_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'job_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Job Id'}},\n",
       "     {'name': 'cluster_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Cluster Id'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'attribute_sort_key',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Attribute Sort Key'}},\n",
       "     {'name': 'attribute_sort_order',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'enum': ['asc', 'desc'], 'type': 'string'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Attribute Sort Order'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FetchClusteringJobRunsResult'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'List Organizations',\n",
       "    'description': 'Get all orgs visible to this auth',\n",
       "    'operationId': 'list_organizations_api_v1_orgs_get',\n",
       "    'security': [{'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'skip_create',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean', 'default': False, 'title': 'Skip Create'}},\n",
       "     {'name': 'include_tier',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Include Tier'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/OrganizationPGSchemaSlim'},\n",
       "         'title': 'Response List Organizations Api V1 Orgs Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['orgs'],\n",
       "    'summary': 'Create Organization',\n",
       "    'operationId': 'create_organization_api_v1_orgs_post',\n",
       "    'security': [{'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/OrganizationCreate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/OrganizationPGSchemaSlim'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs/current/setup': {'post': {'tags': ['orgs'],\n",
       "    'summary': 'Create Customers And Get Stripe Setup Intent',\n",
       "    'operationId': 'create_customers_and_get_stripe_setup_intent_api_v1_orgs_current_setup_post',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/StripeSetupIntentResponse'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'Get Organization Info',\n",
       "    'operationId': 'get_organization_info_api_v1_orgs_current_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Organization'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/info': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'Get Current Organization Info',\n",
       "    'operationId': 'get_current_organization_info_api_v1_orgs_current_info_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/OrganizationInfo'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]},\n",
       "   'patch': {'tags': ['orgs'],\n",
       "    'summary': 'Update Current Organization Info',\n",
       "    'operationId': 'update_current_organization_info_api_v1_orgs_current_info_patch',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/OrganizationUpdate'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/OrganizationInfo'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/billing': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'Get Organization Billing Info',\n",
       "    'operationId': 'get_organization_billing_info_api_v1_orgs_current_billing_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/OrganizationBillingInfo'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/dashboard': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'Get Dashboard',\n",
       "    'operationId': 'get_dashboard_api_v1_orgs_current_dashboard_get',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'type',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'$ref': '#/components/schemas/OrganizationDashboardType'}},\n",
       "     {'name': 'color_scheme',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'anyOf': [{'$ref': '#/components/schemas/OrganizationDashboardColorScheme'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Color Scheme'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/OrganizationDashboardSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs/current/payment-method': {'post': {'tags': ['orgs'],\n",
       "    'summary': 'On Payment Method Created',\n",
       "    'operationId': 'on_payment_method_created_api_v1_orgs_current_payment_method_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/StripePaymentInformation'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/business-info': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'Get Company Info',\n",
       "    'operationId': 'get_company_info_api_v1_orgs_current_business_info_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/StripeBusinessInfo-Output'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['orgs'],\n",
       "    'summary': 'Set Company Info',\n",
       "    'operationId': 'set_company_info_api_v1_orgs_current_business_info_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/StripeBusinessInfo-Input'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/plan': {'post': {'tags': ['orgs'],\n",
       "    'summary': 'Change Payment Plan',\n",
       "    'operationId': 'change_payment_plan_api_v1_orgs_current_plan_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ChangePaymentPlanSchema'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/roles': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'List Organization Roles',\n",
       "    'operationId': 'list_organization_roles_api_v1_orgs_current_roles_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/Role'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response List Organization Roles Api V1 Orgs Current Roles Get'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['orgs'],\n",
       "    'summary': 'Create Organization Roles',\n",
       "    'operationId': 'create_organization_roles_api_v1_orgs_current_roles_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CreateRoleRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Role'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/roles/{role_id}': {'delete': {'tags': ['orgs'],\n",
       "    'summary': 'Delete Organization Roles',\n",
       "    'operationId': 'delete_organization_roles_api_v1_orgs_current_roles__role_id__delete',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'role_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Role Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Role'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['orgs'],\n",
       "    'summary': 'Update Organization Roles',\n",
       "    'operationId': 'update_organization_roles_api_v1_orgs_current_roles__role_id__patch',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'role_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Role Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/UpdateRoleRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Role'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs/permissions': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'List Permissions',\n",
       "    'operationId': 'list_permissions_api_v1_orgs_permissions_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/PermissionResponse'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response List Permissions Api V1 Orgs Permissions Get'}}}}},\n",
       "    'security': [{'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/pending': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'List Pending Organization Invites',\n",
       "    'description': 'Get all pending orgs visible to this auth',\n",
       "    'operationId': 'list_pending_organization_invites_api_v1_orgs_pending_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/OrganizationPGSchemaSlim'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response List Pending Organization Invites Api V1 Orgs Pending Get'}}}}},\n",
       "    'security': [{'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/members': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'Get Current Org Members',\n",
       "    'operationId': 'get_current_org_members_api_v1_orgs_current_members_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/OrganizationMembers'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['orgs'],\n",
       "    'summary': 'Add Member To Current Org',\n",
       "    'operationId': 'add_member_to_current_org_api_v1_orgs_current_members_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PendingIdentityCreate'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PendingIdentity'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/members/active': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'Get Current Active Org Members',\n",
       "    'operationId': 'get_current_active_org_members_api_v1_orgs_current_members_active_get',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 500,\n",
       "       'minimum': 1,\n",
       "       'default': 50,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'emails',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'array',\n",
       "       'items': {'type': 'string'},\n",
       "       'default': [],\n",
       "       'title': 'Emails'}},\n",
       "     {'name': 'q',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'description': 'Search query for email',\n",
       "       'title': 'Q'},\n",
       "      'description': 'Search query for email'},\n",
       "     {'name': 'ls_user_ids',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'array',\n",
       "       'items': {'type': 'string', 'format': 'uuid'},\n",
       "       'default': [],\n",
       "       'title': 'Ls User Ids'}},\n",
       "     {'name': 'user_ids',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'array',\n",
       "       'items': {'type': 'string', 'format': 'uuid'},\n",
       "       'deprecated': True,\n",
       "       'title': 'User Ids'},\n",
       "      'deprecated': True},\n",
       "     {'name': 'is_disabled',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Is Disabled'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/OrgMemberIdentity'},\n",
       "         'title': 'Response Get Current Active Org Members Api V1 Orgs Current Members Active Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs/current/members/pending': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'Get Current Pending Org Members',\n",
       "    'operationId': 'get_current_pending_org_members_api_v1_orgs_current_members_pending_get',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 500,\n",
       "       'minimum': 1,\n",
       "       'default': 50,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'emails',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'array',\n",
       "       'items': {'type': 'string'},\n",
       "       'default': [],\n",
       "       'title': 'Emails'}},\n",
       "     {'name': 'q',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'description': 'Search query for email',\n",
       "       'title': 'Q'},\n",
       "      'description': 'Search query for email'}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/OrgPendingIdentity'},\n",
       "         'title': 'Response Get Current Pending Org Members Api V1 Orgs Current Members Pending Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs/current/members/batch': {'post': {'tags': ['orgs'],\n",
       "    'summary': 'Add Members To Current Org Batch',\n",
       "    'description': 'Batch invite up to 500 users to the current org.',\n",
       "    'operationId': 'add_members_to_current_org_batch_api_v1_orgs_current_members_batch_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/PendingIdentityCreate'},\n",
       "        'type': 'array',\n",
       "        'title': 'Payloads'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/PendingIdentity'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response Add Members To Current Org Batch Api V1 Orgs Current Members Batch Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/members/basic/batch': {'post': {'tags': ['orgs'],\n",
       "    'summary': 'Add Basic Auth Members To Current Org',\n",
       "    'description': 'Batch add up to 500 users to the org and specified workspaces in basic auth mode.',\n",
       "    'operationId': 'add_basic_auth_members_to_current_org_api_v1_orgs_current_members_basic_batch_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/BasicAuthMemberCreate'},\n",
       "        'type': 'array',\n",
       "        'title': 'Payloads'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/UserWithPassword'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response Add Basic Auth Members To Current Org Api V1 Orgs Current Members Basic Batch Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/members/{identity_id}/pending': {'delete': {'tags': ['orgs'],\n",
       "    'summary': 'Delete Current Org Pending Member',\n",
       "    'description': 'When an admin deletes a pending member invite.',\n",
       "    'operationId': 'delete_current_org_pending_member_api_v1_orgs_current_members__identity_id__pending_delete',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'identity_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Identity Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs/pending/{organization_id}': {'delete': {'tags': ['orgs'],\n",
       "    'summary': 'Delete Pending Organization Invite',\n",
       "    'operationId': 'delete_pending_organization_invite_api_v1_orgs_pending__organization_id__delete',\n",
       "    'security': [{'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'organization_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Organization Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs/pending/{organization_id}/claim': {'post': {'tags': ['orgs'],\n",
       "    'summary': 'Claim Pending Organization Invite',\n",
       "    'operationId': 'claim_pending_organization_invite_api_v1_orgs_pending__organization_id__claim_post',\n",
       "    'security': [{'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'organization_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Organization Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Identity'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs/current/members/{identity_id}': {'delete': {'tags': ['orgs'],\n",
       "    'summary': 'Remove Member From Current Org',\n",
       "    'description': 'Remove a user from the current organization.',\n",
       "    'operationId': 'remove_member_from_current_org_api_v1_orgs_current_members__identity_id__delete',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'identity_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Identity Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['orgs'],\n",
       "    'summary': 'Update Current Org Member',\n",
       "    'description': \"This is used for updating a user's role (all auth modes) or full_name/password (basic auth)\",\n",
       "    'operationId': 'update_current_org_member_api_v1_orgs_current_members__identity_id__patch',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'identity_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Identity Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/OrgIdentityPatch'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs/members/basic': {'patch': {'tags': ['orgs'],\n",
       "    'summary': 'Update Current User',\n",
       "    'description': \"Update a user's full_name/password (basic auth only)\",\n",
       "    'operationId': 'update_current_user_api_v1_orgs_members_basic_patch',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/BasicAuthUserPatch'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/ttl-settings': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'List Ttl Settings',\n",
       "    'description': 'List out the configured TTL settings for a given org (org-level and tenant-level).',\n",
       "    'operationId': 'list_ttl_settings_api_v1_orgs_ttl_settings_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/TTLSettings'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response List Ttl Settings Api V1 Orgs Ttl Settings Get'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]},\n",
       "   'put': {'tags': ['orgs'],\n",
       "    'summary': 'Upsert Ttl Settings',\n",
       "    'operationId': 'upsert_ttl_settings_api_v1_orgs_ttl_settings_put',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/UpsertTTLSettingsRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TTLSettings'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/sso-settings': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'Get Current Sso Settings',\n",
       "    'description': 'Get SSO provider settings for the current organization.',\n",
       "    'operationId': 'get_current_sso_settings_api_v1_orgs_current_sso_settings_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/SSOProvider'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response Get Current Sso Settings Api V1 Orgs Current Sso Settings Get'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['orgs'],\n",
       "    'summary': 'Create Sso Settings',\n",
       "    'description': 'Create SSO provider settings for the current organization.',\n",
       "    'operationId': 'create_sso_settings_api_v1_orgs_current_sso_settings_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SSOSettingsCreate'}}},\n",
       "     'required': True},\n",
       "    'responses': {'201': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SSOProvider'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/sso-settings/{id}': {'patch': {'tags': ['orgs'],\n",
       "    'summary': 'Update Sso Settings',\n",
       "    'description': 'Update SSO provider settings defaults for the current organization.',\n",
       "    'operationId': 'update_sso_settings_api_v1_orgs_current_sso_settings__id__patch',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SSOSettingsUpdate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SSOProvider'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['orgs'],\n",
       "    'summary': 'Delete Sso Settings',\n",
       "    'description': 'Delete SSO provider settings for the current organization.',\n",
       "    'operationId': 'delete_sso_settings_api_v1_orgs_current_sso_settings__id__delete',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SSOProvider'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs/current/login-methods': {'patch': {'tags': ['orgs'],\n",
       "    'summary': 'Update Allowed Login Methods',\n",
       "    'description': 'Update allowed login methods for the current organization.',\n",
       "    'operationId': 'update_allowed_login_methods_api_v1_orgs_current_login_methods_patch',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/AllowedLoginMethodsUpdate'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'additionalProperties': True,\n",
       "         'type': 'object',\n",
       "         'title': 'Response Update Allowed Login Methods Api V1 Orgs Current Login Methods Patch'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/billing/usage': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'Get Org Usage',\n",
       "    'operationId': 'get_org_usage_api_v1_orgs_current_billing_usage_get',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'starting_on',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'date-time',\n",
       "       'title': 'Starting On'}},\n",
       "     {'name': 'ending_before',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'date-time',\n",
       "       'title': 'Ending Before'}},\n",
       "     {'name': 'on_current_plan',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': True,\n",
       "       'title': 'On Current Plan'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/OrgUsage'},\n",
       "         'title': 'Response Get Org Usage Api V1 Orgs Current Billing Usage Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs/current/billing/granular-usage': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'Get Granular Usage',\n",
       "    'description': 'Get granular usage data with flexible grouping.\\n\\nworkspace_ids filters results to the specified workspaces. Only workspaces\\nthe user has read access to will be included in the results.',\n",
       "    'operationId': 'get_granular_usage_api_v1_orgs_current_billing_granular_usage_get',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'start_time',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'date-time',\n",
       "       'title': 'Start Time'}},\n",
       "     {'name': 'end_time',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'date-time',\n",
       "       'title': 'End Time'}},\n",
       "     {'name': 'workspace_ids',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'array',\n",
       "       'items': {'type': 'string', 'format': 'uuid'},\n",
       "       'title': 'Workspace Ids'}},\n",
       "     {'name': 'group_by',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'$ref': '#/components/schemas/GranularUsageGroupBy',\n",
       "       'default': 'workspace'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/GranularUsageResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs/current/billing/granular-usage/export': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'Export Granular Usage Csv',\n",
       "    'description': 'Export granular usage data as CSV.\\n\\nReturns the same data as the granular-usage endpoint but formatted as a\\ndownloadable CSV file. Only workspaces the user has read access to will\\nbe included in the results.',\n",
       "    'operationId': 'export_granular_usage_csv_api_v1_orgs_current_billing_granular_usage_export_get',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'start_time',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'date-time',\n",
       "       'title': 'Start Time'}},\n",
       "     {'name': 'end_time',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'date-time',\n",
       "       'title': 'End Time'}},\n",
       "     {'name': 'workspace_ids',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'array',\n",
       "       'items': {'type': 'string', 'format': 'uuid'},\n",
       "       'title': 'Workspace Ids'}},\n",
       "     {'name': 'group_by',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'$ref': '#/components/schemas/GranularUsageGroupBy',\n",
       "       'default': 'workspace'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs/current/user/login-methods': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'Get Current User Login Methods',\n",
       "    'description': 'Get login methods for the current user.',\n",
       "    'operationId': 'get_current_user_login_methods_api_v1_orgs_current_user_login_methods_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/ProviderUserSlim'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response Get Current User Login Methods Api V1 Orgs Current User Login Methods Get'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/stripe_checkout_session': {'post': {'tags': ['orgs'],\n",
       "    'summary': 'Create Stripe Checkout Sessions Endpoint',\n",
       "    'description': 'Kick off a Stripe checkout session flow.',\n",
       "    'operationId': 'create_stripe_checkout_sessions_endpoint_api_v1_orgs_current_stripe_checkout_session_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/StripeCheckoutSessionsCreate'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/confirm_checkout_session_completion': {'post': {'tags': ['orgs'],\n",
       "    'summary': 'Confirm Checkout Session Completion Endpoint',\n",
       "    'description': 'Complete a Stripe checkout session flow.',\n",
       "    'operationId': 'confirm_checkout_session_completion_endpoint_api_v1_orgs_current_confirm_checkout_session_completion_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/StripeCheckoutSessionsConfirm'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/stripe_account_links': {'post': {'tags': ['orgs'],\n",
       "    'summary': 'Create Stripe Account Links Endpoint',\n",
       "    'description': 'Kick off a Stripe account link flow.',\n",
       "    'operationId': 'create_stripe_account_links_endpoint_api_v1_orgs_current_stripe_account_links_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/StripeAccountLinksCreate'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/service-keys': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'List Org Service Keys',\n",
       "    'operationId': 'list_org_service_keys_api_v1_orgs_current_service_keys_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/APIKeyGetResponse'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response List Org Service Keys Api V1 Orgs Current Service Keys Get'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['orgs'],\n",
       "    'summary': 'Create Org Service Key',\n",
       "    'description': 'Create org-scoped service key. If workspaces is None, key is org-wide.',\n",
       "    'operationId': 'create_org_service_key_api_v1_orgs_current_service_keys_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/APIKeyCreateRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/APIKeyCreateResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/service-keys/{api_key_id}': {'delete': {'tags': ['orgs'],\n",
       "    'summary': 'Delete Org Service Key',\n",
       "    'operationId': 'delete_org_service_key_api_v1_orgs_current_service_keys__api_key_id__delete',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'api_key_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Api Key Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/APIKeyGetResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs/current/personal-access-tokens': {'get': {'tags': ['orgs'],\n",
       "    'summary': 'List Org Personal Access Tokens',\n",
       "    'operationId': 'list_org_personal_access_tokens_api_v1_orgs_current_personal_access_tokens_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/APIKeyGetResponse'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response List Org Personal Access Tokens Api V1 Orgs Current Personal Access Tokens Get'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['orgs'],\n",
       "    'summary': 'Create Org Personal Access Token',\n",
       "    'operationId': 'create_org_personal_access_token_api_v1_orgs_current_personal_access_tokens_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/APIKeyCreateRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/APIKeyCreateResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/orgs/current/personal-access-tokens/{pat_id}': {'delete': {'tags': ['orgs'],\n",
       "    'summary': 'Delete Org Personal Access Token',\n",
       "    'operationId': 'delete_org_personal_access_token_api_v1_orgs_current_personal_access_tokens__pat_id__delete',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'pat_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Pat Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/APIKeyGetResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/orgs/current/set-default-sso-provision': {'post': {'tags': ['orgs'],\n",
       "    'summary': 'Set Default Sso Provision',\n",
       "    'description': 'Set the current organization as the default for SSO provisioning in self-hosted environments.',\n",
       "    'operationId': 'set_default_sso_provision_api_v1_orgs_current_set_default_sso_provision_post',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'additionalProperties': True,\n",
       "         'type': 'object',\n",
       "         'title': 'Response Set Default Sso Provision Api V1 Orgs Current Set Default Sso Provision Post'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/login': {'post': {'tags': ['auth'],\n",
       "    'summary': 'Login',\n",
       "    'operationId': 'login_api_v1_login_post',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/BasicAuthResponse'}}}}}}},\n",
       "  '/api/v1/sso/email-verification/send': {'post': {'tags': ['auth'],\n",
       "    'summary': 'Send Sso Email Confirmation',\n",
       "    'description': 'Send an email to confirm the email address for an SSO user.',\n",
       "    'operationId': 'send_sso_email_confirmation_api_v1_sso_email_verification_send_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SSOEmailVerificationSendRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'202': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'additionalProperties': True,\n",
       "         'type': 'object',\n",
       "         'title': 'Response Send Sso Email Confirmation Api V1 Sso Email Verification Send Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'Bearer Auth': []}]}},\n",
       "  '/api/v1/sso/email-verification/status': {'post': {'tags': ['auth'],\n",
       "    'summary': 'Check Sso Email Verification Status',\n",
       "    'description': 'Retrieve the email verification status of an SSO user.',\n",
       "    'operationId': 'check_sso_email_verification_status_api_v1_sso_email_verification_status_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SSOEmailVerificationStatusRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SSOEmailVerificationStatusResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/sso/email-verification/confirm': {'post': {'tags': ['auth'],\n",
       "    'summary': 'Confirm Sso User Email',\n",
       "    'description': 'Confirm the email of an SSO user.',\n",
       "    'operationId': 'confirm_sso_user_email_api_v1_sso_email_verification_confirm_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SSOConfirmEmailRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'additionalProperties': True,\n",
       "         'type': 'object',\n",
       "         'title': 'Response Confirm Sso User Email Api V1 Sso Email Verification Confirm Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/sso/settings/{sso_login_slug}': {'get': {'tags': ['auth'],\n",
       "    'summary': 'Get Sso Settings',\n",
       "    'description': 'Get SSO provider settings from login slug.',\n",
       "    'operationId': 'get_sso_settings_api_v1_sso_settings__sso_login_slug__get',\n",
       "    'parameters': [{'name': 'sso_login_slug',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Sso Login Slug'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/SSOProviderSlim'},\n",
       "         'title': 'Response Get Sso Settings Api V1 Sso Settings  Sso Login Slug  Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/api-key': {'get': {'tags': ['api-key'],\n",
       "    'summary': 'Get Api Keys',\n",
       "    'description': \"Get the current tenant's API keys\",\n",
       "    'operationId': 'get_api_keys_api_v1_api_key_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/APIKeyGetResponse'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response Get Api Keys Api V1 Api Key Get'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['api-key'],\n",
       "    'summary': 'Generate Api Key',\n",
       "    'description': 'Generate an api key for the user',\n",
       "    'operationId': 'generate_api_key_api_v1_api_key_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/APIKeyCreateRequest',\n",
       "        'default': {'description': 'Default API key', 'read_only': False}}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/APIKeyCreateResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/api-key/{api_key_id}': {'delete': {'tags': ['api-key'],\n",
       "    'summary': 'Delete Api Key',\n",
       "    'description': 'Delete an api key for the user',\n",
       "    'operationId': 'delete_api_key_api_v1_api_key__api_key_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'api_key_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Api Key Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/APIKeyGetResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/api-key/current': {'get': {'tags': ['api-key'],\n",
       "    'summary': 'Get Personal Access Tokens',\n",
       "    'description': 'DEPRECATED: Use /orgs/current/personal-access-tokens instead',\n",
       "    'operationId': 'get_personal_access_tokens_api_v1_api_key_current_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/APIKeyGetResponse'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response Get Personal Access Tokens Api V1 Api Key Current Get'}}}}},\n",
       "    'deprecated': True,\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['api-key'],\n",
       "    'summary': 'Generate Personal Access Token',\n",
       "    'description': 'DEPRECATED: Use /orgs/current/personal-access-tokens instead',\n",
       "    'operationId': 'generate_personal_access_token_api_v1_api_key_current_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/APIKeyCreateRequest',\n",
       "        'default': {'description': 'Default API key', 'read_only': False}}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/APIKeyCreateResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/api-key/current/{pat_id}': {'delete': {'tags': ['api-key'],\n",
       "    'summary': 'Delete Personal Access Token',\n",
       "    'description': 'DEPRECATED: Use /orgs/current/personal-access-tokens/{pat_id} instead',\n",
       "    'operationId': 'delete_personal_access_token_api_v1_api_key_current__pat_id__delete',\n",
       "    'deprecated': True,\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'pat_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Pat Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/APIKeyGetResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/examples/count': {'get': {'tags': ['examples'],\n",
       "    'summary': 'Count Examples',\n",
       "    'description': 'Count all examples by query params',\n",
       "    'operationId': 'count_examples_api_v1_examples_count_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Id'}},\n",
       "     {'name': 'as_of',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'string'}],\n",
       "       'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.',\n",
       "       'default': 'latest',\n",
       "       'title': 'As Of'},\n",
       "      'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.'},\n",
       "     {'name': 'metadata',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Metadata'}},\n",
       "     {'name': 'full_text_contains',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array', 'items': {'type': 'string'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Full Text Contains'}},\n",
       "     {'name': 'splits',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array', 'items': {'type': 'string'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Splits'}},\n",
       "     {'name': 'dataset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Dataset'}},\n",
       "     {'name': 'filter',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Filter'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'integer',\n",
       "         'title': 'Response Count Examples Api V1 Examples Count Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/examples/{example_id}': {'get': {'tags': ['examples'],\n",
       "    'summary': 'Read Example',\n",
       "    'description': 'Get a specific example.',\n",
       "    'operationId': 'read_example_api_v1_examples__example_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'example_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Example Id'}},\n",
       "     {'name': 'as_of',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'string'}],\n",
       "       'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.',\n",
       "       'default': 'latest',\n",
       "       'title': 'As Of'},\n",
       "      'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.'},\n",
       "     {'name': 'dataset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Dataset'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Example'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['examples'],\n",
       "    'summary': 'Update Example',\n",
       "    'description': 'Update a specific example.',\n",
       "    'operationId': 'update_example_api_v1_examples__example_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'example_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Example Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ExampleUpdate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['examples'],\n",
       "    'summary': 'Delete Example',\n",
       "    'description': \"Soft delete an example. Only deletes the example in the 'latest' version of the dataset.\",\n",
       "    'operationId': 'delete_example_api_v1_examples__example_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'example_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Example Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/examples': {'get': {'tags': ['examples'],\n",
       "    'summary': 'Read Examples',\n",
       "    'description': 'Get all examples by query params',\n",
       "    'operationId': 'read_examples_api_v1_examples_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Id'}},\n",
       "     {'name': 'as_of',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'string'}],\n",
       "       'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.',\n",
       "       'default': 'latest',\n",
       "       'title': 'As Of'},\n",
       "      'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.'},\n",
       "     {'name': 'metadata',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Metadata'}},\n",
       "     {'name': 'full_text_contains',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array', 'items': {'type': 'string'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Full Text Contains'}},\n",
       "     {'name': 'splits',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array', 'items': {'type': 'string'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Splits'}},\n",
       "     {'name': 'dataset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Dataset'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'order',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'$ref': '#/components/schemas/ExampleListOrder',\n",
       "       'default': 'recent'}},\n",
       "     {'name': 'random_seed',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "       'title': 'Random Seed'}},\n",
       "     {'name': 'select',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'array',\n",
       "       'items': {'$ref': '#/components/schemas/ExampleSelect'},\n",
       "       'default': ['id',\n",
       "        'created_at',\n",
       "        'modified_at',\n",
       "        'name',\n",
       "        'dataset_id',\n",
       "        'source_run_id',\n",
       "        'metadata',\n",
       "        'inputs',\n",
       "        'outputs'],\n",
       "       'title': 'Select'}},\n",
       "     {'name': 'descending',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Descending'}},\n",
       "     {'name': 'filter',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Filter'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/Example'},\n",
       "         'title': 'Response Read Examples Api V1 Examples Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['examples'],\n",
       "    'summary': 'Create Example',\n",
       "    'description': 'Create a new example.',\n",
       "    'operationId': 'create_example_api_v1_examples_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Example'}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'description': 'Create class for Example.',\n",
       "        'properties': {'outputs': {'anyOf': [{'additionalProperties': True,\n",
       "            'type': 'object'},\n",
       "           {'type': 'null'}],\n",
       "          'title': 'Outputs'},\n",
       "         'dataset_id': {'format': 'uuid',\n",
       "          'title': 'Dataset Id',\n",
       "          'type': 'string'},\n",
       "         'source_run_id': {'anyOf': [{'format': 'uuid', 'type': 'string'},\n",
       "           {'type': 'null'}],\n",
       "          'title': 'Source Run Id'},\n",
       "         'metadata': {'anyOf': [{'additionalProperties': True,\n",
       "            'type': 'object'},\n",
       "           {'type': 'null'}],\n",
       "          'title': 'Metadata'},\n",
       "         'inputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "           {'type': 'null'}],\n",
       "          'title': 'Inputs'},\n",
       "         'split': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "           {'type': 'string'},\n",
       "           {'type': 'null'}],\n",
       "          'default': 'base',\n",
       "          'title': 'Split'},\n",
       "         'id': {'anyOf': [{'format': 'uuid', 'type': 'string'},\n",
       "           {'type': 'null'}],\n",
       "          'title': 'Id'},\n",
       "         'use_source_run_io': {'default': False,\n",
       "          'title': 'Use Source Run Io',\n",
       "          'type': 'boolean'},\n",
       "         'use_source_run_attachments': {'default': [],\n",
       "          'items': {'type': 'string'},\n",
       "          'title': 'Use Source Run Attachments',\n",
       "          'type': 'array'},\n",
       "         'use_legacy_message_format': {'default': False,\n",
       "          'description': 'Use Legacy Message Format for LLM runs',\n",
       "          'title': 'Use Legacy Message Format',\n",
       "          'type': 'boolean'},\n",
       "         'created_at': {'title': 'Created At', 'type': 'string'}},\n",
       "        'required': ['dataset_id'],\n",
       "        'title': 'ExampleCreate',\n",
       "        'type': 'object'}}}}},\n",
       "   'delete': {'tags': ['examples'],\n",
       "    'summary': 'Delete Examples',\n",
       "    'description': \"Soft delete examples. Only deletes the examples in the 'latest' version of the dataset.\",\n",
       "    'operationId': 'delete_examples_api_v1_examples_delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'example_ids',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'array',\n",
       "       'items': {'type': 'string', 'format': 'uuid'},\n",
       "       'title': 'Example Ids'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/examples/bulk': {'post': {'tags': ['examples'],\n",
       "    'summary': 'Create Examples',\n",
       "    'description': 'Create bulk examples.',\n",
       "    'operationId': 'create_examples_api_v1_examples_bulk_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'items': {'properties': {'outputs': {'anyOf': [{'additionalProperties': True,\n",
       "             'type': 'object'},\n",
       "            {'type': 'null'}],\n",
       "           'title': 'Outputs'},\n",
       "          'dataset_id': {'type': 'string',\n",
       "           'format': 'uuid',\n",
       "           'title': 'Dataset Id'},\n",
       "          'source_run_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "            {'type': 'null'}],\n",
       "           'title': 'Source Run Id'},\n",
       "          'metadata': {'anyOf': [{'additionalProperties': True,\n",
       "             'type': 'object'},\n",
       "            {'type': 'null'}],\n",
       "           'title': 'Metadata'},\n",
       "          'inputs': {'anyOf': [{'additionalProperties': True,\n",
       "             'type': 'object'},\n",
       "            {'type': 'null'}],\n",
       "           'title': 'Inputs'},\n",
       "          'split': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "            {'type': 'string'},\n",
       "            {'type': 'null'}],\n",
       "           'title': 'Split',\n",
       "           'default': 'base'},\n",
       "          'id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "            {'type': 'null'}],\n",
       "           'title': 'Id'},\n",
       "          'use_source_run_io': {'type': 'boolean',\n",
       "           'title': 'Use Source Run Io',\n",
       "           'default': False},\n",
       "          'use_source_run_attachments': {'items': {'type': 'string'},\n",
       "           'type': 'array',\n",
       "           'title': 'Use Source Run Attachments',\n",
       "           'default': []},\n",
       "          'use_legacy_message_format': {'type': 'boolean',\n",
       "           'title': 'Use Legacy Message Format',\n",
       "           'description': 'Use Legacy Message Format for LLM runs',\n",
       "           'default': False},\n",
       "          'created_at': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "           'title': 'Created At'}},\n",
       "         'type': 'object',\n",
       "         'required': ['dataset_id'],\n",
       "         'title': 'ExampleBulkCreate',\n",
       "         'description': 'Example with optional created_at to prevent duplicate versions in bulk operations.'},\n",
       "        'type': 'array',\n",
       "        'description': 'Schema for a batch of examples to be created.'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/Example'},\n",
       "         'type': 'array'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]},\n",
       "   'patch': {'tags': ['examples'],\n",
       "    'summary': 'Legacy Update Examples',\n",
       "    'description': 'Legacy update examples in bulk. For update involving attachments, use PATCH /v1/platform/datasets/{dataset_id}/examples instead.',\n",
       "    'operationId': 'legacy_update_examples_api_v1_examples_bulk_patch',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/ExampleUpdateWithID'},\n",
       "        'type': 'array',\n",
       "        'title': 'Example Updates'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/examples/upload/{dataset_id}': {'post': {'tags': ['examples'],\n",
       "    'summary': 'Upload Examples From Csv',\n",
       "    'description': 'Upload examples from a CSV file.\\n\\nNote: For non-csv upload, please use\\nthe POST /v1/platform/datasets/{dataset_id}/examples endpoint which provides more efficient upload.',\n",
       "    'operationId': 'upload_examples_from_csv_api_v1_examples_upload__dataset_id__post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'multipart/form-data': {'schema': {'$ref': '#/components/schemas/Body_upload_examples_from_csv_api_v1_examples_upload__dataset_id__post'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/Example'},\n",
       "         'title': 'Response Upload Examples From Csv Api V1 Examples Upload  Dataset Id  Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/examples/validate': {'post': {'tags': ['examples'],\n",
       "    'summary': 'Validate Example',\n",
       "    'description': 'Validate an example.',\n",
       "    'operationId': 'validate_example_api_v1_examples_validate_post',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ExampleValidationResult'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/examples/validate/bulk': {'post': {'tags': ['examples'],\n",
       "    'summary': 'Validate Examples',\n",
       "    'description': 'Validate examples in bulk.',\n",
       "    'operationId': 'validate_examples_api_v1_examples_validate_bulk_post',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/ExampleValidationResult'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response Validate Examples Api V1 Examples Validate Bulk Post'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/datasets': {'get': {'tags': ['datasets'],\n",
       "    'summary': 'Read Datasets',\n",
       "    'description': 'Get all datasets by query params and owner.',\n",
       "    'operationId': 'read_datasets_api_v1_datasets_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Id'}},\n",
       "     {'name': 'data_type',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/DataType'}},\n",
       "        {'$ref': '#/components/schemas/DataType'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Data Type'}},\n",
       "     {'name': 'name',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name'}},\n",
       "     {'name': 'name_contains',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name Contains'}},\n",
       "     {'name': 'metadata',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Metadata'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'sort_by',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'$ref': '#/components/schemas/SortByDatasetColumn',\n",
       "       'default': 'last_session_start_time'}},\n",
       "     {'name': 'sort_by_desc',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean', 'default': True, 'title': 'Sort By Desc'}},\n",
       "     {'name': 'tag_value_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Tag Value Id'}},\n",
       "     {'name': 'exclude_corrections_datasets',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Exclude Corrections Datasets'}},\n",
       "     {'name': 'exclude',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/GetDatasetsSelect'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Exclude'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/Dataset'},\n",
       "         'title': 'Response Read Datasets Api V1 Datasets Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['datasets'],\n",
       "    'summary': 'Create Dataset',\n",
       "    'description': 'Create a new dataset.',\n",
       "    'operationId': 'create_dataset_api_v1_datasets_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/DatasetCreate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Dataset'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/stream': {'get': {'tags': ['datasets'],\n",
       "    'summary': 'Read Datasets Stream',\n",
       "    'description': 'Stream all datasets by query params and owner as JSON patches.',\n",
       "    'operationId': 'read_datasets_stream_api_v1_datasets_stream_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Id'}},\n",
       "     {'name': 'data_type',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/DataType'}},\n",
       "        {'$ref': '#/components/schemas/DataType'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Data Type'}},\n",
       "     {'name': 'name',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name'}},\n",
       "     {'name': 'name_contains',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name Contains'}},\n",
       "     {'name': 'metadata',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Metadata'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'sort_by',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'$ref': '#/components/schemas/SortByDatasetColumn',\n",
       "       'default': 'last_session_start_time'}},\n",
       "     {'name': 'sort_by_desc',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean', 'default': True, 'title': 'Sort By Desc'}},\n",
       "     {'name': 'tag_value_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Tag Value Id'}},\n",
       "     {'name': 'exclude_corrections_datasets',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Exclude Corrections Datasets'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}': {'get': {'tags': ['datasets'],\n",
       "    'summary': 'Read Dataset',\n",
       "    'description': 'Get a specific dataset.',\n",
       "    'operationId': 'read_dataset_api_v1_datasets__dataset_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Dataset'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['datasets'],\n",
       "    'summary': 'Delete Dataset',\n",
       "    'description': 'Delete a specific dataset.',\n",
       "    'operationId': 'delete_dataset_api_v1_datasets__dataset_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['datasets'],\n",
       "    'summary': 'Update Dataset',\n",
       "    'description': 'Update a specific dataset.',\n",
       "    'operationId': 'update_dataset_api_v1_datasets__dataset_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/DatasetUpdate'}}}},\n",
       "    'responses': {'200': {'description': 'Dataset updated successfully',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/DatasetSchemaForUpdate'}}},\n",
       "      'headers': {'X-Updated-Examples-Count': {'description': 'Number of examples updated',\n",
       "        'schema': {'type': 'integer'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/upload': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Upload Csv Dataset',\n",
       "    'description': 'Create a new dataset from a CSV file.',\n",
       "    'operationId': 'upload_csv_dataset_api_v1_datasets_upload_post',\n",
       "    'requestBody': {'content': {'multipart/form-data': {'schema': {'$ref': '#/components/schemas/Body_upload_csv_dataset_api_v1_datasets_upload_post'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Dataset'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/datasets/upload-experiment': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Upload Experiment',\n",
       "    'description': 'Upload an experiment that has already been run.',\n",
       "    'operationId': 'upload_experiment_api_v1_datasets_upload_experiment_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ExperimentResultsUpload'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ExperimentResultsUploadResult'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/datasets/{dataset_id}/versions': {'get': {'tags': ['datasets'],\n",
       "    'summary': 'Get Dataset Versions',\n",
       "    'description': 'Get dataset versions.',\n",
       "    'operationId': 'get_dataset_versions_api_v1_datasets__dataset_id__versions_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}},\n",
       "     {'name': 'search',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Search'}},\n",
       "     {'name': 'example',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Example'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/DatasetVersion'},\n",
       "         'title': 'Response Get Dataset Versions Api V1 Datasets  Dataset Id  Versions Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/versions/diff': {'get': {'tags': ['datasets'],\n",
       "    'summary': 'Diff Dataset Versions',\n",
       "    'description': 'Get diff between two dataset versions.',\n",
       "    'operationId': 'diff_dataset_versions_api_v1_datasets__dataset_id__versions_diff_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}},\n",
       "     {'name': 'from_version',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'string'}],\n",
       "       'title': 'From Version'}},\n",
       "     {'name': 'to_version',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'string'}],\n",
       "       'title': 'To Version'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/DatasetDiffInfo'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/version': {'get': {'tags': ['datasets'],\n",
       "    'summary': 'Get Dataset Version',\n",
       "    'description': 'Get dataset version by as_of or exact tag.',\n",
       "    'operationId': 'get_dataset_version_api_v1_datasets__dataset_id__version_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}},\n",
       "     {'name': 'as_of',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'As Of'}},\n",
       "     {'name': 'tag',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Tag'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/DatasetVersion'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/tags': {'put': {'tags': ['datasets'],\n",
       "    'summary': 'Update Dataset Version',\n",
       "    'description': 'Set a tag on a dataset version.',\n",
       "    'operationId': 'update_dataset_version_api_v1_datasets__dataset_id__tags_put',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PutDatasetVersionsSchema'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/DatasetVersion'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/openai': {'get': {'tags': ['datasets'],\n",
       "    'summary': 'Download Dataset Openai',\n",
       "    'description': 'Download a dataset as OpenAI Evals Jsonl format.',\n",
       "    'operationId': 'download_dataset_openai_api_v1_datasets__dataset_id__openai_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}},\n",
       "     {'name': 'as_of',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'null'}],\n",
       "       'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.',\n",
       "       'title': 'As Of'},\n",
       "      'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.'}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/openai_ft': {'get': {'tags': ['datasets'],\n",
       "    'summary': 'Download Dataset Openai Ft',\n",
       "    'description': 'Download a dataset as OpenAI Jsonl format.',\n",
       "    'operationId': 'download_dataset_openai_ft_api_v1_datasets__dataset_id__openai_ft_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}},\n",
       "     {'name': 'as_of',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'null'}],\n",
       "       'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.',\n",
       "       'title': 'As Of'},\n",
       "      'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.'}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/csv': {'get': {'tags': ['datasets'],\n",
       "    'summary': 'Download Dataset Csv',\n",
       "    'description': 'Download a dataset as CSV format.',\n",
       "    'operationId': 'download_dataset_csv_api_v1_datasets__dataset_id__csv_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}},\n",
       "     {'name': 'as_of',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'null'}],\n",
       "       'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.',\n",
       "       'title': 'As Of'},\n",
       "      'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.'}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/jsonl': {'get': {'tags': ['datasets'],\n",
       "    'summary': 'Download Dataset Jsonl',\n",
       "    'description': 'Download a dataset as CSV format.',\n",
       "    'operationId': 'download_dataset_jsonl_api_v1_datasets__dataset_id__jsonl_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}},\n",
       "     {'name': 'as_of',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'null'}],\n",
       "       'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.',\n",
       "       'title': 'As Of'},\n",
       "      'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.'}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/runs': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Read Examples With Runs',\n",
       "    'description': 'Fetch examples for a dataset, and fetch the runs for each example if they are associated with the given session_ids.',\n",
       "    'operationId': 'read_examples_with_runs_api_v1_datasets__dataset_id__runs_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}},\n",
       "     {'name': 'format',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'const': 'csv', 'type': 'string'},\n",
       "        {'type': 'null'}],\n",
       "       'description': \"Response format, e.g., 'csv'\",\n",
       "       'title': 'Format'},\n",
       "      'description': \"Response format, e.g., 'csv'\"}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/QueryExampleSchemaWithRunsRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'anyOf': [{'type': 'array',\n",
       "           'items': {'$ref': '#/components/schemas/ExampleWithRuns'},\n",
       "           'title': 'ExamplesWithRuns'},\n",
       "          {'type': 'array',\n",
       "           'items': {'$ref': '#/components/schemas/ExampleWithRunsCH'}},\n",
       "          {'type': 'null'}],\n",
       "         'title': 'Response Read Examples With Runs Api V1 Datasets  Dataset Id  Runs Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/group/runs': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Read Examples With Runs Grouped',\n",
       "    'description': 'Fetch examples for a dataset, and fetch the runs for each example if they are associated with the given session_ids.',\n",
       "    'operationId': 'read_examples_with_runs_grouped_api_v1_datasets__dataset_id__group_runs_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/QueryGroupedExamplesWithRuns'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/GroupedExamplesWithRunsResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/runs/delta': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Read Delta',\n",
       "    'description': 'Fetch the number of regressions/improvements for each example in a dataset, between sessions[0] and sessions[1].',\n",
       "    'operationId': 'read_delta_api_v1_datasets__dataset_id__runs_delta_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/QueryFeedbackDelta'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SessionFeedbackDelta'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/runs/delta/stream': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Read Delta Stream',\n",
       "    'description': 'Stream feedback deltas for multiple feedback keys.\\n\\nReturns results in chunks as they become available. Each chunk contains\\nresults for one or more feedback keys. Errors for individual chunks are\\nincluded in the response rather than failing the entire operation.\\n\\nResponse format (SSE):\\n    event: data\\n    data: {\"feedback_deltas\": {\"key1\": {session_id: {...}}, ...}, \"errors\": null}\\n\\n    event: data\\n    data: {\"feedback_deltas\": {\"key2\": {...}}, \"errors\": null}\\n\\n    event: end',\n",
       "    'operationId': 'read_delta_stream_api_v1_datasets__dataset_id__runs_delta_stream_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/QueryFeedbackDeltaBatch'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/experiments/grouped': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Read Grouped Experiments',\n",
       "    'description': 'Stream grouped and aggregated experiments.',\n",
       "    'operationId': 'read_grouped_experiments_api_v1_datasets__dataset_id__experiments_grouped_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/GroupedExperimentsRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/share': {'get': {'tags': ['datasets'],\n",
       "    'summary': 'Read Dataset Share State',\n",
       "    'description': 'Get the state of sharing a dataset',\n",
       "    'operationId': 'read_dataset_share_state_api_v1_datasets__dataset_id__share_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'anyOf': [{'$ref': '#/components/schemas/DatasetShareSchema'},\n",
       "          {'type': 'null'}],\n",
       "         'title': 'Response Read Dataset Share State Api V1 Datasets  Dataset Id  Share Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'put': {'tags': ['datasets'],\n",
       "    'summary': 'Share Dataset',\n",
       "    'description': 'Share a dataset.',\n",
       "    'operationId': 'share_dataset_api_v1_datasets__dataset_id__share_put',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}},\n",
       "     {'name': 'share_projects',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Share Projects'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/DatasetShareSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['datasets'],\n",
       "    'summary': 'Unshare Dataset',\n",
       "    'description': 'Unshare a dataset.',\n",
       "    'operationId': 'unshare_dataset_api_v1_datasets__dataset_id__share_delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/comparative': {'get': {'tags': ['datasets'],\n",
       "    'summary': 'Read Comparative Experiments',\n",
       "    'description': 'Get all comparative experiments for a given dataset.',\n",
       "    'operationId': 'read_comparative_experiments_api_v1_datasets__dataset_id__comparative_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}},\n",
       "     {'name': 'name',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name'}},\n",
       "     {'name': 'name_contains',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name Contains'}},\n",
       "     {'name': 'id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Id'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'sort_by',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'$ref': '#/components/schemas/SortByComparativeExperimentColumn',\n",
       "       'default': 'created_at'}},\n",
       "     {'name': 'sort_by_desc',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': True,\n",
       "       'title': 'Sort By Desc'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/ComparativeExperiment'},\n",
       "         'title': 'Response Read Comparative Experiments Api V1 Datasets  Dataset Id  Comparative Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/comparative': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Create Comparative Experiment',\n",
       "    'description': 'Create a comparative experiment.',\n",
       "    'operationId': 'create_comparative_experiment_api_v1_datasets_comparative_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ComparativeExperimentCreate'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ComparativeExperimentBase'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/datasets/comparative/{comparative_experiment_id}': {'delete': {'tags': ['datasets'],\n",
       "    'summary': 'Delete Comparative Experiment',\n",
       "    'description': 'Delete a specific comparative experiment.',\n",
       "    'operationId': 'delete_comparative_experiment_api_v1_datasets_comparative__comparative_experiment_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'comparative_experiment_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Comparative Experiment Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/clone': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Clone Dataset',\n",
       "    'description': 'Clone a dataset.',\n",
       "    'operationId': 'clone_dataset_api_v1_datasets_clone_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Body_clone_dataset_api_v1_datasets_clone_post'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'additionalProperties': True,\n",
       "          'type': 'object'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response Clone Dataset Api V1 Datasets Clone Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/datasets/{dataset_id}/splits': {'get': {'tags': ['datasets'],\n",
       "    'summary': 'Get Dataset Splits',\n",
       "    'operationId': 'get_dataset_splits_api_v1_datasets__dataset_id__splits_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}},\n",
       "     {'name': 'as_of',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'string'}],\n",
       "       'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.',\n",
       "       'default': 'latest',\n",
       "       'title': 'As Of'},\n",
       "      'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.'}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'type': 'string'},\n",
       "         'title': 'Response Get Dataset Splits Api V1 Datasets  Dataset Id  Splits Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'put': {'tags': ['datasets'],\n",
       "    'summary': 'Update Dataset Splits',\n",
       "    'operationId': 'update_dataset_splits_api_v1_datasets__dataset_id__splits_put',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Body_update_dataset_splits_api_v1_datasets__dataset_id__splits_put'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'},\n",
       "         'title': 'Response Update Dataset Splits Api V1 Datasets  Dataset Id  Splits Put'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/index': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Index',\n",
       "    'description': 'Index a dataset.',\n",
       "    'operationId': 'index_api_v1_datasets__dataset_id__index_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/DatasetIndexRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['datasets'],\n",
       "    'summary': 'Remove Index',\n",
       "    'description': 'Remove an index for a dataset.',\n",
       "    'operationId': 'remove_index_api_v1_datasets__dataset_id__index_delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'get': {'tags': ['datasets'],\n",
       "    'summary': 'Get Index Info',\n",
       "    'description': 'Get index info.',\n",
       "    'operationId': 'get_index_info_api_v1_datasets__dataset_id__index_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/DatasetIndexInfo'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/index/sync': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Sync Index',\n",
       "    'description': 'Sync an index for a dataset.',\n",
       "    'operationId': 'sync_index_api_v1_datasets__dataset_id__index_sync_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/search': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Search',\n",
       "    'description': 'Search a dataset.',\n",
       "    'operationId': 'search_api_v1_datasets__dataset_id__search_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SearchDatasetRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SearchDatasetResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/{dataset_id}/generate': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Generate',\n",
       "    'description': 'Generate synthetic examples for a dataset.',\n",
       "    'operationId': 'generate_api_v1_datasets__dataset_id__generate_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/GenerateSyntheticExamplesBody'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/datasets/playground_experiment/batch': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Dataset Handler',\n",
       "    'operationId': 'dataset_handler_api_v1_datasets_playground_experiment_batch_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PlaygroundRunOverDatasetBatchRequestSchema'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'title': 'Response Dataset Handler Api V1 Datasets Playground Experiment Batch Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/datasets/playground_experiment/stream': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Stream Dataset Handler',\n",
       "    'operationId': 'stream_dataset_handler_api_v1_datasets_playground_experiment_stream_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PlaygroundRunOverDatasetRequestSchema'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/datasets/studio_experiment': {'post': {'tags': ['datasets'],\n",
       "    'summary': 'Studio Experiment',\n",
       "    'operationId': 'studio_experiment_api_v1_datasets_studio_experiment_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/StudioRunOverDatasetRequestSchema'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'title': 'Response Studio Experiment Api V1 Datasets Studio Experiment Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/runs/rules': {'get': {'tags': ['run'],\n",
       "    'summary': 'List Rules',\n",
       "    'description': 'List all run rules.',\n",
       "    'operationId': 'list_rules_api_v1_runs_rules_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Dataset Id'}},\n",
       "     {'name': 'session_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Session Id'}},\n",
       "     {'name': 'type',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'enum': ['session', 'dataset'], 'type': 'string'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Type'}},\n",
       "     {'name': 'name_contains',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name Contains'}},\n",
       "     {'name': 'id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/RunRulesSchema'},\n",
       "         'title': 'Response List Rules Api V1 Runs Rules Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['run'],\n",
       "    'summary': 'Create Rule',\n",
       "    'description': 'Create a new run rule.',\n",
       "    'operationId': 'create_rule_api_v1_runs_rules_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunRulesCreateSchema'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunRulesSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/runs/rules/validate': {'post': {'tags': ['run'],\n",
       "    'summary': 'Validate Rule',\n",
       "    'description': 'Validate a rule by executing it with test data without creating a saved rule.\\n\\nThis endpoint allows testing LLM-as-judge evaluators before saving them. It accepts\\na rule configuration (same as rule creation) and test data, executes the evaluator,\\nand returns the evaluation results in the same format as batch_invoke_evaluator.\\n\\nOnly LLM-as-judge rules (evaluators) are supported. Code evaluators are not allowed.\\n\\nThe evaluator execution traces are written to the database (in the \"evaluators\"\\nproject), which allows users to see the evaluator execution history.',\n",
       "    'operationId': 'validate_rule_api_v1_runs_rules_validate_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunRulesValidateSchema'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'additionalProperties': True,\n",
       "          'type': 'object'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response Validate Rule Api V1 Runs Rules Validate Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/runs/rules/{rule_id}': {'patch': {'tags': ['run'],\n",
       "    'summary': 'Update Rule',\n",
       "    'description': 'Update a run rule.',\n",
       "    'operationId': 'update_rule_api_v1_runs_rules__rule_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'rule_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Rule Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunRulesUpdateSchema'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunRulesSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['run'],\n",
       "    'summary': 'Delete Rule',\n",
       "    'description': 'Delete a run rule.',\n",
       "    'operationId': 'delete_rule_api_v1_runs_rules__rule_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'rule_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Rule Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/runs/threads/{thread_id}': {'get': {'tags': ['run'],\n",
       "    'summary': 'Thread Preview',\n",
       "    'description': 'Get preview of a thread.',\n",
       "    'operationId': 'thread_preview_api_v1_runs_threads__thread_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'thread_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Thread Id'}},\n",
       "     {'name': 'session_id',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},\n",
       "     {'name': 'select',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/ThreadMessagesFormatType'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Select'}},\n",
       "     {'name': 'variables',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array', 'items': {'type': 'string'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Variables'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ThreadPreviewResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/runs/rules/{rule_id}/logs': {'get': {'tags': ['run'],\n",
       "    'summary': 'List Rule Logs',\n",
       "    'description': 'List logs for a particular rule',\n",
       "    'operationId': 'list_rule_logs_api_v1_runs_rules__rule_id__logs_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'rule_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Rule Id'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 1440,\n",
       "       'minimum': 100,\n",
       "       'default': 720,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'start_time',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Start Time'}},\n",
       "     {'name': 'end_time',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'End Time'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/RuleLogSchema'},\n",
       "         'title': 'Response List Rule Logs Api V1 Runs Rules  Rule Id  Logs Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/runs/rules/{rule_id}/last_applied': {'get': {'tags': ['run'],\n",
       "    'summary': 'Get Last Applied Rule',\n",
       "    'description': 'Get the last applied rule.',\n",
       "    'operationId': 'get_last_applied_rule_api_v1_runs_rules__rule_id__last_applied_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'rule_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Rule Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RuleLogSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/runs/rules/{rule_id}/trigger': {'post': {'tags': ['run'],\n",
       "    'summary': 'Trigger Rule',\n",
       "    'description': 'Trigger a run rule manually.',\n",
       "    'operationId': 'trigger_rule_api_v1_runs_rules__rule_id__trigger_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'rule_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Rule Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunRulesSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/runs/rules/trigger': {'post': {'tags': ['run'],\n",
       "    'summary': 'Trigger Rules',\n",
       "    'description': 'Trigger an array of run rules manually.',\n",
       "    'operationId': 'trigger_rules_api_v1_runs_rules_trigger_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TriggerRulesRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/runs/{run_id}': {'get': {'tags': ['run'],\n",
       "    'summary': 'Read Run',\n",
       "    'description': 'Get a specific run.',\n",
       "    'operationId': 'read_run_api_v1_runs__run_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'run_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Run Id'}},\n",
       "     {'name': 'session_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Session Id'}},\n",
       "     {'name': 'start_time',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Start Time'}},\n",
       "     {'name': 'exclude_s3_stored_attributes',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Exclude S3 Stored Attributes'}},\n",
       "     {'name': 'exclude_serialized',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Exclude Serialized'}},\n",
       "     {'name': 'include_messages',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Include Messages'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['run'],\n",
       "    'summary': 'Update Run',\n",
       "    'description': 'Update a run.',\n",
       "    'operationId': 'update_run_api_v1_runs__run_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'run_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Run Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/runs/{run_id}/share': {'get': {'tags': ['run'],\n",
       "    'summary': 'Read Run Share State',\n",
       "    'description': 'Get the state of sharing of a run.',\n",
       "    'operationId': 'read_run_share_state_api_v1_runs__run_id__share_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'run_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Run Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'anyOf': [{'$ref': '#/components/schemas/RunShareSchema'},\n",
       "          {'type': 'null'}],\n",
       "         'title': 'Response Read Run Share State Api V1 Runs  Run Id  Share Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'put': {'tags': ['run'],\n",
       "    'summary': 'Share Run',\n",
       "    'description': 'Share a run.',\n",
       "    'operationId': 'share_run_api_v1_runs__run_id__share_put',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'run_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Run Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunShareSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['run'],\n",
       "    'summary': 'Unshare Run',\n",
       "    'description': 'Unshare a run.',\n",
       "    'operationId': 'unshare_run_api_v1_runs__run_id__share_delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'run_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Run Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/runs/query': {'post': {'tags': ['run'],\n",
       "    'summary': 'Query Runs',\n",
       "    'operationId': 'query_runs_api_v1_runs_query_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/BodyParamsForRunsQuerySchema'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ListRunsResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/runs/generate-query': {'post': {'tags': ['run'],\n",
       "    'summary': 'Generate Query For Runs',\n",
       "    'description': 'Get runs filter expression query for a given natural language query.',\n",
       "    'operationId': 'generate_query_for_runs_api_v1_runs_generate_query_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RequestBodyForRunsGenerateQuery'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ResponseBodyForRunsGenerateQuery'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/runs/stats': {'post': {'tags': ['run'],\n",
       "    'summary': 'Stats Runs',\n",
       "    'description': 'Get all runs by query in body payload.',\n",
       "    'operationId': 'stats_runs_api_v1_runs_stats_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunStatsQueryParams'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'anyOf': [{'$ref': '#/components/schemas/RunStats'},\n",
       "          {'additionalProperties': {'$ref': '#/components/schemas/RunStats'},\n",
       "           'type': 'object'}],\n",
       "         'title': 'Response Stats Runs Api V1 Runs Stats Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/runs/group': {'post': {'tags': ['run'],\n",
       "    'summary': 'Group Runs',\n",
       "    'description': 'Get runs grouped by an expression',\n",
       "    'operationId': 'group_runs_api_v1_runs_group_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'accept',\n",
       "      'in': 'header',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Accept'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunGroupRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/runs/group/stats': {'post': {'tags': ['run'],\n",
       "    'summary': 'Stats Group Runs',\n",
       "    'description': 'Get stats for the grouped runs.',\n",
       "    'operationId': 'stats_group_runs_api_v1_runs_group_stats_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunGroupRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunGroupStats'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/runs/delete': {'post': {'tags': ['run'],\n",
       "    'summary': 'Delete Runs',\n",
       "    'description': 'Delete specific runs by trace IDs or metadata key-value pairs.',\n",
       "    'operationId': 'delete_runs_api_v1_runs_delete_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Body_delete_runs_api_v1_runs_delete_post'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/feedback/formulas': {'post': {'tags': ['feedback'],\n",
       "    'summary': 'Create Feedback Formula Ep',\n",
       "    'operationId': 'create_feedback_formula_ep_api_v1_feedback_formulas_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FeedbackFormulaCreate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FeedbackFormula'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'get': {'tags': ['feedback'],\n",
       "    'summary': 'List Feedback Formula Ep',\n",
       "    'operationId': 'list_feedback_formula_ep_api_v1_feedback_formulas_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'dataset_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Dataset Id'}},\n",
       "     {'name': 'session_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Session Id'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer', 'default': 20, 'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer', 'default': 0, 'title': 'Offset'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/FeedbackFormula'},\n",
       "         'title': 'Response List Feedback Formula Ep Api V1 Feedback Formulas Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/feedback/formulas/{feedback_formula_id}': {'get': {'tags': ['feedback'],\n",
       "    'summary': 'Get Feedback Formula Ep',\n",
       "    'operationId': 'get_feedback_formula_ep_api_v1_feedback_formulas__feedback_formula_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'feedback_formula_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Feedback Formula Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FeedbackFormula'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'put': {'tags': ['feedback'],\n",
       "    'summary': 'Update Feedback Formula Ep',\n",
       "    'operationId': 'update_feedback_formula_ep_api_v1_feedback_formulas__feedback_formula_id__put',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'feedback_formula_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Feedback Formula Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FeedbackFormulaUpdate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FeedbackFormula'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['feedback'],\n",
       "    'summary': 'Delete Feedback Formula Endpoint',\n",
       "    'operationId': 'delete_feedback_formula_endpoint_api_v1_feedback_formulas__feedback_formula_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'feedback_formula_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Feedback Formula Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/feedback/{feedback_id}': {'get': {'tags': ['feedback'],\n",
       "    'summary': 'Read Feedback',\n",
       "    'description': 'Get a specific feedback.',\n",
       "    'operationId': 'read_feedback_api_v1_feedback__feedback_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'feedback_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Feedback Id'}},\n",
       "     {'name': 'include_user_names',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Include User Names'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FeedbackSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['feedback'],\n",
       "    'summary': 'Update Feedback',\n",
       "    'description': 'Replace an existing feedback entry with a new, modified entry.',\n",
       "    'operationId': 'update_feedback_api_v1_feedback__feedback_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'feedback_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Feedback Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FeedbackUpdateSchema'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FeedbackSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['feedback'],\n",
       "    'summary': 'Delete Feedback',\n",
       "    'description': 'Delete a feedback.',\n",
       "    'operationId': 'delete_feedback_api_v1_feedback__feedback_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'feedback_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Feedback Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/feedback': {'get': {'tags': ['feedback'],\n",
       "    'summary': 'Read Feedbacks',\n",
       "    'description': 'List all Feedback by query params.',\n",
       "    'operationId': 'read_feedbacks_api_v1_feedback_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'run',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Run'}},\n",
       "     {'name': 'key',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array', 'items': {'type': 'string'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Key'}},\n",
       "     {'name': 'session',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Session'}},\n",
       "     {'name': 'source',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/SourceType'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Source'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'user',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'User'}},\n",
       "     {'name': 'has_comment',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Has Comment'}},\n",
       "     {'name': 'has_score',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Has Score'}},\n",
       "     {'name': 'level',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'$ref': '#/components/schemas/FeedbackLevel'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Level'}},\n",
       "     {'name': 'max_created_at',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Max Created At'}},\n",
       "     {'name': 'min_created_at',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Min Created At'}},\n",
       "     {'name': 'include_user_names',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Include User Names'}},\n",
       "     {'name': 'comparative_experiment_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Comparative Experiment Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/FeedbackSchema'},\n",
       "         'title': 'Response Read Feedbacks Api V1 Feedback Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['feedback'],\n",
       "    'summary': 'Create Feedback',\n",
       "    'description': 'Create a new feedback.',\n",
       "    'operationId': 'create_feedback_api_v1_feedback_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FeedbackCreateSchema'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FeedbackSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/feedback/eager': {'post': {'tags': ['feedback'],\n",
       "    'summary': 'Eagerly Create Feedback',\n",
       "    'description': 'Create a new feedback.\\n\\nThis method is invoked under the assumption that the run\\nis already visible in the app, thus already present in DB',\n",
       "    'operationId': 'eagerly_create_feedback_api_v1_feedback_eager_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FeedbackCreateSchema'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FeedbackSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/feedback/tokens': {'post': {'tags': ['feedback'],\n",
       "    'summary': 'Create Feedback Ingest Token',\n",
       "    'description': 'Create a new feedback ingest token.',\n",
       "    'operationId': 'create_feedback_ingest_token_api_v1_feedback_tokens_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'anyOf': [{'$ref': '#/components/schemas/FeedbackIngestTokenCreateSchema'},\n",
       "         {'type': 'array',\n",
       "          'items': {'$ref': '#/components/schemas/FeedbackIngestTokenCreateSchema'}}],\n",
       "        'title': 'Feedback Ingest Token'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'anyOf': [{'$ref': '#/components/schemas/FeedbackIngestTokenSchema'},\n",
       "          {'type': 'array',\n",
       "           'items': {'$ref': '#/components/schemas/FeedbackIngestTokenSchema'}}],\n",
       "         'title': 'Response Create Feedback Ingest Token Api V1 Feedback Tokens Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'get': {'tags': ['feedback'],\n",
       "    'summary': 'List Feedback Ingest Tokens',\n",
       "    'description': 'List all feedback ingest tokens for a run.',\n",
       "    'operationId': 'list_feedback_ingest_tokens_api_v1_feedback_tokens_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'run_id',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Run Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/FeedbackIngestTokenSchema'},\n",
       "         'title': 'Response List Feedback Ingest Tokens Api V1 Feedback Tokens Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/feedback/tokens/{token}': {'get': {'tags': ['feedback'],\n",
       "    'summary': 'Create Feedback With Token Get',\n",
       "    'description': 'Create a new feedback with a token.',\n",
       "    'operationId': 'create_feedback_with_token_get_api_v1_feedback_tokens__token__get',\n",
       "    'parameters': [{'name': 'token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Token'}},\n",
       "     {'name': 'score',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'number'},\n",
       "        {'type': 'integer'},\n",
       "        {'type': 'boolean'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Score'}},\n",
       "     {'name': 'value',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'number'},\n",
       "        {'type': 'integer'},\n",
       "        {'type': 'boolean'},\n",
       "        {'type': 'string'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Value'}},\n",
       "     {'name': 'comment',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Comment'}},\n",
       "     {'name': 'correction',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Correction'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['feedback'],\n",
       "    'summary': 'Create Feedback With Token Post',\n",
       "    'description': 'Create a new feedback with a token.',\n",
       "    'operationId': 'create_feedback_with_token_post_api_v1_feedback_tokens__token__post',\n",
       "    'parameters': [{'name': 'token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Token'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FeedbackCreateWithTokenExtendedSchema'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/run': {'get': {'tags': ['public'],\n",
       "    'summary': 'Get Shared Run',\n",
       "    'description': 'Get the shared run.',\n",
       "    'operationId': 'get_shared_run_api_v1_public__share_token__run_get',\n",
       "    'parameters': [{'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}},\n",
       "     {'name': 'exclude_s3_stored_attributes',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Exclude S3 Stored Attributes'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunPublicSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/run/{id}': {'get': {'tags': ['public'],\n",
       "    'summary': 'Get Shared Run By Id',\n",
       "    'description': 'Get the shared run.',\n",
       "    'operationId': 'get_shared_run_by_id_api_v1_public__share_token__run__id__get',\n",
       "    'parameters': [{'name': 'id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Id'}},\n",
       "     {'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}},\n",
       "     {'name': 'exclude_s3_stored_attributes',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Exclude S3 Stored Attributes'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunPublicSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/runs/query': {'post': {'tags': ['public'],\n",
       "    'summary': 'Query Shared Runs',\n",
       "    'description': 'Get run by ids or the shared run if not specifed.',\n",
       "    'operationId': 'query_shared_runs_api_v1_public__share_token__runs_query_post',\n",
       "    'parameters': [{'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/QueryParamsForPublicRunSchema'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ListPublicRunsResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/feedbacks': {'get': {'tags': ['public'],\n",
       "    'summary': 'Read Shared Feedbacks',\n",
       "    'operationId': 'read_shared_feedbacks_api_v1_public__share_token__feedbacks_get',\n",
       "    'parameters': [{'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}},\n",
       "     {'name': 'run',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Run'}},\n",
       "     {'name': 'key',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array', 'items': {'type': 'string'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Key'}},\n",
       "     {'name': 'session',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Session'}},\n",
       "     {'name': 'source',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/SourceType'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Source'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'user',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'User'}},\n",
       "     {'name': 'has_comment',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Has Comment'}},\n",
       "     {'name': 'has_score',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Has Score'}},\n",
       "     {'name': 'level',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'$ref': '#/components/schemas/FeedbackLevel'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Level'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/FeedbackSchema'},\n",
       "         'title': 'Response Read Shared Feedbacks Api V1 Public  Share Token  Feedbacks Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/datasets': {'get': {'tags': ['public'],\n",
       "    'summary': 'Read Shared Dataset',\n",
       "    'description': 'Get dataset by ids or the shared dataset if not specifed.',\n",
       "    'operationId': 'read_shared_dataset_api_v1_public__share_token__datasets_get',\n",
       "    'parameters': [{'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'sort_by',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'$ref': '#/components/schemas/SortByDatasetColumn',\n",
       "       'default': 'last_session_start_time'}},\n",
       "     {'name': 'sort_by_desc',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': True,\n",
       "       'title': 'Sort By Desc'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/DatasetPublicSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/examples/count': {'get': {'tags': ['public'],\n",
       "    'summary': 'Count Shared Examples',\n",
       "    'description': 'Count all examples by query params',\n",
       "    'operationId': 'count_shared_examples_api_v1_public__share_token__examples_count_get',\n",
       "    'parameters': [{'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}},\n",
       "     {'name': 'id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Id'}},\n",
       "     {'name': 'as_of',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'string'}],\n",
       "       'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.',\n",
       "       'default': 'latest',\n",
       "       'title': 'As Of'},\n",
       "      'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.'},\n",
       "     {'name': 'metadata',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Metadata'}},\n",
       "     {'name': 'filter',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Filter'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'integer',\n",
       "         'title': 'Response Count Shared Examples Api V1 Public  Share Token  Examples Count Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/examples': {'get': {'tags': ['public'],\n",
       "    'summary': 'Read Shared Examples',\n",
       "    'description': 'Get example by ids or the shared example if not specifed.',\n",
       "    'operationId': 'read_shared_examples_api_v1_public__share_token__examples_get',\n",
       "    'parameters': [{'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}},\n",
       "     {'name': 'id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Id'}},\n",
       "     {'name': 'as_of',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'string'}],\n",
       "       'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.',\n",
       "       'default': 'latest',\n",
       "       'title': 'As Of'},\n",
       "      'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.'},\n",
       "     {'name': 'metadata',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Metadata'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'select',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'array',\n",
       "       'items': {'$ref': '#/components/schemas/ExampleSelect'},\n",
       "       'default': ['id',\n",
       "        'created_at',\n",
       "        'modified_at',\n",
       "        'name',\n",
       "        'dataset_id',\n",
       "        'metadata',\n",
       "        'inputs',\n",
       "        'outputs',\n",
       "        'attachment_urls'],\n",
       "       'title': 'Select'}},\n",
       "     {'name': 'filter',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Filter'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/Example'},\n",
       "         'title': 'Response Read Shared Examples Api V1 Public  Share Token  Examples Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/datasets/sessions': {'get': {'tags': ['public'],\n",
       "    'summary': 'Read Shared Dataset Tracer Sessions',\n",
       "    'description': 'Get projects run on a dataset that has been shared.',\n",
       "    'operationId': 'read_shared_dataset_tracer_sessions_api_v1_public__share_token__datasets_sessions_get',\n",
       "    'parameters': [{'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}},\n",
       "     {'name': 'id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Id'}},\n",
       "     {'name': 'name',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name'}},\n",
       "     {'name': 'name_contains',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name Contains'}},\n",
       "     {'name': 'dataset_version',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Dataset Version'}},\n",
       "     {'name': 'sort_by',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'$ref': '#/components/schemas/SessionSortableColumns',\n",
       "       'default': 'start_time'}},\n",
       "     {'name': 'sort_by_desc',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean', 'default': True, 'title': 'Sort By Desc'}},\n",
       "     {'name': 'sort_by_feedback_key',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Sort By Feedback Key'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'facets',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean', 'default': False, 'title': 'Facets'}},\n",
       "     {'name': 'accept',\n",
       "      'in': 'header',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Accept'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/TracerSession'},\n",
       "         'title': 'Response Read Shared Dataset Tracer Sessions Api V1 Public  Share Token  Datasets Sessions Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/datasets/sessions-bulk': {'get': {'tags': ['public'],\n",
       "    'summary': 'Read Shared Dataset Tracer Sessions Bulk',\n",
       "    'description': 'Get sessions from multiple datasets using share tokens.',\n",
       "    'operationId': 'read_shared_dataset_tracer_sessions_bulk_api_v1_public_datasets_sessions_bulk_get',\n",
       "    'parameters': [{'name': 'share_tokens',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'array',\n",
       "       'items': {'type': 'string'},\n",
       "       'title': 'Share Tokens'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/TracerSession'},\n",
       "         'title': 'Response Read Shared Dataset Tracer Sessions Bulk Api V1 Public Datasets Sessions Bulk Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/examples/runs': {'post': {'tags': ['public'],\n",
       "    'summary': 'Read Shared Dataset Examples With Runs',\n",
       "    'description': 'Get examples with associated runs from sessions in a dataset that has been shared.',\n",
       "    'operationId': 'read_shared_dataset_examples_with_runs_api_v1_public__share_token__examples_runs_post',\n",
       "    'parameters': [{'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/QueryExampleSchemaWithRuns'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'anyOf': [{'type': 'array',\n",
       "           'items': {'$ref': '#/components/schemas/PublicExampleWithRuns'}},\n",
       "          {'type': 'array',\n",
       "           'items': {'$ref': '#/components/schemas/ExampleWithRunsCH'}}],\n",
       "         'title': 'Response Read Shared Dataset Examples With Runs Api V1 Public  Share Token  Examples Runs Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/datasets/runs/delta': {'post': {'tags': ['public'],\n",
       "    'summary': 'Read Shared Delta',\n",
       "    'description': 'Fetch the number of regressions/improvements for each example in a dataset, between sessions[0] and sessions[1].',\n",
       "    'operationId': 'read_shared_delta_api_v1_public__share_token__datasets_runs_delta_post',\n",
       "    'parameters': [{'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/QueryFeedbackDelta'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SessionFeedbackDelta'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/datasets/runs/delta/stream': {'post': {'tags': ['public'],\n",
       "    'summary': 'Read Shared Delta Stream',\n",
       "    'description': 'Stream feedback deltas for multiple feedback keys.\\n\\nReturns results in chunks as they become available. Each chunk contains\\nresults for one or more feedback keys. Errors for individual chunks are\\nincluded in the response rather than failing the entire operation.\\n\\nResponse format (SSE):\\n    event: data\\n    data: {\"feedback_deltas\": {\"key1\": {session_id: {...}}, ...}, \"errors\": null}\\n\\n    event: data\\n    data: {\"feedback_deltas\": {\"key2\": {...}}, \"errors\": null}\\n\\n    event: end',\n",
       "    'operationId': 'read_shared_delta_stream_api_v1_public__share_token__datasets_runs_delta_stream_post',\n",
       "    'parameters': [{'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/QueryFeedbackDeltaBatch'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/datasets/runs/query': {'post': {'tags': ['public'],\n",
       "    'summary': 'Query Shared Dataset Runs',\n",
       "    'description': 'Get runs in projects run over a dataset that has been shared.',\n",
       "    'operationId': 'query_shared_dataset_runs_api_v1_public__share_token__datasets_runs_query_post',\n",
       "    'parameters': [{'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/BodyParamsForRunSchema'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ListPublicDatasetRunsResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/datasets/runs/generate-query': {'post': {'tags': ['public'],\n",
       "    'summary': 'Generate Query For Shared Dataset Runs',\n",
       "    'description': 'Get runs in projects run over a dataset that has been shared.',\n",
       "    'operationId': 'generate_query_for_shared_dataset_runs_api_v1_public__share_token__datasets_runs_generate_query_post',\n",
       "    'parameters': [{'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RequestBodyForRunsGenerateQuery'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ResponseBodyForRunsGenerateQuery'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/datasets/runs/stats': {'post': {'tags': ['public'],\n",
       "    'summary': 'Stats Shared Dataset Runs',\n",
       "    'description': 'Get run stats in projects run over a dataset that has been shared.',\n",
       "    'operationId': 'stats_shared_dataset_runs_api_v1_public__share_token__datasets_runs_stats_post',\n",
       "    'parameters': [{'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunStatsQueryParams'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunStats'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/datasets/runs/{run_id}': {'get': {'tags': ['public'],\n",
       "    'summary': 'Read Shared Dataset Run',\n",
       "    'description': 'Get runs in projects run over a dataset that has been shared.',\n",
       "    'operationId': 'read_shared_dataset_run_api_v1_public__share_token__datasets_runs__run_id__get',\n",
       "    'parameters': [{'name': 'run_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Run Id'}},\n",
       "     {'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}},\n",
       "     {'name': 'exclude_s3_stored_attributes',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Exclude S3 Stored Attributes'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunPublicDatasetSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/datasets/feedback': {'get': {'tags': ['public'],\n",
       "    'summary': 'Read Shared Dataset Feedback',\n",
       "    'description': 'Get feedback for runs in projects run over a dataset that has been shared.',\n",
       "    'operationId': 'read_shared_dataset_feedback_api_v1_public__share_token__datasets_feedback_get',\n",
       "    'parameters': [{'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}},\n",
       "     {'name': 'run',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Run'}},\n",
       "     {'name': 'key',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array', 'items': {'type': 'string'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Key'}},\n",
       "     {'name': 'session',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Session'}},\n",
       "     {'name': 'source',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/SourceType'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Source'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'user',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'User'}},\n",
       "     {'name': 'has_comment',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Has Comment'}},\n",
       "     {'name': 'has_score',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Has Score'}},\n",
       "     {'name': 'level',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'$ref': '#/components/schemas/FeedbackLevel'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Level'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/FeedbackSchema'},\n",
       "         'title': 'Response Read Shared Dataset Feedback Api V1 Public  Share Token  Datasets Feedback Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/{share_token}/datasets/comparative': {'get': {'tags': ['public'],\n",
       "    'summary': 'Read Shared Comparative Experiments',\n",
       "    'description': 'Get all comparative experiments for a given dataset.',\n",
       "    'operationId': 'read_shared_comparative_experiments_api_v1_public__share_token__datasets_comparative_get',\n",
       "    'parameters': [{'name': 'share_token',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Share Token'}},\n",
       "     {'name': 'name',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name'}},\n",
       "     {'name': 'name_contains',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name Contains'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'sort_by',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'$ref': '#/components/schemas/SortByComparativeExperimentColumn',\n",
       "       'default': 'created_at'}},\n",
       "     {'name': 'sort_by_desc',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': True,\n",
       "       'title': 'Sort By Desc'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/PublicComparativeExperiment'},\n",
       "         'title': 'Response Read Shared Comparative Experiments Api V1 Public  Share Token  Datasets Comparative Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/schemas/{version}/message.json': {'get': {'tags': ['public'],\n",
       "    'summary': 'Get Message Json Schema',\n",
       "    'operationId': 'get_message_json_schema_api_v1_public_schemas__version__message_json_get',\n",
       "    'parameters': [{'name': 'version',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Version'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/public/schemas/{version}/tooldef.json': {'get': {'tags': ['public'],\n",
       "    'summary': 'Get Tool Def Json Schema',\n",
       "    'operationId': 'get_tool_def_json_schema_api_v1_public_schemas__version__tooldef_json_get',\n",
       "    'parameters': [{'name': 'version',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Version'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/annotation-queues': {'get': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Get Annotation Queues',\n",
       "    'operationId': 'get_annotation_queues_api_v1_annotation_queues_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'ids',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Ids'}},\n",
       "     {'name': 'name',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name'}},\n",
       "     {'name': 'name_contains',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name Contains'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'tag_value_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Tag Value Id'}},\n",
       "     {'name': 'dataset_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Dataset Id'}},\n",
       "     {'name': 'queue_type',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'enum': ['single', 'pairwise'], 'type': 'string'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Queue Type'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/AnnotationQueueSchemaWithSize'},\n",
       "         'title': 'Response Get Annotation Queues Api V1 Annotation Queues Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Create Annotation Queue',\n",
       "    'operationId': 'create_annotation_queue_api_v1_annotation_queues_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/AnnotationQueueCreateSchema'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/AnnotationQueueSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/annotation-queues/populate': {'post': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Populate Annotation Queue',\n",
       "    'description': 'Populate annotation queue with runs from an experiment.',\n",
       "    'operationId': 'populate_annotation_queue_api_v1_annotation_queues_populate_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PopulateAnnotationQueueSchema'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/annotation-queues/{queue_id}': {'delete': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Delete Annotation Queue',\n",
       "    'operationId': 'delete_annotation_queue_api_v1_annotation_queues__queue_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'queue_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Queue Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Update Annotation Queue',\n",
       "    'operationId': 'update_annotation_queue_api_v1_annotation_queues__queue_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'queue_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Queue Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/AnnotationQueueUpdateSchema'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'get': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Get Annotation Queue',\n",
       "    'operationId': 'get_annotation_queue_api_v1_annotation_queues__queue_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'queue_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Queue Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/AnnotationQueueSchemaWithRubric'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/annotation-queues/{queue_id}/runs': {'post': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Add Runs To Annotation Queue',\n",
       "    'operationId': 'add_runs_to_annotation_queue_api_v1_annotation_queues__queue_id__runs_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'queue_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Queue Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'anyOf': [{'type': 'array',\n",
       "          'items': {'type': 'string', 'format': 'uuid'}},\n",
       "         {'type': 'array',\n",
       "          'items': {'$ref': '#/components/schemas/AnnotationQueueRunAddSchema'}}],\n",
       "        'title': 'Runs'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/AnnotationQueueRunSchema'},\n",
       "         'title': 'Response Add Runs To Annotation Queue Api V1 Annotation Queues  Queue Id  Runs Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'get': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Get Runs From Annotation Queue',\n",
       "    'operationId': 'get_runs_from_annotation_queue_api_v1_annotation_queues__queue_id__runs_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'queue_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Queue Id'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'archived',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Archived'}},\n",
       "     {'name': 'include_stats',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Include Stats'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/RunSchemaWithAnnotationQueueInfo'},\n",
       "         'title': 'Response Get Runs From Annotation Queue Api V1 Annotation Queues  Queue Id  Runs Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/annotation-queues/{queue_id}/export': {'post': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Export Annotation Queue Archived Runs',\n",
       "    'operationId': 'export_annotation_queue_archived_runs_api_v1_annotation_queues__queue_id__export_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'queue_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Queue Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ExportAnnotationQueueRunsRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/annotation-queues/{queue_id}/run/{index}': {'get': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Get Run From Annotation Queue',\n",
       "    'description': 'Get a run from an annotation queue',\n",
       "    'operationId': 'get_run_from_annotation_queue_api_v1_annotation_queues__queue_id__run__index__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'queue_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Queue Id'}},\n",
       "     {'name': 'index',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'integer', 'title': 'Index'}},\n",
       "     {'name': 'include_extra',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Include Extra'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RunSchemaWithAnnotationQueueInfo'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/annotation-queues/{run_id}/queues': {'get': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Get Annotation Queues For Run',\n",
       "    'operationId': 'get_annotation_queues_for_run_api_v1_annotation_queues__run_id__queues_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'run_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Run Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/AnnotationQueueSchema'},\n",
       "         'title': 'Response Get Annotation Queues For Run Api V1 Annotation Queues  Run Id  Queues Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/annotation-queues/{queue_id}/runs/{queue_run_id}': {'patch': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Update Run In Annotation Queue',\n",
       "    'operationId': 'update_run_in_annotation_queue_api_v1_annotation_queues__queue_id__runs__queue_run_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'queue_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Queue Id'}},\n",
       "     {'name': 'queue_run_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Queue Run Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/AnnotationQueueRunUpdateSchema'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Delete Run From Annotation Queue',\n",
       "    'operationId': 'delete_run_from_annotation_queue_api_v1_annotation_queues__queue_id__runs__queue_run_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'queue_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Queue Id'}},\n",
       "     {'name': 'queue_run_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Queue Run Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/annotation-queues/{queue_id}/runs/delete': {'post': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Delete Runs From Annotation Queue',\n",
       "    'operationId': 'delete_runs_from_annotation_queue_api_v1_annotation_queues__queue_id__runs_delete_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'queue_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Queue Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/AnnotationQueueBulkDeleteRunsRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/annotation-queues/{queue_id}/total_size': {'get': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Get Total Size From Annotation Queue',\n",
       "    'operationId': 'get_total_size_from_annotation_queue_api_v1_annotation_queues__queue_id__total_size_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'queue_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Queue Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/AnnotationQueueSizeSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/annotation-queues/{queue_id}/total_archived': {'get': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Get Total Archived From Annotation Queue',\n",
       "    'operationId': 'get_total_archived_from_annotation_queue_api_v1_annotation_queues__queue_id__total_archived_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'queue_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Queue Id'}},\n",
       "     {'name': 'start_time',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Start Time'}},\n",
       "     {'name': 'end_time',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'End Time'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/AnnotationQueueSizeSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/annotation-queues/{queue_id}/size': {'get': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Get Size From Annotation Queue',\n",
       "    'operationId': 'get_size_from_annotation_queue_api_v1_annotation_queues__queue_id__size_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'queue_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Queue Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/AnnotationQueueSizeSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/annotation-queues/status/{annotation_queue_run_id}': {'post': {'tags': ['annotation-queues'],\n",
       "    'summary': 'Create Identity Annotation Queue Run Status',\n",
       "    'operationId': 'create_identity_annotation_queue_run_status_api_v1_annotation_queues_status__annotation_queue_run_id__post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'annotation_queue_run_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Annotation Queue Run Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/IdentityAnnotationQueueRunStatusCreateSchema'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/ace/execute': {'post': {'tags': ['ace'],\n",
       "    'summary': 'Execute',\n",
       "    'description': 'Execute some custom code for testing purposes.',\n",
       "    'operationId': 'execute_api_v1_ace_execute_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Body_execute_api_v1_ace_execute_post'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'additionalProperties': True,\n",
       "         'type': 'object',\n",
       "         'title': 'Response Execute Api V1 Ace Execute Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/bulk-exports': {'get': {'tags': ['bulk-exports'],\n",
       "    'summary': 'Get Bulk Exports',\n",
       "    'description': \"Get the current workspace's bulk exports\",\n",
       "    'operationId': 'get_bulk_exports_api_v1_bulk_exports_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/BulkExport'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response Get Bulk Exports Api V1 Bulk Exports Get'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['bulk-exports'],\n",
       "    'summary': 'Create Bulk Export',\n",
       "    'description': 'Create a new bulk export',\n",
       "    'operationId': 'create_bulk_export_api_v1_bulk_exports_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/BulkExportCreate'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/BulkExport'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/bulk-exports/destinations': {'get': {'tags': ['bulk-exports'],\n",
       "    'summary': 'Get Bulk Export Destinations',\n",
       "    'description': \"Get the current workspace's bulk export destinations\",\n",
       "    'operationId': 'get_bulk_export_destinations_api_v1_bulk_exports_destinations_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/BulkExportDestination'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response Get Bulk Export Destinations Api V1 Bulk Exports Destinations Get'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['bulk-exports'],\n",
       "    'summary': 'Create Bulk Export Destination',\n",
       "    'description': 'Create a new bulk export destination',\n",
       "    'operationId': 'create_bulk_export_destination_api_v1_bulk_exports_destinations_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/BulkExportDestinationCreate'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/BulkExportDestination'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/bulk-exports/runs': {'get': {'tags': ['bulk-exports'],\n",
       "    'summary': 'Get Bulk Export Runs Filtered',\n",
       "    'description': 'Get all bulk export runs for exports that were created from a scheduled bulk export',\n",
       "    'operationId': 'get_bulk_export_runs_filtered_api_v1_bulk_exports_runs_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'source_bulk_export_id',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Source Bulk Export Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/BulkExportRun'},\n",
       "         'title': 'Response Get Bulk Export Runs Filtered Api V1 Bulk Exports Runs Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/bulk-exports/{bulk_export_id}': {'get': {'tags': ['bulk-exports'],\n",
       "    'summary': 'Get Bulk Export',\n",
       "    'description': 'Get a single bulk export by ID',\n",
       "    'operationId': 'get_bulk_export_api_v1_bulk_exports__bulk_export_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'bulk_export_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Bulk Export Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/BulkExport'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['bulk-exports'],\n",
       "    'summary': 'Cancel Bulk Export',\n",
       "    'description': 'Cancel a bulk export by ID',\n",
       "    'operationId': 'cancel_bulk_export_api_v1_bulk_exports__bulk_export_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'bulk_export_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Bulk Export Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/BulkExportUpdate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/BulkExport'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/bulk-exports/destinations/{destination_id}': {'get': {'tags': ['bulk-exports'],\n",
       "    'summary': 'Get Bulk Export Destination',\n",
       "    'description': 'Get a single bulk export destination by ID',\n",
       "    'operationId': 'get_bulk_export_destination_api_v1_bulk_exports_destinations__destination_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'destination_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Destination Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/BulkExportDestination'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/bulk-exports/{bulk_export_id}/runs': {'get': {'tags': ['bulk-exports'],\n",
       "    'summary': 'Get Bulk Export Runs',\n",
       "    'description': \"Get a bulk export's runs\",\n",
       "    'operationId': 'get_bulk_export_runs_api_v1_bulk_exports__bulk_export_id__runs_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'bulk_export_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Bulk Export Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/BulkExportRun'},\n",
       "         'title': 'Response Get Bulk Export Runs Api V1 Bulk Exports  Bulk Export Id  Runs Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/bulk-exports/{bulk_export_id}/runs/{run_id}': {'get': {'tags': ['bulk-exports'],\n",
       "    'summary': 'Get Bulk Export Run',\n",
       "    'description': \"Get a single bulk export's run by ID\",\n",
       "    'operationId': 'get_bulk_export_run_api_v1_bulk_exports__bulk_export_id__runs__run_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'bulk_export_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Bulk Export Id'}},\n",
       "     {'name': 'run_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Run Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/BulkExportRun'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/tenants': {'get': {'tags': ['tenant'],\n",
       "    'summary': 'List Tenants',\n",
       "    'description': 'Get all tenants visible to this auth',\n",
       "    'operationId': 'list_tenants_api_v1_tenants_get',\n",
       "    'security': [{'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'skip_create',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean', 'default': False, 'title': 'Skip Create'}},\n",
       "     {'name': 'include_deleted',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Include Deleted'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/TenantForUser'},\n",
       "         'title': 'Response List Tenants Api V1 Tenants Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['tenant'],\n",
       "    'summary': 'Create Tenant',\n",
       "    'description': 'Create a new organization and corresponding workspace.',\n",
       "    'operationId': 'create_tenant_api_v1_tenants_post',\n",
       "    'security': [{'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TenantCreate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/app__schemas__Tenant'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/info': {'get': {'tags': ['info'],\n",
       "    'summary': 'Get Server Info',\n",
       "    'description': 'Get information about the current deployment of LangSmith.',\n",
       "    'operationId': 'get_server_info_api_v1_info_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/InfoGetResponse'}}}}}}},\n",
       "  '/api/v1/info/health': {'get': {'tags': ['info'],\n",
       "    'summary': 'Get Health Info',\n",
       "    'description': 'Get health information about the current deployment of LangSmith.',\n",
       "    'operationId': 'get_health_info_api_v1_info_health_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HealthInfoGetResponse'}}}}}}},\n",
       "  '/api/v1/feedback-configs': {'get': {'tags': ['feedback-configs'],\n",
       "    'summary': 'List Feedback Configs Endpoint',\n",
       "    'operationId': 'list_feedback_configs_endpoint_api_v1_feedback_configs_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'key',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string'},\n",
       "         'maxItems': 50},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Key'}},\n",
       "     {'name': 'name_contains',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Name Contains'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'integer', 'maximum': 100, 'minimum': 1},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'sort_by_desc',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean', 'default': True, 'title': 'Sort By Desc'}},\n",
       "     {'name': 'read_after_write',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Read After Write'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/FeedbackConfigSchema'},\n",
       "         'title': 'Response List Feedback Configs Endpoint Api V1 Feedback Configs Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['feedback-configs'],\n",
       "    'summary': 'Create Feedback Config Endpoint',\n",
       "    'operationId': 'create_feedback_config_endpoint_api_v1_feedback_configs_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CreateFeedbackConfigSchema'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FeedbackConfigSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['feedback-configs'],\n",
       "    'summary': 'Update Feedback Config Endpoint',\n",
       "    'operationId': 'update_feedback_config_endpoint_api_v1_feedback_configs_patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/UpdateFeedbackConfigSchema'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/FeedbackConfigSchema'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['feedback-configs'],\n",
       "    'summary': 'Delete Feedback Config Endpoint',\n",
       "    'description': 'Soft delete a feedback config by marking it as deleted.\\n\\nThe config can be recreated later with the same key (simple reuse pattern).\\nExisting feedback records with this key will remain unchanged.',\n",
       "    'operationId': 'delete_feedback_config_endpoint_api_v1_feedback_configs_delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'feedback_key',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Feedback Key'}}],\n",
       "    'responses': {'204': {'description': 'Successful Response'},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/metrics/queue/{queue_name}': {'get': {'tags': ['metrics'],\n",
       "    'summary': 'Get Queue Metrics',\n",
       "    'description': 'Return cached SAQ queue counts for the requested queue.',\n",
       "    'operationId': 'get_queue_metrics_api_v1_metrics_queue__queue_name__get',\n",
       "    'parameters': [{'name': 'queue_name',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Queue Name'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/QueueInfoResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/model-price-map': {'get': {'tags': ['model-price-map'],\n",
       "    'summary': 'Read Model Price Map',\n",
       "    'operationId': 'read_model_price_map_api_v1_model_price_map_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['model-price-map'],\n",
       "    'summary': 'Create New Model Price',\n",
       "    'operationId': 'create_new_model_price_api_v1_model_price_map_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ModelPriceMapCreateSchema'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/model-price-map/{id}': {'put': {'tags': ['model-price-map'],\n",
       "    'summary': 'Update Model Price',\n",
       "    'operationId': 'update_model_price_api_v1_model_price_map__id__put',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ModelPriceMapUpdateSchema'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['model-price-map'],\n",
       "    'summary': 'Delete Model Price',\n",
       "    'operationId': 'delete_model_price_api_v1_model_price_map__id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/usage-limits': {'get': {'tags': ['usage-limits'],\n",
       "    'summary': 'List Usage Limits',\n",
       "    'description': 'List out the configured usage limits for a given tenant.',\n",
       "    'operationId': 'list_usage_limits_api_v1_usage_limits_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/UsageLimit'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response List Usage Limits Api V1 Usage Limits Get'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]},\n",
       "   'put': {'tags': ['usage-limits'],\n",
       "    'summary': 'Upsert Usage Limit',\n",
       "    'description': 'Create a new usage limit.',\n",
       "    'operationId': 'upsert_usage_limit_api_v1_usage_limits_put',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/UpsertUsageLimit'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/UsageLimit'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/usage-limits/org': {'get': {'tags': ['usage-limits'],\n",
       "    'summary': 'List Org Usage Limits',\n",
       "    'description': 'List out the configured usage limits for a given organization.',\n",
       "    'operationId': 'list_org_usage_limits_api_v1_usage_limits_org_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/UsageLimit'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response List Org Usage Limits Api V1 Usage Limits Org Get'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/usage-limits/{usage_limit_id}': {'delete': {'tags': ['usage-limits'],\n",
       "    'summary': 'Delete Usage Limit',\n",
       "    'description': 'Delete a specific usage limit.',\n",
       "    'operationId': 'delete_usage_limit_api_v1_usage_limits__usage_limit_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'usage_limit_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Usage Limit Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/ttl-settings': {'get': {'tags': ['ttl-settings'],\n",
       "    'summary': 'List Ttl Settings',\n",
       "    'description': 'List out the configured TTL settings for a given tenant.',\n",
       "    'operationId': 'list_ttl_settings_api_v1_ttl_settings_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/TTLSettings'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response List Ttl Settings Api V1 Ttl Settings Get'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]},\n",
       "   'put': {'tags': ['ttl-settings'],\n",
       "    'summary': 'Upsert Ttl Settings',\n",
       "    'operationId': 'upsert_ttl_settings_api_v1_ttl_settings_put',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/UpsertTTLSettingsRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TTLSettings'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/prompts/invoke_prompt': {'post': {'tags': ['prompts'],\n",
       "    'summary': 'Invoke Prompt',\n",
       "    'operationId': 'invoke_prompt_api_v1_prompts_invoke_prompt_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/InvokePromptPayload'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/prompts/canvas': {'post': {'tags': ['prompts'],\n",
       "    'summary': 'Prompt Canvas',\n",
       "    'operationId': 'prompt_canvas_api_v1_prompts_canvas_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PlaygroundPromptCanvasPayload'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/prompt-webhooks': {'get': {'tags': ['prompt-webhooks'],\n",
       "    'summary': 'List Prompt Webhooks',\n",
       "    'description': 'List all prompt webhooks for the current tenant.',\n",
       "    'operationId': 'list_prompt_webhooks_api_v1_prompt_webhooks_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/PromptWebhook'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response List Prompt Webhooks Api V1 Prompt Webhooks Get'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['prompt-webhooks'],\n",
       "    'summary': 'Create Prompt Webhook',\n",
       "    'description': 'Create a new prompt webhook.',\n",
       "    'operationId': 'create_prompt_webhook_api_v1_prompt_webhooks_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PromptWebhookCreate'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PromptWebhook'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/prompt-webhooks/{webhook_id}': {'get': {'tags': ['prompt-webhooks'],\n",
       "    'summary': 'Get Prompt Webhook',\n",
       "    'description': 'Get a specific prompt webhook.',\n",
       "    'operationId': 'get_prompt_webhook_api_v1_prompt_webhooks__webhook_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'webhook_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Webhook Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PromptWebhook'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['prompt-webhooks'],\n",
       "    'summary': 'Update Prompt Webhook',\n",
       "    'description': 'Update a specific prompt webhook.',\n",
       "    'operationId': 'update_prompt_webhook_api_v1_prompt_webhooks__webhook_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'webhook_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Webhook Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PromptWebhookUpdate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PromptWebhook'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['prompt-webhooks'],\n",
       "    'summary': 'Delete Prompt Webhook',\n",
       "    'description': 'Delete a specific prompt webhook.',\n",
       "    'operationId': 'delete_prompt_webhook_api_v1_prompt_webhooks__webhook_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'webhook_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Webhook Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/prompt-webhooks/test': {'post': {'tags': ['prompt-webhooks'],\n",
       "    'summary': 'Test Prompt Webhook',\n",
       "    'description': 'Test a specific prompt webhook.',\n",
       "    'operationId': 'test_prompt_webhook_api_v1_prompt_webhooks_test_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PromptWebhookTest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'additionalProperties': {'type': 'string'},\n",
       "         'type': 'object',\n",
       "         'title': 'Response Test Prompt Webhook Api V1 Prompt Webhooks Test Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/workspaces': {'get': {'tags': ['workspaces'],\n",
       "    'summary': 'List Workspaces',\n",
       "    'description': 'Get all workspaces visible to this auth in the current org. Does not create a new workspace/org.',\n",
       "    'operationId': 'list_workspaces_api_v1_workspaces_get',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'include_deleted',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Include Deleted'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/TenantForUser'},\n",
       "         'title': 'Response List Workspaces Api V1 Workspaces Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['workspaces'],\n",
       "    'summary': 'Create Workspace',\n",
       "    'description': 'Create a new workspace.',\n",
       "    'operationId': 'create_workspace_api_v1_workspaces_post',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/WorkspaceCreate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/app__schemas__Tenant'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/{workspace_id}': {'patch': {'tags': ['workspaces'],\n",
       "    'summary': 'Patch Workspace',\n",
       "    'operationId': 'patch_workspace_api_v1_workspaces__workspace_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'workspace_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Workspace Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/WorkspacePatch'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/app__schemas__Tenant'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['workspaces'],\n",
       "    'summary': 'Delete Workspace',\n",
       "    'operationId': 'delete_workspace_api_v1_workspaces__workspace_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'workspace_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Workspace Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/pending': {'get': {'tags': ['workspaces'],\n",
       "    'summary': 'List Pending Workspace Invites',\n",
       "    'description': 'Get all workspaces visible to this auth',\n",
       "    'operationId': 'list_pending_workspace_invites_api_v1_workspaces_pending_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/app__schemas__Tenant'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response List Pending Workspace Invites Api V1 Workspaces Pending Get'}}}}},\n",
       "    'security': [{'Bearer Auth': []}]}},\n",
       "  '/api/v1/workspaces/pending/{id}': {'delete': {'tags': ['workspaces'],\n",
       "    'summary': 'Delete Pending Workspace Invite',\n",
       "    'operationId': 'delete_pending_workspace_invite_api_v1_workspaces_pending__id__delete',\n",
       "    'security': [{'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/pending/{workspace_id}/claim': {'post': {'tags': ['workspaces'],\n",
       "    'summary': 'Claim Pending Workspace Invite',\n",
       "    'operationId': 'claim_pending_workspace_invite_api_v1_workspaces_pending__workspace_id__claim_post',\n",
       "    'deprecated': True,\n",
       "    'security': [{'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'workspace_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Workspace Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/current/stats': {'get': {'tags': ['workspaces'],\n",
       "    'summary': 'Get Current Workspace Stats',\n",
       "    'operationId': 'get_current_workspace_stats_api_v1_workspaces_current_stats_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'tag_value_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Tag Value Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TenantStats'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/current/members': {'get': {'tags': ['workspaces'],\n",
       "    'summary': 'Get Current Workspace Members',\n",
       "    'operationId': 'get_current_workspace_members_api_v1_workspaces_current_members_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TenantMembers'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['workspaces'],\n",
       "    'summary': 'Add Member To Current Workspace',\n",
       "    'description': 'Add an existing organization member to the current workspace.',\n",
       "    'operationId': 'add_member_to_current_workspace_api_v1_workspaces_current_members_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/IdentityCreate'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Identity'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/workspaces/current/members/active': {'get': {'tags': ['workspaces'],\n",
       "    'summary': 'Get Current Active Workspace Members',\n",
       "    'operationId': 'get_current_active_workspace_members_api_v1_workspaces_current_members_active_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 500,\n",
       "       'minimum': 1,\n",
       "       'default': 50,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'emails',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'array',\n",
       "       'items': {'type': 'string'},\n",
       "       'default': [],\n",
       "       'title': 'Emails'}},\n",
       "     {'name': 'q',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'description': 'Search query for email',\n",
       "       'title': 'Q'},\n",
       "      'description': 'Search query for email'},\n",
       "     {'name': 'ls_user_ids',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'array',\n",
       "       'items': {'type': 'string', 'format': 'uuid'},\n",
       "       'default': [],\n",
       "       'title': 'Ls User Ids'}},\n",
       "     {'name': 'user_ids',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'array',\n",
       "       'items': {'type': 'string', 'format': 'uuid'},\n",
       "       'deprecated': True,\n",
       "       'title': 'User Ids'},\n",
       "      'deprecated': True},\n",
       "     {'name': 'is_disabled',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Is Disabled'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/MemberIdentity'},\n",
       "         'title': 'Response Get Current Active Workspace Members Api V1 Workspaces Current Members Active Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/current/members/pending': {'get': {'tags': ['workspaces'],\n",
       "    'summary': 'Get Current Pending Workspace Members',\n",
       "    'operationId': 'get_current_pending_workspace_members_api_v1_workspaces_current_members_pending_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 500,\n",
       "       'minimum': 1,\n",
       "       'default': 50,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'emails',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'array',\n",
       "       'items': {'type': 'string'},\n",
       "       'default': [],\n",
       "       'title': 'Emails'}},\n",
       "     {'name': 'q',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'description': 'Search query for email',\n",
       "       'title': 'Q'},\n",
       "      'description': 'Search query for email'}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/PendingIdentity'},\n",
       "         'title': 'Response Get Current Pending Workspace Members Api V1 Workspaces Current Members Pending Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/current/members/batch': {'post': {'tags': ['workspaces'],\n",
       "    'summary': 'Add Members To Current Workspace Batch',\n",
       "    'description': 'Batch invite up to 500 users to the current workspace and organization.',\n",
       "    'operationId': 'add_members_to_current_workspace_batch_api_v1_workspaces_current_members_batch_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/PendingIdentityCreate'},\n",
       "        'type': 'array',\n",
       "        'title': 'Payloads'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/PendingIdentity'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response Add Members To Current Workspace Batch Api V1 Workspaces Current Members Batch Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []},\n",
       "     {'Organization ID': []}]}},\n",
       "  '/api/v1/workspaces/current/shared': {'get': {'tags': ['workspaces'],\n",
       "    'summary': 'Get Shared Tokens',\n",
       "    'description': 'List all shared entities and their tokens by the workspace.',\n",
       "    'operationId': 'get_shared_tokens_api_v1_workspaces_current_shared_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 50,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TenantShareTokensResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['workspaces'],\n",
       "    'summary': 'Bulk Unshare Entities',\n",
       "    'description': 'Bulk unshare entities by share tokens for the workspace.',\n",
       "    'operationId': 'bulk_unshare_entities_api_v1_workspaces_current_shared_delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TenantBulkUnshareRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/current/members/{identity_id}': {'delete': {'tags': ['workspaces'],\n",
       "    'summary': 'Delete Current Workspace Member',\n",
       "    'operationId': 'delete_current_workspace_member_api_v1_workspaces_current_members__identity_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'identity_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Identity Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['workspaces'],\n",
       "    'summary': 'Patch Current Workspace Member',\n",
       "    'operationId': 'patch_current_workspace_member_api_v1_workspaces_current_members__identity_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'identity_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Identity Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/IdentityPatch'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/current/members/{identity_id}/pending': {'delete': {'tags': ['workspaces'],\n",
       "    'summary': 'Delete Current Workspace Pending Member',\n",
       "    'operationId': 'delete_current_workspace_pending_member_api_v1_workspaces_current_members__identity_id__pending_delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'identity_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Identity Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/current/usage_limits': {'get': {'tags': ['workspaces'],\n",
       "    'summary': 'Get Current Workspace Usage Limits Info',\n",
       "    'operationId': 'get_current_workspace_usage_limits_info_api_v1_workspaces_current_usage_limits_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TenantUsageLimitInfo'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/workspaces/current/secrets': {'get': {'tags': ['workspaces'],\n",
       "    'summary': 'List Current Workspace Secrets',\n",
       "    'operationId': 'list_current_workspace_secrets_api_v1_workspaces_current_secrets_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/SecretKey'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response List Current Workspace Secrets Api V1 Workspaces Current Secrets Get'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['workspaces'],\n",
       "    'summary': 'Upsert Current Workspace Secrets',\n",
       "    'operationId': 'upsert_current_workspace_secrets_api_v1_workspaces_current_secrets_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/SecretUpsert'},\n",
       "        'type': 'array',\n",
       "        'title': 'Secrets'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/workspaces/current/secrets/encrypted': {'get': {'tags': ['workspaces'],\n",
       "    'summary': 'Get Current Workspace Encrypted Secrets',\n",
       "    'description': 'Get encrypted workspace secrets for use with Agent Builder and external services.',\n",
       "    'operationId': 'get_current_workspace_encrypted_secrets_api_v1_workspaces_current_secrets_encrypted_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'service',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'enum': ['agent_builder', 'polly'],\n",
       "       'type': 'string',\n",
       "       'description': 'Service requesting encrypted secrets',\n",
       "       'title': 'Service'},\n",
       "      'description': 'Service requesting encrypted secrets'},\n",
       "     {'name': 'key_names',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array', 'items': {'type': 'string'}},\n",
       "        {'type': 'null'}],\n",
       "       'description': 'Optional list of workspace secret keys to return',\n",
       "       'title': 'Key Names'},\n",
       "      'description': 'Optional list of workspace secret keys to return'}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/InternalSecretsResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/current/tag-keys': {'get': {'tags': ['workspaces'],\n",
       "    'summary': 'List Tag Keys',\n",
       "    'operationId': 'list_tag_keys_api_v1_workspaces_current_tag_keys_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/TagKey'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response List Tag Keys Api V1 Workspaces Current Tag Keys Get'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['workspaces'],\n",
       "    'summary': 'Create Tag Key',\n",
       "    'operationId': 'create_tag_key_api_v1_workspaces_current_tag_keys_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TagKeyCreate'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TagKey'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/workspaces/current/tag-keys/{tag_key_id}': {'patch': {'tags': ['workspaces'],\n",
       "    'summary': 'Update Tag Key',\n",
       "    'operationId': 'update_tag_key_api_v1_workspaces_current_tag_keys__tag_key_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'tag_key_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Tag Key Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TagKeyUpdate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TagKey'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'get': {'tags': ['workspaces'],\n",
       "    'summary': 'Get Tag Key',\n",
       "    'operationId': 'get_tag_key_api_v1_workspaces_current_tag_keys__tag_key_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'tag_key_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Tag Key Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TagKey'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['workspaces'],\n",
       "    'summary': 'Delete Tag Key',\n",
       "    'operationId': 'delete_tag_key_api_v1_workspaces_current_tag_keys__tag_key_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'tag_key_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Tag Key Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/current/tag-keys/{tag_key_id}/tag-values': {'post': {'tags': ['workspaces'],\n",
       "    'summary': 'Create Tag Value',\n",
       "    'operationId': 'create_tag_value_api_v1_workspaces_current_tag_keys__tag_key_id__tag_values_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'tag_key_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Tag Key Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TagValueCreate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TagValue'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'get': {'tags': ['workspaces'],\n",
       "    'summary': 'List Tag Values',\n",
       "    'operationId': 'list_tag_values_api_v1_workspaces_current_tag_keys__tag_key_id__tag_values_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'tag_key_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Tag Key Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/TagValue'},\n",
       "         'title': 'Response List Tag Values Api V1 Workspaces Current Tag Keys  Tag Key Id  Tag Values Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/current/tag-keys/{tag_key_id}/tag-values/{tag_value_id}': {'get': {'tags': ['workspaces'],\n",
       "    'summary': 'Get Tag Value',\n",
       "    'operationId': 'get_tag_value_api_v1_workspaces_current_tag_keys__tag_key_id__tag_values__tag_value_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'tag_key_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Tag Key Id'}},\n",
       "     {'name': 'tag_value_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Tag Value Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TagValue'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['workspaces'],\n",
       "    'summary': 'Update Tag Value',\n",
       "    'operationId': 'update_tag_value_api_v1_workspaces_current_tag_keys__tag_key_id__tag_values__tag_value_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'tag_key_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Tag Key Id'}},\n",
       "     {'name': 'tag_value_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Tag Value Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TagValueUpdate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TagValue'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['workspaces'],\n",
       "    'summary': 'Delete Tag Value',\n",
       "    'operationId': 'delete_tag_value_api_v1_workspaces_current_tag_keys__tag_key_id__tag_values__tag_value_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'tag_key_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Tag Key Id'}},\n",
       "     {'name': 'tag_value_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Tag Value Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/current/taggings': {'post': {'tags': ['workspaces'],\n",
       "    'summary': 'Create Tagging',\n",
       "    'operationId': 'create_tagging_api_v1_workspaces_current_taggings_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/TaggingCreate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Tagging'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'get': {'tags': ['workspaces'],\n",
       "    'summary': 'List Taggings',\n",
       "    'operationId': 'list_taggings_api_v1_workspaces_current_taggings_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'tag_value_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Tag Value Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/TaggingsResponse'},\n",
       "         'title': 'Response List Taggings Api V1 Workspaces Current Taggings Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/current/taggings/{tagging_id}': {'delete': {'tags': ['workspaces'],\n",
       "    'summary': 'Delete Tagging',\n",
       "    'operationId': 'delete_tagging_api_v1_workspaces_current_taggings__tagging_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'tagging_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Tagging Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/current/tags': {'get': {'tags': ['workspaces'],\n",
       "    'summary': 'List Tags',\n",
       "    'operationId': 'list_tags_api_v1_workspaces_current_tags_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/TagKeyWithValues'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response List Tags Api V1 Workspaces Current Tags Get'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/workspaces/current/tags/resource': {'get': {'tags': ['workspaces'],\n",
       "    'summary': 'List Tags For Resource',\n",
       "    'operationId': 'list_tags_for_resource_api_v1_workspaces_current_tags_resource_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'resource_type',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'$ref': '#/components/schemas/ResourceType'}},\n",
       "     {'name': 'resource_id',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Resource Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/TagKeyWithValuesAndTaggings'},\n",
       "         'title': 'Response List Tags For Resource Api V1 Workspaces Current Tags Resource Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/workspaces/current/tags/resources': {'post': {'tags': ['workspaces'],\n",
       "    'summary': 'List Tags For Resources',\n",
       "    'operationId': 'list_tags_for_resources_api_v1_workspaces_current_tags_resources_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/ListTagsForResourceRequest'},\n",
       "        'type': 'array',\n",
       "        'title': 'Resources'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'additionalProperties': {'items': {'$ref': '#/components/schemas/TagKeyWithValuesAndTaggings'},\n",
       "          'type': 'array'},\n",
       "         'propertyNames': {'format': 'uuid'},\n",
       "         'type': 'object',\n",
       "         'title': 'Response List Tags For Resources Api V1 Workspaces Current Tags Resources Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/playground-settings': {'get': {'tags': ['playground-settings'],\n",
       "    'summary': 'List Playground Settings',\n",
       "    'description': 'Get all playground settings for this tenant id.',\n",
       "    'operationId': 'list_playground_settings_api_v1_playground_settings_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/PlaygroundSettingsResponse'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response List Playground Settings Api V1 Playground Settings Get'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['playground-settings'],\n",
       "    'summary': 'Create Playground Settings',\n",
       "    'description': 'Create playground settings.',\n",
       "    'operationId': 'create_playground_settings_api_v1_playground_settings_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PlaygroundSettingsCreateRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PlaygroundSettingsResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/playground-settings/{playground_settings_id}': {'patch': {'tags': ['playground-settings'],\n",
       "    'summary': 'Update Playground Settings',\n",
       "    'description': 'Update playground settings.',\n",
       "    'operationId': 'update_playground_settings_api_v1_playground_settings__playground_settings_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'playground_settings_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Playground Settings Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PlaygroundSettingsUpdateRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PlaygroundSettingsResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['playground-settings'],\n",
       "    'summary': 'Delete Playground Settings',\n",
       "    'description': 'Delete playground settings.',\n",
       "    'operationId': 'delete_playground_settings_api_v1_playground_settings__playground_settings_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'playground_settings_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Playground Settings Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/me/onboarding_state': {'get': {'tags': ['me'],\n",
       "    'summary': 'Get Onboarding State',\n",
       "    'description': 'Get onboarding state for the current user.',\n",
       "    'operationId': 'get_onboarding_state_api_v1_me_onboarding_state_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/UserOnboardingStateResponse'}}}}},\n",
       "    'security': [{'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['me'],\n",
       "    'summary': 'Create Onboarding State',\n",
       "    'description': 'Initialize onboarding state for the current user.',\n",
       "    'operationId': 'create_onboarding_state_api_v1_me_onboarding_state_post',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/UserOnboardingStateResponse'}}}}},\n",
       "    'security': [{'Bearer Auth': []}]}},\n",
       "  '/api/v1/me/onboarding_state/{field}': {'put': {'tags': ['me'],\n",
       "    'summary': 'Update Onboarding State Field',\n",
       "    'description': 'Update a specific onboarding completion field for the current user.\\n\\nValid fields:\\n- tracing_completed_at\\n- lgstudio_completed_at\\n- playground_completed_at\\n- evaluation_completed_at\\n- success_viewed_at',\n",
       "    'operationId': 'update_onboarding_state_field_api_v1_me_onboarding_state__field__put',\n",
       "    'security': [{'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'field',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Field'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/UserOnboardingStateResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/me/ls_user_id': {'get': {'tags': ['me'],\n",
       "    'summary': 'Get Ls User Id',\n",
       "    'description': 'Get the LangSmith user ID for the current user.',\n",
       "    'operationId': 'get_ls_user_id_api_v1_me_ls_user_id_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'string',\n",
       "         'title': 'Response Get Ls User Id Api V1 Me Ls User Id Get'}}}}},\n",
       "    'security': [{'Bearer Auth': []}]}},\n",
       "  '/api/v1/service-accounts': {'get': {'tags': ['service-accounts'],\n",
       "    'summary': 'Get Service Accounts',\n",
       "    'description': \"Get the current organization's service accounts.\",\n",
       "    'operationId': 'get_service_accounts_api_v1_service_accounts_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'items': {'$ref': '#/components/schemas/ServiceAccount'},\n",
       "         'type': 'array',\n",
       "         'title': 'Response Get Service Accounts Api V1 Service Accounts Get'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]},\n",
       "   'post': {'tags': ['service-accounts'],\n",
       "    'summary': 'Create Service Account',\n",
       "    'description': 'Create a service account',\n",
       "    'operationId': 'create_service_account_api_v1_service_accounts_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ServiceAccountCreateRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ServiceAccountCreateResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/service-accounts/{service_account_id}': {'delete': {'tags': ['service-accounts'],\n",
       "    'summary': 'Delete Service Account',\n",
       "    'description': 'Delete a service account',\n",
       "    'operationId': 'delete_service_account_api_v1_service_accounts__service_account_id__delete',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'service_account_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Service Account Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ServiceAccountDeleteResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/charts/section/clone': {'post': {'tags': ['charts'],\n",
       "    'summary': 'Clone Section',\n",
       "    'description': 'Clone a dashboard.',\n",
       "    'operationId': 'clone_section_api_v1_charts_section_clone_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsSectionsCloneRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsSectionResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/charts/section': {'get': {'tags': ['charts'],\n",
       "    'summary': 'Read Sections',\n",
       "    'description': 'Get all sections for the tenant.',\n",
       "    'operationId': 'read_sections_api_v1_charts_section_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'title_contains',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Title Contains'}},\n",
       "     {'name': 'ids',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Ids'}},\n",
       "     {'name': 'sort_by',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'default': 'created_at',\n",
       "       'title': 'Sort By'}},\n",
       "     {'name': 'sort_by_desc',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'default': True,\n",
       "       'title': 'Sort By Desc'}},\n",
       "     {'name': 'tag_value_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Tag Value Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/CustomChartsSectionResponse'},\n",
       "         'title': 'Response Read Sections Api V1 Charts Section Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['charts'],\n",
       "    'summary': 'Create Section',\n",
       "    'description': 'Create a new section.',\n",
       "    'operationId': 'create_section_api_v1_charts_section_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsSectionCreate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsSectionResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/charts': {'post': {'tags': ['charts'],\n",
       "    'summary': 'Read Charts',\n",
       "    'description': 'Get all charts for the tenant.',\n",
       "    'operationId': 'read_charts_api_v1_charts_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/charts/preview': {'post': {'tags': ['charts'],\n",
       "    'summary': 'Read Chart Preview',\n",
       "    'description': 'Get a preview for a chart without actually creating it.',\n",
       "    'operationId': 'read_chart_preview_api_v1_charts_preview_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartPreviewRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SingleCustomChartResponseBase'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/charts/create': {'post': {'tags': ['charts'],\n",
       "    'summary': 'Create Chart',\n",
       "    'description': 'Create a new chart.',\n",
       "    'operationId': 'create_chart_api_v1_charts_create_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartCreate'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/charts/{chart_id}': {'post': {'tags': ['charts'],\n",
       "    'summary': 'Read Single Chart',\n",
       "    'description': 'Get a single chart by ID.',\n",
       "    'operationId': 'read_single_chart_api_v1_charts__chart_id__post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'chart_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Chart Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SingleCustomChartResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['charts'],\n",
       "    'summary': 'Update Chart',\n",
       "    'description': 'Update a chart.',\n",
       "    'operationId': 'update_chart_api_v1_charts__chart_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'chart_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Chart Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartUpdate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['charts'],\n",
       "    'summary': 'Delete Chart',\n",
       "    'description': 'Delete a chart.',\n",
       "    'operationId': 'delete_chart_api_v1_charts__chart_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'chart_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Chart Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/charts/section/{section_id}': {'post': {'tags': ['charts'],\n",
       "    'summary': 'Read Single Section',\n",
       "    'description': 'Get a single section by ID.',\n",
       "    'operationId': 'read_single_section_api_v1_charts_section__section_id__post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'section_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Section Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsSectionRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsSection'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['charts'],\n",
       "    'summary': 'Update Section',\n",
       "    'description': 'Update a section.',\n",
       "    'operationId': 'update_section_api_v1_charts_section__section_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'section_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Section Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsSectionUpdate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsSectionResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['charts'],\n",
       "    'summary': 'Delete Section',\n",
       "    'description': 'Delete a section.',\n",
       "    'operationId': 'delete_section_api_v1_charts_section__section_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'section_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Section Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/org-charts/section': {'get': {'tags': ['charts'],\n",
       "    'summary': 'Org Read Sections',\n",
       "    'description': 'Get all sections for the tenant.',\n",
       "    'operationId': 'org_read_sections_api_v1_org_charts_section_get',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'title_contains',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Title Contains'}},\n",
       "     {'name': 'ids',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Ids'}},\n",
       "     {'name': 'sort_by',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'default': 'created_at',\n",
       "       'title': 'Sort By'}},\n",
       "     {'name': 'sort_by_desc',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'default': True,\n",
       "       'title': 'Sort By Desc'}},\n",
       "     {'name': 'tag_value_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Tag Value Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/CustomChartsSectionResponse'},\n",
       "         'title': 'Response Org Read Sections Api V1 Org Charts Section Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['charts'],\n",
       "    'summary': 'Org Create Section',\n",
       "    'description': 'Create a new section.',\n",
       "    'operationId': 'org_create_section_api_v1_org_charts_section_post',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsSectionCreate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsSectionResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/org-charts': {'post': {'tags': ['charts'],\n",
       "    'summary': 'Org Read Charts',\n",
       "    'description': 'Get all charts for the tenant.',\n",
       "    'operationId': 'org_read_charts_api_v1_org_charts_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/org-charts/preview': {'post': {'tags': ['charts'],\n",
       "    'summary': 'Org Read Chart Preview',\n",
       "    'description': 'Get a preview for a chart without actually creating it.',\n",
       "    'operationId': 'org_read_chart_preview_api_v1_org_charts_preview_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartPreviewRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SingleCustomChartResponseBase'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/org-charts/create': {'post': {'tags': ['charts'],\n",
       "    'summary': 'Org Create Chart',\n",
       "    'description': 'Create a new chart.',\n",
       "    'operationId': 'org_create_chart_api_v1_org_charts_create_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartCreate'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}]}},\n",
       "  '/api/v1/org-charts/{chart_id}': {'post': {'tags': ['charts'],\n",
       "    'summary': 'Org Read Single Chart',\n",
       "    'description': 'Get a single chart by ID.',\n",
       "    'operationId': 'org_read_single_chart_api_v1_org_charts__chart_id__post',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'chart_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Chart Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SingleCustomChartResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['charts'],\n",
       "    'summary': 'Org Update Chart',\n",
       "    'description': 'Update a chart.',\n",
       "    'operationId': 'org_update_chart_api_v1_org_charts__chart_id__patch',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'chart_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Chart Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartUpdate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['charts'],\n",
       "    'summary': 'Org Delete Chart',\n",
       "    'description': 'Delete a chart.',\n",
       "    'operationId': 'org_delete_chart_api_v1_org_charts__chart_id__delete',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'chart_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Chart Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/org-charts/section/{section_id}': {'post': {'tags': ['charts'],\n",
       "    'summary': 'Org Read Single Section',\n",
       "    'description': 'Get a single section by ID.',\n",
       "    'operationId': 'org_read_single_section_api_v1_org_charts_section__section_id__post',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'section_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Section Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsRequestBase'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsSection'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['charts'],\n",
       "    'summary': 'Org Update Section',\n",
       "    'description': 'Update a section.',\n",
       "    'operationId': 'org_update_section_api_v1_org_charts_section__section_id__patch',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'section_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Section Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsSectionUpdate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CustomChartsSectionResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['charts'],\n",
       "    'summary': 'Org Delete Section',\n",
       "    'description': 'Delete a section.',\n",
       "    'operationId': 'org_delete_section_api_v1_org_charts_section__section_id__delete',\n",
       "    'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'section_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Section Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/mcp/proxy': {'get': {'tags': ['mcp'],\n",
       "    'summary': 'Proxy Get',\n",
       "    'operationId': 'proxy_get_api_v1_mcp_proxy_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'url',\n",
       "      'in': 'query',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Url'}},\n",
       "     {'name': 'accept_stream',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': True,\n",
       "       'title': 'Accept Stream'}},\n",
       "     {'name': 'timeout',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer', 'default': 60, 'title': 'Timeout'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['mcp'],\n",
       "    'summary': 'Proxy',\n",
       "    'operationId': 'proxy_api_v1_mcp_proxy_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ProxyRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/ok': {'get': {'summary': 'Ok',\n",
       "    'operationId': 'ok_api_v1_ok_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}}}}},\n",
       "  '/api/v1/repos': {'get': {'tags': ['repos'],\n",
       "    'summary': 'List Repos',\n",
       "    'description': 'Get all repos.',\n",
       "    'operationId': 'list_repos_api_v1_repos_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'with_latest_manifest',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'With Latest Manifest'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 20,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'tenant_handle',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Tenant Handle'}},\n",
       "     {'name': 'tenant_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Tenant Id'}},\n",
       "     {'name': 'query',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Query'}},\n",
       "     {'name': 'has_commits',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Has Commits'}},\n",
       "     {'name': 'tags',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array', 'items': {'type': 'string'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Tags'}},\n",
       "     {'name': 'is_archived',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'enum': ['true', 'allow', 'false'],\n",
       "         'type': 'string'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Is Archived'}},\n",
       "     {'name': 'is_public',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'$ref': '#/components/schemas/TrueFalseLiteral'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Is Public'}},\n",
       "     {'name': 'upstream_repo_owner',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Upstream Repo Owner'}},\n",
       "     {'name': 'upstream_repo_handle',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Upstream Repo Handle'}},\n",
       "     {'name': 'tag_value_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Tag Value Id'}},\n",
       "     {'name': 'sort_field',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'enum': ['num_likes',\n",
       "          'num_downloads',\n",
       "          'num_views',\n",
       "          'updated_at',\n",
       "          'relevance'],\n",
       "         'type': 'string'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Sort Field'}},\n",
       "     {'name': 'sort_direction',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'const': 'asc', 'type': 'string'},\n",
       "        {'const': 'desc', 'type': 'string'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Sort Direction'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ListReposResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['repos'],\n",
       "    'summary': 'Create Repo',\n",
       "    'description': 'Create a repo.',\n",
       "    'operationId': 'create_repo_api_v1_repos_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CreateRepoRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CreateRepoResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/repos/{owner}/{repo}': {'get': {'tags': ['repos'],\n",
       "    'summary': 'Get Repo',\n",
       "    'description': 'Get a repo.',\n",
       "    'operationId': 'get_repo_api_v1_repos__owner___repo__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'owner',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Owner'}},\n",
       "     {'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/GetRepoResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['repos'],\n",
       "    'summary': 'Update Repo',\n",
       "    'description': 'Update a repo.',\n",
       "    'operationId': 'update_repo_api_v1_repos__owner___repo__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'owner',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Owner'}},\n",
       "     {'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/UpdateRepoRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CreateRepoResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['repos'],\n",
       "    'summary': 'Delete Repo',\n",
       "    'description': 'Delete a repo.',\n",
       "    'operationId': 'delete_repo_api_v1_repos__owner___repo__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'owner',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Owner'}},\n",
       "     {'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/repos/{owner}/{repo}/fork': {'post': {'tags': ['repos'],\n",
       "    'summary': 'Fork Repo',\n",
       "    'description': 'Fork a repo.',\n",
       "    'operationId': 'fork_repo_api_v1_repos__owner___repo__fork_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'owner',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Owner'}},\n",
       "     {'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ForkRepoRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/GetRepoResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/repos/tags': {'get': {'tags': ['repos'],\n",
       "    'summary': 'List Repo Tags',\n",
       "    'description': 'Get all repo tags.',\n",
       "    'operationId': 'list_repo_tags_api_v1_repos_tags_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 20,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}},\n",
       "     {'name': 'tenant_handle',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Tenant Handle'}},\n",
       "     {'name': 'tenant_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Tenant Id'}},\n",
       "     {'name': 'query',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Query'}},\n",
       "     {'name': 'has_commits',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "       'title': 'Has Commits'}},\n",
       "     {'name': 'tags',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array', 'items': {'type': 'string'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Tags'}},\n",
       "     {'name': 'is_archived',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'enum': ['true', 'allow', 'false'],\n",
       "         'type': 'string'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Is Archived'}},\n",
       "     {'name': 'is_public',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'$ref': '#/components/schemas/TrueFalseLiteral'},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Is Public'}},\n",
       "     {'name': 'upstream_repo_owner',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Upstream Repo Owner'}},\n",
       "     {'name': 'upstream_repo_handle',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "       'title': 'Upstream Repo Handle'}},\n",
       "     {'name': 'tag_value_id',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'anyOf': [{'type': 'array',\n",
       "         'items': {'type': 'string', 'format': 'uuid'}},\n",
       "        {'type': 'null'}],\n",
       "       'title': 'Tag Value Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ListTagsResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/repos/optimize-job': {'post': {'tags': ['repos'],\n",
       "    'summary': 'Optimize Prompt Job',\n",
       "    'description': 'Optimize prompt',\n",
       "    'operationId': 'optimize_prompt_job_api_v1_repos_optimize_job_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/OptimizePromptJobRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/OptimizePromptResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/likes/{owner}/{repo}': {'post': {'tags': ['likes'],\n",
       "    'summary': 'Like Repo',\n",
       "    'description': 'Like a repo.',\n",
       "    'operationId': 'like_repo_api_v1_likes__owner___repo__post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'owner',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Owner'}},\n",
       "     {'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/LikeRepoRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/LikeRepoResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/settings': {'get': {'tags': ['settings'],\n",
       "    'summary': 'Get Settings',\n",
       "    'description': 'Get settings.',\n",
       "    'operationId': 'get_settings_api_v1_settings_get',\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/app__hub__crud__tenants__Tenant'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/settings/handle': {'post': {'tags': ['settings'],\n",
       "    'summary': 'Set Tenant Handle',\n",
       "    'description': 'Set tenant handle.',\n",
       "    'operationId': 'set_tenant_handle_api_v1_settings_handle_post',\n",
       "    'requestBody': {'content': {'application/json': {'schema': {'$ref': '#/components/schemas/SetTenantHandleRequest'}}},\n",
       "     'required': True},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/app__hub__crud__tenants__Tenant'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}},\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}},\n",
       "  '/api/v1/comments/{owner}/{repo}': {'post': {'tags': ['comments'],\n",
       "    'summary': 'Create Comment',\n",
       "    'operationId': 'create_comment_api_v1_comments__owner___repo__post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'owner',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Owner'}},\n",
       "     {'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CreateCommentRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'get': {'tags': ['comments'],\n",
       "    'summary': 'Get Comments',\n",
       "    'operationId': 'get_comments_api_v1_comments__owner___repo__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'owner',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Owner'}},\n",
       "     {'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 20,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ListCommentsResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/comments/{owner}/{repo}/{parent_comment_id}': {'get': {'tags': ['comments'],\n",
       "    'summary': 'Get Sub Comments',\n",
       "    'operationId': 'get_sub_comments_api_v1_comments__owner___repo___parent_comment_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'owner',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Owner'}},\n",
       "     {'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}},\n",
       "     {'name': 'parent_comment_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Parent Comment Id'}},\n",
       "     {'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'maximum': 100,\n",
       "       'minimum': 1,\n",
       "       'default': 20,\n",
       "       'title': 'Limit'}},\n",
       "     {'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'required': False,\n",
       "      'schema': {'type': 'integer',\n",
       "       'minimum': 0,\n",
       "       'default': 0,\n",
       "       'title': 'Offset'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ListCommentsResponse'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['comments'],\n",
       "    'summary': 'Create Sub Comment',\n",
       "    'operationId': 'create_sub_comment_api_v1_comments__owner___repo___parent_comment_id__post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'owner',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Owner'}},\n",
       "     {'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}},\n",
       "     {'name': 'parent_comment_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Parent Comment Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CreateCommentRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/Comment'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/comments/{owner}/{repo}/{parent_comment_id}/like': {'post': {'tags': ['comments'],\n",
       "    'summary': 'Like Comment',\n",
       "    'operationId': 'like_comment_api_v1_comments__owner___repo___parent_comment_id__like_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'owner',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Owner'}},\n",
       "     {'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}},\n",
       "     {'name': 'parent_comment_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Parent Comment Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': True,\n",
       "         'title': 'Response Like Comment Api V1 Comments  Owner   Repo   Parent Comment Id  Like Post'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['comments'],\n",
       "    'summary': 'Unlike Comment',\n",
       "    'operationId': 'unlike_comment_api_v1_comments__owner___repo___parent_comment_id__like_delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'owner',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Owner'}},\n",
       "     {'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}},\n",
       "     {'name': 'parent_comment_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string',\n",
       "       'format': 'uuid',\n",
       "       'title': 'Parent Comment Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': True,\n",
       "         'title': 'Response Unlike Comment Api V1 Comments  Owner   Repo   Parent Comment Id  Like Delete'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/repos/{owner}/{repo}/tags': {'get': {'tags': ['tags'],\n",
       "    'summary': 'Get Tags',\n",
       "    'operationId': 'get_tags_api_v1_repos__owner___repo__tags_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/RepoTag'},\n",
       "         'title': 'Response Get Tags Api V1 Repos  Owner   Repo  Tags Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['tags'],\n",
       "    'summary': 'Create Tag',\n",
       "    'operationId': 'create_tag_api_v1_repos__owner___repo__tags_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RepoTagRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RepoTag'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/repos/{owner}/{repo}/tags/{tag_name}': {'get': {'tags': ['tags'],\n",
       "    'summary': 'Get Tag',\n",
       "    'operationId': 'get_tag_api_v1_repos__owner___repo__tags__tag_name__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}},\n",
       "     {'name': 'tag_name',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Tag Name'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RepoTag'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['tags'],\n",
       "    'summary': 'Update Tag',\n",
       "    'operationId': 'update_tag_api_v1_repos__owner___repo__tags__tag_name__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}},\n",
       "     {'name': 'tag_name',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Tag Name'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RepoUpdateTagRequest'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/RepoTag'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['tags'],\n",
       "    'summary': 'Delete Tag',\n",
       "    'operationId': 'delete_tag_api_v1_repos__owner___repo__tags__tag_name__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}},\n",
       "     {'name': 'tag_name',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Tag Name'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/repos/{owner}/{repo}/optimization-jobs': {'get': {'tags': ['optimization-jobs'],\n",
       "    'summary': 'List Jobs',\n",
       "    'description': 'List all prompt optimization jobs.',\n",
       "    'operationId': 'list_jobs_api_v1_repos__owner___repo__optimization_jobs_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/PromptOptimizationJob'},\n",
       "         'title': 'Response List Jobs Api V1 Repos  Owner   Repo  Optimization Jobs Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['optimization-jobs'],\n",
       "    'summary': 'Create Job',\n",
       "    'description': 'Create a new prompt optimization job.',\n",
       "    'operationId': 'create_job_api_v1_repos__owner___repo__optimization_jobs_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'title': 'Repo'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PromptOptimizationJobCreate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PromptOptimizationJob'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/repos/{owner}/{repo}/optimization-jobs/{job_id}': {'get': {'tags': ['optimization-jobs'],\n",
       "    'summary': 'Get Job',\n",
       "    'description': 'Get a specific optimization job.',\n",
       "    'operationId': 'get_job_api_v1_repos__owner___repo__optimization_jobs__job_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'job_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Job Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PromptOptimizationJobWithLogs'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'patch': {'tags': ['optimization-jobs'],\n",
       "    'summary': 'Update Job',\n",
       "    'description': 'Replace an existing prompt optimization job with a new, modified job.',\n",
       "    'operationId': 'update_job_api_v1_repos__owner___repo__optimization_jobs__job_id__patch',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'job_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Job Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PromptOptimizationJobUpdate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PromptOptimizationJob'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['optimization-jobs'],\n",
       "    'summary': 'Delete Job',\n",
       "    'description': 'Delete a prompt optimization job.',\n",
       "    'operationId': 'delete_job_api_v1_repos__owner___repo__optimization_jobs__job_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'job_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Job Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/repos/{owner}/{repo}/optimization-jobs/{job_id}/logs': {'get': {'tags': ['optimization-jobs'],\n",
       "    'summary': 'List Job Logs',\n",
       "    'description': 'List all logs for a specific prompt optimization job.',\n",
       "    'operationId': 'list_job_logs_api_v1_repos__owner___repo__optimization_jobs__job_id__logs_get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'job_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Job Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/PromptOptimizationJobLog'},\n",
       "         'title': 'Response List Job Logs Api V1 Repos  Owner   Repo  Optimization Jobs  Job Id  Logs Get'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'post': {'tags': ['optimization-jobs'],\n",
       "    'summary': 'Create Log',\n",
       "    'description': 'Create a new log entry for a prompt optimization job.',\n",
       "    'operationId': 'create_log_api_v1_repos__owner___repo__optimization_jobs__job_id__logs_post',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'job_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Job Id'}}],\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PromptOptimizationJobLogCreate'}}}},\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PromptOptimizationJobLog'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/api/v1/repos/{owner}/{repo}/optimization-jobs/{job_id}/logs/{log_id}': {'get': {'tags': ['optimization-jobs'],\n",
       "    'summary': 'Get Log',\n",
       "    'description': 'Get a specific prompt optimization job log.',\n",
       "    'operationId': 'get_log_api_v1_repos__owner___repo__optimization_jobs__job_id__logs__log_id__get',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'log_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Log Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/PromptOptimizationJobLog'}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}},\n",
       "   'delete': {'tags': ['optimization-jobs'],\n",
       "    'summary': 'Delete Log',\n",
       "    'description': 'Delete a prompt optimization job log.',\n",
       "    'operationId': 'delete_log_api_v1_repos__owner___repo__optimization_jobs__job_id__logs__log_id__delete',\n",
       "    'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}],\n",
       "    'parameters': [{'name': 'log_id',\n",
       "      'in': 'path',\n",
       "      'required': True,\n",
       "      'schema': {'type': 'string', 'format': 'uuid', 'title': 'Log Id'}}],\n",
       "    'responses': {'200': {'description': 'Successful Response',\n",
       "      'content': {'application/json': {'schema': {}}}},\n",
       "     '422': {'description': 'Validation Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}},\n",
       "  '/commits/{owner}/{repo}': {'get': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Lists all commits for a repository with pagination support.\\nThis endpoint supports both authenticated and unauthenticated access.\\nAuthenticated users can access private repos, while unauthenticated users can only access public repos.\\nThe include_stats parameter controls whether download and view statistics are computed (defaults to true).',\n",
       "    'consumes': ['application/json'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['commits'],\n",
       "    'summary': 'List commits',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'description': \"Repository owner (tenant handle) or '-' for private repos\",\n",
       "      'name': 'owner',\n",
       "      'in': 'path',\n",
       "      'required': True},\n",
       "     {'type': 'string',\n",
       "      'description': 'Repository handle',\n",
       "      'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True},\n",
       "     {'description': 'IncludeStats determines whether to compute num_downloads and num_views',\n",
       "      'name': 'include_stats',\n",
       "      'in': 'query',\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': True,\n",
       "       'title': 'Include Stats'}},\n",
       "     {'description': 'Limit is the pagination limit',\n",
       "      'name': 'limit',\n",
       "      'in': 'query',\n",
       "      'schema': {'type': 'integer',\n",
       "       'default': 20,\n",
       "       'minimum': 1,\n",
       "       'maximum': 100,\n",
       "       'title': 'Limit'}},\n",
       "     {'description': 'Offset is the pagination offset',\n",
       "      'name': 'offset',\n",
       "      'in': 'query',\n",
       "      'schema': {'type': 'integer',\n",
       "       'default': 0,\n",
       "       'minimum': 0,\n",
       "       'title': 'Offset'}}],\n",
       "    'responses': {'200': {'description': 'OK',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/commits.ListCommitsResponse'}}}},\n",
       "     '400': {'description': 'Bad Request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/commits.ErrorResponse'}}}},\n",
       "     '404': {'description': 'Not Found',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/commits.ErrorResponse'}}}},\n",
       "     '500': {'description': 'Internal Server Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/commits.ErrorResponse'}}}}}},\n",
       "   'post': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Creates a new commit in a repository.\\nRequires authentication and write access to the repository.',\n",
       "    'consumes': ['application/json'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['commits'],\n",
       "    'summary': 'Create a commit',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'description': \"Repository owner (tenant handle) or '-' for private repos\",\n",
       "      'name': 'owner',\n",
       "      'in': 'path',\n",
       "      'required': True},\n",
       "     {'type': 'string',\n",
       "      'description': 'Repository handle',\n",
       "      'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'201': {'description': 'Created',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/commits.CreateCommitResponse'}}}},\n",
       "     '400': {'description': 'Bad Request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/commits.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/commits.ErrorResponse'}}}},\n",
       "     '404': {'description': 'Not Found',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/commits.ErrorResponse'}}}},\n",
       "     '500': {'description': 'Internal Server Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/commits.ErrorResponse'}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/commits.CreateCommitReq'}}}}}},\n",
       "  '/commits/{owner}/{repo}/{commit}': {'get': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Retrieves a specific commit by hash, tag, or \"latest\" for a repository.\\nThis endpoint supports both authenticated and unauthenticated access.\\nAuthenticated users can access private repos, while unauthenticated users can only access public repos.\\nCommit resolution logic:\\n- \"latest\" or empty: Get the most recent commit\\n- Less than 8 characters: Only check for tags\\n- 8 or more characters: Prioritize commit hash over tag, check both',\n",
       "    'consumes': ['application/json'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['commits'],\n",
       "    'summary': 'Get a commit',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'description': \"Repository owner (tenant handle) or '-' for private repos\",\n",
       "      'name': 'owner',\n",
       "      'in': 'path',\n",
       "      'required': True},\n",
       "     {'type': 'string',\n",
       "      'description': 'Repository handle',\n",
       "      'name': 'repo',\n",
       "      'in': 'path',\n",
       "      'required': True},\n",
       "     {'type': 'string',\n",
       "      'description': \"Commit hash, tag, or 'latest'\",\n",
       "      'name': 'commit',\n",
       "      'in': 'path',\n",
       "      'required': True},\n",
       "     {'name': 'get_examples',\n",
       "      'in': 'query',\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Get Examples'}},\n",
       "     {'name': 'include_model',\n",
       "      'in': 'query',\n",
       "      'schema': {'type': 'boolean',\n",
       "       'default': False,\n",
       "       'title': 'Include Model'}},\n",
       "     {'name': 'is_view',\n",
       "      'in': 'query',\n",
       "      'schema': {'type': 'boolean', 'default': False, 'title': 'Is View'}}],\n",
       "    'responses': {'200': {'description': 'OK',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/commits.CommitResponse'}}}},\n",
       "     '400': {'description': 'Bad Request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/commits.ErrorResponse'}}}},\n",
       "     '404': {'description': 'Not Found',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/commits.ErrorResponse'}}}},\n",
       "     '500': {'description': 'Internal Server Error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/commits.ErrorResponse'}}}}}}},\n",
       "  '/datasets/{dataset_id}/experiment-view-overrides': {'get': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Retrieves all experiment view override configurations for a specific dataset.\\nThis endpoint returns column display overrides including color gradients,\\nprecision settings, and column visibility configurations that customize how\\nexperiment results are displayed in the UI.\\n\\nThe response includes all column overrides with their display settings:\\n- Column identifiers (must start with inputs, outputs, reference_outputs, feedback, metrics, attachments, or metadata)\\n- Color gradients for numeric data visualization\\n- Precision settings for numeric columns (1-6 decimal places)\\n- Hide flags to control column visibility',\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['experiment-view-overrides'],\n",
       "    'summary': 'Get experiment view override configurations for a dataset',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'example': '\"550e8400-e29b-41d4-a716-446655440000\"',\n",
       "      'description': 'Dataset ID',\n",
       "      'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'200': {'description': 'Successfully retrieved experiment view override configurations',\n",
       "      'content': {'application/json': {'schema': {'type': 'array',\n",
       "         'items': {'$ref': '#/components/schemas/experiment_view_overrides.ExperimentViewOverride'}}}}},\n",
       "     '400': {'description': 'Invalid dataset ID format\" example({\"error\":\"invalid dataset ID format\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '401': {'description': 'Unauthorized access\" example({\"error\":\"Unauthorized\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '404': {'description': 'Dataset not found or not accessible\" example({\"error\":\"dataset not found or not accessible\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '500': {'description': 'Internal server error\" example({\"error\":\"internal server error\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}}}},\n",
       "   'post': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Creates a new experiment view override configuration for a dataset with column display settings.\\nThis endpoint allows you to customize how experiment results are displayed by configuring\\ncolumn-specific overrides including colors, precision, and visibility.\\n\\nThe request must include a \\'column_overrides\\' array with at least one override configuration.\\nEach column override can specify:\\n- column: Required field name (must start with inputs, outputs, reference_outputs, feedback, metrics, attachments, or metadata)\\n- color_gradient: Optional array of [number, color] tuples for numeric data visualization\\n- precision: Optional number (1-6) for decimal places in numeric columns\\n- hide: Optional boolean to control column visibility\\n\\nExample request body:\\n{\\n\"column_overrides\": [\\n{\\n\"column\": \"outputs.accuracy\",\\n\"color_gradient\": [[0.0, \"#ff0000\"], [0.5, \"#ffff00\"], [1.0, \"#00ff00\"]],\\n\"precision\": 3\\n},\\n{\\n\"column\": \"inputs.model_type\",\\n\"hide\": false\\n}\\n]\\n}\\n\\nThis operation fails if an override already exists for the dataset (use PATCH to update).',\n",
       "    'consumes': ['application/json'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['experiment-view-overrides'],\n",
       "    'summary': 'Create new experiment view override configuration for a dataset',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'example': '\"550e8400-e29b-41d4-a716-446655440000\"',\n",
       "      'description': 'Dataset ID',\n",
       "      'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'201': {'description': 'Successfully created experiment view override',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/experiment_view_overrides.ExperimentViewOverride'}}}},\n",
       "     '400': {'description': 'Invalid request data\" example({\"error\":\"column_overrides field is required\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '401': {'description': 'Unauthorized access\" example({\"error\":\"Unauthorized\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '404': {'description': 'Dataset not found\" example({\"error\":\"dataset not found or not accessible\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '409': {'description': 'Override already exists\" example({\"error\":\"experiment view override already exists\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '422': {'description': 'Validation error\" example({\"error\":\"column name at index 0 must start with one of: inputs, outputs, reference_outputs, feedback, metrics, attachments, metadata\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '500': {'description': 'Internal server error\" example({\"error\":\"internal server error\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/experiment_view_overrides.ExperimentViewOverridePostRequest'}}}}}},\n",
       "  '/datasets/{dataset_id}/experiment-view-overrides/{id}': {'get': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Retrieves a specific experiment view override configuration using both dataset ID and override ID.\\nThis endpoint provides more precise access to experiment view overrides when you have\\nthe specific override ID, useful for direct links or cached references.\\n\\nThe response includes the same column override information as the dataset-level endpoint:\\n- Column identifiers with validation prefixes\\n- Color gradient settings for numeric data visualization\\n- Numeric precision configurations\\n- Column visibility controls\\n\\nBoth the dataset and override must exist and be accessible by the authenticated user.',\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['experiment-view-overrides'],\n",
       "    'summary': 'Get experiment view override configuration by specific ID',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'example': '\"550e8400-e29b-41d4-a716-446655440000\"',\n",
       "      'description': 'Dataset ID',\n",
       "      'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True},\n",
       "     {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'example': '\"123e4567-e89b-12d3-a456-426614174000\"',\n",
       "      'description': 'Experiment view override ID',\n",
       "      'name': 'id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'200': {'description': 'Successfully retrieved experiment view override configuration',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/experiment_view_overrides.ExperimentViewOverride'}}}},\n",
       "     '400': {'description': 'Invalid ID format\" example({\"error\":\"invalid experiment view override ID format\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '401': {'description': 'Unauthorized access\" example({\"error\":\"Unauthorized\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '404': {'description': 'Override not found\" example({\"error\":\"experiment view override not found\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '500': {'description': 'Internal server error\" example({\"error\":\"internal server error\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}}}},\n",
       "   'delete': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': \"Permanently deletes an experiment view override configuration for a dataset.\\nThis operation removes all column override settings including color gradients,\\nprecision configurations, and visibility settings.\\n\\nAfter deletion, the experiment view will revert to default column display settings.\\nThis action cannot be undone - you will need to recreate the override configuration\\nif you want to restore custom column settings.\\n\\nBoth the dataset and override must exist and be accessible by the authenticated user.\\nThe operation will fail if the override doesn't exist or if the user doesn't have\\nappropriate permissions for the dataset.\",\n",
       "    'tags': ['experiment-view-overrides'],\n",
       "    'summary': 'Delete experiment view override configuration',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'example': '\"550e8400-e29b-41d4-a716-446655440000\"',\n",
       "      'description': 'Dataset ID',\n",
       "      'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True},\n",
       "     {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'example': '\"123e4567-e89b-12d3-a456-426614174000\"',\n",
       "      'description': 'Experiment view override ID',\n",
       "      'name': 'id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'204': {'description': 'Successfully deleted experiment view override (no content returned)'},\n",
       "     '400': {'description': 'Invalid ID format\" example({\"error\":\"invalid experiment view override ID format\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '401': {'description': 'Unauthorized access\" example({\"error\":\"Unauthorized\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '404': {'description': 'Override not found\" example({\"error\":\"experiment view override not found\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '500': {'description': 'Internal server error\" example({\"error\":\"internal server error\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}}}},\n",
       "   'patch': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Updates an existing experiment view override configuration by completely replacing\\nthe column overrides for the specified dataset and override ID.\\n\\nThis endpoint performs a complete replacement of the column overrides configuration.\\nAll existing column overrides will be replaced with the new configuration provided\\nin the request body. To add or modify individual columns, include the complete\\ndesired configuration in the request.\\n\\nThe request format is identical to the create endpoint:\\n- column_overrides: Required array with at least one override configuration\\n- Each override can specify color gradients, precision, and visibility\\n\\nExample request body:\\n{\\n\"column_overrides\": [\\n{\\n\"column\": \"metrics.f1_score\",\\n\"color_gradient\": [[0.0, \"#ff4444\"], [0.8, \"#44ff44\"]],\\n\"precision\": 4\\n},\\n{\\n\"column\": \"feedback.rating\",\\n\"hide\": false\\n}\\n]\\n}\\n\\nBoth the dataset and override must exist and be accessible by the authenticated user.',\n",
       "    'consumes': ['application/json'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['experiment-view-overrides'],\n",
       "    'summary': 'Update existing experiment view override configuration',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'example': '\"550e8400-e29b-41d4-a716-446655440000\"',\n",
       "      'description': 'Dataset ID',\n",
       "      'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True},\n",
       "     {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'example': '\"123e4567-e89b-12d3-a456-426614174000\"',\n",
       "      'description': 'Experiment view override ID',\n",
       "      'name': 'id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'200': {'description': 'Successfully updated experiment view override',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/experiment_view_overrides.ExperimentViewOverride'}}}},\n",
       "     '400': {'description': 'Invalid request data\" example({\"error\":\"invalid experiment view override ID format\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '401': {'description': 'Unauthorized access\" example({\"error\":\"Unauthorized\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '404': {'description': 'Override not found\" example({\"error\":\"experiment view override not found\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '422': {'description': 'Validation error\" example({\"error\":\"\\'precision\\' must be between 1 and 6 for column at index 0\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '500': {'description': 'Internal server error\" example({\"error\":\"internal server error\"})',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/experiment_view_overrides.ExperimentViewOverridePatchRequest'}}}}}},\n",
       "  '/feedback/batch': {'post': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Ingests a batch of feedback objects in a single JSON array payload.',\n",
       "    'consumes': ['application/json'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['feedback'],\n",
       "    'summary': 'Ingest Feedback (Batch JSON)',\n",
       "    'parameters': [],\n",
       "    'responses': {'202': {'description': 'Feedback batch ingested',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'allOf': [{'type': 'string'},\n",
       "           {'type': 'object',\n",
       "            'properties': {'message': {'type': 'string'}}}]}}}}},\n",
       "     '400': {'description': 'Bad Request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '409': {'description': 'Conflict',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '422': {'description': 'Unprocessable Entity',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '429': {'description': 'Too Many Requests',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'type': 'array',\n",
       "        'items': {'$ref': '#/components/schemas/feedback.FeedbackCreateSchema'}}}}}}},\n",
       "  '/runs': {'post': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Queues a single run for ingestion. The request body must be a JSON-encoded run object that follows the Run schema.',\n",
       "    'consumes': ['application/json'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['runs'],\n",
       "    'summary': 'Create a Run',\n",
       "    'parameters': [],\n",
       "    'responses': {'202': {'description': 'Run created',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'allOf': [{'type': 'string'},\n",
       "           {'type': 'object',\n",
       "            'properties': {'message': {'type': 'string'}}}]}}}}},\n",
       "     '400': {'description': 'Bad Request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '409': {'description': 'Conflict',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '422': {'description': 'Unprocessable Entity',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '429': {'description': 'Too Many Requests',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.Run'}}}}}},\n",
       "  '/runs/batch': {'post': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Ingests a batch of runs in a single JSON payload. The payload must have `post` and/or `patch` arrays containing run objects.\\nPrefer this endpoint over single‑run ingestion when submitting hundreds of runs, but `/runs/multipart` offers better handling for very large fields and attachments.',\n",
       "    'consumes': ['application/json'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['runs'],\n",
       "    'summary': 'Ingest Runs (Batch JSON)',\n",
       "    'parameters': [],\n",
       "    'responses': {'202': {'description': 'Runs batch ingested',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'allOf': [{'type': 'string'},\n",
       "           {'type': 'object',\n",
       "            'properties': {'message': {'type': 'string'}}}]}}}}},\n",
       "     '400': {'description': 'Bad Request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '409': {'description': 'Conflict',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '422': {'description': 'Unprocessable Entity',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '429': {'description': 'Too Many Requests',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'type': 'object',\n",
       "        'properties': {'patch': {'type': 'array',\n",
       "          'items': {'$ref': '#/components/schemas/runs.Run'}},\n",
       "         'post': {'type': 'array',\n",
       "          'items': {'$ref': '#/components/schemas/runs.Run'}}}}}}}}},\n",
       "  '/runs/multipart': {'post': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Ingests multiple runs, feedback objects, and binary attachments in a single `multipart/form-data` request.\\n**Part‑name pattern**: `<event>.<run_id>[.<field>]` where `event` ∈ {`post`, `patch`, `feedback`, `attachment`}.\\n* `post|patch.<run_id>` –\\xa0JSON run payload.\\n* `post|patch.<run_id>.<field>` – out‑of‑band run data (`inputs`, `outputs`, `events`, `error`, `extra`, `serialized`).\\n* `feedback.<run_id>` – JSON feedback payload (must include `trace_id`).\\n* `attachment.<run_id>.<filename>` – arbitrary binary attachment stored in S3.\\n**Headers**: every part must set `Content-Type` **and** either a `Content-Length` header or `length` parameter. Per‑part `Content-Encoding` is **not** allowed; the top‑level request may be `Content-Encoding: gzip` or `Content-Encoding: zstd`.\\n**Best performance** for high‑volume ingestion.',\n",
       "    'consumes': ['multipart/form-data'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['runs'],\n",
       "    'summary': 'Ingest Runs (Multipart)',\n",
       "    'parameters': [],\n",
       "    'responses': {'202': {'description': 'Accepted',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '400': {'description': 'Bad Request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '409': {'description': 'Conflict',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '422': {'description': 'Unprocessable Entity',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '429': {'description': 'Too Many Requests',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'multipart/form-data': {'schema': {'type': 'object',\n",
       "        'properties': {'post.{run_id}': {'type': 'string',\n",
       "          'format': 'binary',\n",
       "          'description': 'Run to create (JSON)'},\n",
       "         'patch.{run_id}': {'type': 'string',\n",
       "          'format': 'binary',\n",
       "          'description': 'Run to update (JSON)'},\n",
       "         'post.{run_id}.inputs': {'type': 'string',\n",
       "          'format': 'binary',\n",
       "          'description': 'Large inputs object (JSON) stored out‑of‑band'},\n",
       "         'patch.{run_id}.outputs': {'type': 'string',\n",
       "          'format': 'binary',\n",
       "          'description': 'Large outputs object (JSON) stored out‑of‑band'},\n",
       "         'feedback.{run_id}': {'type': 'string',\n",
       "          'format': 'binary',\n",
       "          'description': 'Feedback object (JSON) – must include trace_id'},\n",
       "         'attachment.{run_id}.{filename}': {'type': 'string',\n",
       "          'format': 'binary',\n",
       "          'description': 'Binary attachment linked to run {run_id}'}},\n",
       "        'required': []}}}}}},\n",
       "  '/runs/{run_id}': {'patch': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Updates a run identified by its ID. The body should contain only the fields to be changed; unknown fields are ignored.',\n",
       "    'consumes': ['application/json'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['runs'],\n",
       "    'summary': 'Update a Run',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'description': 'Run ID',\n",
       "      'name': 'run_id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'202': {'description': 'Run updated',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'allOf': [{'type': 'string'},\n",
       "           {'type': 'object',\n",
       "            'properties': {'message': {'type': 'string'}}}]}}}}},\n",
       "     '400': {'description': 'Bad Request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '404': {'description': 'Not Found',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '409': {'description': 'Conflict',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '422': {'description': 'Unprocessable Entity',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}},\n",
       "     '429': {'description': 'Too Many Requests',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.ErrorResponse'}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/runs.Run'}}}}}},\n",
       "  '/v1/platform/alerts/{session_id}': {'post': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Creates a new alert rule. The request body must be a JSON-encoded alert rule object that follows the CreateAlertRuleRequest schema.',\n",
       "    'consumes': ['application/json'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['alert_rules'],\n",
       "    'summary': 'Create an alert rule',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'description': 'Session ID',\n",
       "      'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'201': {'description': 'Alert rule created',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.AlertRuleResponse'}}}},\n",
       "     '400': {'description': 'Bad request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '429': {'description': 'Alert Rule Limit Reached',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '500': {'description': 'Internal server error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '503': {'description': 'Service unavailable',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.CreateAlertRuleRequest'}}}}}},\n",
       "  '/v1/platform/alerts/{session_id}/test': {'post': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Tests an alert action which will fire a notification to all configured recipients if the configuration is valid.',\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['alert_rules'],\n",
       "    'summary': 'Test an alert action to determine if configuration is valid',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'description': 'Session ID',\n",
       "      'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True},\n",
       "     {'type': 'string',\n",
       "      'description': 'Alert rule ID',\n",
       "      'name': 'alert_rule_id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'200': {'description': 'Alert action fired successfully',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'allOf': [{'type': 'string'},\n",
       "           {'type': 'object',\n",
       "            'properties': {'message': {'type': 'string'}}}]}}}}},\n",
       "     '400': {'description': 'Bad request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '500': {'description': 'Internal server error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '503': {'description': 'Service unavailable',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}}}}},\n",
       "  '/v1/platform/alerts/{session_id}/{alert_rule_id}': {'get': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Gets an alert rule.',\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['alert_rules'],\n",
       "    'summary': 'Get an alert rule',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'description': 'Session ID',\n",
       "      'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True},\n",
       "     {'type': 'string',\n",
       "      'description': 'Alert rule ID',\n",
       "      'name': 'alert_rule_id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'200': {'description': 'Alert rule',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.AlertRuleResponse'}}}},\n",
       "     '400': {'description': 'Bad request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '404': {'description': 'Not found',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '500': {'description': 'Internal server error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '503': {'description': 'Service unavailable',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}}}},\n",
       "   'delete': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Deletes an alert rule',\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['alert_rules'],\n",
       "    'summary': 'Delete an alert rule',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'description': 'Session ID',\n",
       "      'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True},\n",
       "     {'type': 'string',\n",
       "      'description': 'Alert rule ID',\n",
       "      'name': 'alert_rule_id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'200': {'description': 'Alert rule deleted',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'allOf': [{'type': 'string'},\n",
       "           {'type': 'object',\n",
       "            'properties': {'message': {'type': 'string'}}}]}}}}},\n",
       "     '400': {'description': 'Bad request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '404': {'description': 'Not found',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '500': {'description': 'Internal server error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '503': {'description': 'Service unavailable',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}}}},\n",
       "   'patch': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Updates an alert rule.',\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['alert_rules'],\n",
       "    'summary': 'Update an alert rule',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'description': 'Session ID',\n",
       "      'name': 'session_id',\n",
       "      'in': 'path',\n",
       "      'required': True},\n",
       "     {'type': 'string',\n",
       "      'description': 'Alert rule ID',\n",
       "      'name': 'alert_rule_id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'200': {'description': 'Alert rule updated',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'allOf': [{'type': 'string'},\n",
       "           {'type': 'object',\n",
       "            'properties': {'message': {'type': 'string'}}}]}}}}},\n",
       "     '400': {'description': 'Bad request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '404': {'description': 'Not found',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '500': {'description': 'Internal server error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}},\n",
       "     '503': {'description': 'Service unavailable',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.ErrorResponse'}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/alerts.UpdateAlertRuleRequest'}}}}}},\n",
       "  '/v1/platform/datasets/examples/delete': {'post': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'This endpoint hard deletes *all* versions of a dataset example(s).\\nDeletion is performed by setting inputs, outputs, and metadata to null and deleting attachment files while keeping the example ID, dataset ID, and creation timestamp.\\nIMPORTANT: attachment files can take up to 7 days to be deleted. inputs, outputs and metadata are nullified immediately.',\n",
       "    'consumes': ['application/json'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['examples'],\n",
       "    'summary': 'Hard Delete Examples',\n",
       "    'parameters': [],\n",
       "    'responses': {'200': {'description': 'OK',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ExamplesDeletedResponse'}}}},\n",
       "     '400': {'description': 'Bad Request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ErrorResponse'}}}},\n",
       "     '404': {'description': 'Not Found',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ErrorResponse'}}}},\n",
       "     '422': {'description': 'Unprocessable Entity',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ErrorResponse'}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.DeleteExamplesRequest'}}}}}},\n",
       "  '/v1/platform/datasets/{dataset_id}/examples': {'post': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'This endpoint allows clients to upload examples to a specified dataset by sending a multipart/form-data POST request.\\nEach form part contains either JSON-encoded data or binary attachment files associated with an example.',\n",
       "    'consumes': ['multipart/form-data'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['examples'],\n",
       "    'summary': 'Upload Examples',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'description': 'Dataset ID',\n",
       "      'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'201': {'description': 'Created',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ExamplesCreatedResponse'}}}},\n",
       "     '400': {'description': 'Bad Request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ErrorResponse'}}}},\n",
       "     '409': {'description': 'Conflict',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ErrorResponse'}}}},\n",
       "     '422': {'description': 'Unprocessable Entity',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ErrorResponse'}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'multipart/form-data': {'schema': {'type': 'object',\n",
       "        'properties': {'{example_id}': {'type': 'string',\n",
       "          'format': 'binary',\n",
       "          'description': \"The Example info as JSON. Can have fields 'metadata', 'split', 'use_source_run_io', 'source_run_id', 'created_at', 'modified_at'\"},\n",
       "         '{example_id}.inputs': {'type': 'string',\n",
       "          'format': 'binary',\n",
       "          'description': 'The Example inputs as JSON'},\n",
       "         '{example_id}.outputs': {'type': 'string',\n",
       "          'format': 'binary',\n",
       "          'description': 'THe Example outputs as JSON'},\n",
       "         '{example_id}.attachments.{name}': {'type': 'string',\n",
       "          'format': 'binary',\n",
       "          'description': 'File attachment named {name}'}},\n",
       "        'required': ['{example_id}', '{example_id}.inputs']}}}}},\n",
       "   'patch': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'This endpoint allows clients to update existing examples in a specified dataset by sending a multipart/form-data PATCH request.\\nEach form part contains either JSON-encoded data or binary attachment files to update an example.',\n",
       "    'consumes': ['multipart/form-data'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['examples'],\n",
       "    'summary': 'Update Examples',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'description': 'Dataset ID',\n",
       "      'name': 'dataset_id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'201': {'description': 'Created',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ExamplesUpdatedResponse'}}}},\n",
       "     '400': {'description': 'Bad Request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ErrorResponse'}}}},\n",
       "     '404': {'description': 'Not Found',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ErrorResponse'}}}},\n",
       "     '409': {'description': 'Conflict',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ErrorResponse'}}}},\n",
       "     '422': {'description': 'Unprocessable Entity',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/examples.ErrorResponse'}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'multipart/form-data': {'schema': {'type': 'object',\n",
       "        'properties': {'{example_id}': {'type': 'string',\n",
       "          'format': 'binary',\n",
       "          'description': \"The Example update info as JSON. Can have fields 'metadata', 'split'\"},\n",
       "         '{example_id}.inputs': {'type': 'string',\n",
       "          'format': 'binary',\n",
       "          'description': 'The updated Example inputs as JSON'},\n",
       "         '{example_id}.outputs': {'type': 'string',\n",
       "          'format': 'binary',\n",
       "          'description': 'The updated Example outputs as JSON'},\n",
       "         '{example_id}.attachments_operations': {'type': 'string',\n",
       "          'format': 'binary',\n",
       "          'description': 'JSON describing attachment operations (retain, rename)'},\n",
       "         '{example_id}.attachment.{name}': {'type': 'string',\n",
       "          'format': 'binary',\n",
       "          'description': 'New file attachment named {name}'}},\n",
       "        'required': ['{example_id}']}}}}}},\n",
       "  '/v1/platform/orgs/current/access-policies': {'get': {'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Lists all access policies for the organization.',\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['access_policies'],\n",
       "    'summary': 'List access policies',\n",
       "    'responses': {'200': {'description': 'List of access policies',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/authz_internal.ListAccessPoliciesResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '500': {'description': 'Internal server error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '503': {'description': 'Service unavailable',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}}},\n",
       "    'parameters': []},\n",
       "   'post': {'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Creates a new access policy.',\n",
       "    'consumes': ['application/json'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['access_policies'],\n",
       "    'summary': 'Create an access policy',\n",
       "    'parameters': [],\n",
       "    'responses': {'201': {'description': 'Access policy created',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/authz_internal.AccessPolicyCreateResponse'}}}},\n",
       "     '400': {'description': 'Bad request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '404': {'description': 'Role not found',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '500': {'description': 'Internal server error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '503': {'description': 'Service unavailable',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/authz_internal.CreateAccessPolicyPayload'}}}}}},\n",
       "  '/v1/platform/orgs/current/access-policies/roles/{role_id}/access-policies': {'post': {'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Attaches one or more access policies to a specific role. The request body must contain an array of access policy IDs.',\n",
       "    'consumes': ['application/json'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['access_policies'],\n",
       "    'summary': 'Attach access policies to a role',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'description': 'Role ID',\n",
       "      'name': 'role_id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'204': {'description': 'Access policies attached successfully'},\n",
       "     '400': {'description': 'Bad request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '404': {'description': 'Role not found',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '500': {'description': 'Internal server error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '503': {'description': 'Service unavailable',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/authz_internal.AttachAccessPoliciesPayload'}}}}}},\n",
       "  '/v1/platform/orgs/current/access-policies/{access_policy_id}': {'get': {'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Gets a specific access policy by ID.',\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['access_policies'],\n",
       "    'summary': 'Get an access policy',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'description': 'Access Policy ID',\n",
       "      'name': 'access_policy_id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'200': {'description': 'Access policy details',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/authz_internal.AccessPolicy'}}}},\n",
       "     '400': {'description': 'Bad request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '404': {'description': 'Access policy not found',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '500': {'description': 'Internal server error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '503': {'description': 'Service unavailable',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}}}},\n",
       "   'delete': {'security': [{'API Key': []},\n",
       "     {'Organization ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Deletes a specific access policy by ID.',\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['access_policies'],\n",
       "    'summary': 'Delete an access policy',\n",
       "    'parameters': [{'type': 'string',\n",
       "      'description': 'Access Policy ID',\n",
       "      'name': 'access_policy_id',\n",
       "      'in': 'path',\n",
       "      'required': True}],\n",
       "    'responses': {'204': {'description': 'Access policy deleted successfully'},\n",
       "     '400': {'description': 'Bad request',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '403': {'description': 'Forbidden',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '500': {'description': 'Internal server error',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}},\n",
       "     '503': {'description': 'Service unavailable',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/abac.ErrorResponse'}}}}}}},\n",
       "  '/workspaces/current/ttl-settings': {'get': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Get the longlived trace TTL settings for a workspace',\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['TTL Settings'],\n",
       "    'summary': 'Get workspace TTL settings',\n",
       "    'responses': {'200': {'description': 'OK',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ttl_settings.TTLSettingsResponse'}}}},\n",
       "     '400': {'description': 'Bad Request',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '404': {'description': 'Not Found',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '500': {'description': 'Internal Server Error',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}}},\n",
       "    'parameters': []},\n",
       "   'put': {'security': [{'API Key': []},\n",
       "     {'Tenant ID': []},\n",
       "     {'Bearer Auth': []}],\n",
       "    'description': 'Update the longlived trace TTL for a workspace.',\n",
       "    'consumes': ['application/json'],\n",
       "    'produces': ['application/json'],\n",
       "    'tags': ['TTL Settings'],\n",
       "    'summary': 'Update workspace TTL settings',\n",
       "    'parameters': [],\n",
       "    'responses': {'200': {'description': 'OK',\n",
       "      'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ttl_settings.TTLSettingsResponse'}}}},\n",
       "     '400': {'description': 'Bad Request',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '404': {'description': 'Not Found',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}},\n",
       "     '500': {'description': 'Internal Server Error',\n",
       "      'content': {'application/json': {'schema': {'type': 'object',\n",
       "         'additionalProperties': {'type': 'string'}}}}}},\n",
       "    'requestBody': {'required': True,\n",
       "     'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ttl_settings.UpdateTTLSettingsRequest'}}}}}}},\n",
       " 'components': {'schemas': {'AIMessage': {'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "       {'items': {'anyOf': [{'type': 'string'},\n",
       "          {'additionalProperties': True, 'type': 'object'}]},\n",
       "        'type': 'array'}],\n",
       "      'title': 'Content'},\n",
       "     'additional_kwargs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Additional Kwargs'},\n",
       "     'response_metadata': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Response Metadata'},\n",
       "     'type': {'type': 'string',\n",
       "      'const': 'ai',\n",
       "      'title': 'Type',\n",
       "      'default': 'ai'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "     'tool_calls': {'items': {'$ref': '#/components/schemas/ToolCall'},\n",
       "      'type': 'array',\n",
       "      'title': 'Tool Calls'},\n",
       "     'invalid_tool_calls': {'items': {'$ref': '#/components/schemas/InvalidToolCall'},\n",
       "      'type': 'array',\n",
       "      'title': 'Invalid Tool Calls'},\n",
       "     'usage_metadata': {'anyOf': [{'$ref': '#/components/schemas/UsageMetadata'},\n",
       "       {'type': 'null'}]}},\n",
       "    'additionalProperties': True,\n",
       "    'type': 'object',\n",
       "    'required': ['content'],\n",
       "    'title': 'AIMessage',\n",
       "    'description': 'Message from an AI.\\n\\nAn `AIMessage` is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model and standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.'},\n",
       "   'AIMessageChunk': {'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "       {'items': {'anyOf': [{'type': 'string'},\n",
       "          {'additionalProperties': True, 'type': 'object'}]},\n",
       "        'type': 'array'}],\n",
       "      'title': 'Content'},\n",
       "     'additional_kwargs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Additional Kwargs'},\n",
       "     'response_metadata': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Response Metadata'},\n",
       "     'type': {'type': 'string',\n",
       "      'const': 'AIMessageChunk',\n",
       "      'title': 'Type',\n",
       "      'default': 'AIMessageChunk'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "     'tool_calls': {'items': {'$ref': '#/components/schemas/ToolCall'},\n",
       "      'type': 'array',\n",
       "      'title': 'Tool Calls'},\n",
       "     'invalid_tool_calls': {'items': {'$ref': '#/components/schemas/InvalidToolCall'},\n",
       "      'type': 'array',\n",
       "      'title': 'Invalid Tool Calls'},\n",
       "     'usage_metadata': {'anyOf': [{'$ref': '#/components/schemas/UsageMetadata'},\n",
       "       {'type': 'null'}]},\n",
       "     'tool_call_chunks': {'items': {'$ref': '#/components/schemas/ToolCallChunk'},\n",
       "      'type': 'array',\n",
       "      'title': 'Tool Call Chunks'},\n",
       "     'chunk_position': {'anyOf': [{'type': 'string', 'const': 'last'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Chunk Position'}},\n",
       "    'additionalProperties': True,\n",
       "    'type': 'object',\n",
       "    'required': ['content'],\n",
       "    'title': 'AIMessageChunk',\n",
       "    'description': 'Message chunk from an AI (yielded when streaming).'},\n",
       "   'APIFeedbackSource': {'properties': {'type': {'type': 'string',\n",
       "      'const': 'api',\n",
       "      'title': 'Type',\n",
       "      'default': 'api'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'}},\n",
       "    'type': 'object',\n",
       "    'title': 'APIFeedbackSource',\n",
       "    'description': 'API feedback source.'},\n",
       "   'APIKeyCreateRequest': {'properties': {'description': {'type': 'string',\n",
       "      'title': 'Description',\n",
       "      'default': 'Default API key'},\n",
       "     'read_only': {'type': 'boolean',\n",
       "      'title': 'Read Only',\n",
       "      'default': False,\n",
       "      'deprecated': True},\n",
       "     'expires_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Expires At'},\n",
       "     'workspaces': {'anyOf': [{'items': {'type': 'string', 'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspaces'},\n",
       "     'role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Role Id'},\n",
       "     'org_role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Org Role Id'},\n",
       "     'default_workspace_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Workspace Id'}},\n",
       "    'type': 'object',\n",
       "    'title': 'APIKeyCreateRequest',\n",
       "    'description': 'API key POST schema.\\n\\nexpires_at: Optional datetime when the API key will expire.\\nworkspaces: List of workspace UUIDs this key can access (feature-flagged).\\nrole_id: Optional UUID of the role to assign to API key.\\n    If not provided, uses default role based on read_only flag:\\n    - WORKSPACE_ADMIN if read_only is False\\n    - WORKSPACE_READER if read_only is True\\norg_role_id: UUID of a org role for org-scoped keys\\n    If not provided, defaults to ORG_USER\\ndefault_workspace_id: UUID of the default workspace for PATs.\\n    If not provided, uses the current logic (first available workspace).'},\n",
       "   'APIKeyCreateResponse': {'properties': {'created_at': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Created At'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'short_key': {'type': 'string', 'title': 'Short Key'},\n",
       "     'description': {'type': 'string', 'title': 'Description'},\n",
       "     'read_only': {'type': 'boolean',\n",
       "      'title': 'Read Only',\n",
       "      'default': False,\n",
       "      'deprecated': True},\n",
       "     'last_used_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Used At'},\n",
       "     'expires_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Expires At'},\n",
       "     'workspace_names': {'anyOf': [{'items': {'type': 'string'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Names'},\n",
       "     'key': {'type': 'string', 'title': 'Key'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'short_key', 'description', 'key'],\n",
       "    'title': 'APIKeyCreateResponse',\n",
       "    'description': 'API key POST schema.'},\n",
       "   'APIKeyGetResponse': {'properties': {'created_at': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Created At'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'short_key': {'type': 'string', 'title': 'Short Key'},\n",
       "     'description': {'type': 'string', 'title': 'Description'},\n",
       "     'read_only': {'type': 'boolean',\n",
       "      'title': 'Read Only',\n",
       "      'default': False,\n",
       "      'deprecated': True},\n",
       "     'last_used_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Used At'},\n",
       "     'expires_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Expires At'},\n",
       "     'workspace_names': {'anyOf': [{'items': {'type': 'string'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Names'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'short_key', 'description'],\n",
       "    'title': 'APIKeyGetResponse',\n",
       "    'description': 'API key GET schema.'},\n",
       "   'AccessScope': {'type': 'string',\n",
       "    'enum': ['organization', 'workspace'],\n",
       "    'title': 'AccessScope'},\n",
       "   'AllowedLoginMethodsUpdate': {'properties': {'sso_only': {'anyOf': [{'type': 'boolean'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Sso Only'}},\n",
       "    'type': 'object',\n",
       "    'title': 'AllowedLoginMethodsUpdate'},\n",
       "   'AnnotationQueueBulkDeleteRunsRequest': {'properties': {'delete_all': {'type': 'boolean',\n",
       "      'title': 'Delete All',\n",
       "      'default': False},\n",
       "     'run_ids': {'anyOf': [{'items': {'type': 'string', 'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Run Ids'},\n",
       "     'exclude_run_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Exclude Run Ids'}},\n",
       "    'type': 'object',\n",
       "    'title': 'AnnotationQueueBulkDeleteRunsRequest'},\n",
       "   'AnnotationQueueCreateSchema': {'properties': {'description': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'num_reviewers_per_item': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Num Reviewers Per Item',\n",
       "      'default': 1},\n",
       "     'enable_reservations': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Enable Reservations',\n",
       "      'default': True},\n",
       "     'reservation_minutes': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Reservation Minutes',\n",
       "      'default': 1},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'default_dataset': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Dataset'},\n",
       "     'rubric_items': {'anyOf': [{'items': {'$ref': '#/components/schemas/AnnotationQueueRubricItemSchema'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Rubric Items'},\n",
       "     'rubric_instructions': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Rubric Instructions'},\n",
       "     'session_ids': {'anyOf': [{'items': {'type': 'string', 'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Ids'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name'],\n",
       "    'title': 'AnnotationQueueCreateSchema',\n",
       "    'description': 'AnnotationQueue schema.'},\n",
       "   'AnnotationQueueRubricItemSchema': {'properties': {'feedback_key': {'type': 'string',\n",
       "      'title': 'Feedback Key'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'value_descriptions': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Value Descriptions'},\n",
       "     'score_descriptions': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Score Descriptions'}},\n",
       "    'type': 'object',\n",
       "    'required': ['feedback_key'],\n",
       "    'title': 'AnnotationQueueRubricItemSchema'},\n",
       "   'AnnotationQueueRunAddSchema': {'properties': {'run_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Run Id'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'session_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Id'},\n",
       "     'trace_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace Id'},\n",
       "     'parent_run_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Run Id'},\n",
       "     'trace_tier': {'anyOf': [{'$ref': '#/components/schemas/TraceTier'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'required': ['run_id'],\n",
       "    'title': 'AnnotationQueueRunAddSchema',\n",
       "    'description': 'Schema for adding a run to an annotation queue with optional metadata.'},\n",
       "   'AnnotationQueueRunSchema': {'properties': {'run_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Run Id'},\n",
       "     'queue_id': {'type': 'string', 'format': 'uuid', 'title': 'Queue Id'},\n",
       "     'last_reviewed_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Reviewed Time'},\n",
       "     'added_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Added At'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['run_id', 'queue_id', 'id'],\n",
       "    'title': 'AnnotationQueueRunSchema'},\n",
       "   'AnnotationQueueRunUpdateSchema': {'properties': {'last_reviewed_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Reviewed Time'},\n",
       "     'added_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Added At'}},\n",
       "    'type': 'object',\n",
       "    'title': 'AnnotationQueueRunUpdateSchema'},\n",
       "   'AnnotationQueueSchema': {'properties': {'description': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'num_reviewers_per_item': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Num Reviewers Per Item',\n",
       "      'default': 1},\n",
       "     'enable_reservations': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Enable Reservations',\n",
       "      'default': True},\n",
       "     'reservation_minutes': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Reservation Minutes',\n",
       "      'default': 1},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'source_rule_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Source Rule Id'},\n",
       "     'run_rule_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Run Rule Id'},\n",
       "     'default_dataset': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Dataset'},\n",
       "     'queue_type': {'type': 'string',\n",
       "      'enum': ['single', 'pairwise'],\n",
       "      'title': 'Queue Type'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'id', 'tenant_id', 'queue_type'],\n",
       "    'title': 'AnnotationQueueSchema',\n",
       "    'description': 'AnnotationQueue schema.'},\n",
       "   'AnnotationQueueSchemaWithRubric': {'properties': {'description': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'num_reviewers_per_item': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Num Reviewers Per Item',\n",
       "      'default': 1},\n",
       "     'enable_reservations': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Enable Reservations',\n",
       "      'default': True},\n",
       "     'reservation_minutes': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Reservation Minutes',\n",
       "      'default': 1},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'source_rule_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Source Rule Id'},\n",
       "     'run_rule_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Run Rule Id'},\n",
       "     'default_dataset': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Dataset'},\n",
       "     'queue_type': {'type': 'string',\n",
       "      'enum': ['single', 'pairwise'],\n",
       "      'title': 'Queue Type'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'rubric_items': {'anyOf': [{'items': {'$ref': '#/components/schemas/AnnotationQueueRubricItemSchema'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Rubric Items'},\n",
       "     'rubric_instructions': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Rubric Instructions'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'id', 'tenant_id', 'queue_type'],\n",
       "    'title': 'AnnotationQueueSchemaWithRubric',\n",
       "    'description': 'AnnotationQueue schema with rubric.'},\n",
       "   'AnnotationQueueSchemaWithSize': {'properties': {'description': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'num_reviewers_per_item': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Num Reviewers Per Item',\n",
       "      'default': 1},\n",
       "     'enable_reservations': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Enable Reservations',\n",
       "      'default': True},\n",
       "     'reservation_minutes': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Reservation Minutes',\n",
       "      'default': 1},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'source_rule_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Source Rule Id'},\n",
       "     'run_rule_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Run Rule Id'},\n",
       "     'default_dataset': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Dataset'},\n",
       "     'queue_type': {'type': 'string',\n",
       "      'enum': ['single', 'pairwise'],\n",
       "      'title': 'Queue Type'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'total_runs': {'type': 'integer', 'title': 'Total Runs'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'id', 'tenant_id', 'queue_type', 'total_runs'],\n",
       "    'title': 'AnnotationQueueSchemaWithSize',\n",
       "    'description': 'AnnotationQueue schema with size.'},\n",
       "   'AnnotationQueueSizeSchema': {'properties': {'size': {'type': 'integer',\n",
       "      'title': 'Size'}},\n",
       "    'type': 'object',\n",
       "    'required': ['size'],\n",
       "    'title': 'AnnotationQueueSizeSchema',\n",
       "    'description': 'Size of an Annotation Queue'},\n",
       "   'AnnotationQueueUpdateSchema': {'properties': {'name': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'default_dataset': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Dataset'},\n",
       "     'num_reviewers_per_item': {'anyOf': [{'type': 'integer'},\n",
       "       {'$ref': '#/components/schemas/Missing'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Num Reviewers Per Item',\n",
       "      'default': 1},\n",
       "     'enable_reservations': {'type': 'boolean',\n",
       "      'title': 'Enable Reservations',\n",
       "      'default': True},\n",
       "     'reservation_minutes': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Reservation Minutes'},\n",
       "     'rubric_items': {'anyOf': [{'items': {'$ref': '#/components/schemas/AnnotationQueueRubricItemSchema'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Rubric Items'},\n",
       "     'rubric_instructions': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Rubric Instructions'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'$ref': '#/components/schemas/Missing'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata',\n",
       "      'default': {'__missing__': '__missing__'}}},\n",
       "    'type': 'object',\n",
       "    'title': 'AnnotationQueueUpdateSchema',\n",
       "    'description': 'AnnotationQueue update schema.'},\n",
       "   'AppFeedbackSource': {'properties': {'type': {'type': 'string',\n",
       "      'const': 'app',\n",
       "      'title': 'Type',\n",
       "      'default': 'app'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'}},\n",
       "    'type': 'object',\n",
       "    'title': 'AppFeedbackSource',\n",
       "    'description': 'Feedback from the LangChainPlus App.'},\n",
       "   'Artifact': {'properties': {'id': {'type': 'string', 'title': 'Id'},\n",
       "     'contents': {'items': {'$ref': '#/components/schemas/ArtifactContent'},\n",
       "      'type': 'array',\n",
       "      'title': 'Contents'},\n",
       "     'current_content_index': {'type': 'integer',\n",
       "      'title': 'Current Content Index'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'contents', 'current_content_index'],\n",
       "    'title': 'Artifact'},\n",
       "   'ArtifactContent': {'properties': {'index': {'type': 'integer',\n",
       "      'title': 'Index'},\n",
       "     'content': {'type': 'string', 'title': 'Content'}},\n",
       "    'type': 'object',\n",
       "    'required': ['index', 'content'],\n",
       "    'title': 'ArtifactContent'},\n",
       "   'AttachmentsOperations': {'properties': {'rename': {'additionalProperties': {'type': 'string'},\n",
       "      'type': 'object',\n",
       "      'title': 'Rename',\n",
       "      'description': 'Mapping of old attachment names to new names'},\n",
       "     'retain': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Retain',\n",
       "      'description': 'List of attachment names to keep'}},\n",
       "    'type': 'object',\n",
       "    'title': 'AttachmentsOperations'},\n",
       "   'AuditLogMessage': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'operation_name': {'type': 'string', 'title': 'Operation Name'},\n",
       "     'resource_ids': {'anyOf': [{'items': {'type': 'string', 'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Resource Ids'},\n",
       "     'operation_succeeded': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Operation Succeeded'},\n",
       "     'request_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Request Time'},\n",
       "     'request_method': {'type': 'string', 'title': 'Request Method'},\n",
       "     'request_path': {'type': 'string', 'title': 'Request Path'},\n",
       "     'client_host': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Client Host'},\n",
       "     'client_port': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Client Port'},\n",
       "     'x_forwarded_for': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'X Forwarded For'},\n",
       "     'api_key_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Api Key Id'},\n",
       "     'user_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'User Id'},\n",
       "     'ls_user_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Ls User Id'},\n",
       "     'organization_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Organization Id'},\n",
       "     'workspace_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Id'},\n",
       "     'response_status_code': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Response Status Code'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'operation_name',\n",
       "     'resource_ids',\n",
       "     'operation_succeeded',\n",
       "     'request_time',\n",
       "     'request_method',\n",
       "     'request_path',\n",
       "     'client_host',\n",
       "     'client_port',\n",
       "     'x_forwarded_for',\n",
       "     'api_key_id',\n",
       "     'user_id',\n",
       "     'ls_user_id',\n",
       "     'organization_id',\n",
       "     'workspace_id',\n",
       "     'response_status_code'],\n",
       "    'title': 'AuditLogMessage',\n",
       "    'description': 'Represents an audit log message.'},\n",
       "   'AuditLogOperation': {'type': 'string',\n",
       "    'enum': ['create_api_key',\n",
       "     'delete_api_key',\n",
       "     'create_personal_access_token',\n",
       "     'delete_personal_access_token',\n",
       "     'create_service_key',\n",
       "     'delete_service_key',\n",
       "     'create_role',\n",
       "     'update_role',\n",
       "     'delete_role',\n",
       "     'invite_user_to_org',\n",
       "     'invite_users_to_org_batch',\n",
       "     'add_basic_auth_users_to_org',\n",
       "     'update_basic_auth_user',\n",
       "     'delete_org_pending_member',\n",
       "     'delete_org_member',\n",
       "     'update_org_member',\n",
       "     'create_sso_settings',\n",
       "     'update_sso_settings',\n",
       "     'delete_sso_settings',\n",
       "     'update_default_sso_provision_organization',\n",
       "     'update_login_methods',\n",
       "     'update_organization_info',\n",
       "     'update_business_info',\n",
       "     'update_payment_plan',\n",
       "     'update_payment_method',\n",
       "     'create_payment_setup_intent',\n",
       "     'create_payment_checkout_session',\n",
       "     'confirm_payment_checkout_session',\n",
       "     'create_payment_account_link',\n",
       "     'create_workspace',\n",
       "     'update_workspace',\n",
       "     'delete_workspace',\n",
       "     'add_member_to_workspace',\n",
       "     'add_members_to_workspace_batch',\n",
       "     'delete_workspace_member',\n",
       "     'update_workspace_member',\n",
       "     'delete_workspace_pending_member',\n",
       "     'update_workspace_secrets',\n",
       "     'unshare_entities',\n",
       "     'set_tenant_handle',\n",
       "     'create_tag_key',\n",
       "     'update_tag_key',\n",
       "     'delete_tag_key',\n",
       "     'create_tag_value',\n",
       "     'update_tag_value',\n",
       "     'delete_tag_value',\n",
       "     'create_tagging',\n",
       "     'delete_tagging',\n",
       "     'create_bulk_export',\n",
       "     'cancel_bulk_export',\n",
       "     'create_bulk_export_destination',\n",
       "     'update_ttl_settings',\n",
       "     'update_usage_limit',\n",
       "     'delete_usage_limit',\n",
       "     'create_model_price_map',\n",
       "     'update_model_price_map',\n",
       "     'delete_model_price_map',\n",
       "     'create_chart',\n",
       "     'update_chart',\n",
       "     'delete_chart',\n",
       "     'create_chart_section',\n",
       "     'update_chart_section',\n",
       "     'delete_chart_section',\n",
       "     'clone_chart_section',\n",
       "     'create_org_chart',\n",
       "     'update_org_chart',\n",
       "     'delete_org_chart',\n",
       "     'create_org_chart_section',\n",
       "     'update_org_chart_section',\n",
       "     'delete_org_chart_section',\n",
       "     'create_deployment',\n",
       "     'update_deployment',\n",
       "     'delete_deployment'],\n",
       "    'title': 'AuditLogOperation',\n",
       "    'description': 'Operations that can be logged in audit_logs database table.\\nNOTE: not all of @audit_log_operation(<log name>) names are currently used here.\\n\\nNOTE: OCSF mapping: endpoints with POST method and operation prefixed with \"update_\"\\nmap to OCSF UPDATE activity type.'},\n",
       "   'AuthProvider': {'type': 'string',\n",
       "    'enum': ['email',\n",
       "     'supabase:non-sso',\n",
       "     'supabase:sso',\n",
       "     'oidc',\n",
       "     'custom-oidc'],\n",
       "    'title': 'AuthProvider'},\n",
       "   'AutoEvalFeedbackSource': {'properties': {'type': {'type': 'string',\n",
       "      'const': 'auto_eval',\n",
       "      'title': 'Type',\n",
       "      'default': 'auto_eval'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'}},\n",
       "    'type': 'object',\n",
       "    'title': 'AutoEvalFeedbackSource',\n",
       "    'description': 'Auto eval feedback source.'},\n",
       "   'BasicAuthMemberCreate': {'properties': {'user_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'User Id'},\n",
       "     'ls_user_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Ls User Id'},\n",
       "     'email': {'type': 'string', 'title': 'Email'},\n",
       "     'read_only': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Read Only',\n",
       "      'deprecated': True},\n",
       "     'role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Role Id'},\n",
       "     'password': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Password'},\n",
       "     'full_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Full Name'},\n",
       "     'workspace_role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Role Id'},\n",
       "     'workspace_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Ids'}},\n",
       "    'type': 'object',\n",
       "    'required': ['email'],\n",
       "    'title': 'BasicAuthMemberCreate'},\n",
       "   'BasicAuthResponse': {'properties': {'access_token': {'type': 'string',\n",
       "      'title': 'Access Token'}},\n",
       "    'type': 'object',\n",
       "    'required': ['access_token'],\n",
       "    'title': 'BasicAuthResponse'},\n",
       "   'BasicAuthUserPatch': {'properties': {'password': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Password'},\n",
       "     'full_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Full Name'}},\n",
       "    'type': 'object',\n",
       "    'title': 'BasicAuthUserPatch'},\n",
       "   'BatchIngestConfig': {'properties': {'use_multipart_endpoint': {'type': 'boolean',\n",
       "      'title': 'Use Multipart Endpoint',\n",
       "      'default': True},\n",
       "     'scale_up_qsize_trigger': {'type': 'integer',\n",
       "      'title': 'Scale Up Qsize Trigger',\n",
       "      'default': 1000},\n",
       "     'scale_up_nthreads_limit': {'type': 'integer',\n",
       "      'title': 'Scale Up Nthreads Limit',\n",
       "      'default': 16},\n",
       "     'scale_down_nempty_trigger': {'type': 'integer',\n",
       "      'title': 'Scale Down Nempty Trigger',\n",
       "      'default': 4},\n",
       "     'size_limit': {'type': 'integer', 'title': 'Size Limit', 'default': 100},\n",
       "     'size_limit_bytes': {'type': 'integer',\n",
       "      'title': 'Size Limit Bytes',\n",
       "      'default': 20971520}},\n",
       "    'type': 'object',\n",
       "    'title': 'BatchIngestConfig',\n",
       "    'description': 'Batch ingest config.'},\n",
       "   'BodyParamsForRunSchema': {'properties': {'id': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Id'},\n",
       "     'trace': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace'},\n",
       "     'parent_run': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Run'},\n",
       "     'run_type': {'anyOf': [{'$ref': '#/components/schemas/RunTypeEnum'},\n",
       "       {'type': 'null'}]},\n",
       "     'session': {'anyOf': [{'items': {'type': 'string', 'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session'},\n",
       "     'reference_example': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reference Example'},\n",
       "     'execution_order': {'anyOf': [{'type': 'integer',\n",
       "        'maximum': 1.0,\n",
       "        'minimum': 1.0},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Execution Order'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'error': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Error'},\n",
       "     'query': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Query'},\n",
       "     'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Filter'},\n",
       "     'trace_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Trace Filter'},\n",
       "     'tree_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tree Filter'},\n",
       "     'is_root': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Is Root'},\n",
       "     'data_source_type': {'anyOf': [{'$ref': '#/components/schemas/RunsFilterDataSourceTypeEnum'},\n",
       "       {'type': 'null'}]},\n",
       "     'skip_pagination': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Skip Pagination'},\n",
       "     'search_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Search Filter'},\n",
       "     'use_experimental_search': {'type': 'boolean',\n",
       "      'title': 'Use Experimental Search',\n",
       "      'default': False},\n",
       "     'cursor': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Cursor'},\n",
       "     'limit': {'type': 'integer',\n",
       "      'maximum': 100.0,\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Limit',\n",
       "      'default': 100},\n",
       "     'select': {'items': {'$ref': '#/components/schemas/RunSelect'},\n",
       "      'type': 'array',\n",
       "      'title': 'Select',\n",
       "      'default': ['id',\n",
       "       'name',\n",
       "       'run_type',\n",
       "       'start_time',\n",
       "       'end_time',\n",
       "       'status',\n",
       "       'error',\n",
       "       'extra',\n",
       "       'events',\n",
       "       'inputs',\n",
       "       'outputs',\n",
       "       'parent_run_id',\n",
       "       'manifest_id',\n",
       "       'manifest_s3_id',\n",
       "       'manifest',\n",
       "       'session_id',\n",
       "       'serialized',\n",
       "       'reference_example_id',\n",
       "       'reference_dataset_id',\n",
       "       'total_tokens',\n",
       "       'prompt_tokens',\n",
       "       'prompt_token_details',\n",
       "       'completion_tokens',\n",
       "       'completion_token_details',\n",
       "       'total_cost',\n",
       "       'prompt_cost',\n",
       "       'prompt_cost_details',\n",
       "       'completion_cost',\n",
       "       'completion_cost_details',\n",
       "       'price_model_id',\n",
       "       'first_token_time',\n",
       "       'trace_id',\n",
       "       'dotted_order',\n",
       "       'last_queued_at',\n",
       "       'feedback_stats',\n",
       "       'parent_run_ids',\n",
       "       'tags',\n",
       "       'in_dataset',\n",
       "       'app_path',\n",
       "       'share_token',\n",
       "       'trace_tier',\n",
       "       'trace_first_received_at',\n",
       "       'ttl_seconds',\n",
       "       'trace_upgrade',\n",
       "       'thread_id']},\n",
       "     'order': {'$ref': '#/components/schemas/RunDateOrder', 'default': 'desc'},\n",
       "     'skip_prev_cursor': {'type': 'boolean',\n",
       "      'title': 'Skip Prev Cursor',\n",
       "      'default': False}},\n",
       "    'type': 'object',\n",
       "    'title': 'BodyParamsForRunSchema',\n",
       "    'description': 'Query params for run endpoints.'},\n",
       "   'BodyParamsForRunsQuerySchema': {'properties': {'id': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Id'},\n",
       "     'trace': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace'},\n",
       "     'parent_run': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Run'},\n",
       "     'run_type': {'anyOf': [{'$ref': '#/components/schemas/RunTypeEnum'},\n",
       "       {'type': 'null'}]},\n",
       "     'session': {'anyOf': [{'items': {'type': 'string', 'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session'},\n",
       "     'reference_example': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reference Example'},\n",
       "     'execution_order': {'anyOf': [{'type': 'integer',\n",
       "        'maximum': 1.0,\n",
       "        'minimum': 1.0},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Execution Order'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'error': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Error'},\n",
       "     'query': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Query'},\n",
       "     'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Filter'},\n",
       "     'trace_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Trace Filter'},\n",
       "     'tree_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tree Filter'},\n",
       "     'is_root': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Is Root'},\n",
       "     'data_source_type': {'anyOf': [{'$ref': '#/components/schemas/RunsFilterDataSourceTypeEnum'},\n",
       "       {'type': 'null'}]},\n",
       "     'skip_pagination': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Skip Pagination'},\n",
       "     'search_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Search Filter'},\n",
       "     'use_experimental_search': {'type': 'boolean',\n",
       "      'title': 'Use Experimental Search',\n",
       "      'default': False},\n",
       "     'cursor': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Cursor'},\n",
       "     'limit': {'type': 'integer',\n",
       "      'maximum': 100.0,\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Limit',\n",
       "      'default': 100},\n",
       "     'select': {'items': {'$ref': '#/components/schemas/RunSelect'},\n",
       "      'type': 'array',\n",
       "      'title': 'Select',\n",
       "      'default': ['id',\n",
       "       'name',\n",
       "       'run_type',\n",
       "       'start_time',\n",
       "       'end_time',\n",
       "       'status',\n",
       "       'error',\n",
       "       'extra',\n",
       "       'events',\n",
       "       'inputs',\n",
       "       'outputs',\n",
       "       'parent_run_id',\n",
       "       'manifest_id',\n",
       "       'manifest_s3_id',\n",
       "       'manifest',\n",
       "       'session_id',\n",
       "       'serialized',\n",
       "       'reference_example_id',\n",
       "       'reference_dataset_id',\n",
       "       'total_tokens',\n",
       "       'prompt_tokens',\n",
       "       'prompt_token_details',\n",
       "       'completion_tokens',\n",
       "       'completion_token_details',\n",
       "       'total_cost',\n",
       "       'prompt_cost',\n",
       "       'prompt_cost_details',\n",
       "       'completion_cost',\n",
       "       'completion_cost_details',\n",
       "       'price_model_id',\n",
       "       'first_token_time',\n",
       "       'trace_id',\n",
       "       'dotted_order',\n",
       "       'last_queued_at',\n",
       "       'feedback_stats',\n",
       "       'parent_run_ids',\n",
       "       'tags',\n",
       "       'in_dataset',\n",
       "       'app_path',\n",
       "       'share_token',\n",
       "       'trace_tier',\n",
       "       'trace_first_received_at',\n",
       "       'ttl_seconds',\n",
       "       'trace_upgrade',\n",
       "       'thread_id']},\n",
       "     'order': {'$ref': '#/components/schemas/RunDateOrder', 'default': 'desc'},\n",
       "     'skip_prev_cursor': {'type': 'boolean',\n",
       "      'title': 'Skip Prev Cursor',\n",
       "      'default': False}},\n",
       "    'type': 'object',\n",
       "    'title': 'BodyParamsForRunsQuerySchema',\n",
       "    'description': 'Query params for runs query endpoint.'},\n",
       "   'Body_clone_dataset_api_v1_datasets_clone_post': {'properties': {'target_dataset_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Target Dataset Id'},\n",
       "     'source_dataset_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Source Dataset Id'},\n",
       "     'as_of': {'anyOf': [{'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "         {'type': 'string'}],\n",
       "        'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'As Of'},\n",
       "     'examples': {'items': {'type': 'string', 'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'title': 'Examples',\n",
       "      'default': []},\n",
       "     'split': {'anyOf': [{'type': 'string'},\n",
       "       {'items': {'type': 'string'}, 'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Split'}},\n",
       "    'type': 'object',\n",
       "    'required': ['target_dataset_id', 'source_dataset_id'],\n",
       "    'title': 'Body_clone_dataset_api_v1_datasets_clone_post'},\n",
       "   'Body_delete_runs_api_v1_runs_delete_post': {'properties': {'session_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Id'},\n",
       "     'trace_ids': {'anyOf': [{'items': {'type': 'string', 'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace Ids'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'}},\n",
       "    'type': 'object',\n",
       "    'title': 'Body_delete_runs_api_v1_runs_delete_post'},\n",
       "   'Body_execute_api_v1_ace_execute_post': {'properties': {'args': {'items': {},\n",
       "      'type': 'array',\n",
       "      'title': 'Args'},\n",
       "     'code': {'type': 'string', 'title': 'Code'},\n",
       "     'language': {'type': 'string', 'title': 'Language'}},\n",
       "    'type': 'object',\n",
       "    'required': ['args', 'code', 'language'],\n",
       "    'title': 'Body_execute_api_v1_ace_execute_post'},\n",
       "   'Body_update_dataset_splits_api_v1_datasets__dataset_id__splits_put': {'properties': {'split_name': {'type': 'string',\n",
       "      'title': 'Split Name'},\n",
       "     'examples': {'items': {'type': 'string', 'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'title': 'Examples'},\n",
       "     'remove': {'type': 'boolean', 'title': 'Remove', 'default': False}},\n",
       "    'type': 'object',\n",
       "    'required': ['split_name', 'examples'],\n",
       "    'title': 'Body_update_dataset_splits_api_v1_datasets__dataset_id__splits_put'},\n",
       "   'Body_upload_csv_dataset_api_v1_datasets_upload_post': {'properties': {'file': {'type': 'string',\n",
       "      'format': 'binary',\n",
       "      'title': 'File'},\n",
       "     'input_keys': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Input Keys'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'data_type': {'$ref': '#/components/schemas/DataType', 'default': 'kv'},\n",
       "     'output_keys': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Output Keys',\n",
       "      'default': []},\n",
       "     'metadata_keys': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Metadata Keys',\n",
       "      'default': []},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'inputs_schema_definition': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs Schema Definition'},\n",
       "     'outputs_schema_definition': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs Schema Definition'},\n",
       "     'transformations': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Transformations'},\n",
       "     'input_key_mappings': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Input Key Mappings'},\n",
       "     'output_key_mappings': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Output Key Mappings'},\n",
       "     'metadata_key_mappings': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Metadata Key Mappings'}},\n",
       "    'type': 'object',\n",
       "    'required': ['file', 'input_keys'],\n",
       "    'title': 'Body_upload_csv_dataset_api_v1_datasets_upload_post'},\n",
       "   'Body_upload_examples_from_csv_api_v1_examples_upload__dataset_id__post': {'properties': {'file': {'type': 'string',\n",
       "      'format': 'binary',\n",
       "      'title': 'File'},\n",
       "     'input_keys': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Input Keys'},\n",
       "     'output_keys': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Output Keys'},\n",
       "     'metadata_keys': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Metadata Keys'}},\n",
       "    'type': 'object',\n",
       "    'required': ['file', 'input_keys'],\n",
       "    'title': 'Body_upload_examples_from_csv_api_v1_examples_upload__dataset_id__post'},\n",
       "   'BulkExport': {'properties': {'bulk_export_destination_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Bulk Export Destination Id'},\n",
       "     'session_id': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'},\n",
       "     'start_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Filter'},\n",
       "     'format': {'$ref': '#/components/schemas/BulkExportFormat',\n",
       "      'default': 'Parquet'},\n",
       "     'format_version': {'$ref': '#/components/schemas/BulkExportFormatVersion',\n",
       "      'default': 'v1'},\n",
       "     'compression': {'$ref': '#/components/schemas/BulkExportCompression',\n",
       "      'default': 'gzip'},\n",
       "     'interval_hours': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Interval Hours'},\n",
       "     'export_fields': {'anyOf': [{'items': {'type': 'string'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Export Fields'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'status': {'$ref': '#/components/schemas/BulkExportStatus'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'finished_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Finished At'},\n",
       "     'source_bulk_export_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Source Bulk Export Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['bulk_export_destination_id',\n",
       "     'session_id',\n",
       "     'start_time',\n",
       "     'id',\n",
       "     'tenant_id',\n",
       "     'status',\n",
       "     'created_at',\n",
       "     'updated_at',\n",
       "     'finished_at'],\n",
       "    'title': 'BulkExport'},\n",
       "   'BulkExportCompression': {'type': 'string',\n",
       "    'enum': ['none', 'gzip', 'snappy', 'zstandard'],\n",
       "    'title': 'BulkExportCompression'},\n",
       "   'BulkExportCreate': {'properties': {'bulk_export_destination_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Bulk Export Destination Id'},\n",
       "     'session_id': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'},\n",
       "     'start_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Filter'},\n",
       "     'format': {'$ref': '#/components/schemas/BulkExportFormat',\n",
       "      'default': 'Parquet'},\n",
       "     'format_version': {'$ref': '#/components/schemas/BulkExportFormatVersion',\n",
       "      'default': 'v1'},\n",
       "     'compression': {'$ref': '#/components/schemas/BulkExportCompression',\n",
       "      'default': 'gzip'},\n",
       "     'interval_hours': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Interval Hours'},\n",
       "     'export_fields': {'anyOf': [{'items': {'type': 'string'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Export Fields'}},\n",
       "    'type': 'object',\n",
       "    'required': ['bulk_export_destination_id', 'session_id', 'start_time'],\n",
       "    'title': 'BulkExportCreate'},\n",
       "   'BulkExportDestination': {'properties': {'destination_type': {'$ref': '#/components/schemas/BulkExportDestinationType',\n",
       "      'default': 's3'},\n",
       "     'display_name': {'type': 'string',\n",
       "      'minLength': 1,\n",
       "      'pattern': \"^[a-zA-Z0-9\\\\-_ ']+$\",\n",
       "      'title': 'Display Name'},\n",
       "     'config': {'$ref': '#/components/schemas/BulkExportDestinationS3Config'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'credentials_keys': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Credentials Keys'}},\n",
       "    'type': 'object',\n",
       "    'required': ['display_name',\n",
       "     'config',\n",
       "     'id',\n",
       "     'tenant_id',\n",
       "     'created_at',\n",
       "     'updated_at',\n",
       "     'credentials_keys'],\n",
       "    'title': 'BulkExportDestination'},\n",
       "   'BulkExportDestinationCreate': {'properties': {'destination_type': {'$ref': '#/components/schemas/BulkExportDestinationType',\n",
       "      'default': 's3'},\n",
       "     'display_name': {'type': 'string',\n",
       "      'minLength': 1,\n",
       "      'pattern': \"^[a-zA-Z0-9\\\\-_ ']+$\",\n",
       "      'title': 'Display Name'},\n",
       "     'config': {'$ref': '#/components/schemas/BulkExportDestinationS3Config'},\n",
       "     'credentials': {'anyOf': [{'$ref': '#/components/schemas/BulkExportDestinationS3Credentials'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'required': ['display_name', 'config'],\n",
       "    'title': 'BulkExportDestinationCreate'},\n",
       "   'BulkExportDestinationS3Config': {'properties': {'endpoint_url': {'anyOf': [{'type': 'string',\n",
       "        'maxLength': 2048},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Endpoint Url'},\n",
       "     'prefix': {'type': 'string',\n",
       "      'maxLength': 2048,\n",
       "      'title': 'Prefix',\n",
       "      'default': ''},\n",
       "     'bucket_name': {'anyOf': [{'type': 'string',\n",
       "        'maxLength': 63,\n",
       "        'minLength': 3},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Bucket Name'},\n",
       "     'region': {'anyOf': [{'type': 'string', 'minLength': 1},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Region'},\n",
       "     's3_additional_kwargs': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'S3 Additional Kwargs'},\n",
       "     'include_bucket_in_prefix': {'anyOf': [{'type': 'boolean'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Include Bucket In Prefix',\n",
       "      'description': 'Whether to include bucket name as part of the S3 path prefix. Defaults to False for new destinations. Set to True for legacy compatibility.'}},\n",
       "    'type': 'object',\n",
       "    'title': 'BulkExportDestinationS3Config'},\n",
       "   'BulkExportDestinationS3Credentials': {'properties': {'access_key_id': {'type': 'string',\n",
       "      'maxLength': 255,\n",
       "      'minLength': 1,\n",
       "      'title': 'Access Key Id'},\n",
       "     'secret_access_key': {'type': 'string',\n",
       "      'maxLength': 2048,\n",
       "      'minLength': 1,\n",
       "      'title': 'Secret Access Key'},\n",
       "     'session_token': {'anyOf': [{'type': 'string', 'maxLength': 2048},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Token'}},\n",
       "    'type': 'object',\n",
       "    'required': ['access_key_id', 'secret_access_key'],\n",
       "    'title': 'BulkExportDestinationS3Credentials'},\n",
       "   'BulkExportDestinationType': {'type': 'string',\n",
       "    'enum': ['s3'],\n",
       "    'title': 'BulkExportDestinationType'},\n",
       "   'BulkExportFormat': {'type': 'string',\n",
       "    'enum': ['Parquet'],\n",
       "    'title': 'BulkExportFormat'},\n",
       "   'BulkExportFormatVersion': {'type': 'string',\n",
       "    'enum': ['v1', 'v2_beta'],\n",
       "    'title': 'BulkExportFormatVersion',\n",
       "    'description': 'Enum for bulk export format versions.'},\n",
       "   'BulkExportRun': {'properties': {'bulk_export_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Bulk Export Id'},\n",
       "     'metadata': {'$ref': '#/components/schemas/BulkExportRunMetadata'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'status': {'$ref': '#/components/schemas/BulkExportRunStatus'},\n",
       "     'retry_number': {'type': 'integer',\n",
       "      'title': 'Retry Number',\n",
       "      'default': 0},\n",
       "     'errors': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Errors'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'finished_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Finished At'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'}},\n",
       "    'type': 'object',\n",
       "    'required': ['bulk_export_id',\n",
       "     'metadata',\n",
       "     'id',\n",
       "     'status',\n",
       "     'created_at',\n",
       "     'updated_at',\n",
       "     'finished_at'],\n",
       "    'title': 'BulkExportRun'},\n",
       "   'BulkExportRunMetadata': {'properties': {'prefix': {'type': 'string',\n",
       "      'title': 'Prefix'},\n",
       "     'start_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'End Time'},\n",
       "     'result': {'anyOf': [{'$ref': '#/components/schemas/BulkExportRunProgress'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'required': ['prefix', 'start_time', 'end_time'],\n",
       "    'title': 'BulkExportRunMetadata'},\n",
       "   'BulkExportRunProgress': {'properties': {'rows_written': {'type': 'integer',\n",
       "      'title': 'Rows Written'},\n",
       "     'exported_files': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Exported Files'},\n",
       "     'export_path': {'type': 'string', 'title': 'Export Path'},\n",
       "     'latest_cursor': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Latest Cursor'}},\n",
       "    'type': 'object',\n",
       "    'required': ['rows_written',\n",
       "     'exported_files',\n",
       "     'export_path',\n",
       "     'latest_cursor'],\n",
       "    'title': 'BulkExportRunProgress'},\n",
       "   'BulkExportRunStatus': {'type': 'string',\n",
       "    'enum': ['Cancelled',\n",
       "     'Completed',\n",
       "     'Created',\n",
       "     'Failed',\n",
       "     'TimedOut',\n",
       "     'Running'],\n",
       "    'title': 'BulkExportRunStatus'},\n",
       "   'BulkExportStatus': {'type': 'string',\n",
       "    'enum': ['Cancelled',\n",
       "     'Completed',\n",
       "     'Created',\n",
       "     'IntervalScheduled',\n",
       "     'Failed',\n",
       "     'TimedOut',\n",
       "     'Running'],\n",
       "    'title': 'BulkExportStatus'},\n",
       "   'BulkExportUpdatableStatus': {'type': 'string',\n",
       "    'enum': ['Cancelled'],\n",
       "    'title': 'BulkExportUpdatableStatus'},\n",
       "   'BulkExportUpdate': {'properties': {'status': {'$ref': '#/components/schemas/BulkExportUpdatableStatus',\n",
       "      'default': 'Cancelled'}},\n",
       "    'type': 'object',\n",
       "    'title': 'BulkExportUpdate'},\n",
       "   'ChangePaymentPlanReq': {'type': 'string',\n",
       "    'enum': ['disabled',\n",
       "     'developer',\n",
       "     'plus',\n",
       "     'startup',\n",
       "     'startup_v0',\n",
       "     'partner',\n",
       "     'premier',\n",
       "     'free'],\n",
       "    'title': 'ChangePaymentPlanReq',\n",
       "    'description': 'Enum for payment plans that the user can change to. Developer plans are permanent and enterprise plans will be changed manually.'},\n",
       "   'ChangePaymentPlanSchema': {'properties': {'tier': {'$ref': '#/components/schemas/ChangePaymentPlanReq'}},\n",
       "    'type': 'object',\n",
       "    'required': ['tier'],\n",
       "    'title': 'ChangePaymentPlanSchema',\n",
       "    'description': 'Change payment plan schema.'},\n",
       "   'ChatMessage': {'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "       {'items': {'anyOf': [{'type': 'string'},\n",
       "          {'additionalProperties': True, 'type': 'object'}]},\n",
       "        'type': 'array'}],\n",
       "      'title': 'Content'},\n",
       "     'additional_kwargs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Additional Kwargs'},\n",
       "     'response_metadata': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Response Metadata'},\n",
       "     'type': {'type': 'string',\n",
       "      'const': 'chat',\n",
       "      'title': 'Type',\n",
       "      'default': 'chat'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "     'role': {'type': 'string', 'title': 'Role'}},\n",
       "    'additionalProperties': True,\n",
       "    'type': 'object',\n",
       "    'required': ['content', 'role'],\n",
       "    'title': 'ChatMessage',\n",
       "    'description': 'Message that can be assigned an arbitrary speaker (i.e. role).'},\n",
       "   'ChatMessageChunk': {'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "       {'items': {'anyOf': [{'type': 'string'},\n",
       "          {'additionalProperties': True, 'type': 'object'}]},\n",
       "        'type': 'array'}],\n",
       "      'title': 'Content'},\n",
       "     'additional_kwargs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Additional Kwargs'},\n",
       "     'response_metadata': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Response Metadata'},\n",
       "     'type': {'type': 'string',\n",
       "      'const': 'ChatMessageChunk',\n",
       "      'title': 'Type',\n",
       "      'default': 'ChatMessageChunk'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "     'role': {'type': 'string', 'title': 'Role'}},\n",
       "    'additionalProperties': True,\n",
       "    'type': 'object',\n",
       "    'required': ['content', 'role'],\n",
       "    'title': 'ChatMessageChunk',\n",
       "    'description': 'Chat Message chunk.'},\n",
       "   'ClusteringJobConfigResponse': {'properties': {'id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'string'}],\n",
       "      'title': 'Id'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'config': {'$ref': '#/components/schemas/SavedRunClusteringJobRequest'},\n",
       "     'prebuilt': {'type': 'boolean', 'title': 'Prebuilt'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'name', 'config', 'prebuilt'],\n",
       "    'title': 'ClusteringJobConfigResponse',\n",
       "    'description': 'Full clustering job config with all details.'},\n",
       "   'CodeEvaluatorTopLevel': {'properties': {'code': {'type': 'string',\n",
       "      'title': 'Code'},\n",
       "     'language': {'anyOf': [{'type': 'string',\n",
       "        'enum': ['python', 'javascript']},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Language',\n",
       "      'default': 'python'}},\n",
       "    'type': 'object',\n",
       "    'required': ['code'],\n",
       "    'title': 'CodeEvaluatorTopLevel'},\n",
       "   'Comment': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'comment_by': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Comment By'},\n",
       "     'comment_on': {'type': 'string', 'format': 'uuid', 'title': 'Comment On'},\n",
       "     'parent_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Id'},\n",
       "     'content': {'type': 'string', 'title': 'Content'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'comment_by_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Comment By Name'},\n",
       "     'num_sub_comments': {'type': 'integer', 'title': 'Num Sub Comments'},\n",
       "     'num_likes': {'type': 'integer', 'title': 'Num Likes'},\n",
       "     'liked_by_auth_user': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Liked By Auth User'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'comment_on',\n",
       "     'content',\n",
       "     'created_at',\n",
       "     'updated_at',\n",
       "     'num_sub_comments',\n",
       "     'num_likes'],\n",
       "    'title': 'Comment'},\n",
       "   'CommitManifestResponse': {'properties': {'commit_hash': {'type': 'string',\n",
       "      'title': 'Commit Hash'},\n",
       "     'manifest': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Manifest'},\n",
       "     'examples': {'anyOf': [{'items': {'$ref': '#/components/schemas/RepoExampleResponse'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Examples'}},\n",
       "    'type': 'object',\n",
       "    'required': ['commit_hash', 'manifest'],\n",
       "    'title': 'CommitManifestResponse',\n",
       "    'description': 'Response model for get_commit_manifest.'},\n",
       "   'ComparativeExperiment': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'modified_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Modified At'},\n",
       "     'reference_dataset_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Reference Dataset Id'},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'},\n",
       "     'experiments_info': {'items': {'$ref': '#/components/schemas/SimpleExperimentInfo'},\n",
       "      'type': 'array',\n",
       "      'title': 'Experiments Info'},\n",
       "     'feedback_stats': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Stats'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'tenant_id',\n",
       "     'created_at',\n",
       "     'modified_at',\n",
       "     'reference_dataset_id',\n",
       "     'experiments_info'],\n",
       "    'title': 'ComparativeExperiment',\n",
       "    'description': 'ComparativeExperiment schema.'},\n",
       "   'ComparativeExperimentBase': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'modified_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Modified At'},\n",
       "     'reference_dataset_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Reference Dataset Id'},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'tenant_id',\n",
       "     'created_at',\n",
       "     'modified_at',\n",
       "     'reference_dataset_id'],\n",
       "    'title': 'ComparativeExperimentBase',\n",
       "    'description': 'ComparativeExperiment schema.'},\n",
       "   'ComparativeExperimentCreate': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'experiment_ids': {'items': {'type': 'string', 'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'title': 'Experiment Ids'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'modified_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Modified At'},\n",
       "     'reference_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reference Dataset Id'},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'}},\n",
       "    'type': 'object',\n",
       "    'required': ['experiment_ids'],\n",
       "    'title': 'ComparativeExperimentCreate',\n",
       "    'description': 'Create class for ComparativeExperiment.'},\n",
       "   'ConfiguredBy': {'type': 'string',\n",
       "    'enum': ['system', 'user'],\n",
       "    'title': 'ConfiguredBy'},\n",
       "   'CreateClusteringJobConfigRequest': {'properties': {'name': {'type': 'string',\n",
       "      'maxLength': 255,\n",
       "      'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'config': {'$ref': '#/components/schemas/CreateRunClusteringJobRequest'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'config'],\n",
       "    'title': 'CreateClusteringJobConfigRequest',\n",
       "    'description': 'Request to create a clustering job config.'},\n",
       "   'CreateClusteringJobConfigResponse': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'config': {'$ref': '#/components/schemas/SavedRunClusteringJobRequest'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'name', 'description', 'config'],\n",
       "    'title': 'CreateClusteringJobConfigResponse',\n",
       "    'description': 'Response to create a clustering job config.'},\n",
       "   'CreateCommentRequest': {'properties': {'content': {'type': 'string',\n",
       "      'title': 'Content'}},\n",
       "    'type': 'object',\n",
       "    'required': ['content'],\n",
       "    'title': 'CreateCommentRequest'},\n",
       "   'CreateFeedbackConfigSchema': {'properties': {'feedback_key': {'type': 'string',\n",
       "      'title': 'Feedback Key'},\n",
       "     'feedback_config': {'$ref': '#/components/schemas/FeedbackConfig'},\n",
       "     'is_lower_score_better': {'anyOf': [{'type': 'boolean'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Is Lower Score Better',\n",
       "      'default': False}},\n",
       "    'type': 'object',\n",
       "    'required': ['feedback_key', 'feedback_config'],\n",
       "    'title': 'CreateFeedbackConfigSchema'},\n",
       "   'CreateRepoRequest': {'properties': {'repo_handle': {'type': 'string',\n",
       "      'title': 'Repo Handle'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'readme': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Readme'},\n",
       "     'is_public': {'type': 'boolean', 'title': 'Is Public'},\n",
       "     'tags': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tags'}},\n",
       "    'type': 'object',\n",
       "    'required': ['repo_handle', 'is_public'],\n",
       "    'title': 'CreateRepoRequest',\n",
       "    'description': 'Fields to create a repo'},\n",
       "   'CreateRepoResponse': {'properties': {'repo': {'$ref': '#/components/schemas/RepoWithLookups'}},\n",
       "    'type': 'object',\n",
       "    'required': ['repo'],\n",
       "    'title': 'CreateRepoResponse'},\n",
       "   'CreateRoleRequest': {'properties': {'display_name': {'type': 'string',\n",
       "      'title': 'Display Name'},\n",
       "     'description': {'type': 'string', 'title': 'Description'},\n",
       "     'permissions': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Permissions'}},\n",
       "    'type': 'object',\n",
       "    'required': ['display_name', 'description', 'permissions'],\n",
       "    'title': 'CreateRoleRequest'},\n",
       "   'CreateRunClusteringJobRequest': {'properties': {'start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'last_n_hours': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Last N Hours'},\n",
       "     'hierarchy': {'anyOf': [{'items': {'type': 'integer'}, 'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Hierarchy'},\n",
       "     'partitions': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object',\n",
       "        'maxProperties': 10},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Partitions'},\n",
       "     'sample': {'anyOf': [{'type': 'number'},\n",
       "       {'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Sample'},\n",
       "     'summary_prompt': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Summary Prompt'},\n",
       "     'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Filter'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'attribute_schemas': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Attribute Schemas'},\n",
       "     'user_context': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'User Context'},\n",
       "     'model': {'type': 'string',\n",
       "      'enum': ['openai', 'anthropic'],\n",
       "      'title': 'Model',\n",
       "      'default': 'openai'},\n",
       "     'validate_model_secrets': {'type': 'boolean',\n",
       "      'title': 'Validate Model Secrets',\n",
       "      'default': True}},\n",
       "    'type': 'object',\n",
       "    'title': 'CreateRunClusteringJobRequest',\n",
       "    'description': 'Request to create a run clustering job.'},\n",
       "   'CreateRunClusteringJobResponse': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'status': {'type': 'string', 'title': 'Status'},\n",
       "     'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Error'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'name', 'status'],\n",
       "    'title': 'CreateRunClusteringJobResponse',\n",
       "    'description': 'Response to creating a run clustering job.'},\n",
       "   'CustomChartCreate': {'properties': {'title': {'type': 'string',\n",
       "      'title': 'Title'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'index': {'anyOf': [{'type': 'integer', 'maximum': 100.0, 'minimum': 0.0},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Index'},\n",
       "     'chart_type': {'$ref': '#/components/schemas/CustomChartType'},\n",
       "     'series': {'items': {'$ref': '#/components/schemas/CustomChartSeriesCreate'},\n",
       "      'type': 'array',\n",
       "      'title': 'Series'},\n",
       "     'section_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Section Id'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'common_filters': {'anyOf': [{'$ref': '#/components/schemas/CustomChartSeriesFilters'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'required': ['title', 'chart_type', 'series'],\n",
       "    'title': 'CustomChartCreate'},\n",
       "   'CustomChartCreatePreview': {'properties': {'series': {'items': {'$ref': '#/components/schemas/CustomChartSeries'},\n",
       "      'type': 'array',\n",
       "      'title': 'Series'},\n",
       "     'common_filters': {'anyOf': [{'$ref': '#/components/schemas/CustomChartSeriesFilters'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'required': ['series'],\n",
       "    'title': 'CustomChartCreatePreview'},\n",
       "   'CustomChartMetric': {'type': 'string',\n",
       "    'enum': ['run_count',\n",
       "     'latency_p50',\n",
       "     'latency_p99',\n",
       "     'latency_avg',\n",
       "     'first_token_p50',\n",
       "     'first_token_p99',\n",
       "     'total_tokens',\n",
       "     'prompt_tokens',\n",
       "     'completion_tokens',\n",
       "     'median_tokens',\n",
       "     'completion_tokens_p50',\n",
       "     'prompt_tokens_p50',\n",
       "     'tokens_p99',\n",
       "     'completion_tokens_p99',\n",
       "     'prompt_tokens_p99',\n",
       "     'feedback',\n",
       "     'feedback_score_avg',\n",
       "     'feedback_values',\n",
       "     'total_cost',\n",
       "     'prompt_cost',\n",
       "     'completion_cost',\n",
       "     'error_rate',\n",
       "     'streaming_rate',\n",
       "     'cost_p50',\n",
       "     'cost_p99'],\n",
       "    'title': 'CustomChartMetric',\n",
       "    'description': 'Metrics you can chart. Feedback metrics are not available for organization-scoped charts.'},\n",
       "   'CustomChartPreviewRequest': {'properties': {'bucket_info': {'$ref': '#/components/schemas/CustomChartsRequestBase'},\n",
       "     'chart': {'$ref': '#/components/schemas/CustomChartCreatePreview'}},\n",
       "    'type': 'object',\n",
       "    'required': ['bucket_info', 'chart'],\n",
       "    'title': 'CustomChartPreviewRequest'},\n",
       "   'CustomChartResponse': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'title': {'type': 'string', 'title': 'Title'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'index': {'type': 'integer', 'title': 'Index'},\n",
       "     'chart_type': {'$ref': '#/components/schemas/CustomChartType'},\n",
       "     'section_id': {'type': 'string', 'format': 'uuid', 'title': 'Section Id'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'series': {'anyOf': [{'items': {'$ref': '#/components/schemas/CustomChartSeries'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Series'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'title', 'index', 'chart_type', 'section_id', 'series'],\n",
       "    'title': 'CustomChartResponse'},\n",
       "   'CustomChartSeries': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'filters': {'anyOf': [{'$ref': '#/components/schemas/CustomChartSeriesFilters'},\n",
       "       {'type': 'null'}]},\n",
       "     'metric': {'$ref': '#/components/schemas/CustomChartMetric'},\n",
       "     'feedback_key': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Feedback Key'},\n",
       "     'workspace_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Id'},\n",
       "     'project_metric': {'anyOf': [{'$ref': '#/components/schemas/HostProjectChartMetric'},\n",
       "       {'type': 'null'}]},\n",
       "     'id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'string'}],\n",
       "      'title': 'Id'},\n",
       "     'group_by': {'anyOf': [{'$ref': '#/components/schemas/RunStatsGroupBySeriesResponse'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'metric', 'id'],\n",
       "    'title': 'CustomChartSeries'},\n",
       "   'CustomChartSeriesCreate': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'filters': {'anyOf': [{'$ref': '#/components/schemas/CustomChartSeriesFilters'},\n",
       "       {'type': 'null'}]},\n",
       "     'metric': {'$ref': '#/components/schemas/CustomChartMetric'},\n",
       "     'feedback_key': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Feedback Key'},\n",
       "     'workspace_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Id'},\n",
       "     'project_metric': {'anyOf': [{'$ref': '#/components/schemas/HostProjectChartMetric'},\n",
       "       {'type': 'null'}]},\n",
       "     'group_by': {'anyOf': [{'$ref': '#/components/schemas/RunStatsGroupBy'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'metric'],\n",
       "    'title': 'CustomChartSeriesCreate'},\n",
       "   'CustomChartSeriesFilters': {'properties': {'filter': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Filter'},\n",
       "     'trace_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Trace Filter'},\n",
       "     'tree_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tree Filter'},\n",
       "     'session': {'anyOf': [{'items': {'type': 'string', 'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session'}},\n",
       "    'type': 'object',\n",
       "    'title': 'CustomChartSeriesFilters'},\n",
       "   'CustomChartSeriesUpdate': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'filters': {'anyOf': [{'$ref': '#/components/schemas/CustomChartSeriesFilters'},\n",
       "       {'type': 'null'}]},\n",
       "     'metric': {'$ref': '#/components/schemas/CustomChartMetric'},\n",
       "     'feedback_key': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Feedback Key'},\n",
       "     'workspace_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Id'},\n",
       "     'project_metric': {'anyOf': [{'$ref': '#/components/schemas/HostProjectChartMetric'},\n",
       "       {'type': 'null'}]},\n",
       "     'group_by': {'anyOf': [{'$ref': '#/components/schemas/RunStatsGroupBy'},\n",
       "       {'type': 'null'}]},\n",
       "     'id': {'anyOf': [{'type': 'string', 'format': 'uuid'}, {'type': 'null'}],\n",
       "      'title': 'Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'metric'],\n",
       "    'title': 'CustomChartSeriesUpdate'},\n",
       "   'CustomChartType': {'type': 'string',\n",
       "    'enum': ['line', 'bar'],\n",
       "    'title': 'CustomChartType',\n",
       "    'description': 'Enum for custom chart types.'},\n",
       "   'CustomChartUpdate': {'properties': {'title': {'anyOf': [{'type': 'string'},\n",
       "       {'$ref': '#/components/schemas/Missing'}],\n",
       "      'title': 'Title',\n",
       "      'default': {'__missing__': '__missing__'}},\n",
       "     'description': {'anyOf': [{'type': 'string'},\n",
       "       {'$ref': '#/components/schemas/Missing'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Description',\n",
       "      'default': {'__missing__': '__missing__'}},\n",
       "     'index': {'anyOf': [{'type': 'integer'},\n",
       "       {'$ref': '#/components/schemas/Missing'}],\n",
       "      'title': 'Index',\n",
       "      'default': {'__missing__': '__missing__'},\n",
       "      'ge': 0,\n",
       "      'le': 100},\n",
       "     'chart_type': {'anyOf': [{'$ref': '#/components/schemas/CustomChartType'},\n",
       "       {'$ref': '#/components/schemas/Missing'}],\n",
       "      'title': 'Chart Type',\n",
       "      'default': {'__missing__': '__missing__'}},\n",
       "     'series': {'anyOf': [{'items': {'$ref': '#/components/schemas/CustomChartSeriesUpdate'},\n",
       "        'type': 'array'},\n",
       "       {'$ref': '#/components/schemas/Missing'}],\n",
       "      'title': 'Series',\n",
       "      'default': {'__missing__': '__missing__'}},\n",
       "     'section_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'$ref': '#/components/schemas/Missing'}],\n",
       "      'title': 'Section Id',\n",
       "      'default': {'__missing__': '__missing__'}},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'$ref': '#/components/schemas/Missing'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata',\n",
       "      'default': {'__missing__': '__missing__'}},\n",
       "     'common_filters': {'anyOf': [{'$ref': '#/components/schemas/CustomChartSeriesFilters'},\n",
       "       {'$ref': '#/components/schemas/Missing'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Common Filters',\n",
       "      'default': {'__missing__': '__missing__'}}},\n",
       "    'type': 'object',\n",
       "    'title': 'CustomChartUpdate'},\n",
       "   'CustomChartsDataPoint': {'properties': {'series_id': {'type': 'string',\n",
       "      'title': 'Series Id'},\n",
       "     'timestamp': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Timestamp'},\n",
       "     'value': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'number'},\n",
       "       {'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Value'},\n",
       "     'group': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Group'}},\n",
       "    'type': 'object',\n",
       "    'required': ['series_id', 'timestamp', 'value'],\n",
       "    'title': 'CustomChartsDataPoint'},\n",
       "   'CustomChartsRequest': {'properties': {'timezone': {'type': 'string',\n",
       "      'title': 'Timezone',\n",
       "      'default': 'UTC'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'stride': {'$ref': '#/components/schemas/TimedeltaInput',\n",
       "      'default': {'days': 0, 'hours': 0, 'minutes': 15}},\n",
       "     'omit_data': {'type': 'boolean', 'title': 'Omit Data', 'default': False},\n",
       "     'after_index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'After Index'},\n",
       "     'tag_value_id': {'anyOf': [{'items': {'type': 'string', 'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tag Value Id'}},\n",
       "    'type': 'object',\n",
       "    'title': 'CustomChartsRequest'},\n",
       "   'CustomChartsRequestBase': {'properties': {'timezone': {'type': 'string',\n",
       "      'title': 'Timezone',\n",
       "      'default': 'UTC'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'stride': {'$ref': '#/components/schemas/TimedeltaInput',\n",
       "      'default': {'days': 0, 'hours': 0, 'minutes': 15}},\n",
       "     'omit_data': {'type': 'boolean', 'title': 'Omit Data', 'default': False}},\n",
       "    'type': 'object',\n",
       "    'title': 'CustomChartsRequestBase'},\n",
       "   'CustomChartsResponse': {'properties': {'sections': {'items': {'$ref': '#/components/schemas/CustomChartsSection'},\n",
       "      'type': 'array',\n",
       "      'title': 'Sections'}},\n",
       "    'type': 'object',\n",
       "    'required': ['sections'],\n",
       "    'title': 'CustomChartsResponse'},\n",
       "   'CustomChartsSection': {'properties': {'title': {'type': 'string',\n",
       "      'title': 'Title'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Index'},\n",
       "     'id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'string'}],\n",
       "      'title': 'Id'},\n",
       "     'session_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Id'},\n",
       "     'charts': {'items': {'$ref': '#/components/schemas/SingleCustomChartResponse'},\n",
       "      'type': 'array',\n",
       "      'title': 'Charts'},\n",
       "     'sub_sections': {'anyOf': [{'items': {'$ref': '#/components/schemas/SingleCustomChartSubSectionResponse'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Sub Sections'}},\n",
       "    'type': 'object',\n",
       "    'required': ['title', 'id', 'charts'],\n",
       "    'title': 'CustomChartsSection'},\n",
       "   'CustomChartsSectionCreate': {'properties': {'title': {'type': 'string',\n",
       "      'title': 'Title'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Index'}},\n",
       "    'type': 'object',\n",
       "    'required': ['title'],\n",
       "    'title': 'CustomChartsSectionCreate'},\n",
       "   'CustomChartsSectionRequest': {'properties': {'timezone': {'type': 'string',\n",
       "      'title': 'Timezone',\n",
       "      'default': 'UTC'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'stride': {'$ref': '#/components/schemas/TimedeltaInput',\n",
       "      'default': {'days': 0, 'hours': 0, 'minutes': 15}},\n",
       "     'omit_data': {'type': 'boolean', 'title': 'Omit Data', 'default': False},\n",
       "     'group_by': {'anyOf': [{'$ref': '#/components/schemas/RunStatsGroupBy'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'title': 'CustomChartsSectionRequest'},\n",
       "   'CustomChartsSectionResponse': {'properties': {'title': {'type': 'string',\n",
       "      'title': 'Title'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Index'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'chart_count': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Chart Count'},\n",
       "     'created_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Created At'},\n",
       "     'modified_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Modified At'}},\n",
       "    'type': 'object',\n",
       "    'required': ['title', 'id'],\n",
       "    'title': 'CustomChartsSectionResponse'},\n",
       "   'CustomChartsSectionUpdate': {'properties': {'title': {'anyOf': [{'type': 'string'},\n",
       "       {'$ref': '#/components/schemas/Missing'}],\n",
       "      'title': 'Title',\n",
       "      'default': {'__missing__': '__missing__'}},\n",
       "     'description': {'anyOf': [{'type': 'string'},\n",
       "       {'$ref': '#/components/schemas/Missing'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Description',\n",
       "      'default': {'__missing__': '__missing__'}},\n",
       "     'index': {'anyOf': [{'type': 'integer'},\n",
       "       {'$ref': '#/components/schemas/Missing'}],\n",
       "      'title': 'Index',\n",
       "      'default': {'__missing__': '__missing__'}}},\n",
       "    'type': 'object',\n",
       "    'title': 'CustomChartsSectionUpdate'},\n",
       "   'CustomChartsSectionsCloneRequest': {'properties': {'section_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Section Id'},\n",
       "     'session_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Id'}},\n",
       "    'type': 'object',\n",
       "    'title': 'CustomChartsSectionsCloneRequest'},\n",
       "   'CustomerInfo': {'properties': {'customer_id': {'type': 'string',\n",
       "      'title': 'Customer Id'},\n",
       "     'customer_name': {'type': 'string', 'title': 'Customer Name'}},\n",
       "    'type': 'object',\n",
       "    'required': ['customer_id', 'customer_name'],\n",
       "    'title': 'CustomerInfo',\n",
       "    'description': 'Customer info.'},\n",
       "   'CustomerVisiblePlanInfo': {'properties': {'tier': {'$ref': '#/components/schemas/PaymentPlanTier'},\n",
       "     'started_on': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Started On'},\n",
       "     'ends_on': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Ends On'}},\n",
       "    'type': 'object',\n",
       "    'required': ['tier', 'started_on'],\n",
       "    'title': 'CustomerVisiblePlanInfo',\n",
       "    'description': 'Customer visible plan information.'},\n",
       "   'DataType': {'type': 'string',\n",
       "    'enum': ['kv', 'llm', 'chat'],\n",
       "    'title': 'DataType',\n",
       "    'description': 'Enum for dataset data types.'},\n",
       "   'Dataset': {'properties': {'name': {'type': 'string', 'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'inputs_schema_definition': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs Schema Definition'},\n",
       "     'outputs_schema_definition': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs Schema Definition'},\n",
       "     'externally_managed': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Externally Managed',\n",
       "      'default': False},\n",
       "     'transformations': {'anyOf': [{'items': {'$ref': '#/components/schemas/DatasetTransformation'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Transformations'},\n",
       "     'data_type': {'anyOf': [{'$ref': '#/components/schemas/DataType'},\n",
       "       {'type': 'null'}],\n",
       "      'default': 'kv'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'example_count': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Example Count'},\n",
       "     'session_count': {'type': 'integer', 'title': 'Session Count'},\n",
       "     'modified_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Modified At'},\n",
       "     'last_session_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Session Start Time'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'id', 'tenant_id', 'session_count', 'modified_at'],\n",
       "    'title': 'Dataset',\n",
       "    'description': 'Dataset schema.'},\n",
       "   'DatasetCreate': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'inputs_schema_definition': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs Schema Definition'},\n",
       "     'outputs_schema_definition': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs Schema Definition'},\n",
       "     'externally_managed': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Externally Managed',\n",
       "      'default': False},\n",
       "     'transformations': {'anyOf': [{'items': {'$ref': '#/components/schemas/DatasetTransformation'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Transformations'},\n",
       "     'id': {'anyOf': [{'type': 'string', 'format': 'uuid'}, {'type': 'null'}],\n",
       "      'title': 'Id'},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'},\n",
       "     'data_type': {'$ref': '#/components/schemas/DataType', 'default': 'kv'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name'],\n",
       "    'title': 'DatasetCreate',\n",
       "    'description': 'Create class for Dataset.'},\n",
       "   'DatasetDiffInfo': {'properties': {'examples_modified': {'items': {'type': 'string',\n",
       "       'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'title': 'Examples Modified'},\n",
       "     'examples_added': {'items': {'type': 'string', 'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'title': 'Examples Added'},\n",
       "     'examples_removed': {'items': {'type': 'string', 'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'title': 'Examples Removed'}},\n",
       "    'type': 'object',\n",
       "    'required': ['examples_modified', 'examples_added', 'examples_removed'],\n",
       "    'title': 'DatasetDiffInfo',\n",
       "    'description': 'Dataset diff schema.'},\n",
       "   'DatasetIndexInfo': {'properties': {'dataset_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Dataset Id'},\n",
       "     'tag': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tag',\n",
       "      'default': 'latest'},\n",
       "     'last_updated_version': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Updated Version'}},\n",
       "    'type': 'object',\n",
       "    'required': ['dataset_id'],\n",
       "    'title': 'DatasetIndexInfo',\n",
       "    'description': 'Dataset schema for serving.'},\n",
       "   'DatasetIndexRequest': {'properties': {'tag': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tag',\n",
       "      'default': 'latest'}},\n",
       "    'type': 'object',\n",
       "    'title': 'DatasetIndexRequest',\n",
       "    'description': 'Dataset schema for serving.'},\n",
       "   'DatasetPublicSchema': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'inputs_schema_definition': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs Schema Definition'},\n",
       "     'outputs_schema_definition': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs Schema Definition'},\n",
       "     'externally_managed': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Externally Managed',\n",
       "      'default': False},\n",
       "     'transformations': {'anyOf': [{'items': {'$ref': '#/components/schemas/DatasetTransformation'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Transformations'},\n",
       "     'data_type': {'anyOf': [{'$ref': '#/components/schemas/DataType'},\n",
       "       {'type': 'null'}],\n",
       "      'default': 'kv'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'example_count': {'type': 'integer', 'title': 'Example Count'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'id', 'example_count'],\n",
       "    'title': 'DatasetPublicSchema',\n",
       "    'description': \"Public schema for datasets.\\n\\nDoesn't currently include session counts/stats\\nsince public test project sharing is not yet shipped\"},\n",
       "   'DatasetSchemaForUpdate': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'inputs_schema_definition': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs Schema Definition'},\n",
       "     'outputs_schema_definition': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs Schema Definition'},\n",
       "     'externally_managed': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Externally Managed',\n",
       "      'default': False},\n",
       "     'transformations': {'anyOf': [{'items': {'$ref': '#/components/schemas/DatasetTransformation'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Transformations'},\n",
       "     'data_type': {'anyOf': [{'$ref': '#/components/schemas/DataType'},\n",
       "       {'type': 'null'}],\n",
       "      'default': 'kv'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'id', 'tenant_id'],\n",
       "    'title': 'DatasetSchemaForUpdate'},\n",
       "   'DatasetShareSchema': {'properties': {'dataset_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Dataset Id'},\n",
       "     'share_token': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Share Token'}},\n",
       "    'type': 'object',\n",
       "    'required': ['dataset_id', 'share_token'],\n",
       "    'title': 'DatasetShareSchema'},\n",
       "   'DatasetTransformation': {'properties': {'path': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Path'},\n",
       "     'transformation_type': {'$ref': '#/components/schemas/DatasetTransformationType'}},\n",
       "    'type': 'object',\n",
       "    'required': ['path', 'transformation_type'],\n",
       "    'title': 'DatasetTransformation'},\n",
       "   'DatasetTransformationType': {'type': 'string',\n",
       "    'enum': ['convert_to_openai_message',\n",
       "     'convert_to_openai_tool',\n",
       "     'remove_system_messages',\n",
       "     'remove_extra_fields',\n",
       "     'extract_tools_from_run'],\n",
       "    'title': 'DatasetTransformationType',\n",
       "    'description': 'Enum for dataset transformation types.\\nOrdering determines the order in which transformations are applied if there are multiple transformations on the same path.'},\n",
       "   'DatasetUpdate': {'properties': {'name': {'anyOf': [{'type': 'string'},\n",
       "       {'$ref': '#/components/schemas/Missing'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Name',\n",
       "      'default': {'__missing__': '__missing__'}},\n",
       "     'description': {'anyOf': [{'type': 'string'},\n",
       "       {'$ref': '#/components/schemas/Missing'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Description',\n",
       "      'default': {'__missing__': '__missing__'}},\n",
       "     'inputs_schema_definition': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'$ref': '#/components/schemas/Missing'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs Schema Definition',\n",
       "      'default': {'__missing__': '__missing__'}},\n",
       "     'outputs_schema_definition': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'$ref': '#/components/schemas/Missing'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs Schema Definition',\n",
       "      'default': {'__missing__': '__missing__'}},\n",
       "     'patch_examples': {'anyOf': [{'additionalProperties': {'$ref': '#/components/schemas/ExampleUpdate'},\n",
       "        'propertyNames': {'format': 'uuid'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Patch Examples'},\n",
       "     'transformations': {'anyOf': [{'items': {'$ref': '#/components/schemas/DatasetTransformation'},\n",
       "        'type': 'array'},\n",
       "       {'$ref': '#/components/schemas/Missing'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Transformations',\n",
       "      'default': {'__missing__': '__missing__'}},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'$ref': '#/components/schemas/Missing'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata',\n",
       "      'default': {'__missing__': '__missing__'}}},\n",
       "    'type': 'object',\n",
       "    'title': 'DatasetUpdate',\n",
       "    'description': 'Update class for Dataset.'},\n",
       "   'DatasetVersion': {'properties': {'tags': {'anyOf': [{'items': {'type': 'string'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tags'},\n",
       "     'as_of': {'type': 'string', 'format': 'date-time', 'title': 'As Of'}},\n",
       "    'type': 'object',\n",
       "    'required': ['as_of'],\n",
       "    'title': 'DatasetVersion',\n",
       "    'description': 'Dataset version schema.'},\n",
       "   'DeleteClusteringJobConfigResponse': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'message': {'type': 'string', 'title': 'Message'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'message'],\n",
       "    'title': 'DeleteClusteringJobConfigResponse',\n",
       "    'description': 'Response to delete a clustering job config.'},\n",
       "   'DeleteRunClusteringJobResponse': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'message': {'type': 'string', 'title': 'Message'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'message'],\n",
       "    'title': 'DeleteRunClusteringJobResponse',\n",
       "    'description': 'Response to delete a session cluster job.'},\n",
       "   'DemoConfig': {'properties': {'message_index': {'type': 'integer',\n",
       "      'title': 'Message Index'},\n",
       "     'metaprompt': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Metaprompt'},\n",
       "     'examples': {'items': {'additionalProperties': True, 'type': 'object'},\n",
       "      'type': 'array',\n",
       "      'title': 'Examples'},\n",
       "     'overall_feedback': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Overall Feedback'}},\n",
       "    'type': 'object',\n",
       "    'required': ['message_index',\n",
       "     'metaprompt',\n",
       "     'examples',\n",
       "     'overall_feedback'],\n",
       "    'title': 'DemoConfig'},\n",
       "   'EPromptOptimizationAlgorithm': {'type': 'string',\n",
       "    'enum': ['promptim', 'demo'],\n",
       "    'title': 'EPromptOptimizationAlgorithm'},\n",
       "   'EPromptOptimizationJobLogType': {'type': 'string',\n",
       "    'enum': ['info', 'result', 'error', 'link'],\n",
       "    'title': 'EPromptOptimizationJobLogType'},\n",
       "   'EPromptOptimizationJobStatus': {'type': 'string',\n",
       "    'enum': ['created', 'running', 'successful', 'failed'],\n",
       "    'title': 'EPromptOptimizationJobStatus'},\n",
       "   'EPromptWebhookTrigger': {'type': 'string',\n",
       "    'enum': ['commit', 'tag:create', 'tag:update'],\n",
       "    'title': 'EPromptWebhookTrigger',\n",
       "    'description': 'Valid trigger types for prompt webhooks.'},\n",
       "   'EvaluatorStructuredOutput': {'properties': {'hub_ref': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Hub Ref'},\n",
       "     'prompt': {'anyOf': [{'items': {'prefixItems': [{'type': 'string'},\n",
       "          {'type': 'string'}],\n",
       "         'type': 'array',\n",
       "         'maxItems': 2,\n",
       "         'minItems': 2},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Prompt'},\n",
       "     'template_format': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Template Format'},\n",
       "     'schema': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Schema'},\n",
       "     'variable_mapping': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Variable Mapping'},\n",
       "     'model': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Model'}},\n",
       "    'type': 'object',\n",
       "    'title': 'EvaluatorStructuredOutput',\n",
       "    'description': 'Evaluator structured output schema.'},\n",
       "   'EvaluatorTopLevel': {'properties': {'structured': {'$ref': '#/components/schemas/EvaluatorStructuredOutput'}},\n",
       "    'type': 'object',\n",
       "    'required': ['structured'],\n",
       "    'title': 'EvaluatorTopLevel'},\n",
       "   'Example': {'properties': {'outputs': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs'},\n",
       "     'dataset_id': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'},\n",
       "     'source_run_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Source Run Id'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'inputs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Inputs'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'modified_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Modified At'},\n",
       "     'attachment_urls': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Attachment Urls'}},\n",
       "    'type': 'object',\n",
       "    'required': ['dataset_id', 'inputs', 'id', 'name'],\n",
       "    'title': 'Example',\n",
       "    'description': 'Example schema.'},\n",
       "   'ExampleGroupWithSessions': {'properties': {'filter': {'type': 'string',\n",
       "      'title': 'Filter'},\n",
       "     'count': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Count'},\n",
       "     'total_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Total Tokens'},\n",
       "     'total_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Total Cost'},\n",
       "     'min_start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Min Start Time'},\n",
       "     'max_start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Max Start Time'},\n",
       "     'latency_p50': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Latency P50'},\n",
       "     'latency_p99': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Latency P99'},\n",
       "     'feedback_stats': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Stats'},\n",
       "     'group_key': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'integer'},\n",
       "       {'type': 'number'}],\n",
       "      'title': 'Group Key'},\n",
       "     'sessions': {'items': {'$ref': '#/components/schemas/GroupedRunsSessionStats'},\n",
       "      'type': 'array',\n",
       "      'title': 'Sessions'},\n",
       "     'examples': {'items': {'$ref': '#/components/schemas/ExampleWithRunsCH'},\n",
       "      'type': 'array',\n",
       "      'title': 'Examples'},\n",
       "     'example_count': {'type': 'integer', 'title': 'Example Count'},\n",
       "     'prompt_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Tokens'},\n",
       "     'completion_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Completion Tokens'},\n",
       "     'prompt_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Cost'},\n",
       "     'completion_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Completion Cost'},\n",
       "     'error_rate': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Error Rate'}},\n",
       "    'type': 'object',\n",
       "    'required': ['filter',\n",
       "     'group_key',\n",
       "     'sessions',\n",
       "     'examples',\n",
       "     'example_count'],\n",
       "    'title': 'ExampleGroupWithSessions',\n",
       "    'description': 'Group of examples with a specific metadata value across multiple sessions.\\n\\nExtends RunGroupBase with:\\n- group_key: metadata value that defines this group\\n- sessions: per-session stats for runs matching this metadata value\\n- examples: shared examples across all sessions (intersection logic)\\n            with flat array of runs (each run has session_id field for frontend to determine column)\\n- example_count: unique example count (pagination-aware, same across all sessions due to intersection)\\n\\nInherited from RunGroupBase:\\n- filter: metadata filter for this group (e.g., \"and(eq(is_root, true), and(eq(metadata_key, \\'model\\'), eq(metadata_value, \\'gpt-4\\')))\")\\n- count: total run count across all sessions (includes duplicate runs)\\n- total_tokens, total_cost: aggregate across sessions\\n- min_start_time, max_start_time: time range across sessions\\n- latency_p50, latency_p99: aggregate latency stats across sessions\\n- feedback_stats: weighted average feedback across sessions\\n\\nAdditional aggregate stats (from ExampleWithRunsGroup):\\n- prompt_tokens, completion_tokens: separate token counts\\n- prompt_cost, completion_cost: separate costs\\n- error_rate: average error rate'},\n",
       "   'ExampleListOrder': {'type': 'string',\n",
       "    'enum': ['recent', 'random', 'recently_created', 'id'],\n",
       "    'title': 'ExampleListOrder'},\n",
       "   'ExampleSelect': {'type': 'string',\n",
       "    'enum': ['id',\n",
       "     'created_at',\n",
       "     'modified_at',\n",
       "     'name',\n",
       "     'dataset_id',\n",
       "     'source_run_id',\n",
       "     'metadata',\n",
       "     'inputs',\n",
       "     'outputs',\n",
       "     'attachment_urls'],\n",
       "    'title': 'ExampleSelect'},\n",
       "   'ExampleUpdate': {'properties': {'dataset_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Dataset Id'},\n",
       "     'inputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs'},\n",
       "     'outputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs'},\n",
       "     'attachments_operations': {'anyOf': [{'$ref': '#/components/schemas/AttachmentsOperations'},\n",
       "       {'type': 'null'}]},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'split': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "       {'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Split'},\n",
       "     'overwrite': {'type': 'boolean', 'title': 'Overwrite', 'default': False}},\n",
       "    'type': 'object',\n",
       "    'title': 'ExampleUpdate',\n",
       "    'description': 'Update class for Example.'},\n",
       "   'ExampleUpdateWithID': {'properties': {'dataset_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Dataset Id'},\n",
       "     'inputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs'},\n",
       "     'outputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs'},\n",
       "     'attachments_operations': {'anyOf': [{'$ref': '#/components/schemas/AttachmentsOperations'},\n",
       "       {'type': 'null'}]},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'split': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "       {'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Split'},\n",
       "     'overwrite': {'type': 'boolean', 'title': 'Overwrite', 'default': False},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id'],\n",
       "    'title': 'ExampleUpdateWithID',\n",
       "    'description': 'Bulk update class for Example (includes example id).'},\n",
       "   'ExampleValidationResult': {'properties': {'dataset_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Dataset Id'},\n",
       "     'inputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs'},\n",
       "     'outputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs'},\n",
       "     'created_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Created At'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'source_run_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Source Run Id'},\n",
       "     'split': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "       {'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Split',\n",
       "      'default': 'base'},\n",
       "     'id': {'anyOf': [{'type': 'string', 'format': 'uuid'}, {'type': 'null'}],\n",
       "      'title': 'Id'},\n",
       "     'use_source_run_io': {'type': 'boolean',\n",
       "      'title': 'Use Source Run Io',\n",
       "      'default': False},\n",
       "     'overwrite': {'type': 'boolean', 'title': 'Overwrite', 'default': False}},\n",
       "    'type': 'object',\n",
       "    'title': 'ExampleValidationResult',\n",
       "    'description': 'Validation result for Example, combining fields from Create/Base/Update schemas.'},\n",
       "   'ExampleWithRuns': {'properties': {'outputs': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs'},\n",
       "     'dataset_id': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'},\n",
       "     'source_run_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Source Run Id'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'inputs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Inputs'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'modified_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Modified At'},\n",
       "     'attachment_urls': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Attachment Urls'},\n",
       "     'runs': {'items': {'$ref': '#/components/schemas/RunSchema'},\n",
       "      'type': 'array',\n",
       "      'title': 'Runs'}},\n",
       "    'type': 'object',\n",
       "    'required': ['dataset_id', 'inputs', 'id', 'name', 'runs'],\n",
       "    'title': 'ExampleWithRuns',\n",
       "    'description': 'Example schema with list of runs.'},\n",
       "   'ExampleWithRunsCH': {'properties': {'outputs': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs'},\n",
       "     'dataset_id': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'},\n",
       "     'source_run_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Source Run Id'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'inputs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Inputs'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'modified_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Modified At'},\n",
       "     'attachment_urls': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Attachment Urls'},\n",
       "     'runs': {'items': {'$ref': '#/components/schemas/RunSchemaComparisonView'},\n",
       "      'type': 'array',\n",
       "      'title': 'Runs'}},\n",
       "    'type': 'object',\n",
       "    'required': ['dataset_id', 'inputs', 'id', 'name', 'runs'],\n",
       "    'title': 'ExampleWithRunsCH',\n",
       "    'description': 'Example schema with list of runs from ClickHouse.\\n\\nFor non-grouped endpoint (/datasets/{dataset_id}/runs): runs from single session.\\nFor grouped endpoint (/datasets/{dataset_id}/group/runs): flat array of runs from\\nall sessions, where each run has a session_id field for frontend to determine column placement.'},\n",
       "   'ExperimentResultRow': {'properties': {'row_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Row Id'},\n",
       "     'inputs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Inputs'},\n",
       "     'expected_outputs': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Expected Outputs'},\n",
       "     'actual_outputs': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Actual Outputs'},\n",
       "     'evaluation_scores': {'anyOf': [{'items': {'$ref': '#/components/schemas/FeedbackCreateCoreSchema'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Evaluation Scores'},\n",
       "     'start_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'End Time'},\n",
       "     'run_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Run Name'},\n",
       "     'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Error'},\n",
       "     'run_metadata': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Run Metadata'}},\n",
       "    'type': 'object',\n",
       "    'required': ['inputs', 'start_time', 'end_time'],\n",
       "    'title': 'ExperimentResultRow',\n",
       "    'description': 'Class for a single row in the uploaded experiment results.'},\n",
       "   'ExperimentResultsUpload': {'properties': {'experiment_name': {'type': 'string',\n",
       "      'title': 'Experiment Name'},\n",
       "     'experiment_description': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Experiment Description'},\n",
       "     'dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Dataset Id'},\n",
       "     'dataset_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Dataset Name'},\n",
       "     'dataset_description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Dataset Description'},\n",
       "     'summary_experiment_scores': {'anyOf': [{'items': {'$ref': '#/components/schemas/FeedbackCreateCoreSchema'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Summary Experiment Scores'},\n",
       "     'results': {'items': {'$ref': '#/components/schemas/ExperimentResultRow'},\n",
       "      'type': 'array',\n",
       "      'title': 'Results'},\n",
       "     'experiment_start_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Experiment Start Time'},\n",
       "     'experiment_end_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Experiment End Time'},\n",
       "     'experiment_metadata': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Experiment Metadata'}},\n",
       "    'type': 'object',\n",
       "    'required': ['experiment_name',\n",
       "     'results',\n",
       "     'experiment_start_time',\n",
       "     'experiment_end_time'],\n",
       "    'title': 'ExperimentResultsUpload',\n",
       "    'description': 'Class for uploading the results of an already-run experiment.'},\n",
       "   'ExperimentResultsUploadResult': {'properties': {'dataset': {'$ref': '#/components/schemas/Dataset'},\n",
       "     'experiment': {'$ref': '#/components/schemas/TracerSession'}},\n",
       "    'type': 'object',\n",
       "    'required': ['dataset', 'experiment'],\n",
       "    'title': 'ExperimentResultsUploadResult',\n",
       "    'description': 'Class for uploading the results of an already-run experiment.'},\n",
       "   'ExportAnnotationQueueRunsRequest': {'properties': {'start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'}},\n",
       "    'type': 'object',\n",
       "    'title': 'ExportAnnotationQueueRunsRequest',\n",
       "    'description': 'Export annotation queue runs request schema.'},\n",
       "   'FeedbackCategory': {'properties': {'value': {'type': 'number',\n",
       "      'title': 'Value'},\n",
       "     'label': {'anyOf': [{'type': 'string', 'minLength': 1}, {'type': 'null'}],\n",
       "      'title': 'Label'}},\n",
       "    'type': 'object',\n",
       "    'required': ['value'],\n",
       "    'title': 'FeedbackCategory',\n",
       "    'description': 'Specific value and label pair for feedback'},\n",
       "   'FeedbackConfig': {'properties': {'type': {'$ref': '#/components/schemas/FeedbackType'},\n",
       "     'min': {'anyOf': [{'type': 'number'}, {'type': 'null'}], 'title': 'Min'},\n",
       "     'max': {'anyOf': [{'type': 'number'}, {'type': 'null'}], 'title': 'Max'},\n",
       "     'categories': {'anyOf': [{'items': {'$ref': '#/components/schemas/FeedbackCategory'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Categories'}},\n",
       "    'type': 'object',\n",
       "    'required': ['type'],\n",
       "    'title': 'FeedbackConfig'},\n",
       "   'FeedbackConfigSchema': {'properties': {'feedback_key': {'type': 'string',\n",
       "      'title': 'Feedback Key'},\n",
       "     'feedback_config': {'$ref': '#/components/schemas/FeedbackConfig'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'modified_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Modified At'},\n",
       "     'is_lower_score_better': {'anyOf': [{'type': 'boolean'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Is Lower Score Better'}},\n",
       "    'type': 'object',\n",
       "    'required': ['feedback_key',\n",
       "     'feedback_config',\n",
       "     'tenant_id',\n",
       "     'modified_at'],\n",
       "    'title': 'FeedbackConfigSchema'},\n",
       "   'FeedbackCreateCoreSchema': {'properties': {'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'modified_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Modified At'},\n",
       "     'key': {'type': 'string', 'maxLength': 180, 'title': 'Key'},\n",
       "     'score': {'anyOf': [{'type': 'number'},\n",
       "       {'type': 'integer'},\n",
       "       {'type': 'boolean'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Score'},\n",
       "     'value': {'anyOf': [{'type': 'number'},\n",
       "       {'type': 'integer'},\n",
       "       {'type': 'boolean'},\n",
       "       {'type': 'string'},\n",
       "       {'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Value'},\n",
       "     'comment': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Comment'},\n",
       "     'correction': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Correction'},\n",
       "     'feedback_group_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Group Id'},\n",
       "     'comparative_experiment_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Comparative Experiment Id'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'feedback_source': {'anyOf': [{'oneOf': [{'$ref': '#/components/schemas/AppFeedbackSource'},\n",
       "         {'$ref': '#/components/schemas/APIFeedbackSource'},\n",
       "         {'$ref': '#/components/schemas/ModelFeedbackSource'},\n",
       "         {'$ref': '#/components/schemas/AutoEvalFeedbackSource'}],\n",
       "        'discriminator': {'propertyName': 'type',\n",
       "         'mapping': {'api': '#/components/schemas/APIFeedbackSource',\n",
       "          'app': '#/components/schemas/AppFeedbackSource',\n",
       "          'auto_eval': '#/components/schemas/AutoEvalFeedbackSource',\n",
       "          'model': '#/components/schemas/ModelFeedbackSource'}}},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Source'},\n",
       "     'feedback_config': {'anyOf': [{'$ref': '#/components/schemas/FeedbackConfig'},\n",
       "       {'type': 'null'}]},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'}},\n",
       "    'type': 'object',\n",
       "    'required': ['key'],\n",
       "    'title': 'FeedbackCreateCoreSchema',\n",
       "    'description': 'Schema used for creating feedback without run id or session id.'},\n",
       "   'FeedbackCreateSchema': {'properties': {'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'modified_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Modified At'},\n",
       "     'key': {'type': 'string', 'maxLength': 180, 'title': 'Key'},\n",
       "     'score': {'anyOf': [{'type': 'number'},\n",
       "       {'type': 'integer'},\n",
       "       {'type': 'boolean'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Score'},\n",
       "     'value': {'anyOf': [{'type': 'number'},\n",
       "       {'type': 'integer'},\n",
       "       {'type': 'boolean'},\n",
       "       {'type': 'string'},\n",
       "       {'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Value'},\n",
       "     'comment': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Comment'},\n",
       "     'correction': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Correction'},\n",
       "     'feedback_group_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Group Id'},\n",
       "     'comparative_experiment_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Comparative Experiment Id'},\n",
       "     'run_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Run Id'},\n",
       "     'session_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Id'},\n",
       "     'trace_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace Id'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'feedback_source': {'anyOf': [{'oneOf': [{'$ref': '#/components/schemas/AppFeedbackSource'},\n",
       "         {'$ref': '#/components/schemas/APIFeedbackSource'},\n",
       "         {'$ref': '#/components/schemas/ModelFeedbackSource'},\n",
       "         {'$ref': '#/components/schemas/AutoEvalFeedbackSource'}],\n",
       "        'discriminator': {'propertyName': 'type',\n",
       "         'mapping': {'api': '#/components/schemas/APIFeedbackSource',\n",
       "          'app': '#/components/schemas/AppFeedbackSource',\n",
       "          'auto_eval': '#/components/schemas/AutoEvalFeedbackSource',\n",
       "          'model': '#/components/schemas/ModelFeedbackSource'}}},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Source'},\n",
       "     'feedback_config': {'anyOf': [{'$ref': '#/components/schemas/FeedbackConfig'},\n",
       "       {'type': 'null'}]},\n",
       "     'error': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Error'}},\n",
       "    'type': 'object',\n",
       "    'required': ['key'],\n",
       "    'title': 'FeedbackCreateSchema',\n",
       "    'description': 'Schema used for creating feedback.'},\n",
       "   'FeedbackCreateWithTokenExtendedSchema': {'properties': {'score': {'anyOf': [{'type': 'number'},\n",
       "       {'type': 'integer'},\n",
       "       {'type': 'boolean'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Score'},\n",
       "     'value': {'anyOf': [{'type': 'number'},\n",
       "       {'type': 'integer'},\n",
       "       {'type': 'boolean'},\n",
       "       {'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Value'},\n",
       "     'comment': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Comment'},\n",
       "     'correction': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Correction'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'}},\n",
       "    'type': 'object',\n",
       "    'title': 'FeedbackCreateWithTokenExtendedSchema',\n",
       "    'description': 'Feedback create schema with token.'},\n",
       "   'FeedbackDelta': {'properties': {'improved_examples': {'items': {'type': 'string',\n",
       "       'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'title': 'Improved Examples'},\n",
       "     'regressed_examples': {'items': {'type': 'string', 'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'title': 'Regressed Examples'}},\n",
       "    'type': 'object',\n",
       "    'required': ['improved_examples', 'regressed_examples'],\n",
       "    'title': 'FeedbackDelta',\n",
       "    'description': 'Feedback key with number of improvements and regressions.'},\n",
       "   'FeedbackFormula': {'properties': {'dataset_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Dataset Id'},\n",
       "     'session_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Id'},\n",
       "     'feedback_key': {'type': 'string', 'title': 'Feedback Key'},\n",
       "     'aggregation_type': {'type': 'string',\n",
       "      'enum': ['sum', 'avg'],\n",
       "      'title': 'Aggregation Type'},\n",
       "     'formula_parts': {'items': {'$ref': '#/components/schemas/FeedbackFormulaWeightedVariable'},\n",
       "      'type': 'array',\n",
       "      'maxItems': 50,\n",
       "      'minItems': 1,\n",
       "      'title': 'Formula Parts'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'modified_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Modified At'}},\n",
       "    'type': 'object',\n",
       "    'required': ['feedback_key',\n",
       "     'aggregation_type',\n",
       "     'formula_parts',\n",
       "     'id',\n",
       "     'created_at',\n",
       "     'modified_at'],\n",
       "    'title': 'FeedbackFormula'},\n",
       "   'FeedbackFormulaCreate': {'properties': {'dataset_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Dataset Id'},\n",
       "     'session_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Id'},\n",
       "     'feedback_key': {'type': 'string', 'title': 'Feedback Key'},\n",
       "     'aggregation_type': {'type': 'string',\n",
       "      'enum': ['sum', 'avg'],\n",
       "      'title': 'Aggregation Type'},\n",
       "     'formula_parts': {'items': {'$ref': '#/components/schemas/FeedbackFormulaWeightedVariable'},\n",
       "      'type': 'array',\n",
       "      'maxItems': 50,\n",
       "      'minItems': 1,\n",
       "      'title': 'Formula Parts'}},\n",
       "    'type': 'object',\n",
       "    'required': ['feedback_key', 'aggregation_type', 'formula_parts'],\n",
       "    'title': 'FeedbackFormulaCreate'},\n",
       "   'FeedbackFormulaUpdate': {'properties': {'feedback_key': {'type': 'string',\n",
       "      'title': 'Feedback Key'},\n",
       "     'aggregation_type': {'type': 'string',\n",
       "      'enum': ['sum', 'avg'],\n",
       "      'title': 'Aggregation Type'},\n",
       "     'formula_parts': {'items': {'$ref': '#/components/schemas/FeedbackFormulaWeightedVariable'},\n",
       "      'type': 'array',\n",
       "      'maxItems': 50,\n",
       "      'minItems': 1,\n",
       "      'title': 'Formula Parts'}},\n",
       "    'type': 'object',\n",
       "    'required': ['feedback_key', 'aggregation_type', 'formula_parts'],\n",
       "    'title': 'FeedbackFormulaUpdate'},\n",
       "   'FeedbackFormulaWeightedVariable': {'properties': {'part_type': {'type': 'string',\n",
       "      'const': 'weighted_key',\n",
       "      'title': 'Part Type'},\n",
       "     'weight': {'type': 'number', 'title': 'Weight'},\n",
       "     'key': {'type': 'string', 'minLength': 1, 'title': 'Key'}},\n",
       "    'type': 'object',\n",
       "    'required': ['part_type', 'weight', 'key'],\n",
       "    'title': 'FeedbackFormulaWeightedVariable'},\n",
       "   'FeedbackIngestTokenCreateSchema': {'properties': {'expires_in': {'anyOf': [{'$ref': '#/components/schemas/TimedeltaInput'},\n",
       "       {'type': 'null'}]},\n",
       "     'expires_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Expires At'},\n",
       "     'run_id': {'type': 'string', 'format': 'uuid', 'title': 'Run Id'},\n",
       "     'feedback_key': {'type': 'string', 'title': 'Feedback Key'},\n",
       "     'feedback_config': {'anyOf': [{'$ref': '#/components/schemas/FeedbackConfig'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'required': ['run_id', 'feedback_key'],\n",
       "    'title': 'FeedbackIngestTokenCreateSchema',\n",
       "    'description': 'Feedback ingest token create schema.'},\n",
       "   'FeedbackIngestTokenSchema': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'url': {'type': 'string', 'title': 'Url'},\n",
       "     'expires_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Expires At'},\n",
       "     'feedback_key': {'type': 'string', 'title': 'Feedback Key'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'url', 'expires_at', 'feedback_key'],\n",
       "    'title': 'FeedbackIngestTokenSchema',\n",
       "    'description': 'Feedback ingest token schema.'},\n",
       "   'FeedbackLevel': {'type': 'string',\n",
       "    'enum': ['run', 'session'],\n",
       "    'title': 'FeedbackLevel',\n",
       "    'description': 'Enum for feedback levels.'},\n",
       "   'FeedbackSchema': {'properties': {'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'modified_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Modified At'},\n",
       "     'key': {'type': 'string', 'title': 'Key'},\n",
       "     'score': {'anyOf': [{'type': 'number'},\n",
       "       {'type': 'integer'},\n",
       "       {'type': 'boolean'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Score'},\n",
       "     'value': {'anyOf': [{'type': 'number'},\n",
       "       {'type': 'integer'},\n",
       "       {'type': 'boolean'},\n",
       "       {'type': 'string'},\n",
       "       {'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Value'},\n",
       "     'comment': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Comment'},\n",
       "     'correction': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Correction'},\n",
       "     'feedback_group_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Group Id'},\n",
       "     'comparative_experiment_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Comparative Experiment Id'},\n",
       "     'run_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Run Id'},\n",
       "     'session_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Id'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'trace_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace Id'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'feedback_source': {'anyOf': [{'$ref': '#/components/schemas/FeedbackSource'},\n",
       "       {'type': 'null'}]},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'},\n",
       "     'feedback_thread_id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Feedback Thread Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['key', 'id'],\n",
       "    'title': 'FeedbackSchema',\n",
       "    'description': 'Schema for getting feedback.'},\n",
       "   'FeedbackSource': {'properties': {'type': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Type'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'user_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'User Id'},\n",
       "     'ls_user_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Ls User Id'},\n",
       "     'user_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'User Name'}},\n",
       "    'type': 'object',\n",
       "    'title': 'FeedbackSource',\n",
       "    'description': 'The feedback source loaded from the database.'},\n",
       "   'FeedbackType': {'type': 'string',\n",
       "    'enum': ['continuous', 'categorical', 'freeform'],\n",
       "    'title': 'FeedbackType',\n",
       "    'description': 'Enum for feedback types.'},\n",
       "   'FeedbackUpdateSchema': {'properties': {'score': {'anyOf': [{'type': 'number'},\n",
       "       {'type': 'integer'},\n",
       "       {'type': 'boolean'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Score'},\n",
       "     'value': {'anyOf': [{'type': 'number'},\n",
       "       {'type': 'integer'},\n",
       "       {'type': 'boolean'},\n",
       "       {'type': 'string'},\n",
       "       {'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Value'},\n",
       "     'comment': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Comment'},\n",
       "     'correction': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Correction'},\n",
       "     'feedback_config': {'anyOf': [{'$ref': '#/components/schemas/FeedbackConfig'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'title': 'FeedbackUpdateSchema',\n",
       "    'description': 'Schema used for updating feedback'},\n",
       "   'FetchClusteringJobRunsResult': {'properties': {'runs': {'items': {'additionalProperties': True,\n",
       "       'type': 'object'},\n",
       "      'type': 'array',\n",
       "      'title': 'Runs'},\n",
       "     'offset': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Offset'}},\n",
       "    'type': 'object',\n",
       "    'required': ['runs', 'offset'],\n",
       "    'title': 'FetchClusteringJobRunsResult'},\n",
       "   'FilterView': {'properties': {'filter_string': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Filter String'},\n",
       "     'trace_filter_string': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Trace Filter String'},\n",
       "     'tree_filter_string': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tree Filter String'},\n",
       "     'display_name': {'type': 'string', 'title': 'Display Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'type': {'$ref': '#/components/schemas/FilterViewType',\n",
       "      'default': 'runs'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'session_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'}},\n",
       "    'type': 'object',\n",
       "    'required': ['display_name', 'id', 'created_at', 'updated_at'],\n",
       "    'title': 'FilterView'},\n",
       "   'FilterViewCreate': {'properties': {'filter_string': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Filter String'},\n",
       "     'trace_filter_string': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Trace Filter String'},\n",
       "     'tree_filter_string': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tree Filter String'},\n",
       "     'display_name': {'type': 'string', 'title': 'Display Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'type': {'$ref': '#/components/schemas/FilterViewType',\n",
       "      'default': 'runs'}},\n",
       "    'type': 'object',\n",
       "    'required': ['display_name'],\n",
       "    'title': 'FilterViewCreate'},\n",
       "   'FilterViewType': {'type': 'string',\n",
       "    'enum': ['runs', 'threads'],\n",
       "    'title': 'FilterViewType'},\n",
       "   'FilterViewUpdate': {'properties': {'filter_string': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Filter String'},\n",
       "     'display_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Display Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'trace_filter_string': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Trace Filter String'},\n",
       "     'tree_filter_string': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tree Filter String'},\n",
       "     'type': {'anyOf': [{'$ref': '#/components/schemas/FilterViewType'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'title': 'FilterViewUpdate'},\n",
       "   'ForkRepoRequest': {'properties': {'repo_handle': {'type': 'string',\n",
       "      'title': 'Repo Handle'},\n",
       "     'readme': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Readme'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'tags': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tags'},\n",
       "     'is_public': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Is Public'}},\n",
       "    'type': 'object',\n",
       "    'required': ['repo_handle'],\n",
       "    'title': 'ForkRepoRequest',\n",
       "    'description': 'Fields to fork a repo'},\n",
       "   'FunctionMessage': {'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "       {'items': {'anyOf': [{'type': 'string'},\n",
       "          {'additionalProperties': True, 'type': 'object'}]},\n",
       "        'type': 'array'}],\n",
       "      'title': 'Content'},\n",
       "     'additional_kwargs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Additional Kwargs'},\n",
       "     'response_metadata': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Response Metadata'},\n",
       "     'type': {'type': 'string',\n",
       "      'const': 'function',\n",
       "      'title': 'Type',\n",
       "      'default': 'function'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'}},\n",
       "    'additionalProperties': True,\n",
       "    'type': 'object',\n",
       "    'required': ['content', 'name'],\n",
       "    'title': 'FunctionMessage',\n",
       "    'description': 'Message for passing the result of executing a tool back to a model.\\n\\n`FunctionMessage` are an older version of the `ToolMessage` schema, and\\ndo not contain the `tool_call_id` field.\\n\\nThe `tool_call_id` field is used to associate the tool call request with the\\ntool call response. Useful in situations where a chat model is able\\nto request multiple tool calls in parallel.'},\n",
       "   'FunctionMessageChunk': {'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "       {'items': {'anyOf': [{'type': 'string'},\n",
       "          {'additionalProperties': True, 'type': 'object'}]},\n",
       "        'type': 'array'}],\n",
       "      'title': 'Content'},\n",
       "     'additional_kwargs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Additional Kwargs'},\n",
       "     'response_metadata': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Response Metadata'},\n",
       "     'type': {'type': 'string',\n",
       "      'const': 'FunctionMessageChunk',\n",
       "      'title': 'Type',\n",
       "      'default': 'FunctionMessageChunk'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'}},\n",
       "    'additionalProperties': True,\n",
       "    'type': 'object',\n",
       "    'required': ['content', 'name'],\n",
       "    'title': 'FunctionMessageChunk',\n",
       "    'description': 'Function Message chunk.'},\n",
       "   'GenerateClusteringJobConfigRequest': {'properties': {'user_context': {'additionalProperties': {'type': 'string'},\n",
       "      'type': 'object',\n",
       "      'title': 'User Context'},\n",
       "     'model': {'type': 'string',\n",
       "      'enum': ['openai', 'anthropic'],\n",
       "      'title': 'Model',\n",
       "      'default': 'openai'}},\n",
       "    'type': 'object',\n",
       "    'required': ['user_context'],\n",
       "    'title': 'GenerateClusteringJobConfigRequest',\n",
       "    'description': 'Request to auto-generate a clustering job config.'},\n",
       "   'GenerateClusteringJobConfigResponse': {'properties': {'summary_prompt': {'type': 'string',\n",
       "      'title': 'Summary Prompt'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'attribute_schemas': {'anyOf': [{'additionalProperties': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Attribute Schemas'}},\n",
       "    'type': 'object',\n",
       "    'required': ['summary_prompt'],\n",
       "    'title': 'GenerateClusteringJobConfigResponse',\n",
       "    'description': 'Auto-generated clustering job config.'},\n",
       "   'GenerateSyntheticExamplesBody': {'properties': {'example_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Example Ids'},\n",
       "     'num_examples': {'type': 'integer', 'title': 'Num Examples'}},\n",
       "    'type': 'object',\n",
       "    'required': ['num_examples'],\n",
       "    'title': 'GenerateSyntheticExamplesBody'},\n",
       "   'GetClusteringJobConfigsResponse': {'properties': {'configs': {'items': {'$ref': '#/components/schemas/ClusteringJobConfigResponse'},\n",
       "      'type': 'array',\n",
       "      'title': 'Configs'}},\n",
       "    'type': 'object',\n",
       "    'required': ['configs'],\n",
       "    'title': 'GetClusteringJobConfigsResponse',\n",
       "    'description': 'Response to get clustering job configs.'},\n",
       "   'GetDatasetsSelect': {'type': 'string',\n",
       "    'enum': ['example_count'],\n",
       "    'title': 'GetDatasetsSelect'},\n",
       "   'GetRepoResponse': {'properties': {'repo': {'$ref': '#/components/schemas/RepoWithLookups'}},\n",
       "    'type': 'object',\n",
       "    'required': ['repo'],\n",
       "    'title': 'GetRepoResponse'},\n",
       "   'GetRunClusterResponse': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'parent_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Id'},\n",
       "     'num_children': {'type': 'integer', 'title': 'Num Children'},\n",
       "     'level': {'type': 'integer', 'title': 'Level'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'description': {'type': 'string', 'title': 'Description'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'num_children', 'level', 'name', 'description'],\n",
       "    'title': 'GetRunClusterResponse',\n",
       "    'description': 'Response to get a specific cluster from a specific cluster job.'},\n",
       "   'GetRunClusteringJobResponse': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'status': {'type': 'string', 'title': 'Status'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'shape': {'anyOf': [{'additionalProperties': {'type': 'integer'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Shape'},\n",
       "     'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Error'},\n",
       "     'clusters': {'items': {'$ref': '#/components/schemas/RunCluster'},\n",
       "      'type': 'array',\n",
       "      'title': 'Clusters'},\n",
       "     'report': {'anyOf': [{'$ref': '#/components/schemas/InsightsSummary'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'name', 'status', 'clusters'],\n",
       "    'title': 'GetRunClusteringJobResponse',\n",
       "    'description': 'Response to get a specific cluster job for a session.'},\n",
       "   'GetRunClusteringJobsResponse': {'properties': {'clustering_jobs': {'items': {'$ref': '#/components/schemas/RunClusteringJobPydantic'},\n",
       "      'type': 'array',\n",
       "      'title': 'Clustering Jobs'}},\n",
       "    'type': 'object',\n",
       "    'required': ['clustering_jobs'],\n",
       "    'title': 'GetRunClusteringJobsResponse',\n",
       "    'description': 'Response to get all cluster jobs for a session.'},\n",
       "   'GranularUsageDimensions': {'properties': {'user_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'User Id'},\n",
       "     'user_email': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'User Email'},\n",
       "     'api_key_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Api Key Id'},\n",
       "     'api_key_short_key': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Api Key Short Key'},\n",
       "     'project_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Project Id'},\n",
       "     'project_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Project Name'},\n",
       "     'workspace_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Id'},\n",
       "     'workspace_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Workspace Name'}},\n",
       "    'type': 'object',\n",
       "    'title': 'GranularUsageDimensions',\n",
       "    'description': 'Dimension values for a granular usage record.'},\n",
       "   'GranularUsageGroupBy': {'type': 'string',\n",
       "    'enum': ['user', 'api_key', 'project', 'workspace'],\n",
       "    'title': 'GranularUsageGroupBy',\n",
       "    'description': 'Dimensions for grouping granular usage data.'},\n",
       "   'GranularUsageRecord': {'properties': {'time_bucket': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Time Bucket'},\n",
       "     'dimensions': {'$ref': '#/components/schemas/GranularUsageDimensions'},\n",
       "     'traces': {'type': 'integer', 'title': 'Traces', 'default': 0}},\n",
       "    'type': 'object',\n",
       "    'required': ['dimensions'],\n",
       "    'title': 'GranularUsageRecord',\n",
       "    'description': 'A single granular usage data point.'},\n",
       "   'GranularUsageResponse': {'properties': {'stride': {'$ref': '#/components/schemas/GranularUsageStride'},\n",
       "     'usage': {'items': {'$ref': '#/components/schemas/GranularUsageRecord'},\n",
       "      'type': 'array',\n",
       "      'title': 'Usage'}},\n",
       "    'type': 'object',\n",
       "    'required': ['stride', 'usage'],\n",
       "    'title': 'GranularUsageResponse',\n",
       "    'description': 'Response for granular usage query.'},\n",
       "   'GranularUsageStride': {'properties': {'days': {'type': 'integer',\n",
       "      'title': 'Days',\n",
       "      'default': 0},\n",
       "     'hours': {'type': 'integer', 'title': 'Hours', 'default': 0}},\n",
       "    'type': 'object',\n",
       "    'title': 'GranularUsageStride',\n",
       "    'description': 'Stride configuration for time bucketing - only ONE field should be non-zero.'},\n",
       "   'GroupExampleRunsByField': {'type': 'string',\n",
       "    'enum': ['run_metadata', 'example_metadata'],\n",
       "    'title': 'GroupExampleRunsByField'},\n",
       "   'GroupedExamplesWithRunsResponse': {'properties': {'groups': {'items': {'$ref': '#/components/schemas/ExampleGroupWithSessions'},\n",
       "      'type': 'array',\n",
       "      'title': 'Groups'}},\n",
       "    'type': 'object',\n",
       "    'required': ['groups'],\n",
       "    'title': 'GroupedExamplesWithRunsResponse',\n",
       "    'description': \"Response for grouped comparison view of dataset examples.\\n\\nReturns dataset examples grouped by a run metadata value (e.g., model='gpt-4').\\nOptional filters are applied to all runs before grouping.\\n\\nShows:\\n- Which examples were executed with each metadata value\\n- Per-session aggregate statistics for runs on those examples\\n- The actual example data with their associated runs\\n\\nUsed for comparing how different sessions performed on the same set of examples.\"},\n",
       "   'GroupedExperimentsRequest': {'properties': {'stats_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Stats Start Time'},\n",
       "     'name_contains': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name Contains'},\n",
       "     'tag_value_id': {'anyOf': [{'items': {'type': 'string', 'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tag Value Id'},\n",
       "     'dataset_version': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Dataset Version'},\n",
       "     'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Filter'},\n",
       "     'use_approx_stats': {'type': 'boolean',\n",
       "      'title': 'Use Approx Stats',\n",
       "      'default': False},\n",
       "     'metadata_keys': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'maxItems': 4,\n",
       "      'minItems': 1,\n",
       "      'title': 'Metadata Keys'},\n",
       "     'experiment_limit': {'type': 'integer',\n",
       "      'maximum': 1000.0,\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Experiment Limit',\n",
       "      'default': 1000}},\n",
       "    'type': 'object',\n",
       "    'required': ['metadata_keys'],\n",
       "    'title': 'GroupedExperimentsRequest',\n",
       "    'description': 'Schema for grouped experiment (tracer session) query.'},\n",
       "   'GroupedRunsSessionStats': {'properties': {'start_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'default_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Dataset Id'},\n",
       "     'reference_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reference Dataset Id'},\n",
       "     'trace_tier': {'anyOf': [{'$ref': '#/components/schemas/TraceTier'},\n",
       "       {'type': 'null'}]},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'run_count': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Run Count'},\n",
       "     'latency_p50': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Latency P50'},\n",
       "     'latency_p99': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Latency P99'},\n",
       "     'first_token_p50': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'First Token P50'},\n",
       "     'first_token_p99': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'First Token P99'},\n",
       "     'total_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Total Tokens'},\n",
       "     'prompt_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Tokens'},\n",
       "     'completion_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Completion Tokens'},\n",
       "     'total_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Total Cost'},\n",
       "     'prompt_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Cost'},\n",
       "     'completion_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Completion Cost'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'last_run_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Run Start Time'},\n",
       "     'last_run_start_time_live': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Run Start Time Live'},\n",
       "     'feedback_stats': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Stats'},\n",
       "     'session_feedback_stats': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Feedback Stats'},\n",
       "     'run_facets': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Run Facets'},\n",
       "     'error_rate': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Error Rate'},\n",
       "     'streaming_rate': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Streaming Rate'},\n",
       "     'test_run_number': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Test Run Number'},\n",
       "     'example_count': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Example Count'},\n",
       "     'filter': {'type': 'string', 'title': 'Filter'},\n",
       "     'min_start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Min Start Time'},\n",
       "     'max_start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Max Start Time'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'tenant_id', 'filter'],\n",
       "    'title': 'GroupedRunsSessionStats',\n",
       "    'description': 'TracerSession stats filtered to runs matching a specific metadata value.\\n\\nExtends TracerSession with:\\n- example_count: unique examples (vs run_count = total runs including duplicates)\\n- filter: ClickHouse filter for fetching runs in this session/group\\n- min/max_start_time: time range for runs in this session/group'},\n",
       "   'HTTPValidationError': {'properties': {'detail': {'items': {'$ref': '#/components/schemas/ValidationError'},\n",
       "      'type': 'array',\n",
       "      'title': 'Detail'}},\n",
       "    'type': 'object',\n",
       "    'title': 'HTTPValidationError'},\n",
       "   'HealthInfoGetResponse': {'properties': {'clickhouse_disk_free_pct': {'type': 'number',\n",
       "      'title': 'Clickhouse Disk Free Pct'}},\n",
       "    'type': 'object',\n",
       "    'required': ['clickhouse_disk_free_pct'],\n",
       "    'title': 'HealthInfoGetResponse',\n",
       "    'description': 'The LangSmith server info.'},\n",
       "   'Highlight': {'properties': {'prompt_chunk_start_index': {'type': 'integer',\n",
       "      'title': 'Prompt Chunk Start Index'},\n",
       "     'prompt_chunk_end_index': {'type': 'integer',\n",
       "      'title': 'Prompt Chunk End Index'},\n",
       "     'prompt_chunk': {'type': 'string', 'title': 'Prompt Chunk'},\n",
       "     'highlight_text': {'type': 'string', 'title': 'Highlight Text'}},\n",
       "    'type': 'object',\n",
       "    'required': ['prompt_chunk_start_index',\n",
       "     'prompt_chunk_end_index',\n",
       "     'prompt_chunk',\n",
       "     'highlight_text'],\n",
       "    'title': 'Highlight'},\n",
       "   'HighlightedRun': {'properties': {'run_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Run Id'},\n",
       "     'cluster_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Cluster Id'},\n",
       "     'cluster_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Cluster Name'},\n",
       "     'rank': {'type': 'integer', 'title': 'Rank'},\n",
       "     'highlight_reason': {'type': 'string', 'title': 'Highlight Reason'},\n",
       "     'summary': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Summary'}},\n",
       "    'type': 'object',\n",
       "    'required': ['run_id', 'rank', 'highlight_reason'],\n",
       "    'title': 'HighlightedRun',\n",
       "    'description': 'A trace highlighted in an insights report summary. Up to 10 per insights job.'},\n",
       "   'HostProjectChartMetric': {'type': 'string',\n",
       "    'enum': ['memory_usage',\n",
       "     'cpu_usage',\n",
       "     'disk_usage',\n",
       "     'restart_count',\n",
       "     'replica_count',\n",
       "     'worker_count',\n",
       "     'lg_run_count',\n",
       "     'responses_per_second',\n",
       "     'error_responses_per_second',\n",
       "     'p95_latency'],\n",
       "    'title': 'HostProjectChartMetric',\n",
       "    'description': 'LGP Metrics you can chart.'},\n",
       "   'HumanMessage': {'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "       {'items': {'anyOf': [{'type': 'string'},\n",
       "          {'additionalProperties': True, 'type': 'object'}]},\n",
       "        'type': 'array'}],\n",
       "      'title': 'Content'},\n",
       "     'additional_kwargs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Additional Kwargs'},\n",
       "     'response_metadata': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Response Metadata'},\n",
       "     'type': {'type': 'string',\n",
       "      'const': 'human',\n",
       "      'title': 'Type',\n",
       "      'default': 'human'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'}},\n",
       "    'additionalProperties': True,\n",
       "    'type': 'object',\n",
       "    'required': ['content'],\n",
       "    'title': 'HumanMessage',\n",
       "    'description': 'Message from the user.\\n\\nA `HumanMessage` is a message that is passed in from a user to the model.\\n\\nExample:\\n    ```python\\n    from langchain_core.messages import HumanMessage, SystemMessage\\n\\n    messages = [\\n        SystemMessage(content=\"You are a helpful assistant! Your name is Bob.\"),\\n        HumanMessage(content=\"What is your name?\"),\\n    ]\\n\\n    # Instantiate a chat model and invoke it with the messages\\n    model = ...\\n    print(model.invoke(messages))\\n    ```'},\n",
       "   'HumanMessageChunk': {'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "       {'items': {'anyOf': [{'type': 'string'},\n",
       "          {'additionalProperties': True, 'type': 'object'}]},\n",
       "        'type': 'array'}],\n",
       "      'title': 'Content'},\n",
       "     'additional_kwargs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Additional Kwargs'},\n",
       "     'response_metadata': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Response Metadata'},\n",
       "     'type': {'type': 'string',\n",
       "      'const': 'HumanMessageChunk',\n",
       "      'title': 'Type',\n",
       "      'default': 'HumanMessageChunk'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'}},\n",
       "    'additionalProperties': True,\n",
       "    'type': 'object',\n",
       "    'required': ['content'],\n",
       "    'title': 'HumanMessageChunk',\n",
       "    'description': 'Human Message chunk.'},\n",
       "   'Identity': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'organization_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Organization Id'},\n",
       "     'tenant_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tenant Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'user_id': {'type': 'string', 'format': 'uuid', 'title': 'User Id'},\n",
       "     'ls_user_id': {'type': 'string', 'format': 'uuid', 'title': 'Ls User Id'},\n",
       "     'read_only': {'type': 'boolean',\n",
       "      'title': 'Read Only',\n",
       "      'deprecated': True},\n",
       "     'role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Role Id'},\n",
       "     'role_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Role Name'},\n",
       "     'access_scope': {'$ref': '#/components/schemas/AccessScope',\n",
       "      'default': 'workspace'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'organization_id',\n",
       "     'created_at',\n",
       "     'user_id',\n",
       "     'ls_user_id',\n",
       "     'read_only'],\n",
       "    'title': 'Identity'},\n",
       "   'IdentityAnnotationQueueRunStatusCreateSchema': {'properties': {'status': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Status'},\n",
       "     'override_added_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Override Added At'}},\n",
       "    'type': 'object',\n",
       "    'title': 'IdentityAnnotationQueueRunStatusCreateSchema',\n",
       "    'description': 'Identity annotation queue run status create schema.'},\n",
       "   'IdentityCreate': {'properties': {'user_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'User Id',\n",
       "      'deprecated': True},\n",
       "     'org_identity_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Org Identity Id'},\n",
       "     'ls_user_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Ls User Id'},\n",
       "     'read_only': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Read Only',\n",
       "      'deprecated': True},\n",
       "     'role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Role Id'}},\n",
       "    'type': 'object',\n",
       "    'title': 'IdentityCreate'},\n",
       "   'IdentityPatch': {'properties': {'read_only': {'anyOf': [{'type': 'boolean'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Read Only',\n",
       "      'deprecated': True},\n",
       "     'role_id': {'type': 'string', 'format': 'uuid', 'title': 'Role Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['role_id'],\n",
       "    'title': 'IdentityPatch'},\n",
       "   'InfoGetResponse': {'properties': {'version': {'type': 'string',\n",
       "      'title': 'Version'},\n",
       "     'license_expiration_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'License Expiration Time'},\n",
       "     'batch_ingest_config': {'$ref': '#/components/schemas/BatchIngestConfig'},\n",
       "     'instance_flags': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Instance Flags'},\n",
       "     'customer_info': {'anyOf': [{'$ref': '#/components/schemas/CustomerInfo'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'required': ['version'],\n",
       "    'title': 'InfoGetResponse',\n",
       "    'description': 'The LangSmith server info.'},\n",
       "   'InputTokenDetails': {'properties': {'audio': {'type': 'integer',\n",
       "      'title': 'Audio'},\n",
       "     'cache_creation': {'type': 'integer', 'title': 'Cache Creation'},\n",
       "     'cache_read': {'type': 'integer', 'title': 'Cache Read'}},\n",
       "    'type': 'object',\n",
       "    'title': 'InputTokenDetails',\n",
       "    'description': 'Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n    ```python\\n    {\\n        \"audio\": 10,\\n        \"cache_creation\": 200,\\n        \"cache_read\": 100,\\n    }\\n    ```\\n\\nMay also hold extra provider-specific keys.\\n\\n!!! version-added \"Added in `langchain-core` 0.3.9\"'},\n",
       "   'InsightsSummary': {'properties': {'key_points': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Key Points'},\n",
       "     'title': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Title'},\n",
       "     'highlighted_traces': {'items': {'$ref': '#/components/schemas/HighlightedRun'},\n",
       "      'type': 'array',\n",
       "      'title': 'Highlighted Traces'},\n",
       "     'created_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Created At'}},\n",
       "    'type': 'object',\n",
       "    'title': 'InsightsSummary',\n",
       "    'description': 'High level summary of an insights job that pulls out patterns and specific traces.'},\n",
       "   'InternalSecretsResponse': {'properties': {'encrypted_secrets': {'type': 'string',\n",
       "      'title': 'Encrypted Secrets'}},\n",
       "    'type': 'object',\n",
       "    'required': ['encrypted_secrets'],\n",
       "    'title': 'InternalSecretsResponse'},\n",
       "   'InvalidToolCall': {'properties': {'type': {'type': 'string',\n",
       "      'const': 'invalid_tool_call',\n",
       "      'title': 'Type'},\n",
       "     'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Args'},\n",
       "     'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Error'},\n",
       "     'index': {'anyOf': [{'type': 'integer'}, {'type': 'string'}],\n",
       "      'title': 'Index'},\n",
       "     'extras': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Extras'}},\n",
       "    'type': 'object',\n",
       "    'required': ['type', 'id', 'name', 'args', 'error'],\n",
       "    'title': 'InvalidToolCall',\n",
       "    'description': 'Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)'},\n",
       "   'InvokePromptPayload': {'properties': {'messages': {'items': {'prefixItems': [{'type': 'string'},\n",
       "        {'type': 'string'}],\n",
       "       'type': 'array',\n",
       "       'maxItems': 2,\n",
       "       'minItems': 2},\n",
       "      'type': 'array',\n",
       "      'title': 'Messages'},\n",
       "     'template_format': {'type': 'string', 'title': 'Template Format'},\n",
       "     'inputs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Inputs'}},\n",
       "    'type': 'object',\n",
       "    'required': ['messages', 'template_format', 'inputs'],\n",
       "    'title': 'InvokePromptPayload'},\n",
       "   'LikeRepoRequest': {'properties': {'like': {'type': 'boolean',\n",
       "      'title': 'Like'}},\n",
       "    'type': 'object',\n",
       "    'required': ['like'],\n",
       "    'title': 'LikeRepoRequest'},\n",
       "   'LikeRepoResponse': {'properties': {'likes': {'type': 'integer',\n",
       "      'title': 'Likes'}},\n",
       "    'type': 'object',\n",
       "    'required': ['likes'],\n",
       "    'title': 'LikeRepoResponse'},\n",
       "   'ListAuditLogsOCSFResponse': {'properties': {'cursor': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Cursor'},\n",
       "     'items': {'items': {'$ref': '#/components/schemas/OCSFApiActivity'},\n",
       "      'type': 'array',\n",
       "      'title': 'Items'}},\n",
       "    'type': 'object',\n",
       "    'required': ['cursor', 'items'],\n",
       "    'title': 'ListAuditLogsOCSFResponse',\n",
       "    'description': 'Response model for listing audit logs in OCSF format with pagination.'},\n",
       "   'ListCommentsResponse': {'properties': {'comments': {'items': {'$ref': '#/components/schemas/Comment'},\n",
       "      'type': 'array',\n",
       "      'title': 'Comments'},\n",
       "     'total': {'type': 'integer', 'title': 'Total'}},\n",
       "    'type': 'object',\n",
       "    'required': ['comments', 'total'],\n",
       "    'title': 'ListCommentsResponse'},\n",
       "   'ListPublicDatasetRunsResponse': {'properties': {'runs': {'items': {'$ref': '#/components/schemas/RunPublicDatasetSchema'},\n",
       "      'type': 'array',\n",
       "      'title': 'Runs'},\n",
       "     'cursors': {'additionalProperties': {'anyOf': [{'type': 'string'},\n",
       "        {'type': 'null'}]},\n",
       "      'type': 'object',\n",
       "      'title': 'Cursors'},\n",
       "     'parsed_query': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Parsed Query'}},\n",
       "    'type': 'object',\n",
       "    'required': ['runs', 'cursors'],\n",
       "    'title': 'ListPublicDatasetRunsResponse'},\n",
       "   'ListPublicRunsResponse': {'properties': {'runs': {'items': {'$ref': '#/components/schemas/RunPublicSchema'},\n",
       "      'type': 'array',\n",
       "      'title': 'Runs'},\n",
       "     'cursors': {'additionalProperties': {'anyOf': [{'type': 'string'},\n",
       "        {'type': 'null'}]},\n",
       "      'type': 'object',\n",
       "      'title': 'Cursors'},\n",
       "     'parsed_query': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Parsed Query'}},\n",
       "    'type': 'object',\n",
       "    'required': ['runs', 'cursors'],\n",
       "    'title': 'ListPublicRunsResponse'},\n",
       "   'ListReposResponse': {'properties': {'repos': {'items': {'$ref': '#/components/schemas/RepoWithLookups'},\n",
       "      'type': 'array',\n",
       "      'title': 'Repos'},\n",
       "     'total': {'type': 'integer', 'title': 'Total'}},\n",
       "    'type': 'object',\n",
       "    'required': ['repos', 'total'],\n",
       "    'title': 'ListReposResponse'},\n",
       "   'ListRunsResponse': {'properties': {'runs': {'items': {'$ref': '#/components/schemas/RunSchema'},\n",
       "      'type': 'array',\n",
       "      'title': 'Runs'},\n",
       "     'cursors': {'additionalProperties': {'anyOf': [{'type': 'string'},\n",
       "        {'type': 'null'}]},\n",
       "      'type': 'object',\n",
       "      'title': 'Cursors'},\n",
       "     'search_cursors': {'anyOf': [{'additionalProperties': {'anyOf': [{},\n",
       "          {'type': 'null'}]},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Search Cursors'},\n",
       "     'parsed_query': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Parsed Query'}},\n",
       "    'type': 'object',\n",
       "    'required': ['runs', 'cursors'],\n",
       "    'title': 'ListRunsResponse'},\n",
       "   'ListTagsForResourceRequest': {'properties': {'resource_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Resource Id'},\n",
       "     'resource_type': {'$ref': '#/components/schemas/ResourceType'}},\n",
       "    'type': 'object',\n",
       "    'required': ['resource_id', 'resource_type'],\n",
       "    'title': 'ListTagsForResourceRequest'},\n",
       "   'ListTagsResponse': {'properties': {'tags': {'items': {'$ref': '#/components/schemas/TagCount'},\n",
       "      'type': 'array',\n",
       "      'title': 'Tags'}},\n",
       "    'type': 'object',\n",
       "    'required': ['tags'],\n",
       "    'title': 'ListTagsResponse'},\n",
       "   'MemberIdentity': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'organization_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Organization Id'},\n",
       "     'tenant_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tenant Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'user_id': {'type': 'string', 'format': 'uuid', 'title': 'User Id'},\n",
       "     'ls_user_id': {'type': 'string', 'format': 'uuid', 'title': 'Ls User Id'},\n",
       "     'read_only': {'type': 'boolean',\n",
       "      'title': 'Read Only',\n",
       "      'deprecated': True},\n",
       "     'role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Role Id'},\n",
       "     'role_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Role Name'},\n",
       "     'access_scope': {'$ref': '#/components/schemas/AccessScope',\n",
       "      'default': 'workspace'},\n",
       "     'email': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Email'},\n",
       "     'full_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Full Name'},\n",
       "     'avatar_url': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Avatar Url'},\n",
       "     'linked_login_methods': {'items': {'$ref': '#/components/schemas/ProviderUserSlim'},\n",
       "      'type': 'array',\n",
       "      'title': 'Linked Login Methods',\n",
       "      'default': []},\n",
       "     'display_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Display Name'},\n",
       "     'is_disabled': {'type': 'boolean',\n",
       "      'title': 'Is Disabled',\n",
       "      'default': False},\n",
       "     'org_role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Org Role Id'},\n",
       "     'org_role_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Org Role Name'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'organization_id',\n",
       "     'created_at',\n",
       "     'user_id',\n",
       "     'ls_user_id',\n",
       "     'read_only'],\n",
       "    'title': 'MemberIdentity'},\n",
       "   'Missing': {'properties': {'__missing__': {'type': 'string',\n",
       "      'const': '__missing__',\n",
       "      'title': 'Missing'}},\n",
       "    'type': 'object',\n",
       "    'required': ['__missing__'],\n",
       "    'title': 'Missing'},\n",
       "   'ModelFeedbackSource': {'properties': {'type': {'type': 'string',\n",
       "      'const': 'model',\n",
       "      'title': 'Type',\n",
       "      'default': 'model'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'}},\n",
       "    'type': 'object',\n",
       "    'title': 'ModelFeedbackSource',\n",
       "    'description': 'Model feedback source.'},\n",
       "   'ModelPriceMapCreateSchema': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'match_path': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Match Path',\n",
       "      'default': ['model',\n",
       "       'model_name',\n",
       "       'model_id',\n",
       "       'model_path',\n",
       "       'endpoint_name']},\n",
       "     'match_pattern': {'type': 'string', 'title': 'Match Pattern'},\n",
       "     'prompt_cost': {'anyOf': [{'type': 'number'}, {'type': 'string'}],\n",
       "      'title': 'Prompt Cost'},\n",
       "     'completion_cost': {'anyOf': [{'type': 'number'}, {'type': 'string'}],\n",
       "      'title': 'Completion Cost'},\n",
       "     'prompt_cost_details': {'anyOf': [{'additionalProperties': {'anyOf': [{'type': 'number'},\n",
       "          {'type': 'string'}]},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Prompt Cost Details'},\n",
       "     'completion_cost_details': {'anyOf': [{'additionalProperties': {'anyOf': [{'type': 'number'},\n",
       "          {'type': 'string'}]},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Cost Details'},\n",
       "     'provider': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Provider'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'match_pattern', 'prompt_cost', 'completion_cost'],\n",
       "    'title': 'ModelPriceMapCreateSchema',\n",
       "    'description': 'Model price map create schema.'},\n",
       "   'ModelPriceMapUpdateSchema': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'match_path': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Match Path',\n",
       "      'default': ['model',\n",
       "       'model_name',\n",
       "       'model_id',\n",
       "       'model_path',\n",
       "       'endpoint_name']},\n",
       "     'match_pattern': {'type': 'string', 'title': 'Match Pattern'},\n",
       "     'prompt_cost': {'anyOf': [{'type': 'number'}, {'type': 'string'}],\n",
       "      'title': 'Prompt Cost'},\n",
       "     'completion_cost': {'anyOf': [{'type': 'number'}, {'type': 'string'}],\n",
       "      'title': 'Completion Cost'},\n",
       "     'prompt_cost_details': {'anyOf': [{'additionalProperties': {'anyOf': [{'type': 'number'},\n",
       "          {'type': 'string'}]},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Prompt Cost Details'},\n",
       "     'completion_cost_details': {'anyOf': [{'additionalProperties': {'anyOf': [{'type': 'number'},\n",
       "          {'type': 'string'}]},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Cost Details'},\n",
       "     'provider': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Provider'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'match_pattern', 'prompt_cost', 'completion_cost'],\n",
       "    'title': 'ModelPriceMapUpdateSchema',\n",
       "    'description': 'Model price map update schema.'},\n",
       "   'OCSFActivityId': {'type': 'integer',\n",
       "    'enum': [0, 1, 2, 3, 4, 99],\n",
       "    'title': 'OCSFActivityId',\n",
       "    'description': 'Activity types for API Activity class.'},\n",
       "   'OCSFActor': {'properties': {'user': {'$ref': '#/components/schemas/OCSFUser'}},\n",
       "    'type': 'object',\n",
       "    'required': ['user'],\n",
       "    'title': 'OCSFActor',\n",
       "    'description': 'OCSF actor object.'},\n",
       "   'OCSFApi': {'properties': {'operation': {'$ref': '#/components/schemas/AuditLogOperation'}},\n",
       "    'type': 'object',\n",
       "    'required': ['operation'],\n",
       "    'title': 'OCSFApi',\n",
       "    'description': 'OCSF API details object.'},\n",
       "   'OCSFApiActivity': {'properties': {'class_uid': {'$ref': '#/components/schemas/OCSFClassUid'},\n",
       "     'class_name': {'$ref': '#/components/schemas/OCSFClassName'},\n",
       "     'category_uid': {'$ref': '#/components/schemas/OCSFCategoryUid'},\n",
       "     'category_name': {'$ref': '#/components/schemas/OCSFCategoryName'},\n",
       "     'severity_id': {'$ref': '#/components/schemas/OCSFSeverityId'},\n",
       "     'type_uid': {'$ref': '#/components/schemas/OCSFTypeUid'},\n",
       "     'activity_id': {'$ref': '#/components/schemas/OCSFActivityId'},\n",
       "     'activity_name': {'type': 'string', 'title': 'Activity Name'},\n",
       "     'status_id': {'$ref': '#/components/schemas/OCSFStatusId'},\n",
       "     'status': {'type': 'string', 'title': 'Status'},\n",
       "     'time': {'type': 'integer', 'title': 'Time'},\n",
       "     'metadata': {'$ref': '#/components/schemas/OCSFMetadata'},\n",
       "     'api': {'$ref': '#/components/schemas/OCSFApi'},\n",
       "     'http_request': {'$ref': '#/components/schemas/OCSFHttpRequest'},\n",
       "     'http_response': {'$ref': '#/components/schemas/OCSFHttpResponse'},\n",
       "     'actor': {'$ref': '#/components/schemas/OCSFActor'},\n",
       "     'src_endpoint': {'$ref': '#/components/schemas/OCSFEndpoint'},\n",
       "     'resources': {'items': {'$ref': '#/components/schemas/OCSFResourceDetails'},\n",
       "      'type': 'array',\n",
       "      'title': 'Resources'},\n",
       "     'unmapped': {'$ref': '#/components/schemas/OCSFUnmapped'}},\n",
       "    'type': 'object',\n",
       "    'required': ['class_uid',\n",
       "     'class_name',\n",
       "     'category_uid',\n",
       "     'category_name',\n",
       "     'severity_id',\n",
       "     'type_uid',\n",
       "     'activity_id',\n",
       "     'activity_name',\n",
       "     'status_id',\n",
       "     'status',\n",
       "     'time',\n",
       "     'metadata',\n",
       "     'api',\n",
       "     'http_request',\n",
       "     'http_response',\n",
       "     'actor',\n",
       "     'src_endpoint',\n",
       "     'resources',\n",
       "     'unmapped'],\n",
       "    'title': 'OCSFApiActivity',\n",
       "    'description': 'OCSF API Activity event (Class UID: 6003).\\n\\nThis represents an API call event in the OCSF format.\\nReference: https://schema.ocsf.io/1.7.0/classes/api_activity\\n\\nRemember to try to validate the OCSF event against the official OCSF schema validator API: https://schema.ocsf.io/doc/index.html#/Tools/SchemaWeb.SchemaController.validate\\nOr with `test_ocsf_validates_against_schema()` in test_audit_logs_models.py.'},\n",
       "   'OCSFCategoryName': {'type': 'string',\n",
       "    'enum': ['Application Activity'],\n",
       "    'title': 'OCSFCategoryName',\n",
       "    'description': 'OCSF category names.'},\n",
       "   'OCSFCategoryUid': {'type': 'integer',\n",
       "    'enum': [6],\n",
       "    'title': 'OCSFCategoryUid',\n",
       "    'description': 'OCSF category UIDs.'},\n",
       "   'OCSFClassName': {'type': 'string',\n",
       "    'enum': ['API Activity'],\n",
       "    'title': 'OCSFClassName',\n",
       "    'description': 'OCSF class names.'},\n",
       "   'OCSFClassUid': {'type': 'integer',\n",
       "    'enum': [6003],\n",
       "    'title': 'OCSFClassUid',\n",
       "    'description': 'OCSF class UIDs.'},\n",
       "   'OCSFEndpoint': {'properties': {'ip': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Ip'},\n",
       "     'port': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Port'},\n",
       "     'intermediate_ips': {'anyOf': [{'items': {'type': 'string'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Intermediate Ips'}},\n",
       "    'type': 'object',\n",
       "    'required': ['ip', 'port', 'intermediate_ips'],\n",
       "    'title': 'OCSFEndpoint',\n",
       "    'description': 'OCSF network endpoint object.'},\n",
       "   'OCSFHttpRequest': {'properties': {'http_method': {'type': 'string',\n",
       "      'title': 'Http Method'},\n",
       "     'url': {'$ref': '#/components/schemas/OCSFUrl'}},\n",
       "    'type': 'object',\n",
       "    'required': ['http_method', 'url'],\n",
       "    'title': 'OCSFHttpRequest',\n",
       "    'description': 'OCSF HTTP request object.'},\n",
       "   'OCSFHttpResponse': {'properties': {'code': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Code'}},\n",
       "    'type': 'object',\n",
       "    'required': ['code'],\n",
       "    'title': 'OCSFHttpResponse',\n",
       "    'description': 'OCSF HTTP response object.'},\n",
       "   'OCSFMetadata': {'properties': {'uid': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Uid'},\n",
       "     'product': {'$ref': '#/components/schemas/OCSFProduct'}},\n",
       "    'type': 'object',\n",
       "    'required': ['uid', 'product'],\n",
       "    'title': 'OCSFMetadata',\n",
       "    'description': 'OCSF event metadata.'},\n",
       "   'OCSFProduct': {'properties': {'name': {'type': 'string', 'title': 'Name'},\n",
       "     'vendor_name': {'type': 'string', 'title': 'Vendor Name'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'vendor_name'],\n",
       "    'title': 'OCSFProduct',\n",
       "    'description': 'OCSF product object.'},\n",
       "   'OCSFResourceDetails': {'properties': {'uid': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Uid'}},\n",
       "    'type': 'object',\n",
       "    'required': ['uid'],\n",
       "    'title': 'OCSFResourceDetails',\n",
       "    'description': 'OCSF resource details object.'},\n",
       "   'OCSFSeverityId': {'type': 'integer',\n",
       "    'enum': [99],\n",
       "    'title': 'OCSFSeverityId',\n",
       "    'description': 'Severity levels for OCSF events.'},\n",
       "   'OCSFStatusId': {'type': 'integer',\n",
       "    'enum': [0, 1, 2, 99],\n",
       "    'title': 'OCSFStatusId',\n",
       "    'description': 'Status values for OCSF events.'},\n",
       "   'OCSFTypeUid': {'type': 'integer',\n",
       "    'enum': [600300, 600301, 600302, 600303, 600304, 600399],\n",
       "    'title': 'OCSFTypeUid',\n",
       "    'description': 'OCSF type UIDs for API Activity (class_uid * 100 + activity_id).'},\n",
       "   'OCSFUnmapped': {'properties': {'original_audit_log': {'$ref': '#/components/schemas/AuditLogMessage'}},\n",
       "    'type': 'object',\n",
       "    'required': ['original_audit_log'],\n",
       "    'title': 'OCSFUnmapped',\n",
       "    'description': 'OCSF unmapped attribute for source-specific data.\\n\\nReference: https://schema.ocsf.io/1.7.0/classes/base_event'},\n",
       "   'OCSFUrl': {'properties': {'path': {'type': 'string', 'title': 'Path'}},\n",
       "    'type': 'object',\n",
       "    'required': ['path'],\n",
       "    'title': 'OCSFUrl',\n",
       "    'description': 'OCSF URL object.'},\n",
       "   'OCSFUser': {'properties': {'uid': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Uid'},\n",
       "     'credential_uid': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Credential Uid'}},\n",
       "    'type': 'object',\n",
       "    'required': ['uid', 'credential_uid'],\n",
       "    'title': 'OCSFUser',\n",
       "    'description': 'OCSF user object within actor.'},\n",
       "   'OptimizePromptJobRequest': {'properties': {'algorithm': {'$ref': '#/components/schemas/EPromptOptimizationAlgorithm'},\n",
       "     'config': {'anyOf': [{'$ref': '#/components/schemas/PromptimConfig'},\n",
       "       {'$ref': '#/components/schemas/DemoConfig'}],\n",
       "      'title': 'Config'},\n",
       "     'prompt_name': {'type': 'string', 'title': 'Prompt Name'}},\n",
       "    'type': 'object',\n",
       "    'required': ['algorithm', 'config', 'prompt_name'],\n",
       "    'title': 'OptimizePromptJobRequest',\n",
       "    'description': 'Request to optimize a prompt.'},\n",
       "   'OptimizePromptResponse': {'properties': {'optimization_job_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Optimization Job Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['optimization_job_id'],\n",
       "    'title': 'OptimizePromptResponse',\n",
       "    'description': 'Response from optimizing a prompt.'},\n",
       "   'OrgIdentityPatch': {'properties': {'password': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Password'},\n",
       "     'full_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Full Name'},\n",
       "     'role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Role Id'}},\n",
       "    'type': 'object',\n",
       "    'title': 'OrgIdentityPatch'},\n",
       "   'OrgMemberIdentity': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'organization_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Organization Id'},\n",
       "     'tenant_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tenant Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'user_id': {'type': 'string', 'format': 'uuid', 'title': 'User Id'},\n",
       "     'ls_user_id': {'type': 'string', 'format': 'uuid', 'title': 'Ls User Id'},\n",
       "     'read_only': {'type': 'boolean',\n",
       "      'title': 'Read Only',\n",
       "      'deprecated': True},\n",
       "     'role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Role Id'},\n",
       "     'role_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Role Name'},\n",
       "     'access_scope': {'$ref': '#/components/schemas/AccessScope',\n",
       "      'default': 'workspace'},\n",
       "     'email': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Email'},\n",
       "     'full_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Full Name'},\n",
       "     'avatar_url': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Avatar Url'},\n",
       "     'linked_login_methods': {'items': {'$ref': '#/components/schemas/ProviderUserSlim'},\n",
       "      'type': 'array',\n",
       "      'title': 'Linked Login Methods',\n",
       "      'default': []},\n",
       "     'display_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Display Name'},\n",
       "     'is_disabled': {'type': 'boolean',\n",
       "      'title': 'Is Disabled',\n",
       "      'default': False},\n",
       "     'org_role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Org Role Id'},\n",
       "     'org_role_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Org Role Name'},\n",
       "     'tenant_ids': {'items': {'type': 'string', 'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'title': 'Tenant Ids',\n",
       "      'default': []}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'organization_id',\n",
       "     'created_at',\n",
       "     'user_id',\n",
       "     'ls_user_id',\n",
       "     'read_only'],\n",
       "    'title': 'OrgMemberIdentity'},\n",
       "   'OrgPendingIdentity': {'properties': {'email': {'type': 'string',\n",
       "      'title': 'Email'},\n",
       "     'read_only': {'type': 'boolean',\n",
       "      'title': 'Read Only',\n",
       "      'default': False,\n",
       "      'deprecated': True},\n",
       "     'role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Role Id'},\n",
       "     'workspace_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Ids'},\n",
       "     'workspace_role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Role Id'},\n",
       "     'password': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Password'},\n",
       "     'full_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Full Name'},\n",
       "     'access_scope': {'$ref': '#/components/schemas/AccessScope',\n",
       "      'default': 'workspace'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'user_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'User Id'},\n",
       "     'tenant_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tenant Id'},\n",
       "     'organization_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Organization Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'role_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Role Name'},\n",
       "     'org_role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Org Role Id'},\n",
       "     'org_role_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Org Role Name'},\n",
       "     'tenant_ids': {'items': {'type': 'string', 'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'title': 'Tenant Ids',\n",
       "      'default': []}},\n",
       "    'type': 'object',\n",
       "    'required': ['email', 'id', 'created_at'],\n",
       "    'title': 'OrgPendingIdentity'},\n",
       "   'OrgUsage': {'properties': {'customer_id': {'type': 'string',\n",
       "      'title': 'Customer Id'},\n",
       "     'billable_metric_id': {'type': 'string', 'title': 'Billable Metric Id'},\n",
       "     'billable_metric_name': {'type': 'string',\n",
       "      'title': 'Billable Metric Name'},\n",
       "     'start_timestamp': {'type': 'string', 'title': 'Start Timestamp'},\n",
       "     'end_timestamp': {'type': 'string', 'title': 'End Timestamp'},\n",
       "     'value': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Value'},\n",
       "     'groups': {'anyOf': [{'additionalProperties': {'type': 'number'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Groups'}},\n",
       "    'type': 'object',\n",
       "    'required': ['customer_id',\n",
       "     'billable_metric_id',\n",
       "     'billable_metric_name',\n",
       "     'start_timestamp',\n",
       "     'end_timestamp',\n",
       "     'value',\n",
       "     'groups'],\n",
       "    'title': 'OrgUsage'},\n",
       "   'Organization': {'properties': {'id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Id'},\n",
       "     'display_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Display Name'},\n",
       "     'config': {'$ref': '#/components/schemas/OrganizationConfig'},\n",
       "     'connected_to_stripe': {'type': 'boolean',\n",
       "      'title': 'Connected To Stripe'},\n",
       "     'connected_to_metronome': {'type': 'boolean',\n",
       "      'title': 'Connected To Metronome'},\n",
       "     'is_personal': {'type': 'boolean', 'title': 'Is Personal'},\n",
       "     'tier': {'anyOf': [{'$ref': '#/components/schemas/PaymentPlanTier'},\n",
       "       {'type': 'null'}]},\n",
       "     'payment_method': {'anyOf': [{'$ref': '#/components/schemas/StripePaymentMethodInfo'},\n",
       "       {'type': 'null'}]},\n",
       "     'has_cancelled': {'type': 'boolean', 'title': 'Has Cancelled'},\n",
       "     'end_of_billing_period': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Of Billing Period'},\n",
       "     'current_plan': {'anyOf': [{'$ref': '#/components/schemas/CustomerVisiblePlanInfo'},\n",
       "       {'type': 'null'}]},\n",
       "     'upcoming_plan': {'anyOf': [{'$ref': '#/components/schemas/CustomerVisiblePlanInfo'},\n",
       "       {'type': 'null'}]},\n",
       "     'reached_max_workspaces': {'type': 'boolean',\n",
       "      'title': 'Reached Max Workspaces',\n",
       "      'default': False},\n",
       "     'permissions': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Permissions',\n",
       "      'default': []},\n",
       "     'marketplace_payouts_enabled': {'type': 'boolean',\n",
       "      'title': 'Marketplace Payouts Enabled',\n",
       "      'default': False},\n",
       "     'wallet': {'anyOf': [{'$ref': '#/components/schemas/Wallet'},\n",
       "       {'type': 'null'}]},\n",
       "     'default_sso_provision': {'type': 'boolean',\n",
       "      'title': 'Default Sso Provision',\n",
       "      'default': False}},\n",
       "    'type': 'object',\n",
       "    'required': ['config',\n",
       "     'connected_to_stripe',\n",
       "     'connected_to_metronome',\n",
       "     'is_personal',\n",
       "     'has_cancelled'],\n",
       "    'title': 'Organization',\n",
       "    'description': 'Information about an organization.'},\n",
       "   'OrganizationBillingInfo': {'properties': {'id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Id'},\n",
       "     'display_name': {'type': 'string', 'title': 'Display Name'},\n",
       "     'config': {'$ref': '#/components/schemas/OrganizationConfig'},\n",
       "     'connected_to_stripe': {'type': 'boolean',\n",
       "      'title': 'Connected To Stripe'},\n",
       "     'connected_to_metronome': {'type': 'boolean',\n",
       "      'title': 'Connected To Metronome'},\n",
       "     'is_personal': {'type': 'boolean', 'title': 'Is Personal'},\n",
       "     'tier': {'anyOf': [{'$ref': '#/components/schemas/PaymentPlanTier'},\n",
       "       {'type': 'null'}]},\n",
       "     'payment_method': {'anyOf': [{'$ref': '#/components/schemas/StripePaymentMethodInfo'},\n",
       "       {'type': 'null'}]},\n",
       "     'end_of_billing_period': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Of Billing Period'},\n",
       "     'current_plan': {'anyOf': [{'$ref': '#/components/schemas/CustomerVisiblePlanInfo'},\n",
       "       {'type': 'null'}]},\n",
       "     'upcoming_plan': {'anyOf': [{'$ref': '#/components/schemas/CustomerVisiblePlanInfo'},\n",
       "       {'type': 'null'}]},\n",
       "     'reached_max_workspaces': {'type': 'boolean',\n",
       "      'title': 'Reached Max Workspaces',\n",
       "      'default': False},\n",
       "     'disabled': {'type': 'boolean', 'title': 'Disabled', 'default': False},\n",
       "     'default_sso_provision': {'type': 'boolean',\n",
       "      'title': 'Default Sso Provision',\n",
       "      'default': False}},\n",
       "    'type': 'object',\n",
       "    'required': ['display_name',\n",
       "     'config',\n",
       "     'connected_to_stripe',\n",
       "     'connected_to_metronome',\n",
       "     'is_personal'],\n",
       "    'title': 'OrganizationBillingInfo',\n",
       "    'description': \"Information about an organization's billing configuration.\"},\n",
       "   'OrganizationConfig': {'properties': {'plan_tier': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Plan Tier'},\n",
       "     'max_identities': {'type': 'integer',\n",
       "      'title': 'Max Identities',\n",
       "      'default': 5},\n",
       "     'max_workspaces': {'type': 'integer',\n",
       "      'title': 'Max Workspaces',\n",
       "      'default': 1},\n",
       "     'can_use_rbac': {'type': 'boolean',\n",
       "      'title': 'Can Use Rbac',\n",
       "      'default': False},\n",
       "     'can_use_abac': {'type': 'boolean',\n",
       "      'title': 'Can Use Abac',\n",
       "      'default': False},\n",
       "     'can_use_audit_logs': {'type': 'boolean',\n",
       "      'title': 'Can Use Audit Logs',\n",
       "      'default': False},\n",
       "     'can_add_seats': {'type': 'boolean',\n",
       "      'title': 'Can Add Seats',\n",
       "      'default': True},\n",
       "     'startup_plan_approval_date': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Startup Plan Approval Date'},\n",
       "     'partner_plan_approval_date': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Partner Plan Approval Date'},\n",
       "     'premier_plan_approval_date': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Premier Plan Approval Date'},\n",
       "     'can_disable_public_sharing': {'type': 'boolean',\n",
       "      'title': 'Can Disable Public Sharing',\n",
       "      'default': False},\n",
       "     'can_serve_datasets': {'type': 'boolean',\n",
       "      'title': 'Can Serve Datasets',\n",
       "      'default': False},\n",
       "     'can_use_langgraph_cloud': {'type': 'boolean',\n",
       "      'title': 'Can Use Langgraph Cloud',\n",
       "      'default': False},\n",
       "     'max_langgraph_cloud_deployments': {'type': 'integer',\n",
       "      'title': 'Max Langgraph Cloud Deployments',\n",
       "      'default': 3},\n",
       "     'max_free_langgraph_cloud_deployments': {'type': 'integer',\n",
       "      'title': 'Max Free Langgraph Cloud Deployments',\n",
       "      'default': 0},\n",
       "     'max_sandboxes': {'type': 'integer',\n",
       "      'title': 'Max Sandboxes',\n",
       "      'default': 3},\n",
       "     'max_sandbox_volumes': {'type': 'integer',\n",
       "      'title': 'Max Sandbox Volumes',\n",
       "      'default': 3},\n",
       "     'max_sandbox_cpu': {'type': 'string',\n",
       "      'title': 'Max Sandbox Cpu',\n",
       "      'default': '4'},\n",
       "     'max_sandbox_memory': {'type': 'string',\n",
       "      'title': 'Max Sandbox Memory',\n",
       "      'default': '8Gi'},\n",
       "     'max_sandbox_storage': {'type': 'string',\n",
       "      'title': 'Max Sandbox Storage',\n",
       "      'default': '10Gi'},\n",
       "     'can_use_saml_sso': {'type': 'boolean',\n",
       "      'title': 'Can Use Saml Sso',\n",
       "      'default': False},\n",
       "     'can_use_bulk_export': {'type': 'boolean',\n",
       "      'title': 'Can Use Bulk Export',\n",
       "      'default': False},\n",
       "     'show_updated_sidenav': {'type': 'boolean',\n",
       "      'title': 'Show Updated Sidenav',\n",
       "      'default': False},\n",
       "     'show_updated_resource_tags': {'type': 'boolean',\n",
       "      'title': 'Show Updated Resource Tags',\n",
       "      'default': False},\n",
       "     'kv_dataset_message_support': {'type': 'boolean',\n",
       "      'title': 'Kv Dataset Message Support',\n",
       "      'default': True},\n",
       "     'show_playground_prompt_canvas': {'type': 'boolean',\n",
       "      'title': 'Show Playground Prompt Canvas',\n",
       "      'default': False},\n",
       "     'allow_custom_iframes': {'type': 'boolean',\n",
       "      'title': 'Allow Custom Iframes',\n",
       "      'default': False},\n",
       "     'enable_langgraph_pricing': {'type': 'boolean',\n",
       "      'title': 'Enable Langgraph Pricing',\n",
       "      'default': False},\n",
       "     'enable_thread_view_playground': {'type': 'boolean',\n",
       "      'title': 'Enable Thread View Playground',\n",
       "      'default': False},\n",
       "     'enable_org_usage_charts': {'type': 'boolean',\n",
       "      'title': 'Enable Org Usage Charts',\n",
       "      'default': False},\n",
       "     'use_exact_search_for_prompts': {'type': 'boolean',\n",
       "      'title': 'Use Exact Search For Prompts',\n",
       "      'default': False},\n",
       "     'langgraph_deploy_own_cloud_enabled': {'type': 'boolean',\n",
       "      'title': 'Langgraph Deploy Own Cloud Enabled',\n",
       "      'default': False},\n",
       "     'prompt_optimization_jobs_enabled': {'type': 'boolean',\n",
       "      'title': 'Prompt Optimization Jobs Enabled',\n",
       "      'default': False},\n",
       "     'demo_lgp_new_graph_enabled': {'type': 'boolean',\n",
       "      'title': 'Demo Lgp New Graph Enabled',\n",
       "      'default': False},\n",
       "     'datadog_rum_session_sample_rate': {'type': 'integer',\n",
       "      'title': 'Datadog Rum Session Sample Rate',\n",
       "      'default': 20},\n",
       "     'langgraph_remote_reconciler_enabled': {'type': 'boolean',\n",
       "      'title': 'Langgraph Remote Reconciler Enabled',\n",
       "      'default': False},\n",
       "     'langgraph_enterprise_enabled': {'type': 'boolean',\n",
       "      'title': 'Langgraph Enterprise Enabled',\n",
       "      'default': False},\n",
       "     'langsmith_alerts_poc_enabled': {'type': 'boolean',\n",
       "      'title': 'Langsmith Alerts Poc Enabled',\n",
       "      'default': True},\n",
       "     'tenant_skip_topk_facets': {'type': 'boolean',\n",
       "      'title': 'Tenant Skip Topk Facets',\n",
       "      'default': False},\n",
       "     'lgp_templates_enabled': {'type': 'boolean',\n",
       "      'title': 'Lgp Templates Enabled',\n",
       "      'default': False},\n",
       "     'enable_align_evaluators': {'type': 'boolean',\n",
       "      'title': 'Enable Align Evaluators',\n",
       "      'default': False},\n",
       "     'enable_run_tree_streaming': {'type': 'boolean',\n",
       "      'title': 'Enable Run Tree Streaming',\n",
       "      'default': False},\n",
       "     'enable_querying_v2_endpoints': {'type': 'boolean',\n",
       "      'title': 'Enable Querying V2 Endpoints',\n",
       "      'default': False},\n",
       "     'enable_threads_improvements': {'type': 'boolean',\n",
       "      'title': 'Enable Threads Improvements',\n",
       "      'default': False},\n",
       "     'max_prompt_webhooks': {'type': 'integer',\n",
       "      'title': 'Max Prompt Webhooks',\n",
       "      'default': 1},\n",
       "     'playground_evaluator_strategy': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Playground Evaluator Strategy',\n",
       "      'default': 'sync'},\n",
       "     'can_set_api_key_max_expiry': {'type': 'boolean',\n",
       "      'title': 'Can Set Api Key Max Expiry',\n",
       "      'default': False},\n",
       "     'enable_monthly_usage_charts': {'type': 'boolean',\n",
       "      'title': 'Enable Monthly Usage Charts',\n",
       "      'default': False},\n",
       "     'new_rule_evaluator_creation_version': {'type': 'integer',\n",
       "      'title': 'New Rule Evaluator Creation Version',\n",
       "      'default': 2},\n",
       "     'enable_lgp_listeners_page': {'type': 'boolean',\n",
       "      'title': 'Enable Lgp Listeners Page',\n",
       "      'default': False},\n",
       "     'clio_enabled': {'type': 'boolean',\n",
       "      'title': 'Clio Enabled',\n",
       "      'default': False},\n",
       "     'enable_include_extended_stats': {'type': 'boolean',\n",
       "      'title': 'Enable Include Extended Stats',\n",
       "      'default': False},\n",
       "     'enable_markdown_in_tracing': {'type': 'boolean',\n",
       "      'title': 'Enable Markdown In Tracing',\n",
       "      'default': False},\n",
       "     'enable_pricing_redesign': {'type': 'boolean',\n",
       "      'title': 'Enable Pricing Redesign',\n",
       "      'default': False},\n",
       "     'arbitrary_cost_tracking_enabled': {'type': 'boolean',\n",
       "      'title': 'Arbitrary Cost Tracking Enabled',\n",
       "      'default': False},\n",
       "     'langsmith_deployment_distributed_runtime_enabled': {'type': 'boolean',\n",
       "      'title': 'Langsmith Deployment Distributed Runtime Enabled',\n",
       "      'default': False},\n",
       "     'pairwise_annotation_queues_enabled': {'type': 'boolean',\n",
       "      'title': 'Pairwise Annotation Queues Enabled',\n",
       "      'default': False},\n",
       "     'agent_builder_enabled': {'type': 'boolean',\n",
       "      'title': 'Agent Builder Enabled',\n",
       "      'default': True},\n",
       "     'max_agent_builder_assistants': {'type': 'integer',\n",
       "      'title': 'Max Agent Builder Assistants',\n",
       "      'default': 1000},\n",
       "     'enable_granular_usage_reporting': {'type': 'boolean',\n",
       "      'title': 'Enable Granular Usage Reporting',\n",
       "      'default': False},\n",
       "     'enable_burndown_vs_commit_view': {'type': 'boolean',\n",
       "      'title': 'Enable Burndown Vs Commit View',\n",
       "      'default': False}},\n",
       "    'type': 'object',\n",
       "    'title': 'OrganizationConfig',\n",
       "    'description': 'Organization level configuration. May include any field that exists in tenant config and additional fields.'},\n",
       "   'OrganizationCreate': {'properties': {'display_name': {'type': 'string',\n",
       "      'minLength': 1,\n",
       "      'pattern': '^[a-zA-Z0-9\\\\-_ ]+$',\n",
       "      'title': 'Display Name'},\n",
       "     'is_personal': {'type': 'boolean', 'title': 'Is Personal'}},\n",
       "    'type': 'object',\n",
       "    'required': ['display_name', 'is_personal'],\n",
       "    'title': 'OrganizationCreate',\n",
       "    'description': 'Create organization schema.'},\n",
       "   'OrganizationDashboardColorScheme': {'type': 'string',\n",
       "    'enum': ['light', 'dark'],\n",
       "    'title': 'OrganizationDashboardColorScheme',\n",
       "    'description': 'Enum for acceptable color schemes of dashboards.'},\n",
       "   'OrganizationDashboardSchema': {'properties': {'embeddable_url': {'type': 'string',\n",
       "      'title': 'Embeddable Url'}},\n",
       "    'type': 'object',\n",
       "    'required': ['embeddable_url'],\n",
       "    'title': 'OrganizationDashboardSchema',\n",
       "    'description': 'Organization dashboard for usage or invoices.'},\n",
       "   'OrganizationDashboardType': {'type': 'string',\n",
       "    'enum': ['invoices', 'usage', 'credits'],\n",
       "    'title': 'OrganizationDashboardType',\n",
       "    'description': 'Enum for acceptable types of dashboards.'},\n",
       "   'OrganizationInfo': {'properties': {'id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Id'},\n",
       "     'display_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Display Name'},\n",
       "     'config': {'$ref': '#/components/schemas/OrganizationConfig'},\n",
       "     'is_personal': {'type': 'boolean', 'title': 'Is Personal'},\n",
       "     'tier': {'anyOf': [{'$ref': '#/components/schemas/PaymentPlanTier'},\n",
       "       {'type': 'null'}]},\n",
       "     'reached_max_workspaces': {'type': 'boolean',\n",
       "      'title': 'Reached Max Workspaces',\n",
       "      'default': False},\n",
       "     'permissions': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Permissions',\n",
       "      'default': []},\n",
       "     'disabled': {'type': 'boolean', 'title': 'Disabled', 'default': False},\n",
       "     'member_disabled': {'type': 'boolean',\n",
       "      'title': 'Member Disabled',\n",
       "      'default': False},\n",
       "     'sso_only': {'type': 'boolean', 'title': 'Sso Only', 'default': False},\n",
       "     'jit_provisioning_enabled': {'type': 'boolean',\n",
       "      'title': 'Jit Provisioning Enabled',\n",
       "      'default': True},\n",
       "     'invites_enabled': {'type': 'boolean',\n",
       "      'title': 'Invites Enabled',\n",
       "      'default': True},\n",
       "     'sso_login_slug': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Sso Login Slug'},\n",
       "     'public_sharing_disabled': {'type': 'boolean',\n",
       "      'title': 'Public Sharing Disabled',\n",
       "      'default': False},\n",
       "     'pat_creation_disabled': {'type': 'boolean',\n",
       "      'title': 'Pat Creation Disabled',\n",
       "      'default': False},\n",
       "     'workspace_admin_can_invite_to_org': {'type': 'boolean',\n",
       "      'title': 'Workspace Admin Can Invite To Org',\n",
       "      'default': False},\n",
       "     'marketplace_payouts_enabled': {'type': 'boolean',\n",
       "      'title': 'Marketplace Payouts Enabled',\n",
       "      'default': False},\n",
       "     'wallet': {'anyOf': [{'$ref': '#/components/schemas/Wallet'},\n",
       "       {'type': 'null'}]},\n",
       "     'default_sso_provision': {'type': 'boolean',\n",
       "      'title': 'Default Sso Provision',\n",
       "      'default': False},\n",
       "     'max_api_key_expiry_days': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Max Api Key Expiry Days'}},\n",
       "    'type': 'object',\n",
       "    'required': ['config', 'is_personal'],\n",
       "    'title': 'OrganizationInfo',\n",
       "    'description': 'Information about an organization.'},\n",
       "   'OrganizationMembers': {'properties': {'organization_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Organization Id'},\n",
       "     'members': {'items': {'$ref': '#/components/schemas/OrgMemberIdentity'},\n",
       "      'type': 'array',\n",
       "      'title': 'Members'},\n",
       "     'pending': {'items': {'$ref': '#/components/schemas/OrgPendingIdentity'},\n",
       "      'type': 'array',\n",
       "      'title': 'Pending'}},\n",
       "    'type': 'object',\n",
       "    'required': ['organization_id', 'members', 'pending'],\n",
       "    'title': 'OrganizationMembers',\n",
       "    'description': 'Organization members schema.'},\n",
       "   'OrganizationPGSchemaSlim': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'display_name': {'type': 'string', 'title': 'Display Name'},\n",
       "     'tier': {'anyOf': [{'$ref': '#/components/schemas/PaymentPlanTier'},\n",
       "       {'type': 'null'}]},\n",
       "     'created_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Created At'},\n",
       "     'created_by_user_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Created By User Id'},\n",
       "     'created_by_ls_user_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Created By Ls User Id'},\n",
       "     'modified_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Modified At'},\n",
       "     'is_personal': {'type': 'boolean', 'title': 'Is Personal'},\n",
       "     'disabled': {'type': 'boolean', 'title': 'Disabled'},\n",
       "     'sso_login_slug': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Sso Login Slug'},\n",
       "     'sso_only': {'type': 'boolean', 'title': 'Sso Only', 'default': False},\n",
       "     'jit_provisioning_enabled': {'type': 'boolean',\n",
       "      'title': 'Jit Provisioning Enabled',\n",
       "      'default': True},\n",
       "     'invites_enabled': {'type': 'boolean',\n",
       "      'title': 'Invites Enabled',\n",
       "      'default': True},\n",
       "     'public_sharing_disabled': {'type': 'boolean',\n",
       "      'title': 'Public Sharing Disabled',\n",
       "      'default': False},\n",
       "     'pat_creation_disabled': {'type': 'boolean',\n",
       "      'title': 'Pat Creation Disabled',\n",
       "      'default': False},\n",
       "     'workspace_admin_can_invite_to_org': {'type': 'boolean',\n",
       "      'title': 'Workspace Admin Can Invite To Org',\n",
       "      'default': False},\n",
       "     'default_sso_provision': {'type': 'boolean',\n",
       "      'title': 'Default Sso Provision',\n",
       "      'default': False},\n",
       "     'max_api_key_expiry_days': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Max Api Key Expiry Days'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'display_name', 'is_personal', 'disabled'],\n",
       "    'title': 'OrganizationPGSchemaSlim',\n",
       "    'description': 'Schema for an organization in postgres for list views.'},\n",
       "   'OrganizationUpdate': {'properties': {'display_name': {'type': 'string',\n",
       "      'minLength': 1,\n",
       "      'pattern': '^[a-zA-Z0-9\\\\-_ ]+$',\n",
       "      'title': 'Display Name'},\n",
       "     'public_sharing_disabled': {'anyOf': [{'type': 'boolean'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Public Sharing Disabled'},\n",
       "     'pat_creation_disabled': {'anyOf': [{'type': 'boolean'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Pat Creation Disabled'},\n",
       "     'unshare_all': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Unshare All'},\n",
       "     'jit_provisioning_enabled': {'anyOf': [{'type': 'boolean'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Jit Provisioning Enabled'},\n",
       "     'workspace_admin_can_invite_to_org': {'anyOf': [{'type': 'boolean'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Admin Can Invite To Org'},\n",
       "     'invites_enabled': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Invites Enabled'},\n",
       "     'max_api_key_expiry_days': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Max Api Key Expiry Days'}},\n",
       "    'type': 'object',\n",
       "    'title': 'OrganizationUpdate',\n",
       "    'description': 'Update organization schema.'},\n",
       "   'OutputTokenDetails': {'properties': {'audio': {'type': 'integer',\n",
       "      'title': 'Audio'},\n",
       "     'reasoning': {'type': 'integer', 'title': 'Reasoning'}},\n",
       "    'type': 'object',\n",
       "    'title': 'OutputTokenDetails',\n",
       "    'description': 'Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n    ```python\\n    {\\n        \"audio\": 10,\\n        \"reasoning\": 200,\\n    }\\n    ```\\n\\nMay also hold extra provider-specific keys.\\n\\n!!! version-added \"Added in `langchain-core` 0.3.9\"'},\n",
       "   'PagerdutySeverity': {'type': 'string',\n",
       "    'enum': ['critical', 'warning', 'error', 'info'],\n",
       "    'title': 'PagerdutySeverity',\n",
       "    'description': 'Enum for severity.'},\n",
       "   'PaymentPlanTier': {'type': 'string',\n",
       "    'enum': ['no_plan',\n",
       "     'developer',\n",
       "     'plus',\n",
       "     'enterprise',\n",
       "     'developer_legacy',\n",
       "     'plus_legacy',\n",
       "     'free',\n",
       "     'enterprise_legacy',\n",
       "     'startup',\n",
       "     'startup_v0',\n",
       "     'partner',\n",
       "     'premier'],\n",
       "    'title': 'PaymentPlanTier'},\n",
       "   'PendingIdentity': {'properties': {'email': {'type': 'string',\n",
       "      'title': 'Email'},\n",
       "     'read_only': {'type': 'boolean',\n",
       "      'title': 'Read Only',\n",
       "      'default': False,\n",
       "      'deprecated': True},\n",
       "     'role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Role Id'},\n",
       "     'workspace_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Ids'},\n",
       "     'workspace_role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Role Id'},\n",
       "     'password': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Password'},\n",
       "     'full_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Full Name'},\n",
       "     'access_scope': {'$ref': '#/components/schemas/AccessScope',\n",
       "      'default': 'workspace'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'user_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'User Id'},\n",
       "     'tenant_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tenant Id'},\n",
       "     'organization_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Organization Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'role_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Role Name'},\n",
       "     'org_role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Org Role Id'},\n",
       "     'org_role_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Org Role Name'}},\n",
       "    'type': 'object',\n",
       "    'required': ['email', 'id', 'created_at'],\n",
       "    'title': 'PendingIdentity'},\n",
       "   'PendingIdentityCreate': {'properties': {'email': {'type': 'string',\n",
       "      'title': 'Email'},\n",
       "     'read_only': {'type': 'boolean',\n",
       "      'title': 'Read Only',\n",
       "      'default': False,\n",
       "      'deprecated': True},\n",
       "     'role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Role Id'},\n",
       "     'workspace_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Ids'},\n",
       "     'workspace_role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Workspace Role Id'},\n",
       "     'password': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Password'},\n",
       "     'full_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Full Name'}},\n",
       "    'type': 'object',\n",
       "    'required': ['email'],\n",
       "    'title': 'PendingIdentityCreate'},\n",
       "   'PermissionResponse': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'description': {'type': 'string', 'title': 'Description'},\n",
       "     'access_scope': {'$ref': '#/components/schemas/AccessScope'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'description', 'access_scope'],\n",
       "    'title': 'PermissionResponse'},\n",
       "   'PlaygroundPromptCanvasPayload': {'properties': {'messages': {'items': {'oneOf': [{'$ref': '#/components/schemas/AIMessage'},\n",
       "        {'$ref': '#/components/schemas/HumanMessage'},\n",
       "        {'$ref': '#/components/schemas/ChatMessage'},\n",
       "        {'$ref': '#/components/schemas/SystemMessage'},\n",
       "        {'$ref': '#/components/schemas/FunctionMessage'},\n",
       "        {'$ref': '#/components/schemas/ToolMessage'},\n",
       "        {'$ref': '#/components/schemas/AIMessageChunk'},\n",
       "        {'$ref': '#/components/schemas/HumanMessageChunk'},\n",
       "        {'$ref': '#/components/schemas/ChatMessageChunk'},\n",
       "        {'$ref': '#/components/schemas/SystemMessageChunk'},\n",
       "        {'$ref': '#/components/schemas/FunctionMessageChunk'},\n",
       "        {'$ref': '#/components/schemas/ToolMessageChunk'}]},\n",
       "      'type': 'array',\n",
       "      'title': 'Messages'},\n",
       "     'highlighted': {'anyOf': [{'$ref': '#/components/schemas/Highlight'},\n",
       "       {'type': 'null'}]},\n",
       "     'artifact': {'anyOf': [{'$ref': '#/components/schemas/Artifact'},\n",
       "       {'type': 'null'}]},\n",
       "     'artifact_length': {'anyOf': [{'type': 'string',\n",
       "        'enum': ['shortest', 'short', 'long', 'longest']},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Artifact Length'},\n",
       "     'reading_level': {'anyOf': [{'type': 'string',\n",
       "        'enum': ['child', 'teenager', 'college', 'phd']},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reading Level'},\n",
       "     'custom_action': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Custom Action'},\n",
       "     'template_format': {'type': 'string',\n",
       "      'enum': ['f-string', 'mustache'],\n",
       "      'title': 'Template Format'},\n",
       "     'secrets': {'additionalProperties': {'type': 'string'},\n",
       "      'type': 'object',\n",
       "      'title': 'Secrets'}},\n",
       "    'type': 'object',\n",
       "    'required': ['messages', 'template_format', 'secrets'],\n",
       "    'title': 'PlaygroundPromptCanvasPayload'},\n",
       "   'PlaygroundRunOverDatasetBatchRequestSchema': {'properties': {'manifest': {'title': 'Manifest'},\n",
       "     'secrets': {'additionalProperties': {'type': 'string'},\n",
       "      'type': 'object',\n",
       "      'title': 'Secrets'},\n",
       "     'run_id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Run Id'},\n",
       "     'repo_id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Repo Id'},\n",
       "     'tools': {'anyOf': [{'items': {}, 'type': 'array'}, {'type': 'null'}],\n",
       "      'title': 'Tools'},\n",
       "     'tool_choice': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tool Choice'},\n",
       "     'parallel_tool_calls': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Parallel Tool Calls'},\n",
       "     'options': {'$ref': '#/components/schemas/RunnableConfig'},\n",
       "     'project_name': {'type': 'string', 'title': 'Project Name'},\n",
       "     'repo_handle': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Repo Handle'},\n",
       "     'owner': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Owner'},\n",
       "     'commit': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Commit'},\n",
       "     'evaluator_rules': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Evaluator Rules'},\n",
       "     'requests_per_second': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Requests Per Second'},\n",
       "     'use_or_fallback_to_workspace_secrets': {'type': 'boolean',\n",
       "      'title': 'Use Or Fallback To Workspace Secrets',\n",
       "      'default': False},\n",
       "     'runner_context': {'anyOf': [{'$ref': '#/components/schemas/RunnerContextEnum'},\n",
       "       {'type': 'null'}],\n",
       "      'default': 'langsmith_ui'},\n",
       "     'dataset_id': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'},\n",
       "     'dataset_splits': {'anyOf': [{'items': {'type': 'string'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Dataset Splits'},\n",
       "     'repetitions': {'type': 'integer',\n",
       "      'maximum': 30.0,\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Repetitions',\n",
       "      'default': 1},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'batch_size': {'anyOf': [{'type': 'integer',\n",
       "        'maximum': 100.0,\n",
       "        'minimum': 1.0},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Batch Size'}},\n",
       "    'type': 'object',\n",
       "    'required': ['manifest',\n",
       "     'secrets',\n",
       "     'options',\n",
       "     'project_name',\n",
       "     'dataset_id'],\n",
       "    'title': 'PlaygroundRunOverDatasetBatchRequestSchema'},\n",
       "   'PlaygroundRunOverDatasetRequestSchema': {'properties': {'manifest': {'title': 'Manifest'},\n",
       "     'secrets': {'additionalProperties': {'type': 'string'},\n",
       "      'type': 'object',\n",
       "      'title': 'Secrets'},\n",
       "     'run_id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Run Id'},\n",
       "     'repo_id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Repo Id'},\n",
       "     'tools': {'anyOf': [{'items': {}, 'type': 'array'}, {'type': 'null'}],\n",
       "      'title': 'Tools'},\n",
       "     'tool_choice': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tool Choice'},\n",
       "     'parallel_tool_calls': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Parallel Tool Calls'},\n",
       "     'options': {'$ref': '#/components/schemas/RunnableConfig'},\n",
       "     'project_name': {'type': 'string', 'title': 'Project Name'},\n",
       "     'repo_handle': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Repo Handle'},\n",
       "     'owner': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Owner'},\n",
       "     'commit': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Commit'},\n",
       "     'evaluator_rules': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Evaluator Rules'},\n",
       "     'requests_per_second': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Requests Per Second'},\n",
       "     'use_or_fallback_to_workspace_secrets': {'type': 'boolean',\n",
       "      'title': 'Use Or Fallback To Workspace Secrets',\n",
       "      'default': False},\n",
       "     'runner_context': {'anyOf': [{'$ref': '#/components/schemas/RunnerContextEnum'},\n",
       "       {'type': 'null'}],\n",
       "      'default': 'langsmith_ui'},\n",
       "     'dataset_id': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'},\n",
       "     'dataset_splits': {'anyOf': [{'items': {'type': 'string'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Dataset Splits'},\n",
       "     'repetitions': {'type': 'integer',\n",
       "      'maximum': 30.0,\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Repetitions',\n",
       "      'default': 1},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'}},\n",
       "    'type': 'object',\n",
       "    'required': ['manifest',\n",
       "     'secrets',\n",
       "     'options',\n",
       "     'project_name',\n",
       "     'dataset_id'],\n",
       "    'title': 'PlaygroundRunOverDatasetRequestSchema'},\n",
       "   'PlaygroundSavedOptions': {'properties': {'requests_per_second': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Requests Per Second'}},\n",
       "    'type': 'object',\n",
       "    'title': 'PlaygroundSavedOptions'},\n",
       "   'PlaygroundSettingsCreateRequest': {'properties': {'name': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'settings': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Settings'},\n",
       "     'options': {'anyOf': [{'$ref': '#/components/schemas/PlaygroundSavedOptions'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'required': ['settings'],\n",
       "    'title': 'PlaygroundSettingsCreateRequest'},\n",
       "   'PlaygroundSettingsResponse': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'settings': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Settings'},\n",
       "     'options': {'anyOf': [{'$ref': '#/components/schemas/PlaygroundSavedOptions'},\n",
       "       {'type': 'null'}]},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'settings', 'created_at', 'updated_at'],\n",
       "    'title': 'PlaygroundSettingsResponse'},\n",
       "   'PlaygroundSettingsUpdateRequest': {'properties': {'name': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'settings': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Settings'},\n",
       "     'options': {'anyOf': [{'$ref': '#/components/schemas/PlaygroundSavedOptions'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'title': 'PlaygroundSettingsUpdateRequest'},\n",
       "   'PopulateAnnotationQueueSchema': {'properties': {'queue_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Queue Id'},\n",
       "     'session_ids': {'items': {'type': 'string', 'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'title': 'Session Ids'}},\n",
       "    'type': 'object',\n",
       "    'required': ['queue_id', 'session_ids'],\n",
       "    'title': 'PopulateAnnotationQueueSchema'},\n",
       "   'PromptOptimizationJob': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'repo_id': {'type': 'string', 'format': 'uuid', 'title': 'Repo Id'},\n",
       "     'status': {'$ref': '#/components/schemas/EPromptOptimizationJobStatus'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'algorithm': {'$ref': '#/components/schemas/EPromptOptimizationAlgorithm'},\n",
       "     'config': {'anyOf': [{'$ref': '#/components/schemas/PromptimConfig'},\n",
       "       {'$ref': '#/components/schemas/DemoConfig'}],\n",
       "      'title': 'Config'},\n",
       "     'results': {'items': {'$ref': '#/components/schemas/PromptOptimizationResult'},\n",
       "      'type': 'array',\n",
       "      'title': 'Results'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'repo_id',\n",
       "     'status',\n",
       "     'tenant_id',\n",
       "     'algorithm',\n",
       "     'config',\n",
       "     'created_at',\n",
       "     'updated_at'],\n",
       "    'title': 'PromptOptimizationJob'},\n",
       "   'PromptOptimizationJobCreate': {'properties': {'algorithm': {'$ref': '#/components/schemas/EPromptOptimizationAlgorithm'},\n",
       "     'config': {'anyOf': [{'$ref': '#/components/schemas/PromptimConfig'},\n",
       "       {'$ref': '#/components/schemas/DemoConfig'}],\n",
       "      'title': 'Config'}},\n",
       "    'type': 'object',\n",
       "    'required': ['algorithm', 'config'],\n",
       "    'title': 'PromptOptimizationJobCreate'},\n",
       "   'PromptOptimizationJobLog': {'properties': {'log_type': {'$ref': '#/components/schemas/EPromptOptimizationJobLogType'},\n",
       "     'message': {'type': 'string', 'title': 'Message'},\n",
       "     'data': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Data'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'job_id': {'type': 'string', 'format': 'uuid', 'title': 'Job Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'}},\n",
       "    'type': 'object',\n",
       "    'required': ['log_type', 'message', 'id', 'job_id', 'created_at'],\n",
       "    'title': 'PromptOptimizationJobLog'},\n",
       "   'PromptOptimizationJobLogCreate': {'properties': {'log_type': {'$ref': '#/components/schemas/EPromptOptimizationJobLogType'},\n",
       "     'message': {'type': 'string', 'title': 'Message'},\n",
       "     'data': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Data'}},\n",
       "    'type': 'object',\n",
       "    'required': ['log_type', 'message'],\n",
       "    'title': 'PromptOptimizationJobLogCreate'},\n",
       "   'PromptOptimizationJobUpdate': {'properties': {'status': {'anyOf': [{'$ref': '#/components/schemas/EPromptOptimizationJobStatus'},\n",
       "       {'type': 'null'}]},\n",
       "     'result': {'anyOf': [{'$ref': '#/components/schemas/PromptOptimizationResult'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'title': 'PromptOptimizationJobUpdate'},\n",
       "   'PromptOptimizationJobWithLogs': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'repo_id': {'type': 'string', 'format': 'uuid', 'title': 'Repo Id'},\n",
       "     'status': {'$ref': '#/components/schemas/EPromptOptimizationJobStatus'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'algorithm': {'$ref': '#/components/schemas/EPromptOptimizationAlgorithm'},\n",
       "     'config': {'anyOf': [{'$ref': '#/components/schemas/PromptimConfig'},\n",
       "       {'$ref': '#/components/schemas/DemoConfig'}],\n",
       "      'title': 'Config'},\n",
       "     'results': {'items': {'$ref': '#/components/schemas/PromptOptimizationResult'},\n",
       "      'type': 'array',\n",
       "      'title': 'Results'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'logs': {'items': {'$ref': '#/components/schemas/PromptOptimizationJobLog'},\n",
       "      'type': 'array',\n",
       "      'title': 'Logs'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'repo_id',\n",
       "     'status',\n",
       "     'tenant_id',\n",
       "     'algorithm',\n",
       "     'config',\n",
       "     'created_at',\n",
       "     'updated_at',\n",
       "     'logs'],\n",
       "    'title': 'PromptOptimizationJobWithLogs'},\n",
       "   'PromptOptimizationResult': {'properties': {'timestamp': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Timestamp'},\n",
       "     'x': {'type': 'number', 'title': 'X'},\n",
       "     'y': {'type': 'number', 'title': 'Y'}},\n",
       "    'type': 'object',\n",
       "    'required': ['timestamp', 'x', 'y'],\n",
       "    'title': 'PromptOptimizationResult'},\n",
       "   'PromptWebhook': {'properties': {'url': {'type': 'string',\n",
       "      'minLength': 1,\n",
       "      'format': 'uri',\n",
       "      'title': 'Url'},\n",
       "     'headers': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Headers'},\n",
       "     'include_prompts': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Include Prompts'},\n",
       "     'exclude_prompts': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Exclude Prompts'},\n",
       "     'triggers': {'items': {'$ref': '#/components/schemas/EPromptWebhookTrigger'},\n",
       "      'type': 'array',\n",
       "      'title': 'Triggers'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'}},\n",
       "    'type': 'object',\n",
       "    'required': ['url', 'id', 'tenant_id', 'created_at', 'updated_at'],\n",
       "    'title': 'PromptWebhook',\n",
       "    'description': 'Schema for a prompt webhook.'},\n",
       "   'PromptWebhookBase': {'properties': {'url': {'type': 'string',\n",
       "      'minLength': 1,\n",
       "      'format': 'uri',\n",
       "      'title': 'Url'},\n",
       "     'headers': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Headers'},\n",
       "     'include_prompts': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Include Prompts'},\n",
       "     'exclude_prompts': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Exclude Prompts'},\n",
       "     'triggers': {'items': {'$ref': '#/components/schemas/EPromptWebhookTrigger'},\n",
       "      'type': 'array',\n",
       "      'title': 'Triggers'}},\n",
       "    'type': 'object',\n",
       "    'required': ['url'],\n",
       "    'title': 'PromptWebhookBase',\n",
       "    'description': 'Base schema for prompt webhooks.'},\n",
       "   'PromptWebhookCreate': {'properties': {'url': {'type': 'string',\n",
       "      'minLength': 1,\n",
       "      'format': 'uri',\n",
       "      'title': 'Url'},\n",
       "     'headers': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Headers'},\n",
       "     'include_prompts': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Include Prompts'},\n",
       "     'exclude_prompts': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Exclude Prompts'},\n",
       "     'triggers': {'items': {'$ref': '#/components/schemas/EPromptWebhookTrigger'},\n",
       "      'type': 'array',\n",
       "      'title': 'Triggers'},\n",
       "     'id': {'anyOf': [{'type': 'string', 'format': 'uuid'}, {'type': 'null'}],\n",
       "      'title': 'Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['url'],\n",
       "    'title': 'PromptWebhookCreate',\n",
       "    'description': 'Schema for creating a prompt webhook.'},\n",
       "   'PromptWebhookPayload': {'properties': {'prompt_id': {'type': 'string',\n",
       "      'title': 'Prompt Id'},\n",
       "     'prompt_name': {'type': 'string', 'title': 'Prompt Name'},\n",
       "     'manifest': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Manifest'},\n",
       "     'commit_hash': {'type': 'string', 'title': 'Commit Hash'},\n",
       "     'created_at': {'type': 'string', 'title': 'Created At'},\n",
       "     'created_by': {'type': 'string', 'title': 'Created By'},\n",
       "     'event': {'$ref': '#/components/schemas/EPromptWebhookTrigger'},\n",
       "     'tag_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tag Name'}},\n",
       "    'type': 'object',\n",
       "    'required': ['prompt_id',\n",
       "     'prompt_name',\n",
       "     'manifest',\n",
       "     'commit_hash',\n",
       "     'created_at',\n",
       "     'created_by',\n",
       "     'event'],\n",
       "    'title': 'PromptWebhookPayload'},\n",
       "   'PromptWebhookTest': {'properties': {'webhook': {'$ref': '#/components/schemas/PromptWebhookBase'},\n",
       "     'payload': {'$ref': '#/components/schemas/PromptWebhookPayload'}},\n",
       "    'type': 'object',\n",
       "    'required': ['webhook', 'payload'],\n",
       "    'title': 'PromptWebhookTest',\n",
       "    'description': 'Schema for testing a prompt webhook.'},\n",
       "   'PromptWebhookUpdate': {'properties': {'include_prompts': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Include Prompts'},\n",
       "     'exclude_prompts': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Exclude Prompts'},\n",
       "     'url': {'anyOf': [{'type': 'string', 'minLength': 1, 'format': 'uri'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Url'},\n",
       "     'headers': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Headers'},\n",
       "     'triggers': {'anyOf': [{'items': {'$ref': '#/components/schemas/EPromptWebhookTrigger'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Triggers'}},\n",
       "    'type': 'object',\n",
       "    'title': 'PromptWebhookUpdate',\n",
       "    'description': 'Schema for updating a prompt webhook.'},\n",
       "   'PromptimConfig': {'properties': {'message_index': {'type': 'integer',\n",
       "      'title': 'Message Index'},\n",
       "     'task_description': {'type': 'string', 'title': 'Task Description'},\n",
       "     'dataset_name': {'type': 'string', 'title': 'Dataset Name'},\n",
       "     'train_split': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Train Split'},\n",
       "     'dev_split': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Dev Split'},\n",
       "     'test_split': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Test Split'},\n",
       "     'evaluators': {'items': {'type': 'string', 'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'title': 'Evaluators'},\n",
       "     'num_epochs': {'type': 'integer', 'title': 'Num Epochs'},\n",
       "     'auto_commit': {'type': 'boolean', 'title': 'Auto Commit'}},\n",
       "    'type': 'object',\n",
       "    'required': ['message_index',\n",
       "     'task_description',\n",
       "     'dataset_name',\n",
       "     'train_split',\n",
       "     'dev_split',\n",
       "     'test_split',\n",
       "     'evaluators',\n",
       "     'num_epochs',\n",
       "     'auto_commit'],\n",
       "    'title': 'PromptimConfig'},\n",
       "   'ProviderUserSlim': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'provider': {'anyOf': [{'$ref': '#/components/schemas/AuthProvider'},\n",
       "       {'type': 'null'}]},\n",
       "     'ls_user_id': {'type': 'string', 'format': 'uuid', 'title': 'Ls User Id'},\n",
       "     'saml_provider_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Saml Provider Id'},\n",
       "     'provider_user_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Provider User Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'email': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Email'},\n",
       "     'full_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Full Name'},\n",
       "     'first_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'First Name'},\n",
       "     'last_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Last Name'},\n",
       "     'username': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Username'},\n",
       "     'is_disabled': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Is Disabled'},\n",
       "     'provisioning_method': {'anyOf': [{'$ref': '#/components/schemas/ProvisioningMethod'},\n",
       "       {'type': 'null'}]},\n",
       "     'email_confirmed_at': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Email Confirmed At'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'ls_user_id', 'created_at', 'updated_at'],\n",
       "    'title': 'ProviderUserSlim'},\n",
       "   'ProvisioningMethod': {'type': 'string',\n",
       "    'enum': ['scim', 'saml:jit', 'bootstrap'],\n",
       "    'title': 'ProvisioningMethod'},\n",
       "   'ProxyRequest': {'properties': {'url': {'type': 'string', 'title': 'Url'},\n",
       "     'method': {'type': 'string',\n",
       "      'enum': ['GET', 'POST', 'PUT', 'DELETE', 'PATCH', 'HEAD', 'OPTIONS'],\n",
       "      'title': 'Method',\n",
       "      'default': 'GET'},\n",
       "     'headers': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Headers',\n",
       "      'default': {}},\n",
       "     'timeout': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Timeout',\n",
       "      'default': 120},\n",
       "     'body': {'anyOf': [{}, {'type': 'null'}], 'title': 'Body'},\n",
       "     'oauth_provider_id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Oauth Provider Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['url'],\n",
       "    'title': 'ProxyRequest'},\n",
       "   'PublicComparativeExperiment': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'modified_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Modified At'},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'},\n",
       "     'experiments_info': {'items': {'$ref': '#/components/schemas/SimpleExperimentInfo'},\n",
       "      'type': 'array',\n",
       "      'title': 'Experiments Info'},\n",
       "     'feedback_stats': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Stats'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'created_at', 'modified_at', 'experiments_info'],\n",
       "    'title': 'PublicComparativeExperiment',\n",
       "    'description': 'Publicly-shared ComparativeExperiment schema.'},\n",
       "   'PublicExampleWithRuns': {'properties': {'outputs': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs'},\n",
       "     'dataset_id': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'},\n",
       "     'source_run_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Source Run Id'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'inputs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Inputs'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'modified_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Modified At'},\n",
       "     'attachment_urls': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Attachment Urls'},\n",
       "     'runs': {'items': {'$ref': '#/components/schemas/RunPublicDatasetSchema'},\n",
       "      'type': 'array',\n",
       "      'title': 'Runs'}},\n",
       "    'type': 'object',\n",
       "    'required': ['dataset_id', 'inputs', 'id', 'name', 'runs'],\n",
       "    'title': 'PublicExampleWithRuns',\n",
       "    'description': 'Schema for an example in a publicly-shared dataset with list of runs.'},\n",
       "   'PutDatasetVersionsSchema': {'properties': {'as_of': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'string'}],\n",
       "      'title': 'As Of',\n",
       "      'description': 'Only modifications made on or before this time are included. If None, the latest version of the dataset is used.'},\n",
       "     'tag': {'type': 'string', 'title': 'Tag'}},\n",
       "    'type': 'object',\n",
       "    'required': ['as_of', 'tag'],\n",
       "    'title': 'PutDatasetVersionsSchema'},\n",
       "   'QueryExampleSchemaWithRuns': {'properties': {'session_ids': {'items': {'type': 'string',\n",
       "       'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'minItems': 1,\n",
       "      'title': 'Session Ids'},\n",
       "     'offset': {'type': 'integer',\n",
       "      'minimum': 0.0,\n",
       "      'title': 'Offset',\n",
       "      'default': 0},\n",
       "     'limit': {'type': 'integer',\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Limit',\n",
       "      'default': 10},\n",
       "     'preview': {'type': 'boolean', 'title': 'Preview', 'default': False},\n",
       "     'comparative_experiment_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Comparative Experiment Id'},\n",
       "     'sort_params': {'anyOf': [{'$ref': '#/components/schemas/SortParamsForRunsComparisonView'},\n",
       "       {'type': 'null'}]},\n",
       "     'filters': {'anyOf': [{'additionalProperties': {'items': {'type': 'string'},\n",
       "         'type': 'array'},\n",
       "        'propertyNames': {'format': 'uuid'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Filters'},\n",
       "     'stream': {'type': 'boolean', 'title': 'Stream', 'default': False}},\n",
       "    'type': 'object',\n",
       "    'required': ['session_ids'],\n",
       "    'title': 'QueryExampleSchemaWithRuns'},\n",
       "   'QueryExampleSchemaWithRunsRequest': {'properties': {'session_ids': {'items': {'type': 'string',\n",
       "       'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'minItems': 1,\n",
       "      'title': 'Session Ids'},\n",
       "     'offset': {'type': 'integer',\n",
       "      'minimum': 0.0,\n",
       "      'title': 'Offset',\n",
       "      'default': 0},\n",
       "     'limit': {'anyOf': [{'type': 'integer', 'minimum': 1.0},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Limit'},\n",
       "     'preview': {'type': 'boolean', 'title': 'Preview', 'default': False},\n",
       "     'comparative_experiment_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Comparative Experiment Id'},\n",
       "     'sort_params': {'anyOf': [{'$ref': '#/components/schemas/SortParamsForRunsComparisonView'},\n",
       "       {'type': 'null'}]},\n",
       "     'filters': {'anyOf': [{'additionalProperties': {'items': {'type': 'string'},\n",
       "         'type': 'array'},\n",
       "        'propertyNames': {'format': 'uuid'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Filters'},\n",
       "     'stream': {'type': 'boolean', 'title': 'Stream', 'default': False}},\n",
       "    'type': 'object',\n",
       "    'required': ['session_ids'],\n",
       "    'title': 'QueryExampleSchemaWithRunsRequest',\n",
       "    'description': 'Request DTO for querying examples with runs - used for API input.\\n\\nThis is separate from the internal schema to cleanly handle optional limit values.\\nWhen limit is None, the internal schema will apply appropriate defaults based on\\nformat and stream settings.'},\n",
       "   'QueryFeedbackDelta': {'properties': {'baseline_session_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Baseline Session Id'},\n",
       "     'comparison_session_ids': {'items': {'type': 'string', 'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'title': 'Comparison Session Ids'},\n",
       "     'feedback_key': {'type': 'string', 'title': 'Feedback Key'},\n",
       "     'filters': {'anyOf': [{'additionalProperties': {'items': {'type': 'string'},\n",
       "         'type': 'array'},\n",
       "        'propertyNames': {'format': 'uuid'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Filters'},\n",
       "     'offset': {'type': 'integer',\n",
       "      'minimum': 0.0,\n",
       "      'title': 'Offset',\n",
       "      'default': 0},\n",
       "     'limit': {'type': 'integer',\n",
       "      'maximum': 100.0,\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Limit',\n",
       "      'default': 100},\n",
       "     'comparative_experiment_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Comparative Experiment Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['baseline_session_id',\n",
       "     'comparison_session_ids',\n",
       "     'feedback_key'],\n",
       "    'title': 'QueryFeedbackDelta'},\n",
       "   'QueryFeedbackDeltaBatch': {'properties': {'baseline_session_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Baseline Session Id'},\n",
       "     'comparison_session_ids': {'items': {'type': 'string', 'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'maxItems': 10,\n",
       "      'minItems': 1,\n",
       "      'title': 'Comparison Session Ids'},\n",
       "     'feedback_keys': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'maxItems': 100,\n",
       "      'minItems': 1,\n",
       "      'title': 'Feedback Keys'},\n",
       "     'filters': {'anyOf': [{'additionalProperties': {'items': {'type': 'string'},\n",
       "         'type': 'array'},\n",
       "        'propertyNames': {'format': 'uuid'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Filters'}},\n",
       "    'type': 'object',\n",
       "    'required': ['baseline_session_id',\n",
       "     'comparison_session_ids',\n",
       "     'feedback_keys'],\n",
       "    'title': 'QueryFeedbackDeltaBatch',\n",
       "    'description': 'Request schema for batched feedback delta queries with multiple feedback keys.'},\n",
       "   'QueryGroupedExamplesWithRuns': {'properties': {'session_ids': {'items': {'type': 'string',\n",
       "       'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'minItems': 1,\n",
       "      'title': 'Session Ids'},\n",
       "     'offset': {'type': 'integer',\n",
       "      'minimum': 0.0,\n",
       "      'title': 'Offset',\n",
       "      'default': 0},\n",
       "     'limit': {'type': 'integer',\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Limit',\n",
       "      'default': 10},\n",
       "     'preview': {'type': 'boolean', 'title': 'Preview', 'default': False},\n",
       "     'group_by': {'$ref': '#/components/schemas/GroupExampleRunsByField'},\n",
       "     'metadata_key': {'type': 'string', 'title': 'Metadata Key'},\n",
       "     'per_group_limit': {'type': 'integer',\n",
       "      'maximum': 10.0,\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Per Group Limit',\n",
       "      'default': 5},\n",
       "     'filters': {'anyOf': [{'additionalProperties': {'items': {'type': 'string'},\n",
       "         'type': 'array'},\n",
       "        'propertyNames': {'format': 'uuid'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Filters'}},\n",
       "    'type': 'object',\n",
       "    'required': ['session_ids', 'group_by', 'metadata_key'],\n",
       "    'title': 'QueryGroupedExamplesWithRuns'},\n",
       "   'QueryParamsForPublicRunSchema': {'properties': {'id': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Id'}},\n",
       "    'type': 'object',\n",
       "    'title': 'QueryParamsForPublicRunSchema',\n",
       "    'description': 'Query params for public run endpoints.'},\n",
       "   'QueueInfoResponse': {'properties': {'queued': {'type': 'integer',\n",
       "      'title': 'Queued'},\n",
       "     'active': {'type': 'integer', 'title': 'Active'},\n",
       "     'scheduled': {'type': 'integer', 'title': 'Scheduled'}},\n",
       "    'type': 'object',\n",
       "    'required': ['queued', 'active', 'scheduled'],\n",
       "    'title': 'QueueInfoResponse',\n",
       "    'description': 'Short summary of queue counts.'},\n",
       "   'RepoExampleResponse': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'inputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs'},\n",
       "     'outputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs'},\n",
       "     'session_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Session Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'session_id'],\n",
       "    'title': 'RepoExampleResponse',\n",
       "    'description': 'Response model for example runs'},\n",
       "   'RepoTag': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'repo_id': {'type': 'string', 'format': 'uuid', 'title': 'Repo Id'},\n",
       "     'commit_id': {'type': 'string', 'format': 'uuid', 'title': 'Commit Id'},\n",
       "     'commit_hash': {'type': 'string', 'title': 'Commit Hash'},\n",
       "     'tag_name': {'type': 'string', 'title': 'Tag Name'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'repo_id',\n",
       "     'commit_id',\n",
       "     'commit_hash',\n",
       "     'tag_name',\n",
       "     'created_at',\n",
       "     'updated_at'],\n",
       "    'title': 'RepoTag',\n",
       "    'description': 'Fields for a prompt tag'},\n",
       "   'RepoTagRequest': {'properties': {'tag_name': {'type': 'string',\n",
       "      'title': 'Tag Name'},\n",
       "     'commit_id': {'type': 'string', 'format': 'uuid', 'title': 'Commit Id'},\n",
       "     'skip_webhooks': {'anyOf': [{'type': 'boolean'},\n",
       "       {'items': {'type': 'string', 'format': 'uuid'}, 'type': 'array'}],\n",
       "      'title': 'Skip Webhooks',\n",
       "      'default': False}},\n",
       "    'type': 'object',\n",
       "    'required': ['tag_name', 'commit_id'],\n",
       "    'title': 'RepoTagRequest',\n",
       "    'description': 'Fields to create a prompt tag'},\n",
       "   'RepoUpdateTagRequest': {'properties': {'commit_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Commit Id'},\n",
       "     'skip_webhooks': {'anyOf': [{'type': 'boolean'},\n",
       "       {'items': {'type': 'string', 'format': 'uuid'}, 'type': 'array'}],\n",
       "      'title': 'Skip Webhooks',\n",
       "      'default': False}},\n",
       "    'type': 'object',\n",
       "    'required': ['commit_id'],\n",
       "    'title': 'RepoUpdateTagRequest',\n",
       "    'description': 'Fields to update a prompt tag'},\n",
       "   'RepoWithLookups': {'properties': {'repo_handle': {'type': 'string',\n",
       "      'title': 'Repo Handle'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'readme': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Readme'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'is_public': {'type': 'boolean', 'title': 'Is Public'},\n",
       "     'is_archived': {'type': 'boolean', 'title': 'Is Archived'},\n",
       "     'tags': {'items': {'type': 'string'}, 'type': 'array', 'title': 'Tags'},\n",
       "     'original_repo_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Original Repo Id'},\n",
       "     'upstream_repo_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Upstream Repo Id'},\n",
       "     'owner': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Owner'},\n",
       "     'full_name': {'type': 'string', 'title': 'Full Name'},\n",
       "     'num_likes': {'type': 'integer', 'title': 'Num Likes'},\n",
       "     'num_downloads': {'type': 'integer', 'title': 'Num Downloads'},\n",
       "     'num_views': {'type': 'integer', 'title': 'Num Views'},\n",
       "     'liked_by_auth_user': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Liked By Auth User'},\n",
       "     'last_commit_hash': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Last Commit Hash'},\n",
       "     'num_commits': {'type': 'integer', 'title': 'Num Commits'},\n",
       "     'created_by': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Created By'},\n",
       "     'original_repo_full_name': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Original Repo Full Name'},\n",
       "     'upstream_repo_full_name': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Upstream Repo Full Name'},\n",
       "     'latest_commit_manifest': {'anyOf': [{'$ref': '#/components/schemas/CommitManifestResponse'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'required': ['repo_handle',\n",
       "     'id',\n",
       "     'tenant_id',\n",
       "     'created_at',\n",
       "     'updated_at',\n",
       "     'is_public',\n",
       "     'is_archived',\n",
       "     'tags',\n",
       "     'owner',\n",
       "     'full_name',\n",
       "     'num_likes',\n",
       "     'num_downloads',\n",
       "     'num_views',\n",
       "     'num_commits'],\n",
       "    'title': 'RepoWithLookups',\n",
       "    'description': 'All database fields for repos, plus helpful computed fields.'},\n",
       "   'RequestBodyForRunsGenerateQuery': {'properties': {'query': {'type': 'string',\n",
       "      'title': 'Query'},\n",
       "     'feedback_keys': {'items': {'$ref': '#/components/schemas/RunsGenerateQueryFeedbackKeys'},\n",
       "      'type': 'array',\n",
       "      'title': 'Feedback Keys'}},\n",
       "    'type': 'object',\n",
       "    'required': ['query'],\n",
       "    'title': 'RequestBodyForRunsGenerateQuery'},\n",
       "   'Resource': {'properties': {'tagging_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Tagging Id'},\n",
       "     'resource_name': {'type': 'string', 'title': 'Resource Name'},\n",
       "     'resource_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Resource Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['tagging_id', 'resource_name', 'resource_id'],\n",
       "    'title': 'Resource'},\n",
       "   'ResourceType': {'type': 'string',\n",
       "    'enum': ['prompt',\n",
       "     'project',\n",
       "     'queue',\n",
       "     'deployment',\n",
       "     'experiment',\n",
       "     'dataset',\n",
       "     'dashboard'],\n",
       "    'title': 'ResourceType'},\n",
       "   'ResponseBodyForRunsGenerateQuery': {'properties': {'filter': {'type': 'string',\n",
       "      'title': 'Filter'},\n",
       "     'feedback_urls': {'additionalProperties': {'type': 'string'},\n",
       "      'propertyNames': {'$ref': '#/components/schemas/RunsGenerateQueryFeedbackKeys'},\n",
       "      'type': 'object',\n",
       "      'title': 'Feedback Urls'}},\n",
       "    'type': 'object',\n",
       "    'required': ['filter', 'feedback_urls'],\n",
       "    'title': 'ResponseBodyForRunsGenerateQuery'},\n",
       "   'Role': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'display_name': {'type': 'string', 'title': 'Display Name'},\n",
       "     'description': {'type': 'string', 'title': 'Description'},\n",
       "     'organization_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Organization Id'},\n",
       "     'permissions': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Permissions'},\n",
       "     'access_scope': {'anyOf': [{'$ref': '#/components/schemas/AccessScope'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'name', 'display_name', 'description', 'permissions'],\n",
       "    'title': 'Role'},\n",
       "   'RootModel_Dict_str__list_str___': {'additionalProperties': {'items': {'type': 'string'},\n",
       "     'type': 'array'},\n",
       "    'type': 'object',\n",
       "    'title': 'RootModel[Dict[str, list[str]]]'},\n",
       "   'RuleLogActionOutcome': {'type': 'string',\n",
       "    'enum': ['success', 'skipped', 'error'],\n",
       "    'title': 'RuleLogActionOutcome'},\n",
       "   'RuleLogActionResponse': {'properties': {'outcome': {'$ref': '#/components/schemas/RuleLogActionOutcome'},\n",
       "     'payload': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Payload'}},\n",
       "    'type': 'object',\n",
       "    'required': ['outcome'],\n",
       "    'title': 'RuleLogActionResponse'},\n",
       "   'RuleLogSchema': {'properties': {'rule_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Rule Id'},\n",
       "     'run_id': {'type': 'string', 'format': 'uuid', 'title': 'Run Id'},\n",
       "     'run_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Run Name'},\n",
       "     'run_type': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Run Type'},\n",
       "     'run_session_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Run Session Id'},\n",
       "     'start_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'End Time'},\n",
       "     'application_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Application Time'},\n",
       "     'add_to_annotation_queue': {'anyOf': [{'$ref': '#/components/schemas/RuleLogActionResponse'},\n",
       "       {'type': 'null'}]},\n",
       "     'add_to_dataset': {'anyOf': [{'$ref': '#/components/schemas/RuleLogActionResponse'},\n",
       "       {'type': 'null'}]},\n",
       "     'evaluators': {'anyOf': [{'$ref': '#/components/schemas/RuleLogActionResponse'},\n",
       "       {'type': 'null'}]},\n",
       "     'alerts': {'anyOf': [{'$ref': '#/components/schemas/RuleLogActionResponse'},\n",
       "       {'type': 'null'}]},\n",
       "     'webhooks': {'anyOf': [{'$ref': '#/components/schemas/RuleLogActionResponse'},\n",
       "       {'type': 'null'}]},\n",
       "     'thread_id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Thread Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['rule_id', 'run_id', 'start_time', 'end_time'],\n",
       "    'title': 'RuleLogSchema',\n",
       "    'description': 'Run rules log schema.'},\n",
       "   'RunCluster': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'parent_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Id'},\n",
       "     'level': {'type': 'integer', 'title': 'Level'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'description': {'type': 'string', 'title': 'Description'},\n",
       "     'parent_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Parent Name'},\n",
       "     'num_runs': {'type': 'integer', 'title': 'Num Runs'},\n",
       "     'stats': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Stats'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'level', 'name', 'description', 'num_runs', 'stats'],\n",
       "    'title': 'RunCluster',\n",
       "    'description': 'A single cluster of runs.'},\n",
       "   'RunClusteringJobPydantic': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'status': {'type': 'string', 'title': 'Status'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'shape': {'anyOf': [{'additionalProperties': {'type': 'integer'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Shape'},\n",
       "     'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Error'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'name', 'status'],\n",
       "    'title': 'RunClusteringJobPydantic',\n",
       "    'description': 'Session cluster job'},\n",
       "   'RunDateOrder': {'type': 'string',\n",
       "    'enum': ['asc', 'desc'],\n",
       "    'title': 'RunDateOrder',\n",
       "    'description': 'Enum for run start date order.'},\n",
       "   'RunGroupBy': {'type': 'string',\n",
       "    'enum': ['conversation'],\n",
       "    'title': 'RunGroupBy'},\n",
       "   'RunGroupRequest': {'properties': {'session_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Session Id'},\n",
       "     'group_by': {'$ref': '#/components/schemas/RunGroupBy'},\n",
       "     'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Filter'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'offset': {'type': 'integer',\n",
       "      'minimum': 0.0,\n",
       "      'title': 'Offset',\n",
       "      'default': 0},\n",
       "     'limit': {'type': 'integer',\n",
       "      'maximum': 100.0,\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Limit',\n",
       "      'default': 10}},\n",
       "    'type': 'object',\n",
       "    'required': ['session_id', 'group_by'],\n",
       "    'title': 'RunGroupRequest'},\n",
       "   'RunGroupStats': {'properties': {'run_count': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Run Count'},\n",
       "     'latency_p50': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Latency P50'},\n",
       "     'latency_p99': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Latency P99'},\n",
       "     'first_token_p50': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'First Token P50'},\n",
       "     'first_token_p99': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'First Token P99'},\n",
       "     'total_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Total Tokens'},\n",
       "     'prompt_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Tokens'},\n",
       "     'completion_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Completion Tokens'},\n",
       "     'median_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Median Tokens'},\n",
       "     'completion_tokens_p50': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Tokens P50'},\n",
       "     'prompt_tokens_p50': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Tokens P50'},\n",
       "     'tokens_p99': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Tokens P99'},\n",
       "     'completion_tokens_p99': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Tokens P99'},\n",
       "     'prompt_tokens_p99': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Tokens P99'},\n",
       "     'last_run_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Run Start Time'},\n",
       "     'feedback_stats': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Stats'},\n",
       "     'run_facets': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Run Facets'},\n",
       "     'error_rate': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Error Rate'},\n",
       "     'streaming_rate': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Streaming Rate'},\n",
       "     'total_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Total Cost'},\n",
       "     'prompt_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Cost'},\n",
       "     'completion_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Completion Cost'},\n",
       "     'cost_p50': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Cost P50'},\n",
       "     'cost_p99': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Cost P99'},\n",
       "     'prompt_token_details': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Prompt Token Details'},\n",
       "     'completion_token_details': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Token Details'},\n",
       "     'prompt_cost_details': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Prompt Cost Details'},\n",
       "     'completion_cost_details': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Cost Details'},\n",
       "     'group_count': {'type': 'integer', 'title': 'Group Count'}},\n",
       "    'type': 'object',\n",
       "    'required': ['group_count'],\n",
       "    'title': 'RunGroupStats'},\n",
       "   'RunPublicDatasetSchema': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'inputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs'},\n",
       "     'inputs_preview': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Inputs Preview'},\n",
       "     'run_type': {'$ref': '#/components/schemas/RunTypeEnum'},\n",
       "     'start_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'},\n",
       "     'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Error'},\n",
       "     'execution_order': {'type': 'integer',\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Execution Order',\n",
       "      'default': 1},\n",
       "     'serialized': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Serialized'},\n",
       "     'outputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs'},\n",
       "     'outputs_preview': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Outputs Preview'},\n",
       "     'parent_run_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Run Id'},\n",
       "     'manifest_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Manifest Id'},\n",
       "     'manifest_s3_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Manifest S3 Id'},\n",
       "     'events': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Events'},\n",
       "     'tags': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tags'},\n",
       "     'inputs_s3_urls': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs S3 Urls'},\n",
       "     'outputs_s3_urls': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs S3 Urls'},\n",
       "     's3_urls': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'S3 Urls'},\n",
       "     'trace_id': {'type': 'string', 'format': 'uuid', 'title': 'Trace Id'},\n",
       "     'dotted_order': {'type': 'string', 'title': 'Dotted Order'},\n",
       "     'trace_min_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace Min Start Time'},\n",
       "     'trace_max_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace Max Start Time'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'status': {'type': 'string', 'title': 'Status'},\n",
       "     'child_run_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Child Run Ids'},\n",
       "     'direct_child_run_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Direct Child Run Ids'},\n",
       "     'parent_run_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Run Ids'},\n",
       "     'feedback_stats': {'anyOf': [{'additionalProperties': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Stats'},\n",
       "     'reference_example_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reference Example Id'},\n",
       "     'total_tokens': {'type': 'integer',\n",
       "      'title': 'Total Tokens',\n",
       "      'default': 0},\n",
       "     'prompt_tokens': {'type': 'integer',\n",
       "      'title': 'Prompt Tokens',\n",
       "      'default': 0},\n",
       "     'completion_tokens': {'type': 'integer',\n",
       "      'title': 'Completion Tokens',\n",
       "      'default': 0},\n",
       "     'prompt_token_details': {'anyOf': [{'additionalProperties': {'type': 'integer'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Prompt Token Details'},\n",
       "     'completion_token_details': {'anyOf': [{'additionalProperties': {'type': 'integer'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Token Details'},\n",
       "     'total_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Total Cost'},\n",
       "     'prompt_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Cost'},\n",
       "     'completion_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Completion Cost'},\n",
       "     'prompt_cost_details': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Prompt Cost Details'},\n",
       "     'completion_cost_details': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Cost Details'},\n",
       "     'price_model_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Price Model Id'},\n",
       "     'first_token_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'First Token Time'},\n",
       "     'messages': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Messages'},\n",
       "     'session_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Session Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name',\n",
       "     'run_type',\n",
       "     'trace_id',\n",
       "     'dotted_order',\n",
       "     'id',\n",
       "     'status',\n",
       "     'session_id'],\n",
       "    'title': 'RunPublicDatasetSchema',\n",
       "    'description': 'Schema for a run in a publicly-shared dataset.'},\n",
       "   'RunPublicSchema': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'inputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs'},\n",
       "     'inputs_preview': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Inputs Preview'},\n",
       "     'run_type': {'$ref': '#/components/schemas/RunTypeEnum'},\n",
       "     'start_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'},\n",
       "     'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Error'},\n",
       "     'execution_order': {'type': 'integer',\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Execution Order',\n",
       "      'default': 1},\n",
       "     'serialized': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Serialized'},\n",
       "     'outputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs'},\n",
       "     'outputs_preview': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Outputs Preview'},\n",
       "     'parent_run_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Run Id'},\n",
       "     'manifest_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Manifest Id'},\n",
       "     'manifest_s3_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Manifest S3 Id'},\n",
       "     'events': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Events'},\n",
       "     'tags': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tags'},\n",
       "     'inputs_s3_urls': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs S3 Urls'},\n",
       "     'outputs_s3_urls': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs S3 Urls'},\n",
       "     's3_urls': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'S3 Urls'},\n",
       "     'trace_id': {'type': 'string', 'format': 'uuid', 'title': 'Trace Id'},\n",
       "     'dotted_order': {'type': 'string', 'title': 'Dotted Order'},\n",
       "     'trace_min_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace Min Start Time'},\n",
       "     'trace_max_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace Max Start Time'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'status': {'type': 'string', 'title': 'Status'},\n",
       "     'child_run_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Child Run Ids'},\n",
       "     'direct_child_run_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Direct Child Run Ids'},\n",
       "     'parent_run_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Run Ids'},\n",
       "     'feedback_stats': {'anyOf': [{'additionalProperties': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Stats'},\n",
       "     'reference_example_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reference Example Id'},\n",
       "     'total_tokens': {'type': 'integer',\n",
       "      'title': 'Total Tokens',\n",
       "      'default': 0},\n",
       "     'prompt_tokens': {'type': 'integer',\n",
       "      'title': 'Prompt Tokens',\n",
       "      'default': 0},\n",
       "     'completion_tokens': {'type': 'integer',\n",
       "      'title': 'Completion Tokens',\n",
       "      'default': 0},\n",
       "     'prompt_token_details': {'anyOf': [{'additionalProperties': {'type': 'integer'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Prompt Token Details'},\n",
       "     'completion_token_details': {'anyOf': [{'additionalProperties': {'type': 'integer'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Token Details'},\n",
       "     'total_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Total Cost'},\n",
       "     'prompt_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Cost'},\n",
       "     'completion_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Completion Cost'},\n",
       "     'prompt_cost_details': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Prompt Cost Details'},\n",
       "     'completion_cost_details': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Cost Details'},\n",
       "     'price_model_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Price Model Id'},\n",
       "     'first_token_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'First Token Time'},\n",
       "     'messages': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Messages'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name',\n",
       "     'run_type',\n",
       "     'trace_id',\n",
       "     'dotted_order',\n",
       "     'id',\n",
       "     'status'],\n",
       "    'title': 'RunPublicSchema'},\n",
       "   'RunRulesAlertType': {'type': 'string',\n",
       "    'enum': ['pagerduty'],\n",
       "    'title': 'RunRulesAlertType',\n",
       "    'description': 'Enum for alert types.'},\n",
       "   'RunRulesCreateSchema': {'properties': {'display_name': {'type': 'string',\n",
       "      'title': 'Display Name'},\n",
       "     'session_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Id'},\n",
       "     'is_enabled': {'type': 'boolean', 'title': 'Is Enabled', 'default': True},\n",
       "     'dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Dataset Id'},\n",
       "     'sampling_rate': {'type': 'number', 'title': 'Sampling Rate'},\n",
       "     'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Filter'},\n",
       "     'trace_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Trace Filter'},\n",
       "     'tree_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tree Filter'},\n",
       "     'backfill_from': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Backfill From'},\n",
       "     'use_corrections_dataset': {'type': 'boolean',\n",
       "      'title': 'Use Corrections Dataset',\n",
       "      'default': False},\n",
       "     'num_few_shot_examples': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Num Few Shot Examples'},\n",
       "     'extend_only': {'type': 'boolean',\n",
       "      'title': 'Extend Only',\n",
       "      'default': False},\n",
       "     'transient': {'type': 'boolean', 'title': 'Transient', 'default': False},\n",
       "     'add_to_annotation_queue_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Add To Annotation Queue Id'},\n",
       "     'add_to_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Add To Dataset Id'},\n",
       "     'add_to_dataset_prefer_correction': {'type': 'boolean',\n",
       "      'title': 'Add To Dataset Prefer Correction',\n",
       "      'default': False},\n",
       "     'evaluators': {'anyOf': [{'items': {'$ref': '#/components/schemas/EvaluatorTopLevel'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Evaluators'},\n",
       "     'code_evaluators': {'anyOf': [{'items': {'$ref': '#/components/schemas/CodeEvaluatorTopLevel'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Code Evaluators'},\n",
       "     'alerts': {'anyOf': [{'items': {'$ref': '#/components/schemas/RunRulesPagerdutyAlertSchema'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Alerts'},\n",
       "     'webhooks': {'anyOf': [{'items': {'$ref': '#/components/schemas/RunRulesWebhookSchema'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Webhooks'},\n",
       "     'evaluator_version': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Evaluator Version'},\n",
       "     'create_alignment_queue': {'type': 'boolean',\n",
       "      'title': 'Create Alignment Queue',\n",
       "      'default': False},\n",
       "     'include_extended_stats': {'type': 'boolean',\n",
       "      'title': 'Include Extended Stats',\n",
       "      'default': False},\n",
       "     'group_by': {'anyOf': [{'type': 'string', 'const': 'thread_id'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Group By'}},\n",
       "    'type': 'object',\n",
       "    'required': ['display_name', 'sampling_rate'],\n",
       "    'title': 'RunRulesCreateSchema'},\n",
       "   'RunRulesPagerdutyAlertSchema': {'properties': {'type': {'anyOf': [{'$ref': '#/components/schemas/RunRulesAlertType'},\n",
       "       {'type': 'null'}],\n",
       "      'default': 'pagerduty'},\n",
       "     'routing_key': {'type': 'string', 'title': 'Routing Key'},\n",
       "     'summary': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Summary'},\n",
       "     'severity': {'anyOf': [{'$ref': '#/components/schemas/PagerdutySeverity'},\n",
       "       {'type': 'null'}],\n",
       "      'default': 'warning'}},\n",
       "    'type': 'object',\n",
       "    'required': ['routing_key'],\n",
       "    'title': 'RunRulesPagerdutyAlertSchema'},\n",
       "   'RunRulesSchema': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'is_enabled': {'type': 'boolean', 'title': 'Is Enabled', 'default': True},\n",
       "     'session_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Id'},\n",
       "     'session_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Session Name'},\n",
       "     'dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Dataset Id'},\n",
       "     'dataset_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Dataset Name'},\n",
       "     'display_name': {'type': 'string', 'title': 'Display Name'},\n",
       "     'sampling_rate': {'type': 'number', 'title': 'Sampling Rate'},\n",
       "     'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Filter'},\n",
       "     'trace_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Trace Filter'},\n",
       "     'tree_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tree Filter'},\n",
       "     'add_to_annotation_queue_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Add To Annotation Queue Id'},\n",
       "     'add_to_annotation_queue_name': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Add To Annotation Queue Name'},\n",
       "     'add_to_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Add To Dataset Id'},\n",
       "     'add_to_dataset_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Add To Dataset Name'},\n",
       "     'add_to_dataset_prefer_correction': {'type': 'boolean',\n",
       "      'title': 'Add To Dataset Prefer Correction',\n",
       "      'default': False},\n",
       "     'corrections_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Corrections Dataset Id'},\n",
       "     'use_corrections_dataset': {'type': 'boolean',\n",
       "      'title': 'Use Corrections Dataset',\n",
       "      'default': False},\n",
       "     'num_few_shot_examples': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Num Few Shot Examples'},\n",
       "     'evaluators': {'anyOf': [{'items': {'$ref': '#/components/schemas/EvaluatorTopLevel'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Evaluators'},\n",
       "     'code_evaluators': {'anyOf': [{'items': {'$ref': '#/components/schemas/CodeEvaluatorTopLevel'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Code Evaluators'},\n",
       "     'alerts': {'anyOf': [{'items': {'$ref': '#/components/schemas/RunRulesPagerdutyAlertSchema'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Alerts'},\n",
       "     'webhooks': {'anyOf': [{'items': {'$ref': '#/components/schemas/RunRulesWebhookSchema'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Webhooks'},\n",
       "     'extend_only': {'type': 'boolean',\n",
       "      'title': 'Extend Only',\n",
       "      'default': False},\n",
       "     'include_extended_stats': {'type': 'boolean',\n",
       "      'title': 'Include Extended Stats',\n",
       "      'default': False},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'backfill_from': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Backfill From'},\n",
       "     'transient': {'type': 'boolean', 'title': 'Transient', 'default': False},\n",
       "     'evaluator_version': {'type': 'integer', 'title': 'Evaluator Version'},\n",
       "     'evaluator_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Evaluator Id'},\n",
       "     'alignment_annotation_queue_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Alignment Annotation Queue Id'},\n",
       "     'group_by': {'anyOf': [{'type': 'string', 'const': 'thread_id'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Group By'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'tenant_id',\n",
       "     'display_name',\n",
       "     'sampling_rate',\n",
       "     'webhooks',\n",
       "     'created_at',\n",
       "     'updated_at',\n",
       "     'evaluator_version'],\n",
       "    'title': 'RunRulesSchema',\n",
       "    'description': 'Run rules schema.'},\n",
       "   'RunRulesUpdateSchema': {'properties': {'display_name': {'type': 'string',\n",
       "      'title': 'Display Name'},\n",
       "     'session_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Id'},\n",
       "     'is_enabled': {'type': 'boolean', 'title': 'Is Enabled', 'default': True},\n",
       "     'dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Dataset Id'},\n",
       "     'sampling_rate': {'type': 'number', 'title': 'Sampling Rate'},\n",
       "     'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Filter'},\n",
       "     'trace_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Trace Filter'},\n",
       "     'tree_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tree Filter'},\n",
       "     'backfill_from': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Backfill From'},\n",
       "     'use_corrections_dataset': {'type': 'boolean',\n",
       "      'title': 'Use Corrections Dataset',\n",
       "      'default': False},\n",
       "     'num_few_shot_examples': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Num Few Shot Examples'},\n",
       "     'extend_only': {'type': 'boolean',\n",
       "      'title': 'Extend Only',\n",
       "      'default': False},\n",
       "     'transient': {'type': 'boolean', 'title': 'Transient', 'default': False},\n",
       "     'add_to_annotation_queue_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Add To Annotation Queue Id'},\n",
       "     'add_to_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Add To Dataset Id'},\n",
       "     'add_to_dataset_prefer_correction': {'type': 'boolean',\n",
       "      'title': 'Add To Dataset Prefer Correction',\n",
       "      'default': False},\n",
       "     'evaluators': {'anyOf': [{'items': {'$ref': '#/components/schemas/EvaluatorTopLevel'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Evaluators'},\n",
       "     'code_evaluators': {'anyOf': [{'items': {'$ref': '#/components/schemas/CodeEvaluatorTopLevel'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Code Evaluators'},\n",
       "     'alerts': {'anyOf': [{'items': {'$ref': '#/components/schemas/RunRulesPagerdutyAlertSchema'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Alerts'},\n",
       "     'webhooks': {'anyOf': [{'items': {'$ref': '#/components/schemas/RunRulesWebhookSchema'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Webhooks'},\n",
       "     'evaluator_version': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Evaluator Version'},\n",
       "     'create_alignment_queue': {'type': 'boolean',\n",
       "      'title': 'Create Alignment Queue',\n",
       "      'default': False},\n",
       "     'include_extended_stats': {'type': 'boolean',\n",
       "      'title': 'Include Extended Stats',\n",
       "      'default': False}},\n",
       "    'type': 'object',\n",
       "    'required': ['display_name', 'sampling_rate'],\n",
       "    'title': 'RunRulesUpdateSchema'},\n",
       "   'RunRulesValidateSchema': {'properties': {'display_name': {'type': 'string',\n",
       "      'title': 'Display Name'},\n",
       "     'session_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Id'},\n",
       "     'is_enabled': {'type': 'boolean', 'title': 'Is Enabled', 'default': True},\n",
       "     'dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Dataset Id'},\n",
       "     'sampling_rate': {'type': 'number', 'title': 'Sampling Rate'},\n",
       "     'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Filter'},\n",
       "     'trace_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Trace Filter'},\n",
       "     'tree_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tree Filter'},\n",
       "     'backfill_from': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Backfill From'},\n",
       "     'use_corrections_dataset': {'type': 'boolean',\n",
       "      'title': 'Use Corrections Dataset',\n",
       "      'default': False},\n",
       "     'num_few_shot_examples': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Num Few Shot Examples'},\n",
       "     'extend_only': {'type': 'boolean',\n",
       "      'title': 'Extend Only',\n",
       "      'default': False},\n",
       "     'transient': {'type': 'boolean', 'title': 'Transient', 'default': False},\n",
       "     'add_to_annotation_queue_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Add To Annotation Queue Id'},\n",
       "     'add_to_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Add To Dataset Id'},\n",
       "     'add_to_dataset_prefer_correction': {'type': 'boolean',\n",
       "      'title': 'Add To Dataset Prefer Correction',\n",
       "      'default': False},\n",
       "     'evaluators': {'anyOf': [{'items': {'$ref': '#/components/schemas/EvaluatorTopLevel'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Evaluators'},\n",
       "     'code_evaluators': {'anyOf': [{'items': {'$ref': '#/components/schemas/CodeEvaluatorTopLevel'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Code Evaluators'},\n",
       "     'alerts': {'anyOf': [{'items': {'$ref': '#/components/schemas/RunRulesPagerdutyAlertSchema'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Alerts'},\n",
       "     'webhooks': {'anyOf': [{'items': {'$ref': '#/components/schemas/RunRulesWebhookSchema'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Webhooks'},\n",
       "     'evaluator_version': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Evaluator Version'},\n",
       "     'create_alignment_queue': {'type': 'boolean',\n",
       "      'title': 'Create Alignment Queue',\n",
       "      'default': False},\n",
       "     'include_extended_stats': {'type': 'boolean',\n",
       "      'title': 'Include Extended Stats',\n",
       "      'default': False},\n",
       "     'group_by': {'anyOf': [{'type': 'string', 'const': 'thread_id'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Group By'},\n",
       "     'test_inputs': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Test Inputs'},\n",
       "     'test_outputs': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Test Outputs'},\n",
       "     'test_reference_outputs': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Test Reference Outputs'}},\n",
       "    'type': 'object',\n",
       "    'required': ['display_name', 'sampling_rate'],\n",
       "    'title': 'RunRulesValidateSchema',\n",
       "    'description': 'Schema for validating rules without creating them.\\n\\nExtends RunRulesCreateSchema with test data fields for validation.\\nOnly LLM-as-judge rules (evaluators) are supported, not code_evaluators.'},\n",
       "   'RunRulesWebhookSchema': {'properties': {'url': {'type': 'string',\n",
       "      'title': 'Url'},\n",
       "     'headers': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Headers'}},\n",
       "    'type': 'object',\n",
       "    'required': ['url'],\n",
       "    'title': 'RunRulesWebhookSchema'},\n",
       "   'RunSchema': {'properties': {'name': {'type': 'string', 'title': 'Name'},\n",
       "     'inputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs'},\n",
       "     'inputs_preview': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Inputs Preview'},\n",
       "     'run_type': {'$ref': '#/components/schemas/RunTypeEnum'},\n",
       "     'start_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'},\n",
       "     'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Error'},\n",
       "     'execution_order': {'type': 'integer',\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Execution Order',\n",
       "      'default': 1},\n",
       "     'serialized': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Serialized'},\n",
       "     'outputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs'},\n",
       "     'outputs_preview': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Outputs Preview'},\n",
       "     'parent_run_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Run Id'},\n",
       "     'manifest_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Manifest Id'},\n",
       "     'manifest_s3_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Manifest S3 Id'},\n",
       "     'events': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Events'},\n",
       "     'tags': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tags'},\n",
       "     'inputs_s3_urls': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs S3 Urls'},\n",
       "     'outputs_s3_urls': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs S3 Urls'},\n",
       "     's3_urls': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'S3 Urls'},\n",
       "     'trace_id': {'type': 'string', 'format': 'uuid', 'title': 'Trace Id'},\n",
       "     'dotted_order': {'type': 'string', 'title': 'Dotted Order'},\n",
       "     'trace_min_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace Min Start Time'},\n",
       "     'trace_max_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace Max Start Time'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'status': {'type': 'string', 'title': 'Status'},\n",
       "     'child_run_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Child Run Ids'},\n",
       "     'direct_child_run_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Direct Child Run Ids'},\n",
       "     'parent_run_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Run Ids'},\n",
       "     'feedback_stats': {'anyOf': [{'additionalProperties': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Stats'},\n",
       "     'reference_example_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reference Example Id'},\n",
       "     'total_tokens': {'type': 'integer',\n",
       "      'title': 'Total Tokens',\n",
       "      'default': 0},\n",
       "     'prompt_tokens': {'type': 'integer',\n",
       "      'title': 'Prompt Tokens',\n",
       "      'default': 0},\n",
       "     'completion_tokens': {'type': 'integer',\n",
       "      'title': 'Completion Tokens',\n",
       "      'default': 0},\n",
       "     'prompt_token_details': {'anyOf': [{'additionalProperties': {'type': 'integer'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Prompt Token Details'},\n",
       "     'completion_token_details': {'anyOf': [{'additionalProperties': {'type': 'integer'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Token Details'},\n",
       "     'total_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Total Cost'},\n",
       "     'prompt_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Cost'},\n",
       "     'completion_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Completion Cost'},\n",
       "     'prompt_cost_details': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Prompt Cost Details'},\n",
       "     'completion_cost_details': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Cost Details'},\n",
       "     'price_model_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Price Model Id'},\n",
       "     'first_token_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'First Token Time'},\n",
       "     'messages': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Messages'},\n",
       "     'session_id': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'},\n",
       "     'app_path': {'type': 'string', 'title': 'App Path'},\n",
       "     'last_queued_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Queued At'},\n",
       "     'in_dataset': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'In Dataset'},\n",
       "     'share_token': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Share Token'},\n",
       "     'trace_tier': {'anyOf': [{'$ref': '#/components/schemas/TraceTier'},\n",
       "       {'type': 'null'}]},\n",
       "     'trace_first_received_at': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace First Received At'},\n",
       "     'ttl_seconds': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Ttl Seconds'},\n",
       "     'trace_upgrade': {'type': 'boolean',\n",
       "      'title': 'Trace Upgrade',\n",
       "      'default': False},\n",
       "     'reference_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reference Dataset Id'},\n",
       "     'thread_id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Thread Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name',\n",
       "     'run_type',\n",
       "     'trace_id',\n",
       "     'dotted_order',\n",
       "     'id',\n",
       "     'status',\n",
       "     'session_id',\n",
       "     'app_path'],\n",
       "    'title': 'RunSchema',\n",
       "    'description': 'Run schema.'},\n",
       "   'RunSchemaComparisonView': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'inputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs'},\n",
       "     'inputs_preview': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Inputs Preview'},\n",
       "     'run_type': {'$ref': '#/components/schemas/RunTypeEnum'},\n",
       "     'start_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'},\n",
       "     'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Error'},\n",
       "     'execution_order': {'type': 'integer',\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Execution Order',\n",
       "      'default': 1},\n",
       "     'serialized': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Serialized'},\n",
       "     'outputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs'},\n",
       "     'outputs_preview': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Outputs Preview'},\n",
       "     'parent_run_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Run Id'},\n",
       "     'manifest_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Manifest Id'},\n",
       "     'manifest_s3_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Manifest S3 Id'},\n",
       "     'events': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Events'},\n",
       "     'tags': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tags'},\n",
       "     'inputs_s3_urls': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs S3 Urls'},\n",
       "     'outputs_s3_urls': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs S3 Urls'},\n",
       "     's3_urls': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'S3 Urls'},\n",
       "     'trace_id': {'type': 'string', 'format': 'uuid', 'title': 'Trace Id'},\n",
       "     'dotted_order': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Dotted Order'},\n",
       "     'trace_min_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace Min Start Time'},\n",
       "     'trace_max_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace Max Start Time'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'session_id': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'},\n",
       "     'reference_example_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reference Example Id'},\n",
       "     'total_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Total Tokens'},\n",
       "     'prompt_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Tokens'},\n",
       "     'completion_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Completion Tokens'},\n",
       "     'total_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Total Cost'},\n",
       "     'prompt_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Cost'},\n",
       "     'completion_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Completion Cost'},\n",
       "     'status': {'type': 'string', 'title': 'Status'},\n",
       "     'feedback_stats': {'anyOf': [{'additionalProperties': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Stats'},\n",
       "     'app_path': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'App Path'},\n",
       "     'feedbacks': {'items': {'$ref': '#/components/schemas/FeedbackSchema'},\n",
       "      'type': 'array',\n",
       "      'title': 'Feedbacks'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'run_type', 'trace_id', 'id', 'session_id', 'status'],\n",
       "    'title': 'RunSchemaComparisonView',\n",
       "    'description': 'Run schema for comparison view.'},\n",
       "   'RunSchemaWithAnnotationQueueInfo': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'inputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs'},\n",
       "     'inputs_preview': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Inputs Preview'},\n",
       "     'run_type': {'$ref': '#/components/schemas/RunTypeEnum'},\n",
       "     'start_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'},\n",
       "     'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Error'},\n",
       "     'execution_order': {'type': 'integer',\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Execution Order',\n",
       "      'default': 1},\n",
       "     'serialized': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Serialized'},\n",
       "     'outputs': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs'},\n",
       "     'outputs_preview': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Outputs Preview'},\n",
       "     'parent_run_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Run Id'},\n",
       "     'manifest_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Manifest Id'},\n",
       "     'manifest_s3_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Manifest S3 Id'},\n",
       "     'events': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Events'},\n",
       "     'tags': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tags'},\n",
       "     'inputs_s3_urls': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Inputs S3 Urls'},\n",
       "     'outputs_s3_urls': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Outputs S3 Urls'},\n",
       "     's3_urls': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'S3 Urls'},\n",
       "     'trace_id': {'type': 'string', 'format': 'uuid', 'title': 'Trace Id'},\n",
       "     'dotted_order': {'type': 'string', 'title': 'Dotted Order'},\n",
       "     'trace_min_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace Min Start Time'},\n",
       "     'trace_max_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace Max Start Time'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'status': {'type': 'string', 'title': 'Status'},\n",
       "     'child_run_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Child Run Ids'},\n",
       "     'direct_child_run_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Direct Child Run Ids'},\n",
       "     'parent_run_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Run Ids'},\n",
       "     'feedback_stats': {'anyOf': [{'additionalProperties': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Stats'},\n",
       "     'reference_example_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reference Example Id'},\n",
       "     'total_tokens': {'type': 'integer',\n",
       "      'title': 'Total Tokens',\n",
       "      'default': 0},\n",
       "     'prompt_tokens': {'type': 'integer',\n",
       "      'title': 'Prompt Tokens',\n",
       "      'default': 0},\n",
       "     'completion_tokens': {'type': 'integer',\n",
       "      'title': 'Completion Tokens',\n",
       "      'default': 0},\n",
       "     'prompt_token_details': {'anyOf': [{'additionalProperties': {'type': 'integer'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Prompt Token Details'},\n",
       "     'completion_token_details': {'anyOf': [{'additionalProperties': {'type': 'integer'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Token Details'},\n",
       "     'total_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Total Cost'},\n",
       "     'prompt_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Cost'},\n",
       "     'completion_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Completion Cost'},\n",
       "     'prompt_cost_details': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Prompt Cost Details'},\n",
       "     'completion_cost_details': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Cost Details'},\n",
       "     'price_model_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Price Model Id'},\n",
       "     'first_token_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'First Token Time'},\n",
       "     'messages': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Messages'},\n",
       "     'session_id': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'},\n",
       "     'app_path': {'type': 'string', 'title': 'App Path'},\n",
       "     'last_queued_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Queued At'},\n",
       "     'in_dataset': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'In Dataset'},\n",
       "     'share_token': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Share Token'},\n",
       "     'trace_tier': {'anyOf': [{'$ref': '#/components/schemas/TraceTier'},\n",
       "       {'type': 'null'}]},\n",
       "     'trace_first_received_at': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace First Received At'},\n",
       "     'ttl_seconds': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Ttl Seconds'},\n",
       "     'trace_upgrade': {'type': 'boolean',\n",
       "      'title': 'Trace Upgrade',\n",
       "      'default': False},\n",
       "     'reference_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reference Dataset Id'},\n",
       "     'thread_id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Thread Id'},\n",
       "     'queue_run_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Queue Run Id'},\n",
       "     'last_reviewed_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Reviewed Time'},\n",
       "     'added_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Added At'},\n",
       "     'effective_added_at': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Effective Added At'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name',\n",
       "     'run_type',\n",
       "     'trace_id',\n",
       "     'dotted_order',\n",
       "     'id',\n",
       "     'status',\n",
       "     'session_id',\n",
       "     'app_path',\n",
       "     'queue_run_id'],\n",
       "    'title': 'RunSchemaWithAnnotationQueueInfo',\n",
       "    'description': 'Run schema with annotation queue info.'},\n",
       "   'RunSelect': {'type': 'string',\n",
       "    'enum': ['id',\n",
       "     'name',\n",
       "     'run_type',\n",
       "     'start_time',\n",
       "     'end_time',\n",
       "     'status',\n",
       "     'error',\n",
       "     'extra',\n",
       "     'events',\n",
       "     'inputs',\n",
       "     'inputs_preview',\n",
       "     'inputs_s3_urls',\n",
       "     'inputs_or_signed_url',\n",
       "     'outputs',\n",
       "     'outputs_preview',\n",
       "     'outputs_s3_urls',\n",
       "     'outputs_or_signed_url',\n",
       "     's3_urls',\n",
       "     'error_or_signed_url',\n",
       "     'events_or_signed_url',\n",
       "     'extra_or_signed_url',\n",
       "     'serialized_or_signed_url',\n",
       "     'parent_run_id',\n",
       "     'manifest_id',\n",
       "     'manifest_s3_id',\n",
       "     'manifest',\n",
       "     'session_id',\n",
       "     'serialized',\n",
       "     'reference_example_id',\n",
       "     'reference_dataset_id',\n",
       "     'total_tokens',\n",
       "     'prompt_tokens',\n",
       "     'prompt_token_details',\n",
       "     'completion_tokens',\n",
       "     'completion_token_details',\n",
       "     'total_cost',\n",
       "     'prompt_cost',\n",
       "     'prompt_cost_details',\n",
       "     'completion_cost',\n",
       "     'completion_cost_details',\n",
       "     'price_model_id',\n",
       "     'first_token_time',\n",
       "     'trace_id',\n",
       "     'dotted_order',\n",
       "     'last_queued_at',\n",
       "     'feedback_stats',\n",
       "     'child_run_ids',\n",
       "     'parent_run_ids',\n",
       "     'tags',\n",
       "     'in_dataset',\n",
       "     'app_path',\n",
       "     'share_token',\n",
       "     'trace_tier',\n",
       "     'trace_first_received_at',\n",
       "     'ttl_seconds',\n",
       "     'trace_upgrade',\n",
       "     'thread_id',\n",
       "     'trace_min_max_start_time',\n",
       "     'messages',\n",
       "     'inserted_at'],\n",
       "    'title': 'RunSelect',\n",
       "    'description': 'Enum for available run columns.'},\n",
       "   'RunShareSchema': {'properties': {'run_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Run Id'},\n",
       "     'share_token': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Share Token'}},\n",
       "    'type': 'object',\n",
       "    'required': ['run_id', 'share_token'],\n",
       "    'title': 'RunShareSchema'},\n",
       "   'RunStats': {'properties': {'run_count': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Run Count'},\n",
       "     'latency_p50': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Latency P50'},\n",
       "     'latency_p99': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Latency P99'},\n",
       "     'first_token_p50': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'First Token P50'},\n",
       "     'first_token_p99': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'First Token P99'},\n",
       "     'total_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Total Tokens'},\n",
       "     'prompt_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Tokens'},\n",
       "     'completion_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Completion Tokens'},\n",
       "     'median_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Median Tokens'},\n",
       "     'completion_tokens_p50': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Tokens P50'},\n",
       "     'prompt_tokens_p50': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Tokens P50'},\n",
       "     'tokens_p99': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Tokens P99'},\n",
       "     'completion_tokens_p99': {'anyOf': [{'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Tokens P99'},\n",
       "     'prompt_tokens_p99': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Tokens P99'},\n",
       "     'last_run_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Run Start Time'},\n",
       "     'feedback_stats': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Stats'},\n",
       "     'run_facets': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Run Facets'},\n",
       "     'error_rate': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Error Rate'},\n",
       "     'streaming_rate': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Streaming Rate'},\n",
       "     'total_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Total Cost'},\n",
       "     'prompt_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Cost'},\n",
       "     'completion_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Completion Cost'},\n",
       "     'cost_p50': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Cost P50'},\n",
       "     'cost_p99': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Cost P99'},\n",
       "     'prompt_token_details': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Prompt Token Details'},\n",
       "     'completion_token_details': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Token Details'},\n",
       "     'prompt_cost_details': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Prompt Cost Details'},\n",
       "     'completion_cost_details': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Completion Cost Details'}},\n",
       "    'type': 'object',\n",
       "    'title': 'RunStats'},\n",
       "   'RunStatsGroupBy': {'properties': {'attribute': {'type': 'string',\n",
       "      'enum': ['name', 'run_type', 'tag', 'metadata'],\n",
       "      'title': 'Attribute'},\n",
       "     'path': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Path'},\n",
       "     'max_groups': {'type': 'integer', 'title': 'Max Groups', 'default': 5}},\n",
       "    'type': 'object',\n",
       "    'required': ['attribute'],\n",
       "    'title': 'RunStatsGroupBy',\n",
       "    'description': 'Group by param for run stats.'},\n",
       "   'RunStatsGroupBySeriesResponse': {'properties': {'attribute': {'type': 'string',\n",
       "      'enum': ['name', 'run_type', 'tag', 'metadata'],\n",
       "      'title': 'Attribute'},\n",
       "     'path': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Path'},\n",
       "     'max_groups': {'type': 'integer', 'title': 'Max Groups', 'default': 5},\n",
       "     'set_by': {'anyOf': [{'type': 'string', 'enum': ['section', 'series']},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Set By'}},\n",
       "    'type': 'object',\n",
       "    'required': ['attribute'],\n",
       "    'title': 'RunStatsGroupBySeriesResponse',\n",
       "    'description': 'Include additional information about where the group_by param was set.'},\n",
       "   'RunStatsQueryParams': {'properties': {'id': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Id'},\n",
       "     'trace': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Trace'},\n",
       "     'parent_run': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Parent Run'},\n",
       "     'run_type': {'anyOf': [{'$ref': '#/components/schemas/RunTypeEnum'},\n",
       "       {'type': 'null'}]},\n",
       "     'session': {'anyOf': [{'items': {'type': 'string', 'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session'},\n",
       "     'reference_example': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reference Example'},\n",
       "     'execution_order': {'anyOf': [{'type': 'integer',\n",
       "        'maximum': 1.0,\n",
       "        'minimum': 1.0},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Execution Order'},\n",
       "     'start_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'error': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Error'},\n",
       "     'query': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Query'},\n",
       "     'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Filter'},\n",
       "     'trace_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Trace Filter'},\n",
       "     'tree_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tree Filter'},\n",
       "     'is_root': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Is Root'},\n",
       "     'data_source_type': {'anyOf': [{'$ref': '#/components/schemas/RunsFilterDataSourceTypeEnum'},\n",
       "       {'type': 'null'}]},\n",
       "     'skip_pagination': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Skip Pagination'},\n",
       "     'search_filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Search Filter'},\n",
       "     'use_experimental_search': {'type': 'boolean',\n",
       "      'title': 'Use Experimental Search',\n",
       "      'default': False},\n",
       "     'group_by': {'anyOf': [{'$ref': '#/components/schemas/RunStatsGroupBy'},\n",
       "       {'type': 'null'}]},\n",
       "     'groups': {'anyOf': [{'items': {'anyOf': [{'type': 'string'},\n",
       "          {'type': 'null'}]},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Groups'},\n",
       "     'select': {'anyOf': [{'items': {'$ref': '#/components/schemas/RunStatsSelect'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Select'}},\n",
       "    'type': 'object',\n",
       "    'title': 'RunStatsQueryParams',\n",
       "    'description': 'Query params for run stats.'},\n",
       "   'RunStatsSelect': {'type': 'string',\n",
       "    'enum': ['run_count',\n",
       "     'latency_p50',\n",
       "     'latency_p99',\n",
       "     'latency_avg',\n",
       "     'first_token_p50',\n",
       "     'first_token_p99',\n",
       "     'total_tokens',\n",
       "     'prompt_tokens',\n",
       "     'completion_tokens',\n",
       "     'median_tokens',\n",
       "     'completion_tokens_p50',\n",
       "     'prompt_tokens_p50',\n",
       "     'tokens_p99',\n",
       "     'completion_tokens_p99',\n",
       "     'prompt_tokens_p99',\n",
       "     'last_run_start_time',\n",
       "     'feedback_stats',\n",
       "     'thread_feedback_stats',\n",
       "     'run_facets',\n",
       "     'error_rate',\n",
       "     'streaming_rate',\n",
       "     'total_cost',\n",
       "     'prompt_cost',\n",
       "     'completion_cost',\n",
       "     'cost_p50',\n",
       "     'cost_p99',\n",
       "     'session_feedback_stats',\n",
       "     'all_run_stats',\n",
       "     'all_token_stats',\n",
       "     'prompt_token_details',\n",
       "     'completion_token_details',\n",
       "     'prompt_cost_details',\n",
       "     'completion_cost_details'],\n",
       "    'title': 'RunStatsSelect',\n",
       "    'description': 'Metrics you can select from run stats endpoint.'},\n",
       "   'RunTypeEnum': {'type': 'string',\n",
       "    'enum': ['tool',\n",
       "     'chain',\n",
       "     'llm',\n",
       "     'retriever',\n",
       "     'embedding',\n",
       "     'prompt',\n",
       "     'parser'],\n",
       "    'title': 'RunTypeEnum',\n",
       "    'description': 'Enum for run types.'},\n",
       "   'RunnableConfig': {'properties': {'tags': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Tags'},\n",
       "     'metadata': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Metadata'},\n",
       "     'callbacks': {'anyOf': [{'items': {}, 'type': 'array'},\n",
       "       {},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Callbacks'},\n",
       "     'run_name': {'type': 'string', 'title': 'Run Name'},\n",
       "     'max_concurrency': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Max Concurrency'},\n",
       "     'recursion_limit': {'type': 'integer', 'title': 'Recursion Limit'},\n",
       "     'configurable': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Configurable'},\n",
       "     'run_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Run Id'}},\n",
       "    'type': 'object',\n",
       "    'title': 'RunnableConfig',\n",
       "    'description': 'Configuration for a `Runnable`.\\n\\nSee the [reference docs](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig)\\nfor more details.'},\n",
       "   'RunnerContextEnum': {'type': 'string',\n",
       "    'enum': ['langsmith_ui', 'langsmith_align_evals'],\n",
       "    'title': 'RunnerContextEnum'},\n",
       "   'RunsFilterDataSourceTypeEnum': {'type': 'string',\n",
       "    'enum': ['current',\n",
       "     'historical',\n",
       "     'lite',\n",
       "     'root_lite',\n",
       "     'runs_feedbacks_rmt_wide'],\n",
       "    'title': 'RunsFilterDataSourceTypeEnum',\n",
       "    'description': 'Enum for run data source types.'},\n",
       "   'RunsGenerateQueryFeedbackKeys': {'type': 'string',\n",
       "    'enum': ['user_score',\n",
       "     'user_edited',\n",
       "     'user_removed',\n",
       "     'user_opened_run',\n",
       "     'user_selected_run',\n",
       "     'results_size',\n",
       "     'valid_filter'],\n",
       "    'title': 'RunsGenerateQueryFeedbackKeys'},\n",
       "   'SSOConfirmEmailRequest': {'properties': {'token': {'type': 'string',\n",
       "      'title': 'Token'}},\n",
       "    'type': 'object',\n",
       "    'required': ['token'],\n",
       "    'title': 'SSOConfirmEmailRequest'},\n",
       "   'SSOEmailVerificationSendRequest': {'properties': {'email': {'type': 'string',\n",
       "      'title': 'Email'},\n",
       "     'saml_provider_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Saml Provider Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['email', 'saml_provider_id'],\n",
       "    'title': 'SSOEmailVerificationSendRequest'},\n",
       "   'SSOEmailVerificationStatusRequest': {'properties': {'email': {'type': 'string',\n",
       "      'title': 'Email'},\n",
       "     'saml_provider_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Saml Provider Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['email', 'saml_provider_id'],\n",
       "    'title': 'SSOEmailVerificationStatusRequest'},\n",
       "   'SSOEmailVerificationStatusResponse': {'properties': {'email_confirmed_at': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Email Confirmed At'}},\n",
       "    'type': 'object',\n",
       "    'title': 'SSOEmailVerificationStatusResponse'},\n",
       "   'SSOProvider': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'organization_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Organization Id'},\n",
       "     'provider_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Provider Id'},\n",
       "     'default_workspace_role_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Default Workspace Role Id'},\n",
       "     'default_workspace_ids': {'items': {'type': 'string', 'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'title': 'Default Workspace Ids'},\n",
       "     'metadata_url': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Metadata Url'},\n",
       "     'metadata_xml': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Metadata Xml'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'organization_id',\n",
       "     'provider_id',\n",
       "     'default_workspace_role_id',\n",
       "     'default_workspace_ids'],\n",
       "    'title': 'SSOProvider'},\n",
       "   'SSOProviderSlim': {'properties': {'provider_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Provider Id'},\n",
       "     'organization_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Organization Id'},\n",
       "     'organization_display_name': {'type': 'string',\n",
       "      'title': 'Organization Display Name'}},\n",
       "    'type': 'object',\n",
       "    'required': ['provider_id',\n",
       "     'organization_id',\n",
       "     'organization_display_name'],\n",
       "    'title': 'SSOProviderSlim'},\n",
       "   'SSOSettingsCreate': {'properties': {'default_workspace_role_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Default Workspace Role Id'},\n",
       "     'default_workspace_ids': {'items': {'type': 'string', 'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'title': 'Default Workspace Ids'},\n",
       "     'metadata_xml': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Metadata Xml'},\n",
       "     'metadata_url': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Metadata Url'},\n",
       "     'attribute_mapping': {'additionalProperties': {'type': 'string'},\n",
       "      'type': 'object',\n",
       "      'title': 'Attribute Mapping'}},\n",
       "    'type': 'object',\n",
       "    'required': ['default_workspace_role_id', 'default_workspace_ids'],\n",
       "    'title': 'SSOSettingsCreate'},\n",
       "   'SSOSettingsUpdate': {'properties': {'default_workspace_role_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Workspace Role Id'},\n",
       "     'default_workspace_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Workspace Ids'},\n",
       "     'metadata_url': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Metadata Url'},\n",
       "     'metadata_xml': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Metadata Xml'}},\n",
       "    'type': 'object',\n",
       "    'title': 'SSOSettingsUpdate'},\n",
       "   'SavedRunClusteringJobRequest': {'properties': {'name': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'last_n_hours': {'type': 'integer', 'title': 'Last N Hours'},\n",
       "     'hierarchy': {'anyOf': [{'items': {'type': 'integer'}, 'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Hierarchy'},\n",
       "     'partitions': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Partitions'},\n",
       "     'sample': {'anyOf': [{'type': 'number'},\n",
       "       {'type': 'integer'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Sample'},\n",
       "     'summary_prompt': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Summary Prompt'},\n",
       "     'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Filter'},\n",
       "     'attribute_schemas': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Attribute Schemas'},\n",
       "     'user_context': {'anyOf': [{'additionalProperties': {'type': 'string'},\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'User Context'},\n",
       "     'model': {'type': 'string',\n",
       "      'enum': ['openai', 'anthropic'],\n",
       "      'title': 'Model'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name',\n",
       "     'last_n_hours',\n",
       "     'hierarchy',\n",
       "     'partitions',\n",
       "     'sample',\n",
       "     'summary_prompt',\n",
       "     'filter',\n",
       "     'attribute_schemas',\n",
       "     'model'],\n",
       "    'title': 'SavedRunClusteringJobRequest',\n",
       "    'description': 'Request to create a run clustering job.'},\n",
       "   'SearchDatasetRequest': {'properties': {'inputs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Inputs'},\n",
       "     'limit': {'type': 'integer',\n",
       "      'maximum': 100.0,\n",
       "      'minimum': 1.0,\n",
       "      'title': 'Limit',\n",
       "      'default': 5},\n",
       "     'debug': {'type': 'boolean', 'title': 'Debug', 'default': False},\n",
       "     'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Filter'}},\n",
       "    'type': 'object',\n",
       "    'required': ['inputs'],\n",
       "    'title': 'SearchDatasetRequest',\n",
       "    'description': 'Dataset schema for serving.'},\n",
       "   'SearchDatasetResponse': {'properties': {'examples': {'items': {'$ref': '#/components/schemas/SearchedFewShotExample'},\n",
       "      'type': 'array',\n",
       "      'title': 'Examples'}},\n",
       "    'type': 'object',\n",
       "    'required': ['examples'],\n",
       "    'title': 'SearchDatasetResponse',\n",
       "    'description': 'Dataset schema for serving.'},\n",
       "   'SearchedFewShotExample': {'properties': {'inputs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Inputs'},\n",
       "     'outputs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Outputs'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'debug_info': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Debug Info'}},\n",
       "    'type': 'object',\n",
       "    'required': ['inputs', 'outputs', 'id'],\n",
       "    'title': 'SearchedFewShotExample',\n",
       "    'description': 'Dataset schema for serving.'},\n",
       "   'SecretKey': {'properties': {'key': {'type': 'string', 'title': 'Key'}},\n",
       "    'type': 'object',\n",
       "    'required': ['key'],\n",
       "    'title': 'SecretKey'},\n",
       "   'SecretUpsert': {'properties': {'key': {'type': 'string', 'title': 'Key'},\n",
       "     'value': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Value'}},\n",
       "    'type': 'object',\n",
       "    'required': ['key', 'value'],\n",
       "    'title': 'SecretUpsert'},\n",
       "   'ServiceAccount': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'organization_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Organization Id'},\n",
       "     'default_workspace_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Workspace Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'created_at',\n",
       "     'updated_at',\n",
       "     'name',\n",
       "     'organization_id',\n",
       "     'default_workspace_id'],\n",
       "    'title': 'ServiceAccount'},\n",
       "   'ServiceAccountCreateRequest': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'workspaces': {'items': {'$ref': '#/components/schemas/ServiceAccountWorkspaceAssignment'},\n",
       "      'type': 'array',\n",
       "      'title': 'Workspaces',\n",
       "      'default': []}},\n",
       "    'type': 'object',\n",
       "    'required': ['name'],\n",
       "    'title': 'ServiceAccountCreateRequest'},\n",
       "   'ServiceAccountCreateResponse': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'organization_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Organization Id'},\n",
       "     'default_workspace_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Workspace Id'},\n",
       "     'organization_identity_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Organization Identity Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'created_at',\n",
       "     'updated_at',\n",
       "     'name',\n",
       "     'organization_id',\n",
       "     'default_workspace_id',\n",
       "     'organization_identity_id'],\n",
       "    'title': 'ServiceAccountCreateResponse'},\n",
       "   'ServiceAccountDeleteResponse': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'organization_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Organization Id'},\n",
       "     'default_workspace_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Workspace Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'created_at',\n",
       "     'updated_at',\n",
       "     'name',\n",
       "     'organization_id',\n",
       "     'default_workspace_id'],\n",
       "    'title': 'ServiceAccountDeleteResponse'},\n",
       "   'ServiceAccountWorkspaceAssignment': {'properties': {'workspace_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Workspace Id'},\n",
       "     'role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Role Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['workspace_id'],\n",
       "    'title': 'ServiceAccountWorkspaceAssignment'},\n",
       "   'SessionFeedbackDelta': {'properties': {'feedback_deltas': {'additionalProperties': {'$ref': '#/components/schemas/FeedbackDelta'},\n",
       "      'propertyNames': {'format': 'uuid'},\n",
       "      'type': 'object',\n",
       "      'title': 'Feedback Deltas'}},\n",
       "    'type': 'object',\n",
       "    'required': ['feedback_deltas'],\n",
       "    'title': 'SessionFeedbackDelta',\n",
       "    'description': 'List of feedback keys with number of improvements and regressions for each.'},\n",
       "   'SessionSortableColumns': {'type': 'string',\n",
       "    'enum': ['name',\n",
       "     'start_time',\n",
       "     'last_run_start_time',\n",
       "     'latency_p50',\n",
       "     'latency_p99',\n",
       "     'error_rate',\n",
       "     'feedback',\n",
       "     'runs_count'],\n",
       "    'title': 'SessionSortableColumns'},\n",
       "   'SetTenantHandleRequest': {'properties': {'tenant_handle': {'type': 'string',\n",
       "      'title': 'Tenant Handle'}},\n",
       "    'type': 'object',\n",
       "    'required': ['tenant_handle'],\n",
       "    'title': 'SetTenantHandleRequest'},\n",
       "   'SimpleExperimentInfo': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'name': {'type': 'string', 'title': 'Name'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'name'],\n",
       "    'title': 'SimpleExperimentInfo',\n",
       "    'description': 'Simple experiment info schema for use with comparative experiments'},\n",
       "   'SingleCustomChartResponse': {'properties': {'data': {'items': {'$ref': '#/components/schemas/CustomChartsDataPoint'},\n",
       "      'type': 'array',\n",
       "      'title': 'Data'},\n",
       "     'id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'string'}],\n",
       "      'title': 'Id'},\n",
       "     'title': {'type': 'string', 'title': 'Title'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'},\n",
       "     'index': {'type': 'integer', 'title': 'Index'},\n",
       "     'chart_type': {'$ref': '#/components/schemas/CustomChartType'},\n",
       "     'series': {'items': {'$ref': '#/components/schemas/CustomChartSeries'},\n",
       "      'type': 'array',\n",
       "      'title': 'Series'},\n",
       "     'common_filters': {'anyOf': [{'$ref': '#/components/schemas/CustomChartSeriesFilters'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'required': ['data', 'id', 'title', 'index', 'chart_type', 'series'],\n",
       "    'title': 'SingleCustomChartResponse'},\n",
       "   'SingleCustomChartResponseBase': {'properties': {'data': {'items': {'$ref': '#/components/schemas/CustomChartsDataPoint'},\n",
       "      'type': 'array',\n",
       "      'title': 'Data'}},\n",
       "    'type': 'object',\n",
       "    'required': ['data'],\n",
       "    'title': 'SingleCustomChartResponseBase'},\n",
       "   'SingleCustomChartSubSectionResponse': {'properties': {'title': {'type': 'string',\n",
       "      'title': 'Title'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'index': {'type': 'integer', 'title': 'Index'},\n",
       "     'id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'string'}],\n",
       "      'title': 'Id'},\n",
       "     'charts': {'items': {'$ref': '#/components/schemas/SingleCustomChartResponse'},\n",
       "      'type': 'array',\n",
       "      'title': 'Charts'}},\n",
       "    'type': 'object',\n",
       "    'required': ['title', 'index', 'id', 'charts'],\n",
       "    'title': 'SingleCustomChartSubSectionResponse'},\n",
       "   'SortByComparativeExperimentColumn': {'type': 'string',\n",
       "    'enum': ['name', 'created_at'],\n",
       "    'title': 'SortByComparativeExperimentColumn',\n",
       "    'description': 'Enum for available comparative experiment columns to sort by.'},\n",
       "   'SortByDatasetColumn': {'type': 'string',\n",
       "    'enum': ['name',\n",
       "     'created_at',\n",
       "     'last_session_start_time',\n",
       "     'example_count',\n",
       "     'session_count',\n",
       "     'modified_at'],\n",
       "    'title': 'SortByDatasetColumn',\n",
       "    'description': 'Enum for available dataset columns to sort by.'},\n",
       "   'SortParamsForRunsComparisonView': {'properties': {'sort_by': {'type': 'string',\n",
       "      'title': 'Sort By'},\n",
       "     'sort_order': {'type': 'string',\n",
       "      'enum': ['ASC', 'DESC'],\n",
       "      'title': 'Sort Order',\n",
       "      'default': 'DESC'}},\n",
       "    'type': 'object',\n",
       "    'required': ['sort_by'],\n",
       "    'title': 'SortParamsForRunsComparisonView'},\n",
       "   'SourceType': {'type': 'string',\n",
       "    'enum': ['api', 'model', 'app', 'auto_eval'],\n",
       "    'title': 'SourceType',\n",
       "    'description': 'Enum for feedback source types.'},\n",
       "   'StripeAccountLinksCreate': {'properties': {'success_path': {'type': 'string',\n",
       "      'title': 'Success Path'}},\n",
       "    'type': 'object',\n",
       "    'required': ['success_path'],\n",
       "    'title': 'StripeAccountLinksCreate'},\n",
       "   'StripeBusinessBillingInfo': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'address': {'anyOf': [{'$ref': '#/components/schemas/StripeCustomerAddress'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'required': ['name'],\n",
       "    'title': 'StripeBusinessBillingInfo',\n",
       "    'description': 'Stripe customer billing information.'},\n",
       "   'StripeBusinessInfo-Input': {'properties': {'company_info': {'anyOf': [{'$ref': '#/components/schemas/StripeBusinessBillingInfo'},\n",
       "       {'type': 'null'}]},\n",
       "     'tax_id': {'anyOf': [{'$ref': '#/components/schemas/StripeTaxId'},\n",
       "       {'type': 'null'}]},\n",
       "     'invoice_email': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Invoice Email'},\n",
       "     'is_business': {'type': 'boolean',\n",
       "      'title': 'Is Business',\n",
       "      'default': False}},\n",
       "    'type': 'object',\n",
       "    'title': 'StripeBusinessInfo'},\n",
       "   'StripeBusinessInfo-Output': {'properties': {'company_info': {'anyOf': [{'$ref': '#/components/schemas/StripeBusinessBillingInfo'},\n",
       "       {'type': 'null'}]},\n",
       "     'tax_id': {'anyOf': [{'$ref': '#/components/schemas/StripeTaxId'},\n",
       "       {'type': 'null'}]},\n",
       "     'invoice_email': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Invoice Email'},\n",
       "     'is_business': {'type': 'boolean',\n",
       "      'title': 'Is Business',\n",
       "      'default': False}},\n",
       "    'type': 'object',\n",
       "    'title': 'StripeBusinessInfo'},\n",
       "   'StripeCheckoutSessionsConfirm': {'properties': {'stripe_session_id': {'type': 'string',\n",
       "      'title': 'Stripe Session Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['stripe_session_id'],\n",
       "    'title': 'StripeCheckoutSessionsConfirm'},\n",
       "   'StripeCheckoutSessionsCreate': {'properties': {'amount_cents': {'type': 'integer',\n",
       "      'title': 'Amount Cents'},\n",
       "     'success_path': {'type': 'string', 'title': 'Success Path'}},\n",
       "    'type': 'object',\n",
       "    'required': ['amount_cents', 'success_path'],\n",
       "    'title': 'StripeCheckoutSessionsCreate'},\n",
       "   'StripeCustomerAddress': {'properties': {'line1': {'type': 'string',\n",
       "      'title': 'Line1'},\n",
       "     'line2': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Line2'},\n",
       "     'city': {'type': 'string', 'title': 'City'},\n",
       "     'state': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'State'},\n",
       "     'postal_code': {'type': 'string', 'title': 'Postal Code'},\n",
       "     'country': {'type': 'string', 'title': 'Country'}},\n",
       "    'type': 'object',\n",
       "    'required': ['line1', 'city', 'postal_code', 'country'],\n",
       "    'title': 'StripeCustomerAddress',\n",
       "    'description': 'Stripe customer address.'},\n",
       "   'StripeCustomerBillingInfo': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'address': {'$ref': '#/components/schemas/StripeCustomerAddress'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'address'],\n",
       "    'title': 'StripeCustomerBillingInfo',\n",
       "    'description': 'Stripe customer billing information.'},\n",
       "   'StripePaymentInformation': {'properties': {'billing_info': {'$ref': '#/components/schemas/StripeCustomerBillingInfo'},\n",
       "     'setup_intent': {'type': 'string', 'title': 'Setup Intent'}},\n",
       "    'type': 'object',\n",
       "    'required': ['billing_info', 'setup_intent'],\n",
       "    'title': 'StripePaymentInformation',\n",
       "    'description': 'Stripe payment information.'},\n",
       "   'StripePaymentMethodInfo': {'properties': {'brand': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Brand'},\n",
       "     'last4': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Last4'},\n",
       "     'exp_month': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Exp Month'},\n",
       "     'exp_year': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Exp Year'},\n",
       "     'email': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Email'}},\n",
       "    'type': 'object',\n",
       "    'title': 'StripePaymentMethodInfo',\n",
       "    'description': 'Stripe customer billing info.'},\n",
       "   'StripeSetupIntentResponse': {'properties': {'client_secret': {'type': 'string',\n",
       "      'title': 'Client Secret'}},\n",
       "    'type': 'object',\n",
       "    'required': ['client_secret'],\n",
       "    'title': 'StripeSetupIntentResponse',\n",
       "    'description': 'Stripe setup intent response.'},\n",
       "   'StripeTaxId': {'properties': {'value': {'type': 'string',\n",
       "      'title': 'Value'},\n",
       "     'type': {'type': 'string', 'title': 'Type'}},\n",
       "    'type': 'object',\n",
       "    'required': ['value', 'type'],\n",
       "    'title': 'StripeTaxId',\n",
       "    'description': 'Stripe tax ID.'},\n",
       "   'StudioRunOverDatasetRequestSchema': {'properties': {'project_name': {'type': 'string',\n",
       "      'title': 'Project Name'},\n",
       "     'dataset_id': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'},\n",
       "     'evaluator_rules': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Evaluator Rules'},\n",
       "     'metadata': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Metadata'}},\n",
       "    'type': 'object',\n",
       "    'required': ['project_name', 'dataset_id'],\n",
       "    'title': 'StudioRunOverDatasetRequestSchema'},\n",
       "   'SystemMessage': {'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "       {'items': {'anyOf': [{'type': 'string'},\n",
       "          {'additionalProperties': True, 'type': 'object'}]},\n",
       "        'type': 'array'}],\n",
       "      'title': 'Content'},\n",
       "     'additional_kwargs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Additional Kwargs'},\n",
       "     'response_metadata': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Response Metadata'},\n",
       "     'type': {'type': 'string',\n",
       "      'const': 'system',\n",
       "      'title': 'Type',\n",
       "      'default': 'system'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'}},\n",
       "    'additionalProperties': True,\n",
       "    'type': 'object',\n",
       "    'required': ['content'],\n",
       "    'title': 'SystemMessage',\n",
       "    'description': 'Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n    ```python\\n    from langchain_core.messages import HumanMessage, SystemMessage\\n\\n    messages = [\\n        SystemMessage(content=\"You are a helpful assistant! Your name is Bob.\"),\\n        HumanMessage(content=\"What is your name?\"),\\n    ]\\n\\n    # Define a chat model and invoke it with the messages\\n    print(model.invoke(messages))\\n    ```'},\n",
       "   'SystemMessageChunk': {'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "       {'items': {'anyOf': [{'type': 'string'},\n",
       "          {'additionalProperties': True, 'type': 'object'}]},\n",
       "        'type': 'array'}],\n",
       "      'title': 'Content'},\n",
       "     'additional_kwargs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Additional Kwargs'},\n",
       "     'response_metadata': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Response Metadata'},\n",
       "     'type': {'type': 'string',\n",
       "      'const': 'SystemMessageChunk',\n",
       "      'title': 'Type',\n",
       "      'default': 'SystemMessageChunk'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'}},\n",
       "    'additionalProperties': True,\n",
       "    'type': 'object',\n",
       "    'required': ['content'],\n",
       "    'title': 'SystemMessageChunk',\n",
       "    'description': 'System Message chunk.'},\n",
       "   'TTLSettings': {'properties': {'tenant_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tenant Id'},\n",
       "     'default_trace_tier': {'$ref': '#/components/schemas/TraceTier'},\n",
       "     'apply_to_all_projects': {'type': 'boolean',\n",
       "      'title': 'Apply To All Projects',\n",
       "      'default': False},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'organization_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Organization Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'configured_by': {'$ref': '#/components/schemas/ConfiguredBy'}},\n",
       "    'type': 'object',\n",
       "    'required': ['default_trace_tier',\n",
       "     'id',\n",
       "     'organization_id',\n",
       "     'created_at',\n",
       "     'updated_at',\n",
       "     'configured_by'],\n",
       "    'title': 'TTLSettings',\n",
       "    'description': 'TTL settings model.'},\n",
       "   'TagCount': {'properties': {'tag': {'type': 'string', 'title': 'Tag'},\n",
       "     'count': {'type': 'integer', 'title': 'Count'}},\n",
       "    'type': 'object',\n",
       "    'required': ['tag', 'count'],\n",
       "    'title': 'TagCount'},\n",
       "   'TagKey': {'properties': {'key': {'type': 'string',\n",
       "      'maxLength': 255,\n",
       "      'minLength': 1,\n",
       "      'title': 'Key'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'}},\n",
       "    'type': 'object',\n",
       "    'required': ['key', 'id', 'created_at', 'updated_at'],\n",
       "    'title': 'TagKey'},\n",
       "   'TagKeyCreate': {'properties': {'key': {'type': 'string',\n",
       "      'maxLength': 255,\n",
       "      'minLength': 1,\n",
       "      'title': 'Key'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'}},\n",
       "    'type': 'object',\n",
       "    'required': ['key'],\n",
       "    'title': 'TagKeyCreate'},\n",
       "   'TagKeyUpdate': {'properties': {'key': {'anyOf': [{'type': 'string',\n",
       "        'maxLength': 255,\n",
       "        'minLength': 1},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Key'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'}},\n",
       "    'type': 'object',\n",
       "    'title': 'TagKeyUpdate'},\n",
       "   'TagKeyWithValues': {'properties': {'key': {'type': 'string',\n",
       "      'maxLength': 255,\n",
       "      'minLength': 1,\n",
       "      'title': 'Key'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'values': {'items': {'$ref': '#/components/schemas/TagValue'},\n",
       "      'type': 'array',\n",
       "      'title': 'Values'}},\n",
       "    'type': 'object',\n",
       "    'required': ['key', 'id', 'created_at', 'updated_at'],\n",
       "    'title': 'TagKeyWithValues'},\n",
       "   'TagKeyWithValuesAndTaggings': {'properties': {'key': {'type': 'string',\n",
       "      'maxLength': 255,\n",
       "      'minLength': 1,\n",
       "      'title': 'Key'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'values': {'items': {'$ref': '#/components/schemas/TagValueWithTaggings'},\n",
       "      'type': 'array',\n",
       "      'title': 'Values'}},\n",
       "    'type': 'object',\n",
       "    'required': ['key', 'id', 'created_at', 'updated_at'],\n",
       "    'title': 'TagKeyWithValuesAndTaggings'},\n",
       "   'TagValue': {'properties': {'value': {'type': 'string',\n",
       "      'maxLength': 255,\n",
       "      'minLength': 1,\n",
       "      'title': 'Value'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'tag_key_id': {'type': 'string', 'format': 'uuid', 'title': 'Tag Key Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'}},\n",
       "    'type': 'object',\n",
       "    'required': ['value', 'id', 'tag_key_id', 'created_at', 'updated_at'],\n",
       "    'title': 'TagValue'},\n",
       "   'TagValueCreate': {'properties': {'value': {'type': 'string',\n",
       "      'maxLength': 255,\n",
       "      'minLength': 1,\n",
       "      'title': 'Value'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'}},\n",
       "    'type': 'object',\n",
       "    'required': ['value'],\n",
       "    'title': 'TagValueCreate'},\n",
       "   'TagValueUpdate': {'properties': {'value': {'anyOf': [{'type': 'string',\n",
       "        'maxLength': 255,\n",
       "        'minLength': 1},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Value'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'}},\n",
       "    'type': 'object',\n",
       "    'title': 'TagValueUpdate'},\n",
       "   'TagValueWithTaggings': {'properties': {'value': {'type': 'string',\n",
       "      'maxLength': 255,\n",
       "      'minLength': 1,\n",
       "      'title': 'Value'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'tag_key_id': {'type': 'string', 'format': 'uuid', 'title': 'Tag Key Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'taggings': {'items': {'$ref': '#/components/schemas/Tagging'},\n",
       "      'type': 'array',\n",
       "      'title': 'Taggings'}},\n",
       "    'type': 'object',\n",
       "    'required': ['value', 'id', 'tag_key_id', 'created_at', 'updated_at'],\n",
       "    'title': 'TagValueWithTaggings'},\n",
       "   'Tagging': {'properties': {'tag_value_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Tag Value Id'},\n",
       "     'resource_type': {'$ref': '#/components/schemas/ResourceType'},\n",
       "     'resource_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Resource Id'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'}},\n",
       "    'type': 'object',\n",
       "    'required': ['tag_value_id',\n",
       "     'resource_type',\n",
       "     'resource_id',\n",
       "     'id',\n",
       "     'created_at'],\n",
       "    'title': 'Tagging'},\n",
       "   'TaggingCreate': {'properties': {'tag_value_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Tag Value Id'},\n",
       "     'resource_type': {'$ref': '#/components/schemas/ResourceType'},\n",
       "     'resource_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Resource Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['tag_value_id', 'resource_type', 'resource_id'],\n",
       "    'title': 'TaggingCreate'},\n",
       "   'TaggingsByResourceType': {'properties': {'prompts': {'items': {'$ref': '#/components/schemas/Resource'},\n",
       "      'type': 'array',\n",
       "      'title': 'Prompts',\n",
       "      'default': []},\n",
       "     'projects': {'items': {'$ref': '#/components/schemas/Resource'},\n",
       "      'type': 'array',\n",
       "      'title': 'Projects',\n",
       "      'default': []},\n",
       "     'queues': {'items': {'$ref': '#/components/schemas/Resource'},\n",
       "      'type': 'array',\n",
       "      'title': 'Queues',\n",
       "      'default': []},\n",
       "     'deployments': {'items': {'$ref': '#/components/schemas/Resource'},\n",
       "      'type': 'array',\n",
       "      'title': 'Deployments',\n",
       "      'default': []},\n",
       "     'experiments': {'items': {'$ref': '#/components/schemas/Resource'},\n",
       "      'type': 'array',\n",
       "      'title': 'Experiments',\n",
       "      'default': []},\n",
       "     'datasets': {'items': {'$ref': '#/components/schemas/Resource'},\n",
       "      'type': 'array',\n",
       "      'title': 'Datasets',\n",
       "      'default': []},\n",
       "     'dashboards': {'items': {'$ref': '#/components/schemas/Resource'},\n",
       "      'type': 'array',\n",
       "      'title': 'Dashboards',\n",
       "      'default': []}},\n",
       "    'type': 'object',\n",
       "    'title': 'TaggingsByResourceType'},\n",
       "   'TaggingsResponse': {'properties': {'tag_key': {'type': 'string',\n",
       "      'title': 'Tag Key'},\n",
       "     'tag_key_id': {'type': 'string', 'format': 'uuid', 'title': 'Tag Key Id'},\n",
       "     'tag_value': {'type': 'string', 'title': 'Tag Value'},\n",
       "     'tag_value_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Tag Value Id'},\n",
       "     'resources': {'$ref': '#/components/schemas/TaggingsByResourceType'}},\n",
       "    'type': 'object',\n",
       "    'required': ['tag_key',\n",
       "     'tag_key_id',\n",
       "     'tag_value',\n",
       "     'tag_value_id',\n",
       "     'resources'],\n",
       "    'title': 'TaggingsResponse'},\n",
       "   'TenantBulkUnshareRequest': {'properties': {'share_tokens': {'items': {'type': 'string',\n",
       "       'format': 'uuid'},\n",
       "      'type': 'array',\n",
       "      'minItems': 1,\n",
       "      'title': 'Share Tokens'}},\n",
       "    'type': 'object',\n",
       "    'title': 'TenantBulkUnshareRequest'},\n",
       "   'TenantCreate': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'organization_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Organization Id'},\n",
       "     'display_name': {'type': 'string',\n",
       "      'minLength': 1,\n",
       "      'pattern': \"^[a-zA-Z0-9\\\\-_ ']+$\",\n",
       "      'title': 'Display Name'},\n",
       "     'tenant_handle': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tenant Handle'},\n",
       "     'is_personal': {'type': 'boolean',\n",
       "      'title': 'Is Personal',\n",
       "      'default': False}},\n",
       "    'type': 'object',\n",
       "    'required': ['display_name'],\n",
       "    'title': 'TenantCreate',\n",
       "    'description': 'Creation model for the tenant.'},\n",
       "   'TenantForUser': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'organization_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Organization Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'display_name': {'type': 'string', 'title': 'Display Name'},\n",
       "     'is_personal': {'type': 'boolean', 'title': 'Is Personal'},\n",
       "     'is_deleted': {'type': 'boolean', 'title': 'Is Deleted'},\n",
       "     'tenant_handle': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tenant Handle'},\n",
       "     'read_only': {'type': 'boolean',\n",
       "      'title': 'Read Only',\n",
       "      'default': False,\n",
       "      'deprecated': True},\n",
       "     'role_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Role Id'},\n",
       "     'role_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Role Name'},\n",
       "     'permissions': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Permissions'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'created_at',\n",
       "     'display_name',\n",
       "     'is_personal',\n",
       "     'is_deleted'],\n",
       "    'title': 'TenantForUser'},\n",
       "   'TenantMembers': {'properties': {'tenant_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Tenant Id'},\n",
       "     'members': {'items': {'$ref': '#/components/schemas/MemberIdentity'},\n",
       "      'type': 'array',\n",
       "      'title': 'Members'},\n",
       "     'pending': {'items': {'$ref': '#/components/schemas/PendingIdentity'},\n",
       "      'type': 'array',\n",
       "      'title': 'Pending'}},\n",
       "    'type': 'object',\n",
       "    'required': ['tenant_id', 'members', 'pending'],\n",
       "    'title': 'TenantMembers',\n",
       "    'description': 'Tenant members schema.'},\n",
       "   'TenantShareDatasetToken': {'properties': {'type': {'type': 'string',\n",
       "      'const': 'dataset',\n",
       "      'title': 'Type'},\n",
       "     'share_token': {'type': 'string', 'title': 'Share Token'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'dataset_id': {'type': 'string', 'format': 'uuid', 'title': 'Dataset Id'},\n",
       "     'dataset_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Dataset Name'}},\n",
       "    'type': 'object',\n",
       "    'required': ['type', 'share_token', 'created_at', 'dataset_id'],\n",
       "    'title': 'TenantShareDatasetToken'},\n",
       "   'TenantShareRunToken': {'properties': {'type': {'type': 'string',\n",
       "      'const': 'run',\n",
       "      'title': 'Type'},\n",
       "     'share_token': {'type': 'string', 'title': 'Share Token'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'run_id': {'type': 'string', 'format': 'uuid', 'title': 'Run Id'},\n",
       "     'run_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Run Name'},\n",
       "     'run_type': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Run Type'},\n",
       "     'session_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Id'},\n",
       "     'session_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Session Name'}},\n",
       "    'type': 'object',\n",
       "    'required': ['type', 'share_token', 'created_at', 'run_id'],\n",
       "    'title': 'TenantShareRunToken'},\n",
       "   'TenantShareTokensResponse': {'properties': {'entities': {'items': {'oneOf': [{'$ref': '#/components/schemas/TenantShareRunToken'},\n",
       "        {'$ref': '#/components/schemas/TenantShareDatasetToken'}],\n",
       "       'discriminator': {'propertyName': 'type',\n",
       "        'mapping': {'dataset': '#/components/schemas/TenantShareDatasetToken',\n",
       "         'run': '#/components/schemas/TenantShareRunToken'}}},\n",
       "      'type': 'array',\n",
       "      'title': 'Entities'}},\n",
       "    'type': 'object',\n",
       "    'required': ['entities'],\n",
       "    'title': 'TenantShareTokensResponse'},\n",
       "   'TenantStats': {'properties': {'tenant_id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Tenant Id'},\n",
       "     'dataset_count': {'type': 'integer', 'title': 'Dataset Count'},\n",
       "     'tracer_session_count': {'type': 'integer',\n",
       "      'title': 'Tracer Session Count'},\n",
       "     'repo_count': {'type': 'integer', 'title': 'Repo Count'},\n",
       "     'annotation_queue_count': {'type': 'integer',\n",
       "      'title': 'Annotation Queue Count'},\n",
       "     'deployment_count': {'type': 'integer', 'title': 'Deployment Count'},\n",
       "     'dashboards_count': {'type': 'integer', 'title': 'Dashboards Count'}},\n",
       "    'type': 'object',\n",
       "    'required': ['tenant_id',\n",
       "     'dataset_count',\n",
       "     'tracer_session_count',\n",
       "     'repo_count',\n",
       "     'annotation_queue_count',\n",
       "     'deployment_count',\n",
       "     'dashboards_count'],\n",
       "    'title': 'TenantStats',\n",
       "    'description': 'Stats for a tenant.'},\n",
       "   'TenantUsageLimitInfo': {'properties': {'in_reject_set': {'type': 'boolean',\n",
       "      'title': 'In Reject Set'},\n",
       "     'usage_limit_type': {'anyOf': [{'$ref': '#/components/schemas/TenantUsageLimitType'},\n",
       "       {'type': 'null'}]},\n",
       "     'tenant_limit': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Tenant Limit'}},\n",
       "    'type': 'object',\n",
       "    'required': ['in_reject_set'],\n",
       "    'title': 'TenantUsageLimitInfo'},\n",
       "   'TenantUsageLimitType': {'type': 'string',\n",
       "    'enum': ['payload_size',\n",
       "     'events_ingested_per_hour',\n",
       "     'total_unique_traces',\n",
       "     'events_ingested_per_minute',\n",
       "     'user_defined_monthly_traces',\n",
       "     'user_defined_monthly_longlived_traces',\n",
       "     'user_defined_unknown'],\n",
       "    'title': 'TenantUsageLimitType'},\n",
       "   'ThreadMessagesFormatType': {'type': 'string',\n",
       "    'enum': ['all_messages', 'human_ai_pairs', 'first_human_last_ai'],\n",
       "    'title': 'ThreadMessagesFormatType',\n",
       "    'description': 'Enum for thread messages format types.'},\n",
       "   'ThreadPreviewResponse': {'properties': {'thread_id': {'type': 'string',\n",
       "      'title': 'Thread Id'},\n",
       "     'previews': {'additionalProperties': {'type': 'string'},\n",
       "      'propertyNames': {'$ref': '#/components/schemas/ThreadMessagesFormatType'},\n",
       "      'type': 'object',\n",
       "      'title': 'Previews'}},\n",
       "    'type': 'object',\n",
       "    'required': ['thread_id', 'previews'],\n",
       "    'title': 'ThreadPreviewResponse',\n",
       "    'description': 'Response to preview a thread.'},\n",
       "   'TimedeltaInput': {'properties': {'days': {'type': 'integer',\n",
       "      'title': 'Days',\n",
       "      'default': 0},\n",
       "     'hours': {'type': 'integer', 'title': 'Hours', 'default': 0},\n",
       "     'minutes': {'type': 'integer', 'title': 'Minutes', 'default': 0}},\n",
       "    'type': 'object',\n",
       "    'title': 'TimedeltaInput',\n",
       "    'description': 'Timedelta input.'},\n",
       "   'ToolCall': {'properties': {'name': {'type': 'string', 'title': 'Name'},\n",
       "     'args': {'additionalProperties': True, 'type': 'object', 'title': 'Args'},\n",
       "     'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "     'type': {'type': 'string', 'const': 'tool_call', 'title': 'Type'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'args', 'id'],\n",
       "    'title': 'ToolCall',\n",
       "    'description': 'Represents an AI\\'s request to call a tool.\\n\\nExample:\\n    ```python\\n    {\"name\": \"foo\", \"args\": {\"a\": 1}, \"id\": \"123\"}\\n    ```\\n\\n    This represents a request to call the tool named `\\'foo\\'` with arguments\\n    `{\"a\": 1}` and an identifier of `\\'123\\'`.'},\n",
       "   'ToolCallChunk': {'properties': {'name': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Args'},\n",
       "     'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "     'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Index'},\n",
       "     'type': {'type': 'string', 'const': 'tool_call_chunk', 'title': 'Type'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'args', 'id', 'index'],\n",
       "    'title': 'ToolCallChunk',\n",
       "    'description': 'A chunk of a tool call (yielded when streaming).\\n\\nWhen merging `ToolCallChunk`s (e.g., via `AIMessageChunk.__add__`),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n```python\\nleft_chunks = [ToolCallChunk(name=\"foo\", args=\\'{\"a\":\\', index=0)]\\nright_chunks = [ToolCallChunk(name=None, args=\"1}\", index=0)]\\n\\n(\\n    AIMessageChunk(content=\"\", tool_call_chunks=left_chunks)\\n    + AIMessageChunk(content=\"\", tool_call_chunks=right_chunks)\\n).tool_call_chunks == [ToolCallChunk(name=\"foo\", args=\\'{\"a\":1}\\', index=0)]\\n```'},\n",
       "   'ToolMessage': {'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "       {'items': {'anyOf': [{'type': 'string'},\n",
       "          {'additionalProperties': True, 'type': 'object'}]},\n",
       "        'type': 'array'}],\n",
       "      'title': 'Content'},\n",
       "     'additional_kwargs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Additional Kwargs'},\n",
       "     'response_metadata': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Response Metadata'},\n",
       "     'type': {'type': 'string',\n",
       "      'const': 'tool',\n",
       "      'title': 'Type',\n",
       "      'default': 'tool'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "     'tool_call_id': {'type': 'string', 'title': 'Tool Call Id'},\n",
       "     'artifact': {'title': 'Artifact'},\n",
       "     'status': {'type': 'string',\n",
       "      'enum': ['success', 'error'],\n",
       "      'title': 'Status',\n",
       "      'default': 'success'}},\n",
       "    'additionalProperties': True,\n",
       "    'type': 'object',\n",
       "    'required': ['content', 'tool_call_id'],\n",
       "    'title': 'ToolMessage',\n",
       "    'description': 'Message for passing the result of executing a tool back to a model.\\n\\n`ToolMessage` objects contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\n`tool_call_id` is used to associate the tool call request with the tool call\\nresponse. Useful in situations where a chat model is able to request multiple tool\\ncalls in parallel.\\n\\nExample:\\n    A `ToolMessage` representing a result of `42` from a tool call with id\\n\\n    ```python\\n    from langchain_core.messages import ToolMessage\\n\\n    ToolMessage(content=\"42\", tool_call_id=\"call_Jja7J89XsjrOLA5r!MEOW!SL\")\\n    ```\\n\\nExample:\\n    A `ToolMessage` where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    ```python\\n    from langchain_core.messages import ToolMessage\\n\\n    tool_output = {\\n        \"stdout\": \"From the graph we can see that the correlation between \"\\n        \"x and y is ...\",\\n        \"stderr\": None,\\n        \"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"},\\n    }\\n\\n    ToolMessage(\\n        content=tool_output[\"stdout\"],\\n        artifact=tool_output,\\n        tool_call_id=\"call_Jja7J89XsjrOLA5r!MEOW!SL\",\\n    )\\n    ```'},\n",
       "   'ToolMessageChunk': {'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "       {'items': {'anyOf': [{'type': 'string'},\n",
       "          {'additionalProperties': True, 'type': 'object'}]},\n",
       "        'type': 'array'}],\n",
       "      'title': 'Content'},\n",
       "     'additional_kwargs': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Additional Kwargs'},\n",
       "     'response_metadata': {'additionalProperties': True,\n",
       "      'type': 'object',\n",
       "      'title': 'Response Metadata'},\n",
       "     'type': {'type': 'string',\n",
       "      'const': 'ToolMessageChunk',\n",
       "      'title': 'Type',\n",
       "      'default': 'ToolMessageChunk'},\n",
       "     'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "     'tool_call_id': {'type': 'string', 'title': 'Tool Call Id'},\n",
       "     'artifact': {'title': 'Artifact'},\n",
       "     'status': {'type': 'string',\n",
       "      'enum': ['success', 'error'],\n",
       "      'title': 'Status',\n",
       "      'default': 'success'}},\n",
       "    'additionalProperties': True,\n",
       "    'type': 'object',\n",
       "    'required': ['content', 'tool_call_id'],\n",
       "    'title': 'ToolMessageChunk',\n",
       "    'description': 'Tool Message chunk.'},\n",
       "   'TraceTier': {'type': 'string',\n",
       "    'enum': ['longlived', 'shortlived'],\n",
       "    'title': 'TraceTier'},\n",
       "   'TracerSession': {'properties': {'start_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'default_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Dataset Id'},\n",
       "     'reference_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reference Dataset Id'},\n",
       "     'trace_tier': {'anyOf': [{'$ref': '#/components/schemas/TraceTier'},\n",
       "       {'type': 'null'}]},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'run_count': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Run Count'},\n",
       "     'latency_p50': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Latency P50'},\n",
       "     'latency_p99': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Latency P99'},\n",
       "     'first_token_p50': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'First Token P50'},\n",
       "     'first_token_p99': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'First Token P99'},\n",
       "     'total_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Total Tokens'},\n",
       "     'prompt_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Tokens'},\n",
       "     'completion_tokens': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Completion Tokens'},\n",
       "     'total_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Total Cost'},\n",
       "     'prompt_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Prompt Cost'},\n",
       "     'completion_cost': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Completion Cost'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'last_run_start_time': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Run Start Time'},\n",
       "     'last_run_start_time_live': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Run Start Time Live'},\n",
       "     'feedback_stats': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Feedback Stats'},\n",
       "     'session_feedback_stats': {'anyOf': [{'additionalProperties': True,\n",
       "        'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Session Feedback Stats'},\n",
       "     'run_facets': {'anyOf': [{'items': {'additionalProperties': True,\n",
       "         'type': 'object'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Run Facets'},\n",
       "     'error_rate': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Error Rate'},\n",
       "     'streaming_rate': {'anyOf': [{'type': 'number'}, {'type': 'null'}],\n",
       "      'title': 'Streaming Rate'},\n",
       "     'test_run_number': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'title': 'Test Run Number'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'tenant_id'],\n",
       "    'title': 'TracerSession',\n",
       "    'description': 'TracerSession schema.'},\n",
       "   'TracerSessionCreate': {'properties': {'start_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'default_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Dataset Id'},\n",
       "     'reference_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reference Dataset Id'},\n",
       "     'trace_tier': {'anyOf': [{'$ref': '#/components/schemas/TraceTier'},\n",
       "       {'type': 'null'}]},\n",
       "     'id': {'anyOf': [{'type': 'string', 'format': 'uuid'}, {'type': 'null'}],\n",
       "      'title': 'Id'}},\n",
       "    'type': 'object',\n",
       "    'title': 'TracerSessionCreate',\n",
       "    'description': 'Create class for TracerSession.'},\n",
       "   'TracerSessionUpdate': {'properties': {'name': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'default_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Dataset Id'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'},\n",
       "     'trace_tier': {'anyOf': [{'$ref': '#/components/schemas/TraceTier'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'title': 'TracerSessionUpdate',\n",
       "    'description': 'Update class for TracerSession.'},\n",
       "   'TracerSessionWithoutVirtualFields': {'properties': {'start_time': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Start Time'},\n",
       "     'end_time': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'End Time'},\n",
       "     'extra': {'anyOf': [{'additionalProperties': True, 'type': 'object'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Extra'},\n",
       "     'name': {'type': 'string', 'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'default_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Default Dataset Id'},\n",
       "     'reference_dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Reference Dataset Id'},\n",
       "     'trace_tier': {'anyOf': [{'$ref': '#/components/schemas/TraceTier'},\n",
       "       {'type': 'null'}]},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'last_run_start_time_live': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Last Run Start Time Live'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'tenant_id'],\n",
       "    'title': 'TracerSessionWithoutVirtualFields',\n",
       "    'description': 'TracerSession schema.'},\n",
       "   'TriggerRulesRequest': {'properties': {'rule_ids': {'anyOf': [{'items': {'type': 'string',\n",
       "         'format': 'uuid'},\n",
       "        'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Rule Ids'},\n",
       "     'dataset_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Dataset Id'}},\n",
       "    'type': 'object',\n",
       "    'title': 'TriggerRulesRequest'},\n",
       "   'TrueFalseLiteral': {'type': 'string',\n",
       "    'enum': ['true', 'false'],\n",
       "    'title': 'TrueFalseLiteral'},\n",
       "   'UpdateClusteringJobConfigRequest': {'properties': {'name': {'anyOf': [{'type': 'string',\n",
       "        'maxLength': 255},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Name'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'config': {'anyOf': [{'$ref': '#/components/schemas/CreateRunClusteringJobRequest'},\n",
       "       {'type': 'null'}]}},\n",
       "    'type': 'object',\n",
       "    'title': 'UpdateClusteringJobConfigRequest',\n",
       "    'description': 'Request to update a clustering job config.'},\n",
       "   'UpdateFeedbackConfigSchema': {'properties': {'feedback_key': {'type': 'string',\n",
       "      'title': 'Feedback Key'},\n",
       "     'feedback_config': {'anyOf': [{'$ref': '#/components/schemas/FeedbackConfig'},\n",
       "       {'type': 'null'}]},\n",
       "     'is_lower_score_better': {'anyOf': [{'type': 'boolean'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Is Lower Score Better'}},\n",
       "    'type': 'object',\n",
       "    'required': ['feedback_key'],\n",
       "    'title': 'UpdateFeedbackConfigSchema'},\n",
       "   'UpdateRepoRequest': {'properties': {'description': {'anyOf': [{'type': 'string'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Description'},\n",
       "     'readme': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Readme'},\n",
       "     'tags': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tags'},\n",
       "     'is_public': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Is Public'},\n",
       "     'is_archived': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}],\n",
       "      'title': 'Is Archived'}},\n",
       "    'type': 'object',\n",
       "    'title': 'UpdateRepoRequest',\n",
       "    'description': 'Fields to update a repo'},\n",
       "   'UpdateRoleRequest': {'properties': {'display_name': {'type': 'string',\n",
       "      'title': 'Display Name'},\n",
       "     'description': {'type': 'string', 'title': 'Description'},\n",
       "     'permissions': {'items': {'type': 'string'},\n",
       "      'type': 'array',\n",
       "      'title': 'Permissions'}},\n",
       "    'type': 'object',\n",
       "    'required': ['display_name', 'description', 'permissions'],\n",
       "    'title': 'UpdateRoleRequest'},\n",
       "   'UpdateRunClusteringJobRequest': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name'],\n",
       "    'title': 'UpdateRunClusteringJobRequest',\n",
       "    'description': 'Request to update a session cluster job.'},\n",
       "   'UpdateRunClusteringJobResponse': {'properties': {'name': {'type': 'string',\n",
       "      'title': 'Name'},\n",
       "     'status': {'type': 'string', 'title': 'Status'}},\n",
       "    'type': 'object',\n",
       "    'required': ['name', 'status'],\n",
       "    'title': 'UpdateRunClusteringJobResponse',\n",
       "    'description': 'Response to update a session cluster job.'},\n",
       "   'UpsertTTLSettingsRequest': {'properties': {'tenant_id': {'anyOf': [{'type': 'string',\n",
       "        'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tenant Id'},\n",
       "     'default_trace_tier': {'$ref': '#/components/schemas/TraceTier'},\n",
       "     'apply_to_all_projects': {'type': 'boolean',\n",
       "      'title': 'Apply To All Projects',\n",
       "      'default': False}},\n",
       "    'type': 'object',\n",
       "    'required': ['default_trace_tier'],\n",
       "    'title': 'UpsertTTLSettingsRequest',\n",
       "    'description': 'Base TTL settings model.'},\n",
       "   'UpsertUsageLimit': {'properties': {'limit_type': {'$ref': '#/components/schemas/UsageLimitType'},\n",
       "     'limit_value': {'type': 'integer', 'title': 'Limit Value'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'}},\n",
       "    'type': 'object',\n",
       "    'required': ['limit_type', 'limit_value'],\n",
       "    'title': 'UpsertUsageLimit',\n",
       "    'description': 'Request body for creating or updating a usage limit.'},\n",
       "   'UsageLimit': {'properties': {'limit_type': {'$ref': '#/components/schemas/UsageLimitType'},\n",
       "     'limit_value': {'type': 'integer', 'title': 'Limit Value'},\n",
       "     'id': {'type': 'string', 'format': 'uuid', 'title': 'Id'},\n",
       "     'tenant_id': {'type': 'string', 'format': 'uuid', 'title': 'Tenant Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'}},\n",
       "    'type': 'object',\n",
       "    'required': ['limit_type',\n",
       "     'limit_value',\n",
       "     'tenant_id',\n",
       "     'created_at',\n",
       "     'updated_at'],\n",
       "    'title': 'UsageLimit',\n",
       "    'description': 'Usage limit model.'},\n",
       "   'UsageLimitType': {'type': 'string',\n",
       "    'enum': ['monthly_traces', 'monthly_longlived_traces'],\n",
       "    'title': 'UsageLimitType',\n",
       "    'description': 'Type of usage limit.'},\n",
       "   'UsageMetadata': {'properties': {'input_tokens': {'type': 'integer',\n",
       "      'title': 'Input Tokens'},\n",
       "     'output_tokens': {'type': 'integer', 'title': 'Output Tokens'},\n",
       "     'total_tokens': {'type': 'integer', 'title': 'Total Tokens'},\n",
       "     'input_token_details': {'$ref': '#/components/schemas/InputTokenDetails'},\n",
       "     'output_token_details': {'$ref': '#/components/schemas/OutputTokenDetails'}},\n",
       "    'type': 'object',\n",
       "    'required': ['input_tokens', 'output_tokens', 'total_tokens'],\n",
       "    'title': 'UsageMetadata',\n",
       "    'description': 'Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n    ```python\\n    {\\n        \"input_tokens\": 350,\\n        \"output_tokens\": 240,\\n        \"total_tokens\": 590,\\n        \"input_token_details\": {\\n            \"audio\": 10,\\n            \"cache_creation\": 200,\\n            \"cache_read\": 100,\\n        },\\n        \"output_token_details\": {\\n            \"audio\": 10,\\n            \"reasoning\": 200,\\n        },\\n    }\\n    ```\\n\\n!!! warning \"Behavior changed in `langchain-core` 0.3.9\"\\n\\n    Added `input_token_details` and `output_token_details`.\\n\\n!!! note \"LangSmith SDK\"\\n\\n    The LangSmith SDK also has a `UsageMetadata` class. While the two share fields,\\n    LangSmith\\'s `UsageMetadata` has additional fields to capture cost information\\n    used by the LangSmith platform.'},\n",
       "   'UserOnboardingStateResponse': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'ls_user_id': {'type': 'string', 'format': 'uuid', 'title': 'Ls User Id'},\n",
       "     'tracing_completed_at': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Tracing Completed At'},\n",
       "     'lgstudio_completed_at': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Lgstudio Completed At'},\n",
       "     'playground_completed_at': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Playground Completed At'},\n",
       "     'evaluation_completed_at': {'anyOf': [{'type': 'string',\n",
       "        'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Evaluation Completed At'},\n",
       "     'success_viewed_at': {'anyOf': [{'type': 'string', 'format': 'date-time'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Success Viewed At'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'ls_user_id', 'created_at', 'updated_at'],\n",
       "    'title': 'UserOnboardingStateResponse'},\n",
       "   'UserWithPassword': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'ls_user_id': {'type': 'string', 'format': 'uuid', 'title': 'Ls User Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'updated_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Updated At'},\n",
       "     'email': {'type': 'string', 'title': 'Email'},\n",
       "     'full_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Full Name'},\n",
       "     'avatar_url': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Avatar Url'},\n",
       "     'password': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Password'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'ls_user_id', 'created_at', 'updated_at', 'email'],\n",
       "    'title': 'UserWithPassword'},\n",
       "   'ValidationError': {'properties': {'loc': {'items': {'anyOf': [{'type': 'string'},\n",
       "        {'type': 'integer'}]},\n",
       "      'type': 'array',\n",
       "      'title': 'Location'},\n",
       "     'msg': {'type': 'string', 'title': 'Message'},\n",
       "     'type': {'type': 'string', 'title': 'Error Type'}},\n",
       "    'type': 'object',\n",
       "    'required': ['loc', 'msg', 'type'],\n",
       "    'title': 'ValidationError'},\n",
       "   'Wallet': {'properties': {'credit_balance_micros': {'type': 'integer',\n",
       "      'title': 'Credit Balance Micros'},\n",
       "     'inflight_balance_micros': {'type': 'integer',\n",
       "      'title': 'Inflight Balance Micros'}},\n",
       "    'type': 'object',\n",
       "    'required': ['credit_balance_micros', 'inflight_balance_micros'],\n",
       "    'title': 'Wallet'},\n",
       "   'WorkspaceCreate': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'display_name': {'type': 'string',\n",
       "      'minLength': 1,\n",
       "      'pattern': \"^[a-zA-Z0-9\\\\-_ '@()]+$\",\n",
       "      'title': 'Display Name'},\n",
       "     'tenant_handle': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tenant Handle'}},\n",
       "    'type': 'object',\n",
       "    'required': ['display_name'],\n",
       "    'title': 'WorkspaceCreate',\n",
       "    'description': 'Creation model for the workspace.'},\n",
       "   'WorkspacePatch': {'properties': {'display_name': {'type': 'string',\n",
       "      'minLength': 1,\n",
       "      'pattern': \"^[a-zA-Z0-9\\\\-_ '@()]+$\",\n",
       "      'title': 'Display Name'}},\n",
       "    'type': 'object',\n",
       "    'required': ['display_name'],\n",
       "    'title': 'WorkspacePatch',\n",
       "    'description': 'Patch model for the workspace.'},\n",
       "   'app__hub__crud__tenants__Tenant': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'display_name': {'type': 'string', 'title': 'Display Name'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'tenant_handle': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tenant Handle'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id', 'display_name', 'created_at'],\n",
       "    'title': 'Tenant'},\n",
       "   'app__schemas__Tenant': {'properties': {'id': {'type': 'string',\n",
       "      'format': 'uuid',\n",
       "      'title': 'Id'},\n",
       "     'organization_id': {'anyOf': [{'type': 'string', 'format': 'uuid'},\n",
       "       {'type': 'null'}],\n",
       "      'title': 'Organization Id'},\n",
       "     'created_at': {'type': 'string',\n",
       "      'format': 'date-time',\n",
       "      'title': 'Created At'},\n",
       "     'display_name': {'type': 'string', 'title': 'Display Name'},\n",
       "     'is_personal': {'type': 'boolean', 'title': 'Is Personal'},\n",
       "     'is_deleted': {'type': 'boolean', 'title': 'Is Deleted'},\n",
       "     'tenant_handle': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'title': 'Tenant Handle'}},\n",
       "    'type': 'object',\n",
       "    'required': ['id',\n",
       "     'created_at',\n",
       "     'display_name',\n",
       "     'is_personal',\n",
       "     'is_deleted'],\n",
       "    'title': 'Tenant',\n",
       "    'description': 'Tenant schema.'},\n",
       "   'abac.ErrorResponse': {'type': 'object',\n",
       "    'properties': {'error': {'type': 'string',\n",
       "      'example': 'Invalid request: missing required fields'}}},\n",
       "   'alerts.AlertAction': {'type': 'object',\n",
       "    'required': ['config', 'target'],\n",
       "    'properties': {'alert_rule_id': {'type': 'string'},\n",
       "     'config': {'type': 'object'},\n",
       "     'created_at': {'type': 'string'},\n",
       "     'id': {'type': 'string'},\n",
       "     'target': {'type': 'string', 'enum': ['pagerduty', 'webhook']},\n",
       "     'updated_at': {'type': 'string'}}},\n",
       "   'alerts.AlertActionBase': {'type': 'object',\n",
       "    'required': ['config', 'target'],\n",
       "    'properties': {'alert_rule_id': {'type': 'string'},\n",
       "     'config': {'type': 'object'},\n",
       "     'id': {'type': 'string'},\n",
       "     'target': {'type': 'string', 'enum': ['pagerduty', 'webhook']}}},\n",
       "   'alerts.AlertRule': {'type': 'object',\n",
       "    'required': ['aggregation',\n",
       "     'attribute',\n",
       "     'description',\n",
       "     'name',\n",
       "     'operator',\n",
       "     'type',\n",
       "     'window_minutes'],\n",
       "    'properties': {'aggregation': {'type': 'string',\n",
       "      'enum': ['avg', 'sum', 'pct']},\n",
       "     'attribute': {'type': 'string',\n",
       "      'enum': ['latency',\n",
       "       'error_count',\n",
       "       'feedback_score',\n",
       "       'run_latency',\n",
       "       'run_count']},\n",
       "     'created_at': {'type': 'string'},\n",
       "     'denominator_filter': {'type': 'string'},\n",
       "     'description': {'type': 'string'},\n",
       "     'filter': {'type': 'string'},\n",
       "     'id': {'type': 'string'},\n",
       "     'name': {'type': 'string'},\n",
       "     'operator': {'type': 'string', 'enum': ['gte', 'lte']},\n",
       "     'threshold': {'type': 'number'},\n",
       "     'threshold_multiplier': {'type': 'number'},\n",
       "     'threshold_window_minutes': {'type': 'integer', 'maximum': 60},\n",
       "     'type': {'type': 'string', 'enum': ['threshold', 'change']},\n",
       "     'updated_at': {'type': 'string'},\n",
       "     'window_minutes': {'description': 'max 15 minutes for alert rule',\n",
       "      'type': 'integer',\n",
       "      'maximum': 15}}},\n",
       "   'alerts.AlertRuleBase': {'type': 'object',\n",
       "    'required': ['aggregation',\n",
       "     'attribute',\n",
       "     'description',\n",
       "     'name',\n",
       "     'operator',\n",
       "     'type',\n",
       "     'window_minutes'],\n",
       "    'properties': {'aggregation': {'type': 'string',\n",
       "      'enum': ['avg', 'sum', 'pct']},\n",
       "     'attribute': {'type': 'string',\n",
       "      'enum': ['latency',\n",
       "       'error_count',\n",
       "       'feedback_score',\n",
       "       'run_latency',\n",
       "       'run_count']},\n",
       "     'denominator_filter': {'type': 'string'},\n",
       "     'description': {'type': 'string'},\n",
       "     'filter': {'type': 'string'},\n",
       "     'id': {'type': 'string'},\n",
       "     'name': {'type': 'string'},\n",
       "     'operator': {'type': 'string', 'enum': ['gte', 'lte']},\n",
       "     'threshold': {'type': 'number'},\n",
       "     'threshold_multiplier': {'type': 'number'},\n",
       "     'threshold_window_minutes': {'type': 'integer', 'maximum': 60},\n",
       "     'type': {'type': 'string', 'enum': ['threshold', 'change']},\n",
       "     'window_minutes': {'description': 'max 15 minutes for alert rule',\n",
       "      'type': 'integer',\n",
       "      'maximum': 15}}},\n",
       "   'alerts.AlertRuleResponse': {'type': 'object',\n",
       "    'properties': {'actions': {'type': 'array',\n",
       "      'items': {'$ref': '#/components/schemas/alerts.AlertAction'}},\n",
       "     'rule': {'$ref': '#/components/schemas/alerts.AlertRule'}}},\n",
       "   'alerts.CreateAlertRuleRequest': {'type': 'object',\n",
       "    'required': ['actions', 'rule'],\n",
       "    'properties': {'actions': {'type': 'array',\n",
       "      'items': {'$ref': '#/components/schemas/alerts.AlertActionBase'}},\n",
       "     'rule': {'$ref': '#/components/schemas/alerts.AlertRuleBase'}}},\n",
       "   'alerts.ErrorResponse': {'type': 'object',\n",
       "    'properties': {'error': {'type': 'string',\n",
       "      'example': 'Invalid request: missing required fields'}}},\n",
       "   'alerts.UpdateAlertRuleRequest': {'type': 'object',\n",
       "    'required': ['actions', 'rule'],\n",
       "    'properties': {'actions': {'type': 'array',\n",
       "      'items': {'$ref': '#/components/schemas/alerts.AlertActionBase'}},\n",
       "     'rule': {'$ref': '#/components/schemas/alerts.AlertRuleBase'}}},\n",
       "   'authz_internal.AbacAttributeName': {'type': 'string',\n",
       "    'enum': ['resource_tag_key'],\n",
       "    'x-enum-varnames': ['AbacAttributeNameResourceTagKey']},\n",
       "   'authz_internal.AbacOperator': {'type': 'string',\n",
       "    'enum': ['equals',\n",
       "     'not_equals',\n",
       "     'equals_ignore_case',\n",
       "     'not_equals_ignore_case',\n",
       "     'matches',\n",
       "     'not_matches',\n",
       "     'equals_if_exists',\n",
       "     'not_equals_if_exists',\n",
       "     'equals_ignore_case_if_exists',\n",
       "     'not_equals_ignore_case_if_exists',\n",
       "     'matches_if_exists',\n",
       "     'not_matches_if_exists'],\n",
       "    'x-enum-varnames': ['AbacOperatorEquals',\n",
       "     'AbacOperatorNotEquals',\n",
       "     'AbacOperatorEqualsIgnoreCase',\n",
       "     'AbacOperatorNotEqualsIgnoreCase',\n",
       "     'AbacOperatorMatches',\n",
       "     'AbacOperatorNotMatches',\n",
       "     'AbacOperatorEqualsIfExists',\n",
       "     'AbacOperatorNotEqualsIfExists',\n",
       "     'AbacOperatorEqualsIgnoreCaseIfExists',\n",
       "     'AbacOperatorNotEqualsIgnoreCaseIfExists',\n",
       "     'AbacOperatorMatchesIfExists',\n",
       "     'AbacOperatorNotMatchesIfExists']},\n",
       "   'authz_internal.AccessPolicy': {'type': 'object',\n",
       "    'properties': {'condition_groups': {'type': 'array',\n",
       "      'items': {'$ref': '#/components/schemas/authz_internal.ConditionGroup'}},\n",
       "     'created_at': {'type': 'string'},\n",
       "     'description': {'type': 'string'},\n",
       "     'effect': {'type': 'string'},\n",
       "     'id': {'type': 'string'},\n",
       "     'name': {'type': 'string'},\n",
       "     'role_ids': {'type': 'array', 'items': {'type': 'string'}},\n",
       "     'updated_at': {'type': 'string'}}},\n",
       "   'authz_internal.AccessPolicyCreateResponse': {'type': 'object',\n",
       "    'properties': {'id': {'type': 'string'}}},\n",
       "   'authz_internal.AttachAccessPoliciesPayload': {'type': 'object',\n",
       "    'properties': {'access_policy_ids': {'type': 'array',\n",
       "      'items': {'type': 'string'}}}},\n",
       "   'authz_internal.Condition': {'type': 'object',\n",
       "    'properties': {'attribute_key': {'type': 'string'},\n",
       "     'attribute_name': {'$ref': '#/components/schemas/authz_internal.AbacAttributeName'},\n",
       "     'attribute_value': {'type': 'string'},\n",
       "     'operator': {'$ref': '#/components/schemas/authz_internal.AbacOperator'}}},\n",
       "   'authz_internal.ConditionGroup': {'type': 'object',\n",
       "    'properties': {'conditions': {'type': 'array',\n",
       "      'items': {'$ref': '#/components/schemas/authz_internal.Condition'}},\n",
       "     'permission': {'$ref': '#/components/schemas/authz_internal.Permission'},\n",
       "     'resource_type': {'type': 'string'}}},\n",
       "   'authz_internal.CreateAccessPolicyPayload': {'type': 'object',\n",
       "    'properties': {'condition_groups': {'type': 'array',\n",
       "      'items': {'$ref': '#/components/schemas/authz_internal.ConditionGroup'}},\n",
       "     'description': {'type': 'string'},\n",
       "     'effect': {'type': 'string'},\n",
       "     'name': {'type': 'string'},\n",
       "     'role_ids': {'description': 'if present, attach to the specified roles',\n",
       "      'type': 'array',\n",
       "      'items': {'type': 'string'}}}},\n",
       "   'authz_internal.ListAccessPoliciesResponse': {'type': 'object',\n",
       "    'properties': {'access_policies': {'type': 'array',\n",
       "      'items': {'$ref': '#/components/schemas/authz_internal.AccessPolicy'}}}},\n",
       "   'authz_internal.Permission': {'type': 'string',\n",
       "    'enum': ['annotation-queues:create',\n",
       "     'annotation-queues:delete',\n",
       "     'annotation-queues:read',\n",
       "     'annotation-queues:update',\n",
       "     'charts:create',\n",
       "     'charts:delete',\n",
       "     'charts:read',\n",
       "     'charts:update',\n",
       "     'datasets:create',\n",
       "     'datasets:delete',\n",
       "     'datasets:read',\n",
       "     'datasets:share',\n",
       "     'datasets:update',\n",
       "     'deployments:create',\n",
       "     'deployments:delete',\n",
       "     'deployments:read',\n",
       "     'deployments:update',\n",
       "     'feedback:create',\n",
       "     'feedback:delete',\n",
       "     'feedback:read',\n",
       "     'feedback:update',\n",
       "     'projects:create',\n",
       "     'projects:delete',\n",
       "     'projects:read',\n",
       "     'projects:update',\n",
       "     'prompts:create',\n",
       "     'prompts:delete',\n",
       "     'prompts:read',\n",
       "     'prompts:update',\n",
       "     'prompts:share',\n",
       "     'rules:create',\n",
       "     'rules:delete',\n",
       "     'rules:read',\n",
       "     'rules:update',\n",
       "     'runs:create',\n",
       "     'runs:read',\n",
       "     'runs:share',\n",
       "     'runs:delete',\n",
       "     'workspaces:manage',\n",
       "     'workspaces:read',\n",
       "     'alerts:create',\n",
       "     'alerts:update',\n",
       "     'alerts:delete',\n",
       "     'alerts:read',\n",
       "     'organization:pats:create',\n",
       "     'organization:read',\n",
       "     'organization:manage'],\n",
       "    'x-enum-varnames': ['AnnotationQueuesCreate',\n",
       "     'AnnotationQueuesDelete',\n",
       "     'AnnotationQueuesRead',\n",
       "     'AnnotationQueuesUpdate',\n",
       "     'ChartsCreate',\n",
       "     'ChartsDelete',\n",
       "     'ChartsRead',\n",
       "     'ChartsUpdate',\n",
       "     'DatasetsCreate',\n",
       "     'DatasetsDelete',\n",
       "     'DatasetsRead',\n",
       "     'DatasetsShare',\n",
       "     'DatasetsUpdate',\n",
       "     'DeploymentsCreate',\n",
       "     'DeploymentsDelete',\n",
       "     'DeploymentsRead',\n",
       "     'DeploymentsUpdate',\n",
       "     'FeedbackCreate',\n",
       "     'FeedbackDelete',\n",
       "     'FeedbackRead',\n",
       "     'FeedbackUpdate',\n",
       "     'ProjectsCreate',\n",
       "     'ProjectsDelete',\n",
       "     'ProjectsRead',\n",
       "     'ProjectsUpdate',\n",
       "     'PromptsCreate',\n",
       "     'PromptsDelete',\n",
       "     'PromptsRead',\n",
       "     'PromptsUpdate',\n",
       "     'PromptsShare',\n",
       "     'RulesCreate',\n",
       "     'RulesDelete',\n",
       "     'RulesRead',\n",
       "     'RulesUpdate',\n",
       "     'RunsCreate',\n",
       "     'RunsRead',\n",
       "     'RunsShare',\n",
       "     'RunsDelete',\n",
       "     'WorkspacesManage',\n",
       "     'WorkspacesRead',\n",
       "     'AlertsCreate',\n",
       "     'AlertsUpdate',\n",
       "     'AlertsDelete',\n",
       "     'AlertsRead',\n",
       "     'OrganizationPATsCreate',\n",
       "     'OrganizationRead',\n",
       "     'OrganizationManage']},\n",
       "   'commits.CommitResponse': {'type': 'object',\n",
       "    'properties': {'commit_hash': {'type': 'string'},\n",
       "     'examples': {'type': 'array',\n",
       "      'items': {'$ref': '#/components/schemas/commits.ExampleRun'}},\n",
       "     'manifest': {'type': 'object'}}},\n",
       "   'commits.CommitWithLookups': {'type': 'object',\n",
       "    'properties': {'commit_hash': {'description': 'The hash of the commit',\n",
       "      'type': 'string'},\n",
       "     'created_at': {'description': 'When the commit was created',\n",
       "      'type': 'string',\n",
       "      'format': 'date-time'},\n",
       "     'example_run_ids': {'description': 'Example run IDs associated with the commit',\n",
       "      'type': 'array',\n",
       "      'items': {'type': 'string', 'format': 'uuid'}},\n",
       "     'full_name': {'description': \"Author's full name\", 'type': 'string'},\n",
       "     'id': {'description': 'The commit ID',\n",
       "      'type': 'string',\n",
       "      'format': 'uuid'},\n",
       "     'manifest': {'description': 'The manifest of the commit',\n",
       "      'type': 'object'},\n",
       "     'manifest_sha': {'description': 'The SHA of the manifest',\n",
       "      'type': 'array',\n",
       "      'items': {'type': 'integer'}},\n",
       "     'num_downloads': {'description': 'Number of API downloads',\n",
       "      'type': 'integer'},\n",
       "     'num_views': {'description': 'Number of web views', 'type': 'integer'},\n",
       "     'parent_commit_hash': {'description': 'The hash of the parent commit',\n",
       "      'type': 'string'},\n",
       "     'parent_id': {'description': 'The ID of the parent commit',\n",
       "      'type': 'string',\n",
       "      'format': 'uuid'},\n",
       "     'repo_id': {'description': 'Repository ID',\n",
       "      'type': 'string',\n",
       "      'format': 'uuid'},\n",
       "     'updated_at': {'description': 'When the commit was last updated',\n",
       "      'type': 'string',\n",
       "      'format': 'date-time'}}},\n",
       "   'commits.CreateCommitReq': {'type': 'object',\n",
       "    'properties': {'manifest': {'type': 'object'},\n",
       "     'parent_commit': {'type': 'string'},\n",
       "     'skip_webhooks': {'description': 'SkipWebhooks allows skipping webhook notifications. Can be true (boolean) to skip all, or an array of webhook UUIDs to skip specific ones.'}}},\n",
       "   'commits.CreateCommitResponse': {'type': 'object',\n",
       "    'properties': {'commit': {'$ref': '#/components/schemas/commits.CommitWithLookups'}}},\n",
       "   'commits.ErrorResponse': {'type': 'object',\n",
       "    'properties': {'error': {'description': 'Error message',\n",
       "      'type': 'string'}}},\n",
       "   'commits.ExampleRun': {'type': 'object',\n",
       "    'properties': {'id': {'type': 'string', 'format': 'uuid'},\n",
       "     'inputs': {'type': 'object'},\n",
       "     'outputs': {'type': 'object'},\n",
       "     'session_id': {'type': 'string', 'format': 'uuid'},\n",
       "     'start_time': {'type': 'string'}}},\n",
       "   'commits.ListCommitsResponse': {'type': 'object',\n",
       "    'properties': {'commits': {'description': 'List of commits with lookup information',\n",
       "      'type': 'array',\n",
       "      'items': {'$ref': '#/components/schemas/commits.CommitWithLookups'}},\n",
       "     'total': {'description': 'Total number of commits', 'type': 'integer'}}},\n",
       "   'examples.DeleteExamplesRequest': {'type': 'object',\n",
       "    'required': ['example_ids', 'hard_delete'],\n",
       "    'properties': {'example_ids': {'description': 'ExampleIDs is a list of UUIDs identifying the examples to delete.',\n",
       "      'type': 'array',\n",
       "      'maxItems': 1000,\n",
       "      'minItems': 1,\n",
       "      'items': {'type': 'string'}},\n",
       "     'hard_delete': {'description': 'HardDelete indicates whether to perform a hard delete.\\nCurrently only True is supported.',\n",
       "      'type': 'boolean'}}},\n",
       "   'examples.ErrorResponse': {'type': 'object',\n",
       "    'properties': {'details': {'description': 'Optional error details as JSON string',\n",
       "      'type': 'string',\n",
       "      'example': '{\"field\":\"dataset_id\",\"reason\":\"required\"}'},\n",
       "     'error': {'description': 'Error message',\n",
       "      'type': 'string',\n",
       "      'example': 'Invalid request: missing required fields'}}},\n",
       "   'examples.ExamplesCreatedResponse': {'type': 'object',\n",
       "    'properties': {'count': {'type': 'integer', 'example': 1},\n",
       "     'example_ids': {'type': 'array',\n",
       "      'items': {'type': 'string'},\n",
       "      'example': ['[\"123e4567-e89b-12d3-a456-426614174000\"]']}}},\n",
       "   'examples.ExamplesDeletedResponse': {'type': 'object',\n",
       "    'properties': {'count': {'type': 'integer', 'example': 1},\n",
       "     'example_ids': {'type': 'array',\n",
       "      'items': {'type': 'string'},\n",
       "      'example': ['[\"123e4567-e89b-12d3-a456-426614174000\"]']}}},\n",
       "   'examples.ExamplesUpdatedResponse': {'type': 'object',\n",
       "    'properties': {'count': {'type': 'integer', 'example': 1},\n",
       "     'example_ids': {'type': 'array',\n",
       "      'items': {'type': 'string'},\n",
       "      'example': ['[\"123e4567-e89b-12d3-a456-426614174000\"]']}}},\n",
       "   'experiment_view_overrides.ColumnOverride': {'type': 'object',\n",
       "    'required': ['column'],\n",
       "    'properties': {'color_gradient': {'type': 'array',\n",
       "      'maxItems': 20,\n",
       "      'items': {'type': 'array', 'items': {}}},\n",
       "     'color_map': {'type': 'object', 'additionalProperties': True},\n",
       "     'column': {'type': 'string', 'maxLength': 200},\n",
       "     'disable_colors': {'type': 'boolean'},\n",
       "     'hide': {'type': 'boolean'},\n",
       "     'precision': {'type': 'integer', 'maximum': 6, 'minimum': 1}}},\n",
       "   'experiment_view_overrides.ExperimentViewOverride': {'type': 'object',\n",
       "    'properties': {'column_overrides': {'type': 'array',\n",
       "      'items': {'$ref': '#/components/schemas/experiment_view_overrides.ColumnOverride'}},\n",
       "     'created_at': {'type': 'string'},\n",
       "     'dataset_id': {'type': 'string'},\n",
       "     'id': {'type': 'string'},\n",
       "     'modified_at': {'type': 'string'}}},\n",
       "   'experiment_view_overrides.ExperimentViewOverridePatchRequest': {'type': 'object',\n",
       "    'required': ['column_overrides'],\n",
       "    'properties': {'column_overrides': {'type': 'array',\n",
       "      'maxItems': 50,\n",
       "      'minItems': 1,\n",
       "      'items': {'$ref': '#/components/schemas/experiment_view_overrides.ColumnOverride'}}}},\n",
       "   'experiment_view_overrides.ExperimentViewOverridePostRequest': {'type': 'object',\n",
       "    'required': ['column_overrides'],\n",
       "    'properties': {'column_overrides': {'type': 'array',\n",
       "      'maxItems': 50,\n",
       "      'minItems': 1,\n",
       "      'items': {'$ref': '#/components/schemas/experiment_view_overrides.ColumnOverride'}}}},\n",
       "   'feedback.FeedbackCreateSchema': {'type': 'object',\n",
       "    'properties': {'comment': {'type': 'string'},\n",
       "     'comparative_experiment_id': {'type': 'string'},\n",
       "     'correction': {},\n",
       "     'created_at': {'type': 'string'},\n",
       "     'error': {'type': 'boolean'},\n",
       "     'feedback_config': {'$ref': '#/components/schemas/types.FeedbackConfig'},\n",
       "     'feedback_group_id': {'type': 'string'},\n",
       "     'feedback_source': {'$ref': '#/components/schemas/feedback.FeedbackSource'},\n",
       "     'id': {'type': 'string'},\n",
       "     'key': {'type': 'string'},\n",
       "     'modified_at': {'type': 'string'},\n",
       "     'run_id': {'type': 'string'},\n",
       "     'score': {},\n",
       "     'session_id': {'type': 'string'},\n",
       "     'start_time': {'type': 'string'},\n",
       "     'trace_id': {'type': 'string'},\n",
       "     'value': {}}},\n",
       "   'feedback.FeedbackSource': {'type': 'object',\n",
       "    'properties': {'ls_user_id': {'type': 'string'},\n",
       "     'metadata': {'type': 'object', 'additionalProperties': True},\n",
       "     'type': {'type': 'string'},\n",
       "     'user_id': {'type': 'string'}}},\n",
       "   'runs.ErrorResponse': {'type': 'object',\n",
       "    'properties': {'details': {'description': 'Optional error details as JSON string',\n",
       "      'type': 'string',\n",
       "      'example': '{\"field\":\"dataset_id\",\"reason\":\"required\"}'},\n",
       "     'error': {'description': 'Error message',\n",
       "      'type': 'string',\n",
       "      'example': 'Invalid request: missing required fields'}}},\n",
       "   'runs.Run': {'type': 'object',\n",
       "    'properties': {'dotted_order': {'type': 'string'},\n",
       "     'end_time': {'type': 'string'},\n",
       "     'error': {'type': 'string'},\n",
       "     'events': {'type': 'array',\n",
       "      'items': {'type': 'object', 'additionalProperties': True}},\n",
       "     'extra': {'type': 'object', 'additionalProperties': True},\n",
       "     'id': {'type': 'string'},\n",
       "     'input_attachments': {'type': 'object', 'additionalProperties': True},\n",
       "     'inputs': {'type': 'object', 'additionalProperties': True},\n",
       "     'name': {'type': 'string'},\n",
       "     'output_attachments': {'type': 'object', 'additionalProperties': True},\n",
       "     'outputs': {'type': 'object', 'additionalProperties': True},\n",
       "     'parent_run_id': {'type': 'string'},\n",
       "     'reference_example_id': {'type': 'string'},\n",
       "     'run_type': {'type': 'string',\n",
       "      'enum': ['tool',\n",
       "       'chain',\n",
       "       'llm',\n",
       "       'retriever',\n",
       "       'embedding',\n",
       "       'prompt',\n",
       "       'parser']},\n",
       "     'serialized': {'type': 'object', 'additionalProperties': True},\n",
       "     'session_id': {'type': 'string'},\n",
       "     'session_name': {'type': 'string'},\n",
       "     'start_time': {'type': 'string'},\n",
       "     'status': {'type': 'string'},\n",
       "     'tags': {'type': 'array', 'items': {'type': 'string'}},\n",
       "     'trace_id': {'type': 'string'}}},\n",
       "   'ttl_settings.TTLSettingsResponse': {'type': 'object',\n",
       "    'properties': {'is_custom': {'type': 'boolean'},\n",
       "     'longlived_ttl_days': {'type': 'integer'},\n",
       "     'tenant_id': {'type': 'string'}}},\n",
       "   'ttl_settings.UpdateTTLSettingsRequest': {'type': 'object',\n",
       "    'properties': {'longlived_ttl_days': {'type': 'integer'}}},\n",
       "   'types.FeedbackCategory': {'type': 'object',\n",
       "    'properties': {'label': {'type': 'string', 'minLength': 1},\n",
       "     'value': {'type': 'number'}}},\n",
       "   'types.FeedbackConfig': {'type': 'object',\n",
       "    'properties': {'categories': {'type': 'array',\n",
       "      'items': {'$ref': '#/components/schemas/types.FeedbackCategory'}},\n",
       "     'max': {'type': 'number'},\n",
       "     'min': {'type': 'number'},\n",
       "     'type': {'$ref': '#/components/schemas/types.FeedbackType'}}},\n",
       "   'types.FeedbackType': {'type': 'string',\n",
       "    'enum': ['continuous', 'categorical', 'freeform'],\n",
       "    'x-enum-varnames': ['FeedbackTypeContinuous',\n",
       "     'FeedbackTypeCategorical',\n",
       "     'FeedbackTypeFreeform']}},\n",
       "  'securitySchemes': {'API Key': {'type': 'apiKey',\n",
       "    'in': 'header',\n",
       "    'name': 'X-API-Key'},\n",
       "   'Organization ID': {'type': 'apiKey',\n",
       "    'in': 'header',\n",
       "    'name': 'X-Organization-Id'},\n",
       "   'Bearer Auth': {'type': 'http',\n",
       "    'description': 'Bearer tokens are used to authenticate from the UI. Must also specify x-tenant-id or x-organization-id (for org scoped apis).',\n",
       "    'scheme': 'bearer'},\n",
       "   'Tenant ID': {'type': 'apiKey', 'in': 'header', 'name': 'X-Tenant-Id'}}},\n",
       " 'definitions': {'abac.ErrorResponse': {'type': 'object',\n",
       "   'properties': {'error': {'type': 'string',\n",
       "     'example': 'Invalid request: missing required fields'}}},\n",
       "  'alerts.AlertAction': {'type': 'object',\n",
       "   'required': ['config', 'target'],\n",
       "   'properties': {'alert_rule_id': {'type': 'string'},\n",
       "    'config': {'type': 'object'},\n",
       "    'created_at': {'type': 'string'},\n",
       "    'id': {'type': 'string'},\n",
       "    'target': {'type': 'string', 'enum': ['pagerduty', 'webhook']},\n",
       "    'updated_at': {'type': 'string'}}},\n",
       "  'alerts.AlertActionBase': {'type': 'object',\n",
       "   'required': ['config', 'target'],\n",
       "   'properties': {'alert_rule_id': {'type': 'string'},\n",
       "    'config': {'type': 'object'},\n",
       "    'id': {'type': 'string'},\n",
       "    'target': {'type': 'string', 'enum': ['pagerduty', 'webhook']}}},\n",
       "  'alerts.AlertRule': {'type': 'object',\n",
       "   'required': ['aggregation',\n",
       "    'attribute',\n",
       "    'description',\n",
       "    'name',\n",
       "    'operator',\n",
       "    'type',\n",
       "    'window_minutes'],\n",
       "   'properties': {'aggregation': {'type': 'string',\n",
       "     'enum': ['avg', 'sum', 'pct']},\n",
       "    'attribute': {'type': 'string',\n",
       "     'enum': ['latency',\n",
       "      'error_count',\n",
       "      'feedback_score',\n",
       "      'run_latency',\n",
       "      'run_count']},\n",
       "    'created_at': {'type': 'string'},\n",
       "    'denominator_filter': {'type': 'string'},\n",
       "    'description': {'type': 'string'},\n",
       "    'filter': {'type': 'string'},\n",
       "    'id': {'type': 'string'},\n",
       "    'name': {'type': 'string'},\n",
       "    'operator': {'type': 'string', 'enum': ['gte', 'lte']},\n",
       "    'threshold': {'type': 'number'},\n",
       "    'threshold_multiplier': {'type': 'number'},\n",
       "    'threshold_window_minutes': {'type': 'integer', 'maximum': 60},\n",
       "    'type': {'type': 'string', 'enum': ['threshold', 'change']},\n",
       "    'updated_at': {'type': 'string'},\n",
       "    'window_minutes': {'description': 'max 15 minutes for alert rule',\n",
       "     'type': 'integer',\n",
       "     'maximum': 15}}},\n",
       "  'alerts.AlertRuleBase': {'type': 'object',\n",
       "   'required': ['aggregation',\n",
       "    'attribute',\n",
       "    'description',\n",
       "    'name',\n",
       "    'operator',\n",
       "    'type',\n",
       "    'window_minutes'],\n",
       "   'properties': {'aggregation': {'type': 'string',\n",
       "     'enum': ['avg', 'sum', 'pct']},\n",
       "    'attribute': {'type': 'string',\n",
       "     'enum': ['latency',\n",
       "      'error_count',\n",
       "      'feedback_score',\n",
       "      'run_latency',\n",
       "      'run_count']},\n",
       "    'denominator_filter': {'type': 'string'},\n",
       "    'description': {'type': 'string'},\n",
       "    'filter': {'type': 'string'},\n",
       "    'id': {'type': 'string'},\n",
       "    'name': {'type': 'string'},\n",
       "    'operator': {'type': 'string', 'enum': ['gte', 'lte']},\n",
       "    'threshold': {'type': 'number'},\n",
       "    'threshold_multiplier': {'type': 'number'},\n",
       "    'threshold_window_minutes': {'type': 'integer', 'maximum': 60},\n",
       "    'type': {'type': 'string', 'enum': ['threshold', 'change']},\n",
       "    'window_minutes': {'description': 'max 15 minutes for alert rule',\n",
       "     'type': 'integer',\n",
       "     'maximum': 15}}},\n",
       "  'alerts.AlertRuleResponse': {'type': 'object',\n",
       "   'properties': {'actions': {'type': 'array',\n",
       "     'items': {'$ref': '#/components/schemas/alerts.AlertAction'}},\n",
       "    'rule': {'$ref': '#/components/schemas/alerts.AlertRule'}}},\n",
       "  'alerts.CreateAlertRuleRequest': {'type': 'object',\n",
       "   'required': ['actions', 'rule'],\n",
       "   'properties': {'actions': {'type': 'array',\n",
       "     'items': {'$ref': '#/components/schemas/alerts.AlertActionBase'}},\n",
       "    'rule': {'$ref': '#/components/schemas/alerts.AlertRuleBase'}}},\n",
       "  'alerts.ErrorResponse': {'type': 'object',\n",
       "   'properties': {'error': {'type': 'string',\n",
       "     'example': 'Invalid request: missing required fields'}}},\n",
       "  'alerts.UpdateAlertRuleRequest': {'type': 'object',\n",
       "   'required': ['actions', 'rule'],\n",
       "   'properties': {'actions': {'type': 'array',\n",
       "     'items': {'$ref': '#/components/schemas/alerts.AlertActionBase'}},\n",
       "    'rule': {'$ref': '#/components/schemas/alerts.AlertRuleBase'}}},\n",
       "  'authz_internal.AbacAttributeName': {'type': 'string',\n",
       "   'enum': ['resource_tag_key'],\n",
       "   'x-enum-varnames': ['AbacAttributeNameResourceTagKey']},\n",
       "  'authz_internal.AbacOperator': {'type': 'string',\n",
       "   'enum': ['equals',\n",
       "    'not_equals',\n",
       "    'equals_ignore_case',\n",
       "    'not_equals_ignore_case',\n",
       "    'matches',\n",
       "    'not_matches',\n",
       "    'equals_if_exists',\n",
       "    'not_equals_if_exists',\n",
       "    'equals_ignore_case_if_exists',\n",
       "    'not_equals_ignore_case_if_exists',\n",
       "    'matches_if_exists',\n",
       "    'not_matches_if_exists'],\n",
       "   'x-enum-varnames': ['AbacOperatorEquals',\n",
       "    'AbacOperatorNotEquals',\n",
       "    'AbacOperatorEqualsIgnoreCase',\n",
       "    'AbacOperatorNotEqualsIgnoreCase',\n",
       "    'AbacOperatorMatches',\n",
       "    'AbacOperatorNotMatches',\n",
       "    'AbacOperatorEqualsIfExists',\n",
       "    'AbacOperatorNotEqualsIfExists',\n",
       "    'AbacOperatorEqualsIgnoreCaseIfExists',\n",
       "    'AbacOperatorNotEqualsIgnoreCaseIfExists',\n",
       "    'AbacOperatorMatchesIfExists',\n",
       "    'AbacOperatorNotMatchesIfExists']},\n",
       "  'authz_internal.AccessPolicy': {'type': 'object',\n",
       "   'properties': {'condition_groups': {'type': 'array',\n",
       "     'items': {'$ref': '#/components/schemas/authz_internal.ConditionGroup'}},\n",
       "    'created_at': {'type': 'string'},\n",
       "    'description': {'type': 'string'},\n",
       "    'effect': {'type': 'string'},\n",
       "    'id': {'type': 'string'},\n",
       "    'name': {'type': 'string'},\n",
       "    'role_ids': {'type': 'array', 'items': {'type': 'string'}},\n",
       "    'updated_at': {'type': 'string'}}},\n",
       "  'authz_internal.AccessPolicyCreateResponse': {'type': 'object',\n",
       "   'properties': {'id': {'type': 'string'}}},\n",
       "  'authz_internal.AttachAccessPoliciesPayload': {'type': 'object',\n",
       "   'properties': {'access_policy_ids': {'type': 'array',\n",
       "     'items': {'type': 'string'}}}},\n",
       "  'authz_internal.Condition': {'type': 'object',\n",
       "   'properties': {'attribute_key': {'type': 'string'},\n",
       "    'attribute_name': {'$ref': '#/components/schemas/authz_internal.AbacAttributeName'},\n",
       "    'attribute_value': {'type': 'string'},\n",
       "    'operator': {'$ref': '#/components/schemas/authz_internal.AbacOperator'}}},\n",
       "  'authz_internal.ConditionGroup': {'type': 'object',\n",
       "   'properties': {'conditions': {'type': 'array',\n",
       "     'items': {'$ref': '#/components/schemas/authz_internal.Condition'}},\n",
       "    'permission': {'$ref': '#/components/schemas/authz_internal.Permission'},\n",
       "    'resource_type': {'type': 'string'}}},\n",
       "  'authz_internal.CreateAccessPolicyPayload': {'type': 'object',\n",
       "   'properties': {'condition_groups': {'type': 'array',\n",
       "     'items': {'$ref': '#/components/schemas/authz_internal.ConditionGroup'}},\n",
       "    'description': {'type': 'string'},\n",
       "    'effect': {'type': 'string'},\n",
       "    'name': {'type': 'string'},\n",
       "    'role_ids': {'description': 'if present, attach to the specified roles',\n",
       "     'type': 'array',\n",
       "     'items': {'type': 'string'}}}},\n",
       "  'authz_internal.ListAccessPoliciesResponse': {'type': 'object',\n",
       "   'properties': {'access_policies': {'type': 'array',\n",
       "     'items': {'$ref': '#/components/schemas/authz_internal.AccessPolicy'}}}},\n",
       "  'authz_internal.Permission': {'type': 'string',\n",
       "   'enum': ['annotation-queues:create',\n",
       "    'annotation-queues:delete',\n",
       "    'annotation-queues:read',\n",
       "    'annotation-queues:update',\n",
       "    'charts:create',\n",
       "    'charts:delete',\n",
       "    'charts:read',\n",
       "    'charts:update',\n",
       "    'datasets:create',\n",
       "    'datasets:delete',\n",
       "    'datasets:read',\n",
       "    'datasets:share',\n",
       "    'datasets:update',\n",
       "    'deployments:create',\n",
       "    'deployments:delete',\n",
       "    'deployments:read',\n",
       "    'deployments:update',\n",
       "    'feedback:create',\n",
       "    'feedback:delete',\n",
       "    'feedback:read',\n",
       "    'feedback:update',\n",
       "    'projects:create',\n",
       "    'projects:delete',\n",
       "    'projects:read',\n",
       "    'projects:update',\n",
       "    'prompts:create',\n",
       "    'prompts:delete',\n",
       "    'prompts:read',\n",
       "    'prompts:update',\n",
       "    'prompts:share',\n",
       "    'rules:create',\n",
       "    'rules:delete',\n",
       "    'rules:read',\n",
       "    'rules:update',\n",
       "    'runs:create',\n",
       "    'runs:read',\n",
       "    'runs:share',\n",
       "    'runs:delete',\n",
       "    'workspaces:manage',\n",
       "    'workspaces:read',\n",
       "    'alerts:create',\n",
       "    'alerts:update',\n",
       "    'alerts:delete',\n",
       "    'alerts:read',\n",
       "    'organization:pats:create',\n",
       "    'organization:read',\n",
       "    'organization:manage'],\n",
       "   'x-enum-varnames': ['AnnotationQueuesCreate',\n",
       "    'AnnotationQueuesDelete',\n",
       "    'AnnotationQueuesRead',\n",
       "    'AnnotationQueuesUpdate',\n",
       "    'ChartsCreate',\n",
       "    'ChartsDelete',\n",
       "    'ChartsRead',\n",
       "    'ChartsUpdate',\n",
       "    'DatasetsCreate',\n",
       "    'DatasetsDelete',\n",
       "    'DatasetsRead',\n",
       "    'DatasetsShare',\n",
       "    'DatasetsUpdate',\n",
       "    'DeploymentsCreate',\n",
       "    'DeploymentsDelete',\n",
       "    'DeploymentsRead',\n",
       "    'DeploymentsUpdate',\n",
       "    'FeedbackCreate',\n",
       "    'FeedbackDelete',\n",
       "    'FeedbackRead',\n",
       "    'FeedbackUpdate',\n",
       "    'ProjectsCreate',\n",
       "    'ProjectsDelete',\n",
       "    'ProjectsRead',\n",
       "    'ProjectsUpdate',\n",
       "    'PromptsCreate',\n",
       "    'PromptsDelete',\n",
       "    'PromptsRead',\n",
       "    'PromptsUpdate',\n",
       "    'PromptsShare',\n",
       "    'RulesCreate',\n",
       "    'RulesDelete',\n",
       "    'RulesRead',\n",
       "    'RulesUpdate',\n",
       "    'RunsCreate',\n",
       "    'RunsRead',\n",
       "    'RunsShare',\n",
       "    'RunsDelete',\n",
       "    'WorkspacesManage',\n",
       "    'WorkspacesRead',\n",
       "    'AlertsCreate',\n",
       "    'AlertsUpdate',\n",
       "    'AlertsDelete',\n",
       "    'AlertsRead',\n",
       "    'OrganizationPATsCreate',\n",
       "    'OrganizationRead',\n",
       "    'OrganizationManage']},\n",
       "  'commits.CommitResponse': {'type': 'object',\n",
       "   'properties': {'commit_hash': {'type': 'string'},\n",
       "    'examples': {'type': 'array',\n",
       "     'items': {'$ref': '#/components/schemas/commits.ExampleRun'}},\n",
       "    'manifest': {'type': 'object'}}},\n",
       "  'commits.CommitWithLookups': {'type': 'object',\n",
       "   'properties': {'commit_hash': {'description': 'The hash of the commit',\n",
       "     'type': 'string'},\n",
       "    'created_at': {'description': 'When the commit was created',\n",
       "     'type': 'string',\n",
       "     'format': 'date-time'},\n",
       "    'example_run_ids': {'description': 'Example run IDs associated with the commit',\n",
       "     'type': 'array',\n",
       "     'items': {'type': 'string', 'format': 'uuid'}},\n",
       "    'full_name': {'description': \"Author's full name\", 'type': 'string'},\n",
       "    'id': {'description': 'The commit ID', 'type': 'string', 'format': 'uuid'},\n",
       "    'manifest': {'description': 'The manifest of the commit',\n",
       "     'type': 'object'},\n",
       "    'manifest_sha': {'description': 'The SHA of the manifest',\n",
       "     'type': 'array',\n",
       "     'items': {'type': 'integer'}},\n",
       "    'num_downloads': {'description': 'Number of API downloads',\n",
       "     'type': 'integer'},\n",
       "    'num_views': {'description': 'Number of web views', 'type': 'integer'},\n",
       "    'parent_commit_hash': {'description': 'The hash of the parent commit',\n",
       "     'type': 'string'},\n",
       "    'parent_id': {'description': 'The ID of the parent commit',\n",
       "     'type': 'string',\n",
       "     'format': 'uuid'},\n",
       "    'repo_id': {'description': 'Repository ID',\n",
       "     'type': 'string',\n",
       "     'format': 'uuid'},\n",
       "    'updated_at': {'description': 'When the commit was last updated',\n",
       "     'type': 'string',\n",
       "     'format': 'date-time'}}},\n",
       "  'commits.CreateCommitReq': {'type': 'object',\n",
       "   'properties': {'manifest': {'type': 'object'},\n",
       "    'parent_commit': {'type': 'string'},\n",
       "    'skip_webhooks': {'description': 'SkipWebhooks allows skipping webhook notifications. Can be true (boolean) to skip all, or an array of webhook UUIDs to skip specific ones.'}}},\n",
       "  'commits.CreateCommitResponse': {'type': 'object',\n",
       "   'properties': {'commit': {'$ref': '#/components/schemas/commits.CommitWithLookups'}}},\n",
       "  'commits.ErrorResponse': {'type': 'object',\n",
       "   'properties': {'error': {'description': 'Error message',\n",
       "     'type': 'string'}}},\n",
       "  'commits.ExampleRun': {'type': 'object',\n",
       "   'properties': {'id': {'type': 'string', 'format': 'uuid'},\n",
       "    'inputs': {'type': 'object'},\n",
       "    'outputs': {'type': 'object'},\n",
       "    'session_id': {'type': 'string', 'format': 'uuid'},\n",
       "    'start_time': {'type': 'string'}}},\n",
       "  'commits.ListCommitsResponse': {'type': 'object',\n",
       "   'properties': {'commits': {'description': 'List of commits with lookup information',\n",
       "     'type': 'array',\n",
       "     'items': {'$ref': '#/components/schemas/commits.CommitWithLookups'}},\n",
       "    'total': {'description': 'Total number of commits', 'type': 'integer'}}},\n",
       "  'examples.DeleteExamplesRequest': {'type': 'object',\n",
       "   'required': ['example_ids', 'hard_delete'],\n",
       "   'properties': {'example_ids': {'description': 'ExampleIDs is a list of UUIDs identifying the examples to delete.',\n",
       "     'type': 'array',\n",
       "     'maxItems': 1000,\n",
       "     'minItems': 1,\n",
       "     'items': {'type': 'string'}},\n",
       "    'hard_delete': {'description': 'HardDelete indicates whether to perform a hard delete.\\nCurrently only True is supported.',\n",
       "     'type': 'boolean'}}},\n",
       "  'examples.ErrorResponse': {'type': 'object',\n",
       "   'properties': {'details': {'description': 'Optional error details as JSON string',\n",
       "     'type': 'string',\n",
       "     'example': '{\"field\":\"dataset_id\",\"reason\":\"required\"}'},\n",
       "    'error': {'description': 'Error message',\n",
       "     'type': 'string',\n",
       "     'example': 'Invalid request: missing required fields'}}},\n",
       "  'examples.ExamplesCreatedResponse': {'type': 'object',\n",
       "   'properties': {'count': {'type': 'integer', 'example': 1},\n",
       "    'example_ids': {'type': 'array',\n",
       "     'items': {'type': 'string'},\n",
       "     'example': ['[\"123e4567-e89b-12d3-a456-426614174000\"]']}}},\n",
       "  'examples.ExamplesDeletedResponse': {'type': 'object',\n",
       "   'properties': {'count': {'type': 'integer', 'example': 1},\n",
       "    'example_ids': {'type': 'array',\n",
       "     'items': {'type': 'string'},\n",
       "     'example': ['[\"123e4567-e89b-12d3-a456-426614174000\"]']}}},\n",
       "  'examples.ExamplesUpdatedResponse': {'type': 'object',\n",
       "   'properties': {'count': {'type': 'integer', 'example': 1},\n",
       "    'example_ids': {'type': 'array',\n",
       "     'items': {'type': 'string'},\n",
       "     'example': ['[\"123e4567-e89b-12d3-a456-426614174000\"]']}}},\n",
       "  'experiment_view_overrides.ColumnOverride': {'type': 'object',\n",
       "   'required': ['column'],\n",
       "   'properties': {'color_gradient': {'type': 'array',\n",
       "     'maxItems': 20,\n",
       "     'items': {'type': 'array', 'items': {}}},\n",
       "    'color_map': {'type': 'object', 'additionalProperties': True},\n",
       "    'column': {'type': 'string', 'maxLength': 200},\n",
       "    'disable_colors': {'type': 'boolean'},\n",
       "    'hide': {'type': 'boolean'},\n",
       "    'precision': {'type': 'integer', 'maximum': 6, 'minimum': 1}}},\n",
       "  'experiment_view_overrides.ExperimentViewOverride': {'type': 'object',\n",
       "   'properties': {'column_overrides': {'type': 'array',\n",
       "     'items': {'$ref': '#/components/schemas/experiment_view_overrides.ColumnOverride'}},\n",
       "    'created_at': {'type': 'string'},\n",
       "    'dataset_id': {'type': 'string'},\n",
       "    'id': {'type': 'string'},\n",
       "    'modified_at': {'type': 'string'}}},\n",
       "  'experiment_view_overrides.ExperimentViewOverridePatchRequest': {'type': 'object',\n",
       "   'required': ['column_overrides'],\n",
       "   'properties': {'column_overrides': {'type': 'array',\n",
       "     'maxItems': 50,\n",
       "     'minItems': 1,\n",
       "     'items': {'$ref': '#/components/schemas/experiment_view_overrides.ColumnOverride'}}}},\n",
       "  'experiment_view_overrides.ExperimentViewOverridePostRequest': {'type': 'object',\n",
       "   'required': ['column_overrides'],\n",
       "   'properties': {'column_overrides': {'type': 'array',\n",
       "     'maxItems': 50,\n",
       "     'minItems': 1,\n",
       "     'items': {'$ref': '#/components/schemas/experiment_view_overrides.ColumnOverride'}}}},\n",
       "  'feedback.FeedbackCreateSchema': {'type': 'object',\n",
       "   'properties': {'comment': {'type': 'string'},\n",
       "    'comparative_experiment_id': {'type': 'string'},\n",
       "    'correction': {},\n",
       "    'created_at': {'type': 'string'},\n",
       "    'error': {'type': 'boolean'},\n",
       "    'feedback_config': {'$ref': '#/components/schemas/types.FeedbackConfig'},\n",
       "    'feedback_group_id': {'type': 'string'},\n",
       "    'feedback_source': {'$ref': '#/components/schemas/feedback.FeedbackSource'},\n",
       "    'id': {'type': 'string'},\n",
       "    'key': {'type': 'string'},\n",
       "    'modified_at': {'type': 'string'},\n",
       "    'run_id': {'type': 'string'},\n",
       "    'score': {},\n",
       "    'session_id': {'type': 'string'},\n",
       "    'start_time': {'type': 'string'},\n",
       "    'trace_id': {'type': 'string'},\n",
       "    'value': {}}},\n",
       "  'feedback.FeedbackSource': {'type': 'object',\n",
       "   'properties': {'ls_user_id': {'type': 'string'},\n",
       "    'metadata': {'type': 'object', 'additionalProperties': True},\n",
       "    'type': {'type': 'string'},\n",
       "    'user_id': {'type': 'string'}}},\n",
       "  'runs.ErrorResponse': {'type': 'object',\n",
       "   'properties': {'details': {'description': 'Optional error details as JSON string',\n",
       "     'type': 'string',\n",
       "     'example': '{\"field\":\"dataset_id\",\"reason\":\"required\"}'},\n",
       "    'error': {'description': 'Error message',\n",
       "     'type': 'string',\n",
       "     'example': 'Invalid request: missing required fields'}}},\n",
       "  'runs.Run': {'type': 'object',\n",
       "   'properties': {'dotted_order': {'type': 'string'},\n",
       "    'end_time': {'type': 'string'},\n",
       "    'error': {'type': 'string'},\n",
       "    'events': {'type': 'array',\n",
       "     'items': {'type': 'object', 'additionalProperties': True}},\n",
       "    'extra': {'type': 'object', 'additionalProperties': True},\n",
       "    'id': {'type': 'string'},\n",
       "    'input_attachments': {'type': 'object', 'additionalProperties': True},\n",
       "    'inputs': {'type': 'object', 'additionalProperties': True},\n",
       "    'name': {'type': 'string'},\n",
       "    'output_attachments': {'type': 'object', 'additionalProperties': True},\n",
       "    'outputs': {'type': 'object', 'additionalProperties': True},\n",
       "    'parent_run_id': {'type': 'string'},\n",
       "    'reference_example_id': {'type': 'string'},\n",
       "    'run_type': {'type': 'string',\n",
       "     'enum': ['tool',\n",
       "      'chain',\n",
       "      'llm',\n",
       "      'retriever',\n",
       "      'embedding',\n",
       "      'prompt',\n",
       "      'parser']},\n",
       "    'serialized': {'type': 'object', 'additionalProperties': True},\n",
       "    'session_id': {'type': 'string'},\n",
       "    'session_name': {'type': 'string'},\n",
       "    'start_time': {'type': 'string'},\n",
       "    'status': {'type': 'string'},\n",
       "    'tags': {'type': 'array', 'items': {'type': 'string'}},\n",
       "    'trace_id': {'type': 'string'}}},\n",
       "  'ttl_settings.TTLSettingsResponse': {'type': 'object',\n",
       "   'properties': {'is_custom': {'type': 'boolean'},\n",
       "    'longlived_ttl_days': {'type': 'integer'},\n",
       "    'tenant_id': {'type': 'string'}}},\n",
       "  'ttl_settings.UpdateTTLSettingsRequest': {'type': 'object',\n",
       "   'properties': {'longlived_ttl_days': {'type': 'integer'}}},\n",
       "  'types.FeedbackCategory': {'type': 'object',\n",
       "   'properties': {'label': {'type': 'string', 'minLength': 1},\n",
       "    'value': {'type': 'number'}}},\n",
       "  'types.FeedbackConfig': {'type': 'object',\n",
       "   'properties': {'categories': {'type': 'array',\n",
       "     'items': {'$ref': '#/components/schemas/types.FeedbackCategory'}},\n",
       "    'max': {'type': 'number'},\n",
       "    'min': {'type': 'number'},\n",
       "    'type': {'$ref': '#/components/schemas/types.FeedbackType'}}},\n",
       "  'types.FeedbackType': {'type': 'string',\n",
       "   'enum': ['continuous', 'categorical', 'freeform'],\n",
       "   'x-enum-varnames': ['FeedbackTypeContinuous',\n",
       "    'FeedbackTypeCategorical',\n",
       "    'FeedbackTypeFreeform']}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL of JSON data\n",
    "url = \"https://api.smith.langchain.com/openapi.json\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "json_data = requests.get(url).json()\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79e4c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "json_splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "json_chunks = json_splitter.split_json(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e61e03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'description': 'The LangSmith API is used to programmatically create and manage LangSmith resources.\\n\\n## Host\\nhttps://api.smith.langchain.com\\n\\n## Authentication\\nTo authenticate with the LangSmith API, set the `X-Api-Key` header\\nto a valid [LangSmith API key](https://docs.langchain.com/langsmith/create-account-api-key#create-an-api-key).\\n\\n'}}\n",
      "{'info': {'version': '0.1.0'}, 'paths': {'/api/v1/audit-logs': {'get': {'tags': ['audit-logs'], 'summary': 'Get Audit Logs'}}}}\n",
      "{'paths': {'/api/v1/audit-logs': {'get': {'description': \"Retrieve audit log records for the authenticated user's organization in OCSF format.\\n\\nRequires both start_time and end_time parameters to filter logs within a date range.\\nSupports cursor-based pagination.\\n\\nReturns results in OCSF API Activity (Class UID: 6003) format,\\nwhich is compatible with security monitoring and SIEM tools.\\nReference: https://schema.ocsf.io/1.7.0/classes/api_activity\"}}}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in json_chunks[:3]:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54ca0c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"description\": \"The LangSmith API is used to programmatically create and manage LangSmith resources.\\n\\n## Host\\nhttps://api.smith.langchain.com\\n\\n## Authentication\\nTo authenticate with the LangSmith API, set the `X-Api-Key` header\\nto a valid [LangSmith API key](https://docs.langchain.com/langsmith/create-account-api-key#create-an-api-key).\\n\\n\"}}'\n",
      "page_content='{\"info\": {\"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/audit-logs\": {\"get\": {\"tags\": [\"audit-logs\"], \"summary\": \"Get Audit Logs\"}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/audit-logs\": {\"get\": {\"description\": \"Retrieve audit log records for the authenticated user's organization in OCSF format.\\n\\nRequires both start_time and end_time parameters to filter logs within a date range.\\nSupports cursor-based pagination.\\n\\nReturns results in OCSF API Activity (Class UID: 6003) format,\\nwhich is compatible with security monitoring and SIEM tools.\\nReference: https://schema.ocsf.io/1.7.0/classes/api_activity\"}}}}'\n"
     ]
    }
   ],
   "source": [
    "docs = json_splitter.create_documents(texts=[json_data])\n",
    "for doc in docs[:3]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59680255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"description\": \"The LangSmith API is used to programmatically create and manage LangSmith resources.\\n\\n## Host\\nhttps://api.smith.langchain.com\\n\\n## Authentication\\nTo authenticate with the LangSmith API, set the `X-Api-Key` header\\nto a valid [LangSmith API key](https://docs.langchain.com/langsmith/create-account-api-key#create-an-api-key).\\n\\n\"}}\n",
      "{\"info\": {\"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/audit-logs\": {\"get\": {\"tags\": [\"audit-logs\"], \"summary\": \"Get Audit Logs\"}}}}\n"
     ]
    }
   ],
   "source": [
    "texts = json_splitter.split_text(json_data)\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb1337",
   "metadata": {},
   "source": [
    "### **COMBINING ALL TRANSFORMED DOCUMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd2be846",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transformed_docs = []\n",
    "\n",
    "# Add PDF chunks\n",
    "all_transformed_docs.extend(final_docs)\n",
    "\n",
    "# Add text file chunks (recursive + character)\n",
    "all_transformed_docs.extend(text)\n",
    "all_transformed_docs.extend(char_docs)\n",
    "\n",
    "# Add HTML chunks\n",
    "all_transformed_docs.extend(html_headers_splits)\n",
    "\n",
    "# Add JSON documents\n",
    "all_transformed_docs.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b2d81f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5650"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of transformed documents\n",
    "len(all_transformed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6d1d18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 0, 'page_label': '1'}, page_content='Deep Learning\\nwith Python\\nFRANÇOIS CHOLLET\\nMANNING\\nSHELTER ISLAND'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 1, 'page_label': '2'}, page_content='ISBN 9781617294433\\nPrinted in the United States of America\\n©2018 by Manning Publications Co.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 2, 'page_label': '3'}, page_content='brief contents\\nPART 1F UNDAMENTALS OF DEEP LEARNING .................................. 1\\n1 ■ What is deep learning? 3\\n2 ■ Before we begin: the mathematical building blocks of neural \\nnetworks 25\\n3 ■ Getting started with neural networks 56\\n4 ■ Fundamentals of machine learning 93\\nPART 2D EEP LEARNING IN PRACTICE ........................................ 117\\n5 ■ Deep learning for computer vision 119\\n6 ■ Deep learning for text and sequences 178\\n7 ■ Advanced deep-learning best practices 233'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 2, 'page_label': '3'}, page_content='7 ■ Advanced deep-learning best practices 233\\n8 ■ Generative deep learning 269\\n9 ■ Conclusions 314'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 3, 'page_label': '4'}, page_content='contents\\npreface xiii\\nacknowledgments xv\\nabout this book xvi\\nabout the author xx\\nabout the cover xxi\\nPART 1F UNDAMENTALS OF DEEP LEARNING ...................1\\n1 What is deep learning? 3\\n1.1 Artificial intelligence, machine learning, \\nand deep learning 4\\nArtificial intelligence 4 ■ Machine learning 4 ■ Learning \\nrepresentations from data 6 ■ The “deep” in deep learning 8\\nUnderstanding how deep learning works, in three figures 9\\nWhat deep learning has achieved so far 11 ■ Don’t believe'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 3, 'page_label': '4'}, page_content='the short-term hype 12 ■ The promise of AI 13\\n1.2 Before deep learning: a brief history of machine \\nlearning 14\\nProbabilistic modeling 14 ■ Early neural networks 14\\nKernel methods 15 ■ Decision trees, random forests, \\nand gradient boosting machines 16 ■ Back to neural \\nnetworks 17 ■ What makes deep learning different 17\\nThe modern machine-learning landscape 18'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 4, 'page_label': '5'}, page_content='1.3 Why deep learning? Why now? 20\\nHardware 20 ■ Data 21 ■ Algorithms 21 ■ A new \\nwave of investment 22 ■ The democratization of deep \\nlearning 23 ■ Will it last? 23\\n2 Before we begin: the mathematical building blocks of \\nneural networks 25\\n2.1 A first look at a neural network 27\\n2.2 Data representations for neural networks 31\\nScalars (0D tensors) 31 ■ Vectors (1D tensors) 31\\nMatrices (2D tensors) 31 ■ 3D tensors and higher-\\ndimensional tensors 32 ■ Key attributes 32'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 4, 'page_label': '5'}, page_content='dimensional tensors 32 ■ Key attributes 32\\nManipulating tensors in Numpy 34 ■ The notion \\nof data batches 34 ■ Real-world examples of data \\ntensors 35 ■ Vector data 35 ■ Timeseries data or \\nsequence data 35 ■ Image data 36 ■ Video data 37\\n2.3 The gears of neural ne tworks: tensor operations 38\\nElement-wise operations 38 ■ Broadcasting 39 ■ Tensor \\ndot 40 ■ Tensor reshaping 42 ■ Geometric interpretation \\nof tensor operations 43 ■ A geometric interpretation of deep \\nlearning 44'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 4, 'page_label': '5'}, page_content='learning 44\\n2.4 The engine of neural networks: gradient-based \\noptimization 46\\nWhat’s a derivative? 47 ■ Derivative of a tensor operation: \\nthe gradient 48 ■ Stochastic gradient descent 48\\nChaining derivatives: the Backpropagation algorithm 51\\n2.5 Looking back at our first example 53\\n2.6 Chapter summary 55\\n3 Getting started with neural networks 56\\n3.1 Anatomy of a neural network 58\\nLayers: the building blocks of deep learning 58 ■ Models:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 4, 'page_label': '5'}, page_content='networks of layers 59 ■ Loss functions and optimizers: keys \\nto configuring the learning process 60\\n3.2 Introduction to Keras 61\\nKeras, TensorFlow, Theano, and CNTK 62 ■ Developing \\nwith Keras: a quick overview 62\\n3.3 Setting up a deep-learning workstation 65\\nJupyter notebooks: the preferred way to run deep-learning \\nexperiments 65 ■ Getting Keras running: two options 66'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 5, 'page_label': '6'}, page_content='Running deep-learning jobs in the cloud: pros and cons 66\\nWhat is the best GPU for deep learning? 66\\n3.4 Classifying movie reviews:  a binary classification \\nexample 68\\nThe IMDB dataset 68 ■ Preparing the data 69\\nBuilding your network 70 ■ Validating your approach 73\\nUsing a trained network to generate predictions on new \\ndata 76 ■ Further experiments 77 ■ Wrapping up 77\\n3.5 Classifying newswires: a multiclass classification \\nexample 78\\nThe Reuters dataset 78 ■ Preparing the data 79'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 5, 'page_label': '6'}, page_content='The Reuters dataset 78 ■ Preparing the data 79\\nBuilding your network 79 ■ Validating your approach 80\\nGenerating predictions on new data 83 ■ A different way to \\nhandle the labels and the loss 83 ■ The importance of \\nhaving sufficiently large intermediate layers 83 ■ Further \\nexperiments 84 ■ Wrapping up 84\\n3.6 Predicting house prices : a regression example 85\\nThe Boston Housing Price dataset 85 ■ Preparing the \\ndata 86 ■ Building your network 86 ■ Validating'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 5, 'page_label': '6'}, page_content='data 86 ■ Building your network 86 ■ Validating \\nyour approach using K-fold validation 87 ■ Wrapping up 91\\n3.7 Chapter summary 92\\n4 Fundamentals of machine learning 93\\n4.1 Four branches of machine learning 94\\nSupervised learning 94 ■ Unsupervised learning 94\\nSelf-supervised learning 94 ■ Reinforcement learning 95\\n4.2 Evaluating machine-learning models 97\\nTraining, validation, and test sets 97 ■ Things to \\nkeep in mind 100\\n4.3 Data preprocessing, feature engineering, \\nand feature learning 101'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 5, 'page_label': '6'}, page_content='and feature learning 101\\nData preprocessing for neural networks 101 ■ Feature \\nengineering 102\\n4.4 Overfitting and underfitting 104\\nReducing the network’s size 104 ■ Adding weight \\nregularization 107 ■ Adding dropout 109\\n4.5 The universal workflow of machine learning 111\\nDefining the problem and assembling a dataset 111\\nChoosing a measure of success 112 ■ Deciding on an'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 6, 'page_label': '7'}, page_content='evaluation protocol 112 ■ Preparing your data 112\\nDeveloping a model that does better than a baseline 113\\nScaling up: developing a model that overfits 114\\nRegularizing your model and tuning your hyperparameters 114\\n4.6 Chapter summary 116\\nPART 2D EEP LEARNING IN PRACTICE .........................117\\n5 Deep learning for computer vision 119\\n5.1 Introduction to convnets 120\\nThe convolution operation 122 ■ The max-pooling \\noperation 127\\n5.2 Training a convnet from scratch on a small dataset 130'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 6, 'page_label': '7'}, page_content='The relevance of deep learning for small-data problems 130\\nDownloading the data 131 ■ Building your network 133\\nData preprocessing 135 ■ Using data augmentation 138\\n5.3 Using a pretrained convnet 143\\nFeature extraction 143 ■ Fine-tuning 152 ■ Wrapping \\nup 159\\n5.4 Visualizing what convnets learn 160\\nVisualizing intermediate activations 160 ■ Visualizing \\nconvnet filters 167 ■ Visualizing heatmaps of class \\nactivation 172\\n5.5 Chapter summary 177\\n6 Deep learning for text and sequences 178'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 6, 'page_label': '7'}, page_content='6 Deep learning for text and sequences 178\\n6.1 Working with text data 180\\nOne-hot encoding of words and characters 181 ■ Using \\nword embeddings 184 ■ Putting it all together: from raw \\ntext to word embeddings 188 ■ Wrapping up 195\\n6.2 Understanding recurrent neural networks 196\\nA recurrent layer in Keras 198 ■ Understanding the \\nLSTM and GRU layers 202 ■ A concrete LSTM example \\nin Keras 204 ■ Wrapping up 206\\n6.3 Advanced use of recurrent neural networks 207'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 6, 'page_label': '7'}, page_content='6.3 Advanced use of recurrent neural networks 207\\nA temperature-forecasting problem 207 ■ Preparing the \\ndata 210 ■ A common-sense, non-machine-learning \\nbaseline 212 ■ A basic machine-learning approach 213\\nA first recurrent baseline 215 ■ Using recurrent dropout'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 7, 'page_label': '8'}, page_content='to fight overfitting 216 ■ Stacking recurrent layers 217\\nUsing bidirectional RNNs 219 ■ Going even further 222\\nWrapping up 223\\n6.4 Sequence processing with convnets 225\\nUnderstanding 1D convolution for sequence data 225\\n1D pooling for sequence data 226 ■ Implementing a 1D \\nconvnet 226 ■ Combining CNNs and RNNs to process long \\nsequences 228 ■ Wrapping up 231\\n6.5 Chapter summary 232\\n7 Advanced deep-learning best practices 233\\n7.1 Going beyond the Sequ ential model: the Keras\\n functional API 234'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 7, 'page_label': '8'}, page_content='functional API 234\\nIntroduction to the functional API 236 ■ Multi-input \\nmodels 238 ■ Multi-output models 240 ■ Directed acyclic \\ngraphs of layers 242 ■ Layer weight sharing 246 ■ Models \\nas layers 247 ■ Wrapping up 248\\n7.2 Inspecting and monitoring deep-learning models using \\nKeras callbacks and TensorBoard 249\\nUsing callbacks to act on a model during training 249\\nIntroduction to TensorBoard: the TensorFlow visualization \\nframework 252 ■ Wrapping up 259'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 7, 'page_label': '8'}, page_content='framework 252 ■ Wrapping up 259\\n7.3 Getting the most out of your models 260\\nAdvanced architecture patterns 260 ■ Hyperparameter \\noptimization 263 ■ Model ensembling 264 ■ Wrapping \\nup 266\\n7.4 Chapter summary 268\\n8 Generative deep learning 269\\n8.1 Text generation with LSTM 271\\nA brief history of generative recurrent networks 271 ■ How \\ndo you generate sequence data? 272 ■ The importance of \\nthe sampling strategy 272 ■ Implementing character-level \\nLSTM text generation 274 ■ Wrapping up 279'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 7, 'page_label': '8'}, page_content='LSTM text generation 274 ■ Wrapping up 279\\n8.2 DeepDream 280\\nImplementing DeepDream in Keras 281 ■ Wrapping up 286\\n8.3 Neural style transfer 287\\nThe content loss 288 ■ The style loss 288 ■ Neural style \\ntransfer in Keras 289 ■ Wrapping up 295'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 8, 'page_label': '9'}, page_content='8.4 Generating images with variational autoencoders 296\\nSampling from latent spaces of images 296 ■ Concept vectors \\nfor image editing 297 ■ Variational autoencoders 298\\nWrapping up 304\\n8.5 Introduction to generati ve adversarial networks 305\\nA schematic GAN implementation 307 ■ A bag of tricks 307\\nThe generator 308 ■ The discriminator 309 ■ The adversarial \\nnetwork 310 ■ How to train your DCGAN 310 ■ Wrapping \\nup 312\\n8.6 Chapter summary 313\\n9 Conclusions 314\\n9.1 Key concepts in review 315'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 8, 'page_label': '9'}, page_content='9 Conclusions 314\\n9.1 Key concepts in review 315\\nVarious approaches to AI 315 ■ What makes deep learning \\nspecial within the field of machine learning 315 ■ How to \\nthink about deep learning 316 ■ Key enabling technologies 317\\nThe universal machine-learning workflow 318 ■ Key network \\narchitectures 319 ■ The space of possibilities 322\\n9.2 The limitations of deep learning 325\\nThe risk of anthropomorphizing machine-learning models 325\\nLocal generalization vs. extreme generalization 327'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 8, 'page_label': '9'}, page_content='Wrapping up 329\\n9.3 The future of deep learning 330\\nModels as programs 330 ■ Beyond backpropagation and \\ndifferentiable layers 332 ■ Automated machine learning 332\\nLifelong learning and modular subroutine reuse 333\\nThe long-term vision 335\\n9.4 Staying up to date in a fast-moving field 337\\nPractice on real-world problems using Kaggle 337\\nRead about the latest developments on arXiv 337\\nExplore the Keras ecosystem 338\\n9.5 Final words 339'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 8, 'page_label': '9'}, page_content='9.5 Final words 339\\nappendix A Installing Keras and its dependencies on Ubuntu 340\\nappendix B Running Jupyter note books on an EC2 GPU instance 345\\nindex 353'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 9, 'page_label': '10'}, page_content='preface\\nIf you’ve picked up this book, you’re pr obably aware of the extraordinary progress\\nthat deep learning has represented for the fi eld of artificial intelligence in the recent\\npast. In a mere five years,  we’ve gone from near-unusable image recognition and\\nspeech transcription, to superhuman performance on these tasks.\\n The consequences of this sudden progre ss extend to almost every industry. But in\\norder to begin deploying deep-learning technology to every problem that it could'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 9, 'page_label': '10'}, page_content='solve, we need to make it accessible to as many people as possible, including non-\\nexperts—people who aren’t re searchers or graduate stud ents. For deep learning to\\nreach its full potential, we need to radically democratize it.\\n When I released the first version of th e Keras deep-learning framework in March\\n2015, the democratization of AI wasn’t what I had in mind. I had been doing research\\nin machine learning for several years, and had built Keras to help me with my own'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 9, 'page_label': '10'}, page_content='experiments. But throughout 2015 and 2016, tens of thousands of new people\\nentered the field of deep learning; many of them picked up Keras because it was—and\\nstill is—the easiest framework to get started with. As I watched scores of newcomers\\nuse Keras in unexpected, powerful ways, I ca me to care deeply about the accessibility\\nand democratization of AI. I realized that the further we spread these technologies,\\nthe more useful and valuable they become . Accessibility quickly became an explicit'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 9, 'page_label': '10'}, page_content='goal in the development of Keras, and over a few short years, the Keras developer\\ncommunity has made fantastic achievements on this front. We’ve put deep learning\\ninto the hands of tens of thousands of people, who in turn are using it to solve import-\\nant problems we didn’t even know existed until recently.\\n The book you’re holding is another step on the way to making deep learning avail-\\nable to as many people as  possible. Keras had always needed a companion course to'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 10, 'page_label': '11'}, page_content='simultaneously cover fundamentals of deep learning, Keras usage patterns, and deep-\\nlearning best practices. This book is my best effort to produce such a course. I wrote it\\nwith a focus on making the concepts behi nd deep learning, and their implementa-\\nt i o n ,  a s  a p p r o a c h a b l e  a s  p o s s i b l e .  D o i n g  s o  d i d n ’ t  r e q u i r e  m e  t o  d u m b  d o w n  a n y -\\nthing—I strongly believe that there are no  difficult ideas in deep learning. I hope'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 10, 'page_label': '11'}, page_content='you’ll find this book valuable and that it w ill enable you to begin building intelligent\\napplications and solve the problems that matter to you.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 11, 'page_label': '12'}, page_content='about this book\\nThis book was written for anyone who wishes to explore deep learning from scratch or\\nbroaden their understanding of deep learning. Whether you’re a practicing machine-learn-\\ning engineer, a software developer, or a college student, you’ll find value in these pages.\\n This book offers a practical, hands-on exploration of deep learning. It avoids math-\\nematical notation, preferring instead to ex plain quantitative concepts via code snip-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 11, 'page_label': '12'}, page_content='pets and to build practical intuition about the core ideas of machine learning and\\ndeep learning. \\n You’ll learn from more than 30 code examples that include detailed commentary,\\npractical recommendations, and simple high -level explanations of everything you\\nneed to know to start using deep learning to solve concrete problems.\\n The code examples use the Python deep-learning framework Keras, with Tensor-\\nFlow as a backend engine. Keras, one of the most popular and fastest-growing deep-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 11, 'page_label': '12'}, page_content='learning frameworks, is widely recommended as the best tool to get started with deep\\nlearning. \\n After reading this book, you’ll have a soli d understand of what deep learning is,\\nwhen it’s applicable, and what its limitations  are. You’ll be familiar with the standard\\nworkflow for approaching and solving machine-learning problems, and you’ll know\\nhow to address commonly enco untered issues. You’ll be ab le to use Keras to tackle'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 11, 'page_label': '12'}, page_content='real-world problems ranging fr om computer vision to na tural-language processing:\\nimage classification, timeseries forecasting, sentiment analysis, image and text genera-\\ntion, and more.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 12, 'page_label': '13'}, page_content='Who should read this book\\nThis book is written for people with Python programming experience who want to get\\nstarted with machine learning and deep lear ning. But this book can also be valuable\\nto many different types of readers:\\n\\uf0a1 If you’re a data scientist familiar with machine learning, this book will provide\\nyou with a solid, practical introduction to deep learning, the fastest-growing\\nand most significant subfield of machine learning.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 12, 'page_label': '13'}, page_content='\\uf0a1 If you’re a deep-learning expert looking to get started with the Keras frame-\\nwork, you’ll find this book to be the best Keras crash course available.\\n\\uf0a1 If you’re a graduate studen t studying deep learning in  a formal setting, you’ll\\nfind this book to be a practical comp lement to your education, helping you\\nbuild intuition around the behavior of deep neural networks and familiarizing\\nyou with key best practices.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 12, 'page_label': '13'}, page_content='you with key best practices.\\nEven technically minded people who don’t code regularly will find this book useful as\\nan introduction to both basic and advanced deep-learning concepts.\\n In order to use Keras, you’ll need reasonable Python proficiency. Additionally, famil-\\niarity with the Numpy library will be helpful, although it isn’t required. You don’t need\\nprevious experience with machine learning  or deep learning: this book covers from'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 12, 'page_label': '13'}, page_content='scratch all the necessary basics. You don’t need an advanced mathematics background,\\neither—high school–level mathematics should suffice in order to follow along.\\nRoadmap\\nThis book is structured in two parts. If you have no prior experience with machine\\nlearning, I strongly recommend that you co mplete part 1 before approaching part 2.\\nWe’ll start with simple examples, and as th e book goes on, we’ll get increasingly close\\nto state-of-the-art techniques.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 12, 'page_label': '13'}, page_content='to state-of-the-art techniques.\\n Part 1 is a high-level introduction to deep learning, providing context and defini-\\ntions, and explaining all the notions requ ired to get started with machine learning\\nand neural networks:\\n\\uf0a1 Chapter 1 presents essential contex t and background knowledge around AI,\\nmachine learning, and deep learning.\\n\\uf0a1 Chapter 2 introduces fundamental conc epts necessary in order to approach\\ndeep learning: tensors, te nsor operations, gradient descent, and backpropaga-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 12, 'page_label': '13'}, page_content='tion. This chapter also features the bo ok’s first example of a working neural\\nnetwork.\\n\\uf0a1 Chapter 3 includes everything you need to get started with neural networks: an\\nintroduction to Keras, our deep-learning framework of choice; a guide for set-\\nting up your workstation; and three foundational code examples with detailed\\nexplanations. By the end of this chapter,  you’ll be able to  train simple neural'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 13, 'page_label': '14'}, page_content='networks to handle classification and re gression tasks, and you’ll have a solid\\nidea of what’s happening in the background as you train them.\\n\\uf0a1 Chapter 4 explores the canonical machine- learning workflow. You’ll also learn\\nabout common pitfalls and their solutions.\\nPart 2 takes an in-depth dive into practica l applications of deep learning in computer\\nvision and natural-language processing. Many of the examples introduced in this part'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 13, 'page_label': '14'}, page_content='can be used as templates to solve problems you’ll encounter in the real-world practice\\nof deep learning:\\n\\uf0a1 Chapter 5 examines a range of practical computer-vision examples, with a focus\\non image classification.\\n\\uf0a1 Chapter 6 gives you practice with techniques for processing sequence data, such\\nas text and timeseries.\\n\\uf0a1 Chapter 7 introduces advanc ed techniques for building state-of-the-art deep-\\nlearning models.\\n\\uf0a1 Chapter 8 explains generative models: deep-learning models capable of creat-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 13, 'page_label': '14'}, page_content='ing images and text, with sometimes surprisingly artistic results.\\n\\uf0a1 Chapter 9 is dedicated to consolidating what you’ve learned throughout the\\nbook, as well as opening perspectives on  the limitations of deep learning and\\nexploring its probable future.\\nSoftware/hardware requirements\\nAll of this book’s code examples use the Keras deep-learning framework ( https:/ /\\nkeras.io), which is open source and free to  download. You’ll need access to a UNIX'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 13, 'page_label': '14'}, page_content='machine; it’s possible to us e Windows, too, but I don’t recommend it. Appendix A\\nwalks you through the complete setup. \\n I also recommend that you have a recent NVIDIA GPU on your machine, such as a\\nTITAN X. This isn’t required, but it will make your experience better by allowing you\\nto run the code examples several times faster. See section 3.3 for more information\\nabout setting up a deep-learning workstation.\\n If you don’t have access to a local workstation with a recent NVIDIA GPU, you can'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 13, 'page_label': '14'}, page_content='use a cloud environment, instead. In particular, you can use Google Cloud instances\\n(such as an n1-standard-8 instance with an NVIDIA Tesla K80 add-on) or Amazon Web\\nServices (AWS) GPU instances (such as a p2.xlarge in stance). Appendix B presents in\\ndetail one possible cloud workflow that runs an AWS instance via Jupyter notebooks,\\naccessible in your browser.\\nSource code\\nAll code examples in this book are availa ble for download as Jupyter notebooks from'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 13, 'page_label': '14'}, page_content='the book’s website, www.manning.com/books/deep-learning-with-python, and on\\nGitHub at https:/ /github.com/fchollet/deep-learning-with-python-notebooks.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 14, 'page_label': '15'}, page_content='Book forum\\nPurchase of Deep Learning with Python includes free access to a private web forum run by\\nManning Publications where you can make  comments about the book, ask technical\\nquestions, and receive help from the author and from other users. To access the forum,\\ngo to https:/ /forums.manning.com/forums/deep-learning-with-python. You can also\\nlearn more about Manning’s forums and the rules of conduct at https:/ /forums\\n.manning.com/forums/about.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 14, 'page_label': '15'}, page_content='.manning.com/forums/about.\\n Manning’s commitment to our readers is  to provide a venue where a meaningful\\ndialogue between individual readers and between readers and the author can take\\nplace. It isn’t a commitment to any specific amount of participation on the part of the\\nauthor, whose contribution to the forum remains voluntary (and unpaid). We suggest\\nyou try asking him some chal lenging questions lest his in terest stray! The forum and'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 14, 'page_label': '15'}, page_content='the archives of previous discussions will be accessible from the publisher’s website as\\nlong as the book is in print.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 15, 'page_label': '16'}, page_content='Part 1\\nFundamentals\\nof deep learning\\nChapters 1–4 of this book will give you a foundational understanding of\\nwhat deep learning is, what it can achieve, and how it works. It will also make you\\nfamiliar with the canonical workflow for solving data problems using deep learn-\\ning. If you aren’t already highly knowledgeable about deep learning, you should\\ndefinitely begin by reading part 1 in full before moving on to the practical appli-\\ncations in part 2.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 16, 'page_label': '17'}, page_content='What is deep learning?\\nIn the past few years, artificial intelligence (AI) has been a subject of intense media\\nhype. Machine learning, deep learning, and AI come up in countless articles, often\\noutside of technology-minded publications. We’re promised a futu re of intelligent\\nchatbots, self-driving cars, and virtual a ssistants—a future sometimes painted in a\\ngrim light and other times as utopian, wh ere human jobs will be scarce and most'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 16, 'page_label': '17'}, page_content='economic activity will be handled by robots or AI agents. For a future or current\\npractitioner of machine learning, it’s impo rtant to be able to recognize the signal\\nin the noise so that you can tell world-changing developments from overhyped\\npress releases. Our future is at stake, and it’s a future in which you have an active\\nrole to play: after reading this book, you’ll be one of those who develop the AI\\nagents. So let’s tackle these questions: What has deep learning achieved so far?'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 16, 'page_label': '17'}, page_content='How significant is it? Where are we headed next? Should you believe the hype?\\n This chapter provides essential context around artificial in telligence, machine\\nlearning, and deep learning.\\nThis chapter covers\\n\\uf0a1 High-level definitions of fundamental concepts\\n\\uf0a1 Timeline of the development of machine learning\\n\\uf0a1 Key factors behind deep learning’s rising \\npopularity and future potential'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 17, 'page_label': '18'}, page_content='4 CHAPTER 1 What is deep learning?\\n1.1 Artificial intelligen ce, machine learning, \\nand deep learning\\nFirst, we need to define clearly what  we’re talking about when we mention AI. What\\nare artificial intelligence, machine learning, and deep learning (see figure 1.1)? How\\ndo they relate to each other?\\n1.1.1 Artificial intelligence\\nArtificial intelligence was born in the 195 0s, when a handful of pioneers from the\\nnascent field of computer science started asking whether computers could be made to'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 17, 'page_label': '18'}, page_content='“think”—a question whose ramifications we’r e still exploring today. A concise defini-\\ntion of the field would be as follows: the effort to automate inte llectual tasks normally per-\\nformed by humans. As such, AI is a general field that encompasses machine learning and\\ndeep learning, but that also includes many  more approaches that don’t involve any\\nlearning. Early chess programs, for instance, only involved hardcoded rules crafted by'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 17, 'page_label': '18'}, page_content='programmers, and didn’t qualify as machin e learning. For a fairly long time, many\\nexperts believed that human-le vel artificial intelligence could be achieved by having\\nprogrammers handcraft a suffi ciently large set of explicit rules for manipulating\\nknowledge. This approach is known as symbolic AI, and it was the dominant paradigm\\nin AI from the 1950s to the late 1980s. It reached its peak popularity during the expert\\nsystems boom of the 1980s.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 17, 'page_label': '18'}, page_content='systems boom of the 1980s.\\n Although symbolic AI proved suitable to solve well-defined, logical problems, such as\\nplaying chess, it turned out to be intractable to figure out explicit rules for solving more\\ncomplex, fuzzy problems, such as image cla ssification, speech recognition, and lan-\\nguage translation. A new approach arose to take symbolic AI’s place: machine learning.\\n1.1.2 Machine learning\\nIn Victorian England, Lady Ada Lovelace was a friend and coll aborator of Charles'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 17, 'page_label': '18'}, page_content='Babbage, the inventor of the Analytical Engine : the first-known general-purpose,\\nmechanical computer. Although visionary and far ahead of its time, the Analytical\\nArtificial\\nintelligence\\nMachine\\nlearning\\nDeep\\nlearning\\nFigure 1.1 Artificial intelligence, \\nmachine learning, and deep learning\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 18, 'page_label': '19'}, page_content='5Artificial intelligence, machine learning, and deep learning\\nEngine wasn’t meant as a general-purpose computer when it was designed in the\\n1830s and 1840s, because the concept of general-purpose computation was yet to be\\ninvented. It was merely meant as a way to use mechanical operations to automate cer-\\ntain computations from the field of math ematical analysis—hence, the name Analyti-\\ncal Engine. In 1843, Ada Lovelace remarked on the invention, “The Analytical Engine'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 18, 'page_label': '19'}, page_content='has no pretensions whatever to originate anything. It can do whatever we know how to\\norder it to perform.… Its prov ince is to assist us in making available what we’re\\nalready acquainted with.”\\n This remark was later quoted by AI pioneer Alan Turing as “Lady Lovelace’s objec-\\ntion” in his landmark 1950 paper “Computing Machinery and Intelligence,” 1 which\\nintroduced the Turing test as well as key concepts that would come to shape AI. Turing'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 18, 'page_label': '19'}, page_content='was quoting Ada Lovelace while pondering whether general-purpose computers could\\nbe capable of learning and originality, and he came to the conclusion that they could.\\n Machine learning arises from this question: could a computer go beyond “what we\\nknow how to order it to perform” and learn on its own how to perform a specified task?\\nCould a computer surprise us? Rather th an programmers crafting data-processing\\nrules by hand, could a computer automatically learn these rules by looking at data?'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 18, 'page_label': '19'}, page_content='This question opens the door to a ne w programming paradigm. In classical pro-\\ngramming, the paradigm of symbolic AI, humans input rules (a program) and data to\\nbe processed according to th ese rules, and out come answ ers (see figure 1.2). With\\nmachine learning, humans input data as well  as the answers expected from the data,\\nand out come the rules. These rules can then be applied to new data to produce orig-\\ninal answers.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 18, 'page_label': '19'}, page_content='inal answers.\\nA machine-learning system is trained rather than explicitly programmed. It’s presented\\nwith many examples relevant to a task, and it finds statistical structure in these exam-\\nples that eventually allows the system to come up with rules for automating the task.\\nFor instance, if you wished to automate the task of tagging your vacation pictures, you\\ncould present a machine-learning system wi th many examples of pictures already'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 18, 'page_label': '19'}, page_content='tagged by humans, and the system would lear n statistical rules for associating specific\\npictures to specific tags.\\n1 A. M. Turing, “Computing Machinery and Intelligence,” Mind 59, no. 236 (1950): 433-460.\\nAnswers\\nRules\\nData\\nClassical\\nprogramming\\nRules\\nData\\nAnswers\\nMachine\\nlearning\\nFigure 1.2 Machine learning: \\na new programming paradigm\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 19, 'page_label': '20'}, page_content='6 CHAPTER 1 What is deep learning?\\n Although machine learning only started to  flourish in the 1990s, it has quickly\\nbecome the most popular and mo st successful subfield of AI, a trend driven by the\\navailability of faster hardware and larger datasets. Machine learning is tightly related\\nto mathematical statistics, but it differs from statistics in several important ways.\\nUnlike statistics, machine learning tends to deal with large, complex datasets (such as'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 19, 'page_label': '20'}, page_content='a dataset of millions of images, each consis ting of tens of thousands of pixels) for\\nwhich classical statistical analys is such as Bayesian analysis  would be impractical. As a\\nresult, machine learning, and especially de ep learning, exhibits  comparatively little\\nmathematical theory—maybe too little—and is engineering oriented. It’s a hands-on\\ndiscipline in which ideas are proven empirically more often than theoretically.\\n1.1.3 Learning representations from data'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 19, 'page_label': '20'}, page_content='1.1.3 Learning representations from data\\nTo define deep learning  and understand the difference between deep learning\\nand other machine-learning ap proaches, first we need some idea of what machine-\\nlearning algorithms do. I just stated that machine lear ning discovers rules to execute\\na data-processing task, given examples of what’s expected. So, to do machine learn-\\ning, we need three things:\\n\\uf0a1 Input data points —For instance, if the task is speech recognition, these data'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 19, 'page_label': '20'}, page_content='points could be sound files of people speaking. If the task  is image tagging,\\nthey could be pictures.\\n\\uf0a1 Examples of the expected output —In a speech-recognition task, these could be\\nhuman-generated transcripts of sound files. In an image task, expected outputs\\ncould be tags such as “dog,” “cat,” and so on.\\n\\uf0a1 A way to measure whether the algorithm is doing a good job —This is necessary in\\norder to determine the distance between the algorithm’s current output and'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 19, 'page_label': '20'}, page_content='its expected output. The measurement is used as a feedback signal to adjust\\nthe way the algorithm works. This adjustment step is what we call learning.\\nA machine-learning model tran sforms its input data into meaningful outputs, a pro-\\ncess that is “learned” from exposure to known examples of inputs and outputs. There-\\nfore, the central problem in machine learning and deep learning is to meaningfully\\ntransform data : in other words, to learn useful representations of the input data at'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 19, 'page_label': '20'}, page_content='hand—representations that get us closer to  the expected output. Before we go any\\nfurther: what’s a representation? At its core, it’s a different way to look at data—to rep-\\nresent or encode data. For instance, a color image can be encoded in the \\nRGB format\\n(red-green-blue) or in the HSV format (hue-saturation-value): these are two different\\nrepresentations of the same data. Some task s that may be difficult with one represen-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 19, 'page_label': '20'}, page_content='tation can become easy with another. For example, the task “select all red pixels in the\\nimage” is simpler in the RG format, whereas “make the image less saturated” is simpler\\nin the HSV format. Machine-learning models ar e all about finding appropriate repre-\\nsentations for their input data—transformations of the data that make it more amena-\\nble to the task at hand, such as a classification task.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 20, 'page_label': '21'}, page_content='7Artificial intelligence, machine learning, and deep learning\\n Let’s make this concrete. Cons ider an x-axis, a y-axis, and\\nsome points represented by their coordinates in the (x, y) sys-\\ntem, as shown in figure 1.3.\\n As you can see, we have a few white points and a few black\\npoints. Let’s say we want to develop an algorithm that can take\\nthe coordinates (x, y) of a point and output whether that\\npoint is likely to be black or to be white. In this case,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 20, 'page_label': '21'}, page_content='\\uf0a1 The inputs are the coordinates of our points.\\n\\uf0a1 The expected outputs are the colors of our points.\\n\\uf0a1 A way to measure whether our algorithm is doing a\\ngood job could be, for instance, the percentage of\\npoints that are being correctly classified.\\nWhat we need here is a new representation of our data that cleanly separates the white\\npoints from the black points. One transfor mation we could use, among many other\\npossibilities, would be a coordinate change, illustrated in figure 1.4.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 20, 'page_label': '21'}, page_content='In this new coordinate system, the coordinate s of our points can be said to be a new\\nrepresentation of our data. And it’s a go od one! With this representation, the\\nblack/white classification problem can be expressed as a simple rule: “Black points\\nare such that x > 0,” or “White points are such that x < 0.” This new representation\\nbasically solves the classification problem.\\n In this case, we defined the coordinate change by hand. But if instead we tried sys-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 20, 'page_label': '21'}, page_content='tematically searching for different possible coordinate changes, and used as feedback\\nthe percentage of points being correctly cl assified, then we would be doing machine\\nlearning. Learning, in the context of machine learni ng, describes an automatic search\\nprocess for better representations.\\n All machine-learning algorithms consist of  automatically finding such transforma-\\ntions that turn data into more-useful re presentations for a given task. These opera-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 20, 'page_label': '21'}, page_content='tions can be coordinate changes, as you ju st saw, or linear pr ojections (which may\\ndestroy information), translations, nonlinea r operations (such as “select all points\\nsuch that x > 0”), and so on. Machine-lear ning algorithms aren’t usually creative in\\ny\\n2: Coordinate change\\nx\\ny\\n1: Raw data\\nx\\ny\\n3: Better representation\\nx\\nFigure 1.4 Coordinate change\\ny\\nx\\nFigure 1.3\\nSome sample data\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 21, 'page_label': '22'}, page_content='8 CHAPTER 1 What is deep learning?\\nfinding these transformations; they’re mere ly searching through a predefined set of\\noperations, called a hypothesis space.\\n So that’s what machine learning is, te chnically: searching for useful representa-\\ntions of some input data, within a predef ined space of possibi lities, using guidance\\nfrom a feedback signal. This simple idea allows for solv ing a remarkably broad range\\nof intellectual tasks, from speech recognition to autonomous car driving.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 21, 'page_label': '22'}, page_content='Now that you understand what we mean by learning, let’s take a look at what makes\\ndeep learning special. \\n1.1.4 The “deep” in deep learning\\nDeep learning is a specific subfield of machine learning: a new take on learning repre-\\nsentations from data that puts an emphasis on learning successive layers of increasingly\\nmeaningful representations. The deep in deep learning isn’t a reference to any kind of\\ndeeper understanding achieved by the approach ; rather, it stands for this idea of suc-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 21, 'page_label': '22'}, page_content='cessive layers of representations. How many layers contribute to a model of the data is\\ncalled the depth of the model. Other appropriate names for the field could have been\\nlayered representations learning  and hierarchical representations learning . Modern deep\\nlearning often involves tens or even hundreds of successive layers of representations—\\nand they’re all learned automatically from  exposure to training data. Meanwhile,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 21, 'page_label': '22'}, page_content='other approaches to machine learning tend to focus on learning only one or two lay-\\ners of representations of the data; hence, they’re sometimes called shallow learning.\\n In deep learning, these layered representations are (almost always) learned via\\nmodels called neural networks, structured in literal layers stacked on top of each other.\\nThe term neural network is a reference to neurobiology, but although some of the cen-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 21, 'page_label': '22'}, page_content='tral concepts in deep learning were developed in part by drawing inspiration from our\\nunderstanding of the brain, deep-learning models are not m o d e l s  o f  t h e  b r a i n .\\nThere’s no evidence that the brain implem ents anything like the learning mecha-\\nnisms used in modern deep-learning mode ls. You may come across pop-science arti-\\ncles proclaiming that deep learning work s like the brain or was modeled after the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 21, 'page_label': '22'}, page_content='brain, but that isn’t the case. It would be confusing and counterproductive for new-\\ncomers to the field to think of deep learning as being in any way related to neurobiol-\\nogy; you don’t need that shroud of “just li ke our minds” mystique and mystery, and\\nyou may as well forget anyt hing you may have read abou t hypothetical links between\\ndeep learning and biology. For our purposes , deep learning is a mathematical frame-\\nwork for learning representations from data.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 22, 'page_label': '23'}, page_content='9Artificial intelligence, machine learning, and deep learning\\n What do the representations learned by a deep-learning algorithm look like? Let’s\\nexamine how a network several layers deep (see figure 1.5) transforms an image of a\\ndigit in order to recognize what digit it is.\\nAs you can see in figure 1.6, the network transforms the digit image into representa-\\ntions that are increasingly different from the original image and increasingly informa-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 22, 'page_label': '23'}, page_content='tive about the final result. You can th ink of a deep network as a multistage\\ninformation-distillation operation, where in formation goes through successive filters\\nand comes out increasingly purified (that is, useful with regard to some task).\\nSo that’s what deep learning is, technically: a multistage way to learn data representa-\\ntions. It’s a simple idea—but, as it turn s out, very simple me chanisms, sufficiently\\nscaled, can end up looking like magic.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 22, 'page_label': '23'}, page_content='scaled, can end up looking like magic. \\n1.1.5 Understanding how deep lear ning works, in three figures\\nAt this point, you know that machine learning is about mapping inputs (such as\\nimages) to targets (such as the label “cat”), which is done by observing many examples\\nof input and targets. You also know that de ep neural networks do this input-to-target\\nLayer 1\\nOriginal\\ninput\\nFinal\\noutput\\nLayer 2 Layer 3 Layer 4\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nFigure 1.5 A deep neural \\nnetwork for digit classification'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 22, 'page_label': '23'}, page_content='network for digit classification\\nLayer 1\\nrepresentations\\nOriginal\\ninput\\nLayer 2\\nrepresentations\\nLayer 3\\nrepresentations\\nLayer 4\\nrepresentations\\n(final output)\\nLayer 1 Layer 2 Layer 3 Layer 4\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nFigure 1.6 Deep representations learned by a digit-classification model\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 23, 'page_label': '24'}, page_content='10 CHAPTER 1 What is deep learning?\\nmapping via a deep sequence of simple data  transformations (layers) and that these\\ndata transformations are learned by exposure to examples. Now let’s look at how this\\nlearning happens, concretely.\\n The specification of what a layer does to  its input data is stored in the layer’s\\nweights, which in essence are a bunch of numbers. In technical terms, we’d say that the\\ntransformation implemented by a layer is parameterized by its weights (see figure 1.7).'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 23, 'page_label': '24'}, page_content='(Weights are also sometimes called the parameters of a layer.) In this context, learning\\nmeans finding a set of values for the weights of all layers in a network, such that the\\nnetwork will correctly map example inputs to  their associated targets. But here’s the\\nthing: a deep neural networ k can contain tens of millions of parameters. Finding the\\ncorrect value for all of them may seem like a daunting task, especially given that mod-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 23, 'page_label': '24'}, page_content='ifying the value of one parameter will affect the behavior of all the others!\\nTo control something, first you need to be able to observe it. To control the output of\\na neural network, you need to be able to measure how far this output is from what you\\nexpected. This is the job of the loss function  of the network, also called the objective\\nfunction. The loss function takes the prediction s of the network and the true target'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 23, 'page_label': '24'}, page_content=\"(what you wanted the network to output) and computes a distance score, capturing\\nhow well the network has done on this specific example (see figure 1.8).\\nGoal: finding the\\nright values for\\nthese weights\\nLayer\\n(data transformation)\\nInput X\\nWeights\\nLayer\\n(data transformation)\\nPredictions\\nY'\\nWeights\\nFigure 1.7 A neural network is \\nparameterized by its weights.\\nLayer\\n(data transformation)\\nInput X\\nWeights\\nLayer\\n(data transformation)\\nPredictions\\nY'\\nTrue targets\\nY\\nWeights\\nLoss function\\nLoss score\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 23, 'page_label': '24'}, page_content='True targets\\nY\\nWeights\\nLoss function\\nLoss score\\nFigure 1.8 A loss function measures \\nthe quality of the network’s output.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 24, 'page_label': '25'}, page_content='11Artificial intelligence, machine learning, and deep learning\\nThe fundamental trick in deep learning is to use this score as a feedback signal to\\nadjust the value of the weight s a little, in a direction that will lower the loss score for\\nthe current example (see figure 1.9). This adjustment is the job of the optimizer, which\\nimplements what’s called the Backpropagation algorithm: the central algorithm in deep\\nlearning. The next chapter explains in more detail how backpropagation works.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 24, 'page_label': '25'}, page_content='Initially, the weights of the network are assigned random values, so the network\\nmerely implements a series of random tran sformations. Naturally, its output is far\\nfrom what it should ideally be, and the lo ss score is accordingly very high. But with\\nevery example the network processes, the weig hts are adjusted a little in the correct\\ndirection, and the loss score decreases. This is the training loop, which, repeated a suffi-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 24, 'page_label': '25'}, page_content='cient number of times (typically tens of it erations over thousands of examples), yields\\nweight values that minimize the loss function. A network with a minimal loss is one for\\nwhich the outputs are as close as they can be to the targets: a trained network. Once\\nagain, it’s a simple mechanism that, once scaled, ends up looking like magic. \\n1.1.6 What deep learning has achieved so far\\nAlthough deep learning is a fairly old subfield of machine learning, it only rose to'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 24, 'page_label': '25'}, page_content='prominence in the early 2010s. In the few years since, it has achieved nothing short of\\na revolution in the field, with remarkable results on perceptual problems such as see-\\ning and hearing—problems involving skills that seem natural and intuitive to humans\\nbut have long been elusive for machines.\\n In particular, deep learning has achieved the following breakthroughs, all in his-\\ntorically difficult areas of machine learning:\\n\\uf0a1 Near-human-level image classification'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 24, 'page_label': '25'}, page_content=\"\\uf0a1 Near-human-level image classification\\n\\uf0a1 Near-human-level speech recognition\\n\\uf0a1 Near-human-level handwriting transcription\\n\\uf0a1 Improved machine translation\\nLayer\\n(data transformation)\\nInput X\\nWeights\\nLayer\\n(data transformation)\\nPredictions\\nY'\\nWeight\\nupdate\\nTrue targets\\nY\\nWeights\\nLoss functionOptimizer\\nLoss score\\nFigure 1.9 The loss score is used as a \\nfeedback signal to adjust the weights.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 25, 'page_label': '26'}, page_content='12 CHAPTER 1 What is deep learning?\\n\\uf0a1 Improved text-to-speech conversion\\n\\uf0a1 Digital assistants such as Google Now and Amazon Alexa\\n\\uf0a1 Near-human-level autonomous driving\\n\\uf0a1 Improved ad targeting, as used by Google, Baidu, and Bing\\n\\uf0a1 Improved search results on the web\\n\\uf0a1 Ability to answer natural-language questions\\n\\uf0a1 Superhuman Go playing\\nWe’re still exploring the full extent of what deep learning can do. We’ve started apply-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 25, 'page_label': '26'}, page_content='ing it to a wide variety of problems outside of machine perception and natural-language\\nunderstanding, such as formal reasoning. If successful, this may herald an age where\\ndeep learning assists humans in science, software development, and more. \\n1.1.7 Don’t believe the short-term hype\\nAlthough deep learning has led to remarkab le achievements in recent years, expecta-\\ntions for what the field will be able to ac hieve in the next decade tend to run much'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 25, 'page_label': '26'}, page_content='higher than what will likely be possible. Although some world- changing applications\\nlike autonomous cars are already within reach, many more are likely to remain elusive\\nfor a long time, such as beli evable dialogue systems, human-level machine translation\\nacross arbitrary langua ges, and human-level natural-la nguage understanding. In par-\\nticular, talk of human-level general intelligence shouldn’t be taken too seriously. The risk'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 25, 'page_label': '26'}, page_content='with high expectations for the short term is that, as technology fails to deliver,\\nresearch investment will dry up, slowing progress for a long time.\\n This has happened before. Twice in the pa st, AI went through a cycle of intense\\noptimism followed by disappointment and skepticism, wi th a dearth of funding as a\\nresult. It started with symbolic \\nAI in the 1960s. In those early days, projections about AI\\nwere flying high. One of the best-known pi oneers and proponents of the symbolic AI'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 25, 'page_label': '26'}, page_content='approach was Marvin Minsky, who claimed in 1967, “Within a generation … the prob-\\nlem of creating ‘artificial intelligence’ will substantially be solved.” Three years later, in\\n1970, he made a more precisely quantified prediction: “In from three to eight years we\\nwill have a machine with the general intelligence of an average human being.” In 2016,\\nsuch an achievement still appears to be far in the future—so far that we have no way to'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 25, 'page_label': '26'}, page_content='predict how long it will take—but in the 1960s and early 1970s, several experts believed\\nit to be right around the corner (as do many  people today). A few years later, as these\\nhigh expectations failed to materialize,  researchers and gove rnment funds turned\\naway from the field, marking the start of the first \\nAI winter (a reference to a nuclear win-\\nter, because this was shortly after the height of the Cold War).'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 25, 'page_label': '26'}, page_content='It wouldn’t be the last one. In the 1980s, a new take on symbolic AI, expert systems,\\nstarted gathering steam among large companies. A few initial success stories triggered\\na wave of investment, with corporations around the world starting their own in-house\\nAI departments to develop ex pert systems. Around 1985,  companies were spending\\nover $1 billion each year on the technolo gy; but by the early 1990s, these systems had'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 25, 'page_label': '26'}, page_content='proven expensive to maintain, difficult to scale, and limited in scope, and interest\\ndied down. Thus began the second AI winter.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 26, 'page_label': '27'}, page_content='13Artificial intelligence, machine learning, and deep learning\\n We may be currently witnessing the third cycle of AI hype and disappointment—\\nand we’re still in the phase of intense optimism. It’s best to moderate our expectations\\nfor the short term and make sure people le ss familiar with the technical side of the\\nfield have a clear idea of what deep learning can and can’t deliver. \\n1.1.8 The promise of AI\\nAlthough we may have unrealistic short-term expectations for AI, the long-term pic-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 26, 'page_label': '27'}, page_content='ture is looking bright. We’re only getting started in applying deep learning to many\\nimportant problems for which it could prov e transformative, from medical diagnoses\\nto digital assistants. AI research has been moving forward amazingly quickly in the past\\nfive years, in large part due to a level of funding never before seen in the short history\\nof AI, but so far relatively little of this progress has made its way into the products and'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 26, 'page_label': '27'}, page_content='processes that form our world. Most of the research findings of deep learning aren’t\\nyet applied, or at least not applied to th e full range of problems they can solve across\\nall industries. Your doctor doesn’t yet use \\nAI, and neither does your accountant. You\\nprobably don’t use AI technologies in your day-to-day life. Of course, you can ask your\\nsmartphone simple questions and get reason able answers, you can get fairly useful'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 26, 'page_label': '27'}, page_content='product recommendations on Amazon.com, and you can search for “birthday” on\\nGoogle Photos and instantly find those pict ures of your daughter’s birthday party\\nfrom last month. That’s a far cry from wh ere such technologies used to stand. But\\nsuch tools are still only acce ssories to our daily lives. AI has yet to transition to being\\ncentral to the way we work, think, and live.\\n Right now, it may seem hard to believe that AI could have a large impact on our'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 26, 'page_label': '27'}, page_content='world, because it isn’t yet widely deployed—much as, back in 1995, it would have been\\ndifficult to believe in the future impact of the internet. Back then, most people didn’t\\nsee how the internet was relevant to them and how it was going to change their lives. The\\nsame is true for deep learning and AI today. But make no mistake: AI is coming. In a not-\\nso-distant future, AI will be your assistant, even your friend; it will answer your questions,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 26, 'page_label': '27'}, page_content='help educate your kids, and watch over your health. It will deliver your groceries to your\\ndoor and drive you from point A to point B. It will be your interface to an increasingly\\ncomplex and information-intensive world. And, even more important, AI will help\\nhumanity as a whole move forward, by assisting human scientists in new breakthrough\\ndiscoveries across all scientific fields, from genomics to mathematics.\\n On the way, we may face a few setbacks and maybe a new AI winter—in much the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 26, 'page_label': '27'}, page_content='same way the internet industry was overhyped in 1998–1999 and suffered from a crash\\nthat dried up investment throughout the early 2000s. But we’ll get there eventually. \\nAI\\nwill end up being applied to nearly every process that makes up our society and our\\ndaily lives, much like the internet is today.\\n Don’t believe the short-term hype, but do  believe in the long-term vision. It may\\ntake a while for AI to be deployed to its true pote ntial—a potential the full extent of'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 26, 'page_label': '27'}, page_content='which no one has yet dared to dream—but AI is coming, and it will transform our\\nworld in a fantastic way. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 27, 'page_label': '28'}, page_content='14 CHAPTER 1 What is deep learning?\\n1.2 Before deep learning: \\na brief history of machine learning\\nDeep learning has reached a level of pub lic attention and industry investment never\\nbefore seen in the history of AI, but it isn’t the first succe ssful form of machine learn-\\ning. It’s safe to say that most of the mach ine-learning algorithms used in the industry\\ntoday aren’t deep-learning algorithms. Deep learning isn’t always the right tool for the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 27, 'page_label': '28'}, page_content='job—sometimes there isn’t enough data for deep learning to be applicable, and some-\\ntimes the problem is better solved by a diff erent algorithm. If d eep learning is your\\nfirst contact with machine learning, then you may find yourself in a situation where all\\nyou have is the deep-learning hammer, and every machine-learning problem starts to\\nlook like a nail. The only way not to fall into this trap is to be familiar with other\\napproaches and practice them when appropriate.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 27, 'page_label': '28'}, page_content='approaches and practice them when appropriate.\\n A detailed discussion of classical machin e-learning approaches is outside of the\\nscope of this book, but we’ll briefly go over  them and describe the historical context\\nin which they were developed. This will allow us to place deep learning in the broader\\ncontext of machine learning and better un derstand where deep learning comes from\\nand why it matters.\\n1.2.1 Probabilistic modeling'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 27, 'page_label': '28'}, page_content='and why it matters.\\n1.2.1 Probabilistic modeling\\nProbabilistic modeling is the application of the principles  of statistics to data analysis. It\\nwas one of the earliest forms of machine learning, and it’s still widely used to this day.\\nOne of the best-known algorithms in this category is the Naive Bayes algorithm.\\n Naive Bayes is a type of machine-learning classifier based on applying Bayes’ theo-\\nrem while assuming that the fe atures in the input data are all independent (a strong,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 27, 'page_label': '28'}, page_content='or “naive” assumption, which is where the name comes from). This form of data analy-\\nsis predates computers and was applied by hand decade s before its first computer\\nimplementation (most likely dating back to the 1950s). Bayes’ theorem and the foun-\\ndations of statistics date back to the eighteenth century, and these are all you need to\\nstart using Naive Bayes classifiers.\\n A closely related model is the logistic regression (logreg for short) , which is some-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 27, 'page_label': '28'}, page_content='times considered to be the “hello world” of modern machine learning. Don’t be mis-\\nled by its name—logreg is a classification algorithm rather than a regression\\nalgorithm. Much like Naive Bayes, logreg pr edates computing by a long time, yet it’s\\nstill useful to this day, thanks to its simple and versatile nature. It’s often the first thing\\na data scientist will try on a dataset to get a feel for the classification task at hand. \\n1.2.2 Early neural networks'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 27, 'page_label': '28'}, page_content='1.2.2 Early neural networks\\nEarly iterations of neural networks have  been completely supplanted by the modern\\nvariants covered in these pages, but it’s helpful to be aware of how deep learning origi-\\nnated. Although the core ideas of neural networks were investigated in toy forms as early\\nas the 1950s, the approach took decades to get started. For a long time, the missing piece\\nwas an efficient way to trai n large neural networks. This changed in the mid-1980s,\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 28, 'page_label': '29'}, page_content='15Before deep learning: a brief history of machine learning\\nwhen  multiple people independently rediscovered the Backpropagation algorithm—\\na way to train chains of parametric oper ations using gradient-descent optimization\\n(later in the book, we’ll precisely define these concepts)—and st arted applying it to\\nneural networks.\\n T h e  f i r s t  s u c c e s s f u l  p r a c t i c a l  a p p l i c a t i o n  o f  n e u r a l  n e t s  c a m e  i n  1 9 8 9  f r o m  B e l l'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 28, 'page_label': '29'}, page_content='Labs, when Yann LeCun combined the earlier ideas of convolutional neural networks\\nand backpropagation, and a pplied them to the problem of classifying handwritten\\ndigits. The resulting network, dubbed LeNet, was used by the United States Postal Ser-\\nvice in the 1990s to automate the reading of ZIP codes on mail envelopes. \\n1.2.3 Kernel methods\\nAs neural networks started to gain some  respect among researchers in the 1990s,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 28, 'page_label': '29'}, page_content='thanks to this first success, a new approa ch to machine learning rose to fame and\\nquickly sent neural nets back to oblivion: kernel methods. Kernel methods are a group of\\nclassification algorithms, the best known of which is the support vector machine  (SVM).\\nThe modern formulation of an SVM was developed by Vladimir\\nVapnik and Corinna Cortes in the early 1990s at Bell Labs and\\npublished in 1995, 2 although an older li near formulation was'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 28, 'page_label': '29'}, page_content='published by Vapnik and Alexey Chervonenkis as early as 1963.3\\n SVMs aim at solving classificati on problems by finding good\\ndecision boundaries (see figure 1.10) between two sets of points\\nbelonging to two different categories. A decision boundary can\\nbe thought of as a line or surf ace separating your training data\\ninto two spaces corresponding to two categories. To classify new\\ndata points, you just need to check which side of the decision\\nboundary they fall on.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 28, 'page_label': '29'}, page_content='boundary they fall on.\\n \\nSVMs proceed to find these boundaries in two steps:\\n1 The data is mapped to a new high-dimensional representation where the\\ndecision boundary can be expressed as a hyperplane (if the data was two-\\ndimensional, as in figure 1.10, a hyperplane would be a straight line).\\n2 A good decision boundary (a separation hyperplane) is com puted by trying to\\nmaximize the distance between the hyperplane and the closest data points from'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 28, 'page_label': '29'}, page_content='each class, a step called maximizing the margin. This allows the boundary to gen-\\neralize well to new samples outside of the training dataset.\\nThe technique of mapping data to a high-dimensional representation where a classifi-\\ncation problem becomes simpler may look good on paper, but in practice it’s\\noften computationally intractable. That’s where the kernel trick comes in (the key idea'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 28, 'page_label': '29'}, page_content='that kernel methods are named after). Here’s  t h e  g i s t  o f  i t :  t o  f i n d  g o o d  d e c i s i o n\\n2 Vladimir Vapnik and Corinna Cortes, “Support-Vector Networks,” Machine Learning 20, no. 3 (1995): 273–297.\\n3 Vladimir Vapnik and Alexey Chervonenkis, “A Note on One Class of Perceptrons,” Automation and Remote Con-\\ntrol 25 (1964).\\nFigure 1.10\\nA decision boundary\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 29, 'page_label': '30'}, page_content='16 CHAPTER 1 What is deep learning?\\nhyperplanes in the ne w representation space, you don’ t have to explicitly compute\\nthe coordinates of your points in the new space; you just need to compute the dis-\\ntance between pairs of points in that spac e, which can be done efficiently using a ker-\\nnel function. A kernel function is a computationa lly tractable operation that maps any\\ntwo points in your initial space to the di stance between these points in your target'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 29, 'page_label': '30'}, page_content='representation space, completely bypassing  the explicit computation of the new rep-\\nresentation. Kernel functions are typically crafted by hand rather than learned from\\ndata—in the case of an SVM, only the separation hyperplane is learned.\\n At the time they were developed, SVMs exhibited state-of-the-art performance on\\nsimple classification problems and were one of the few machine-learning methods\\nbacked by extensive theory and amenable to serious mathematical analysis, making'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 29, 'page_label': '30'}, page_content='them well understood and easily interpreta ble. Because of these useful properties,\\nSVMs became extremely popular in the field for a long time.\\n But SVMs proved hard to scale to large datasets and didn’t provide good results for\\nperceptual problems such as im age classification. Because an SVM is a shallow\\nmethod, applying an SVM to perceptual problems requires first extracting useful rep-\\nresentations manually (a step called feature engineering), which is difficult and brittle.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 29, 'page_label': '30'}, page_content='1.2.4 Decision trees, random forests, and gradient boosting machines\\nDecision trees are flowchart-like structur es that let you classify input data points or pre-\\ndict output values given inputs (see figure 1.11). They’re easy to visualize and inter-\\npret. Decisions trees learned from data began to receive significant research interest\\nin the 2000s, and by 2010 they were often preferred to kernel methods.\\nIn particular, the Random Forest  algorithm introduced a robust, practical take on'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 29, 'page_label': '30'}, page_content='decision-tree learning that involves buildi ng a large number of specialized decision\\ntrees and then ensembling their outputs. Random forests are a pplicable to a wide\\nrange of problems—you could say that they’re almost always the second-best algorithm\\nfor any shallow machine-learning task. When the popular machine-learning competi-\\ntion website Kaggle ( http:/ /kaggle.com) got started in 2010, random forests quickly'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 29, 'page_label': '30'}, page_content='became a favorite on the platform—until 2014, when gradient boosting machines  took\\nover. A gradient boosting machine, much li ke a random forest, is a machine-learning\\ntechnique based on ensembling weak predic tion models, generally decision trees. It\\nQuestion\\nCategory Category\\nQuestion\\nInput data\\nQuestion\\nCategory Category\\nFigure 1.11 A decision tree: the parameters \\nthat are learned are the questions about the \\ndata. A question could be, for instance, “Is'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 29, 'page_label': '30'}, page_content='data. A question could be, for instance, “Is \\ncoefficient 2 in the data greater than 3.5?”\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 30, 'page_label': '31'}, page_content='17Before deep learning: a brief history of machine learning\\nuses gradient boosting, a way to improve any machine-learning model by iteratively train-\\ning new models that specialize  in addressing the weak points of the previous models.\\nApplied to decision trees, the use of the gradient boosting technique results in models\\nthat strictly outperform random forests most of the time, while having similar proper-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 30, 'page_label': '31'}, page_content='ties. It may be one of the best, if not the best, algorithm for dealing with nonperceptual\\ndata today. Alongside deep learning, it’s one of the most commonly used techniques in\\nKaggle competitions. \\n1.2.5 Back to neural networks\\nAround 2010, although neural networks we re almost completely shunned by the sci-\\ne n t i f i c  c o m m u n i t y  a t  l a r g e ,  a  n u m b e r  o f  p e o p l e  s t i l l  w o r k i n g  o n  n e u r a l  n e t w o r k s'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 30, 'page_label': '31'}, page_content='started to make important breakthroughs: th e groups of Geoffrey Hinton at the Uni-\\nversity of Toronto, Yoshua Bengio at the University of Montreal, Yann LeCun at New\\nYork University, and IDSIA in Switzerland.\\n In 2011, Dan Ciresan from IDSIA began to win academic image-classification com-\\npetitions with GPU-trained deep neural networks—the first practical success of mod-\\nern deep learning. But the watershed mo ment came in 2012, with the entry of'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 30, 'page_label': '31'}, page_content='Hinton’s group in the yearly large-scale im age-classification challenge ImageNet. The\\nImageNet challenge was notoriously difficult at the time, consisting of classifying high-\\nresolution color images into 1,000 different categories after training on 1.4 million\\nimages. In 2011, the top-five accuracy of  the winning model, based on classical\\napproaches to computer vision, was only 74.3%. Then, in 2012, a team led by Alex'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 30, 'page_label': '31'}, page_content='Krizhevsky and advised by Geo ffrey Hinton was able to ac hieve a top-five accuracy of\\n83.6%—a significant breakthrough. The co mpetition has been dominated by deep\\nconvolutional neural networks every year since. By 2015, the winner reached an accu-\\nracy of 96.4%, and the classification task  on ImageNet was cons idered to be a com-\\npletely solved problem.\\n Since 2012, deep convolut ional neural networks (convnets) have become the go-to'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 30, 'page_label': '31'}, page_content='algorithm for all computer vision tasks; mo re generally, they work on all perceptual\\ntasks. At major computer vi sion conferences in 2015 and 2016, it was nearly impossi-\\nble to find presentations that didn’t involve convnets in some form. At the same time,\\ndeep learning has also found applications in many other types of problems, such as\\nnatural-language processing. It has completely replaced SVMs and decision trees in a'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 30, 'page_label': '31'}, page_content='wide range of applications. For instance, for several years, the European Organization\\nfor Nuclear Research, CERN, used decision tree–based methods for analysis of particle\\ndata from the ATLAS detector at the Large Hadron Collider ( LHC); but CERN eventu-\\nally switched to Keras-base d deep neural networks due to their higher performance\\nand ease of training on large datasets. \\n1.2.6 What makes deep learning different'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 30, 'page_label': '31'}, page_content='1.2.6 What makes deep learning different\\nThe primary reason deep learning took off so  quickly is that it offered better perfor-\\nmance on many problems. But that’s not th e only reason. Deep learning also makes\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 31, 'page_label': '32'}, page_content='18 CHAPTER 1 What is deep learning?\\nproblem-solving much easier, because it completely automates what used to be the\\nmost crucial step in a machine-learning workflow: feature engineering.\\n Previous machine-learning  techniques—shallow lear ning—only involved trans-\\nforming the input data into one or two succ essive representation spaces, usually via\\nsimple transformations such as high -dimensional non-linear projections ( SVMs) or'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 31, 'page_label': '32'}, page_content='decision trees. But the refined representa tions required by co mplex problems gener-\\nally can’t be attained by such techniques. As such, humans had to go to great lengths\\nto make the initial input data more amenab le to processing by these methods: they\\nhad to manually engineer good layers of representations for their data. This is called\\nfeature engineering. Deep learning, on the other hand , completely automates this step:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 31, 'page_label': '32'}, page_content='with deep learning, you learn all features in one pass rather than having to engineer\\nthem yourself. This has greatly simplified machine-learning workflows, often replac-\\ning sophisticated multistage pipelines with a single, simple, end-to-end deep-learning\\nmodel.\\n You may ask, if the crux of the issue is to have multiple successive layers of repre-\\nsentations, could shallow methods be appl ied repeatedly to emulate the effects of'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 31, 'page_label': '32'}, page_content='deep learning? In practice, there are fast-diminishing re turns to successive applica-\\ntions of shallow-learning methods, because the optimal first representation layer in a three-\\nlayer model isn’t the optimal first lay er in a one-layer or two-layer model . What is transforma-\\ntive about deep learning is th at it allows a model to learn all layers of representation\\njointly, at the same time, rather than in succession ( greedily, as it’s called). With joint'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 31, 'page_label': '32'}, page_content='feature learning, whenever the model adjusts one of its internal features, all other fea-\\ntures that depend on it automatically ad apt to the change, without requiring human\\nintervention. Everything is supervised by a single feedback signal: every change in the\\nmodel serves the end goal. This is much more powerful than greedily stacking shallow\\nmodels, because it allows for complex, ab stract representations to be learned by'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 31, 'page_label': '32'}, page_content='b r e a k i n g  t h e m  d o w n  i n t o  l o n g  s e r i e s  o f  intermediate spaces (layers); each space is\\nonly a simple transformation away from the previous one.\\n These are the two essential characteristics of how deep learning learns from data:\\nthe incremental, layer-by-layer way in which incr easingly complex representations are developed ,\\nand the fact that these intermediate incremental representations are learned jointly, each layer'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 31, 'page_label': '32'}, page_content='being updated to follow both the representational needs of the layer above and the\\nneeds of the layer below. Together, these two properties have made deep learning\\nvastly more successful than previous approaches to machine learning. \\n1.2.7 The modern machine-learning landscape\\nA great way to get a sense of the current landscape of machine-learning algorithms\\nand tools is to look at machine-learning competitions on Kaggle. Due to its highly'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 31, 'page_label': '32'}, page_content='competitive environment (some contests ha ve thousands of entrants and million-\\ndollar prizes) and to the wide variety of  machine-learning problems covered, Kaggle\\noffers a realistic way to assess what works and what doesn’t. So, what kind of algorithm\\nis reliably winning competitions? What tools do top entrants use?\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 32, 'page_label': '33'}, page_content='19Before deep learning: a brief history of machine learning\\n In 2016 and 2017, Kaggle was dominate d by two approaches: gradient boosting\\nmachines and deep learning . Specifically, gradient boosting is used for problems\\nwhere structured data is available, whereas deep learning is used for perceptual prob-\\nlems such as image classifica tion. Practitioners of the fo rmer almost always use the\\nexcellent XGBoost library, which offers support for the two most popular languages of'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 32, 'page_label': '33'}, page_content='data science: Python and R. Meanwhile, most of the Kaggle entrants using deep learn-\\ning use the Keras library, due to its ease of use, flexibility, and support of Python.\\n These are the two techniques you should be  the most familiar with in order to be\\nsuccessful in applied machine learning today: gradient boosting machines, for shallow-\\nlearning problems; and deep learning, for perceptual pr oblems. In technical terms,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 32, 'page_label': '33'}, page_content='this means you’ll need to be familiar with XGBoost and Keras—the two libraries that\\ncurrently dominate Kaggle competitions. With  this book in hand, you’re already one\\nbig step closer. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 33, 'page_label': '34'}, page_content='20 CHAPTER 1 What is deep learning?\\n1.3 Why deep learning? Why now?\\nThe two key ideas of deep learning for co mputer vision—convolutional neural net-\\nworks and backpropagation—were already well understood in 1989. The Long Short-\\nTerm Memory ( LSTM) algorithm, which is fundamental to deep learning for\\ntimeseries, was developed in 1997 and ha s barely changed since. So why did deep\\nlearning only take off after 2012? What changed in these two decades?'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 33, 'page_label': '34'}, page_content='In general, three technical forces are driving advances in machine learning:\\n\\uf0a1 Hardware\\n\\uf0a1 Datasets and benchmarks\\n\\uf0a1 Algorithmic advances\\nBecause the field is guided by experimental findings rather than by theory, algorith-\\nmic advances only become possible when appropriate data and hardware are available\\nto try new ideas (or scale up old ideas, as is often the case). Machine learning isn’t\\nmathematics or physics, where major advances can be done with a pen and a piece of'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 33, 'page_label': '34'}, page_content='paper. It’s an engineering science.\\n The real bottlenecks throughout the 1990s and 2000s were data and hardware. But\\nhere’s what happened during  that time: the internet to ok off, and high-performance\\ngraphics chips were developed for the needs of the gaming market.\\n1.3.1 Hardware\\nBetween 1990 and 2010, off-the-shelf CPUs became faster by a factor of approximately\\n5,000. As a result, nowadays it’s possible to run small deep-learning models on your'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 33, 'page_label': '34'}, page_content='laptop, whereas this would have been intractable 25 years ago.\\n But typical deep-learning models used in  computer vision or speech recognition\\nrequire orders of magnitude more computational power th an what your laptop can\\ndeliver. Throughout the 2000s, companies like NVIDIA and AMD have been investing\\nbillions of dollars in developing fast, massively parallel chips (graphical processing\\nunits [\\nGPUs]) to power the graphics of incr easingly photorealistic video games—'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 33, 'page_label': '34'}, page_content='cheap, single-purpose supercomputers designed to render complex 3D scenes on your\\nscreen in real time. This investment came  to benefit the scientific community when,\\nin 2007, NVIDIA launched CUDA (https:/ /developer.nvidia.com/about-cuda), a pro-\\ngramming interface for its line of GPUs. A small number of GPUs started replacing\\nmassive clusters of CPUs in various highly parallelizable applications, beginning with'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 33, 'page_label': '34'}, page_content='physics modeling. Deep neural networks, consisting mostly of many small matrix mul-\\ntiplications, are also highly parallelizable;  and around 2011, so me researchers began\\nto write CUDA implementations of neural nets—Dan Ciresan 4 and Alex Krizhevsky 5\\nwere among the first.\\n4 See “Flexible, High Performance Convolutional Neural Networks for Image Classification,” Proceedings of the\\n22nd International Joint Conference on Artificial Intelligence  (2011), www.ijcai.org/Proceedings/11/Papers/'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 33, 'page_label': '34'}, page_content='210.pdf.\\n5 See “ImageNet Classification with Deep Convolutional Neural Networks,” Advances in Neural Information Pro-\\ncessing Systems 25 (2012), http://mng.bz/2286.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 34, 'page_label': '35'}, page_content='21Why deep learning? Why now?\\n What happened is that the gaming market subsidized supercomputing for the next\\ngeneration of artificial intelligence ap plications. Sometimes, big things begin as\\ngames. Today, the NVIDIA TITAN X, a gaming GPU that cost $1,000 at the end of 2015,\\ncan deliver a peak of 6.6 TFLOPS in single precision: 6.6 trillion float32 operations\\nper second. That’s about 350 times more th an what you can get out of a modern lap-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 34, 'page_label': '35'}, page_content='top. On a TITAN X, it takes only a couple of days to train an ImageNet model of the\\nsort that would have won the ILSVRC competition a few years ago. Meanwhile, large\\ncompanies train deep-learning models on clusters of hundreds of GPUs of a type\\ndeveloped specifically for the need s of deep learning, such as the NVIDIA Tesla K80 .\\nThe sheer computational power of such clus ters is something that would never have\\nbeen possible without modern GPUs.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 34, 'page_label': '35'}, page_content='been possible without modern GPUs.\\n What’s more, the deep-learning industry is starting to go beyond GPUs and is\\ninvesting in increasingly specialized, effici ent chips for deep learning. In 2016, at its\\nannual I/O convention, Google revealed its tensor processing unit ( TPU) project: a\\nnew chip design developed from the ground up to run deep neural networks, which is\\nreportedly 10 times faster and far more energy efficient than top-of-the-line GPUs. \\n1.3.2 Data'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 34, 'page_label': '35'}, page_content='1.3.2 Data\\nAI is sometimes heralded as the new industrial revolution. If deep learning is the steam\\nengine of this revolution, then data is its coal: the raw material that powers our intelli-\\ngent machines, without which nothing would be possible. When it comes to data, in\\naddition to the exponential progress in stor age hardware over the past 20 years (fol-\\nlowing Moore’s law), the game changer has been the rise of the internet, making it fea-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 34, 'page_label': '35'}, page_content='sible to collect and distribute  very large datasets for machine learning. Today, large\\ncompanies work with image datasets, video datasets, and natural-language datasets that\\ncouldn’t have been collected without th e internet. User-generated image tags on\\nFlickr, for instance, have been a treasure trove of data for computer vision. So are You-\\nTube videos. And Wikipedia is a key dataset for natural-language processing.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 34, 'page_label': '35'}, page_content='If there’s one dataset that has been a cataly st for the rise of deep learning, it’s the\\nImageNet dataset, consisting of 1.4 millio n images that have been hand annotated\\nwith 1,000 image categories (1 category per image). But what makes ImageNet special\\nisn’t just its large size, but also the yearly competition associated with it.6\\n As Kaggle has been demonstrating sinc e 2010, public competitions are an excel-\\nlent way to motivate researchers and engineers to push the envelope. Having common'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 34, 'page_label': '35'}, page_content='benchmarks that researchers compete to be at has greatly helped the recent rise of\\ndeep learning. \\n1.3.3 Algorithms\\nIn addition to hardware and data, until the late 2000s, we were missing a reliable way to\\ntrain very deep neural networks. As a result , neural networks were still fairly shallow,\\n6 The ImageNet Large Scale Visual Recognition Challenge (ILSVRC), www.image-net.org/challenges/LSVRC.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 35, 'page_label': '36'}, page_content='22 CHAPTER 1 What is deep learning?\\nusing only one or two layers of representations; thus, they weren’t able to shine against\\nmore-refined shallow methods such as SVMs and random forests. The key issue was that\\nof gradient propagation through deep stacks of layers. The feedback signal used to train\\nneural networks would fade away as the number of layers increased.\\n This changed around 2009–2010 with the advent of several simple but important'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 35, 'page_label': '36'}, page_content='algorithmic improvements that allowed for better gradient propagation:\\n\\uf0a1 Better activation functions for neural layers\\n\\uf0a1 Better weight-initialization schemes, starting with layer-wise pretraining, which was\\nquickly abandoned\\n\\uf0a1 Better optimization schemes, such as RMSProp and Adam\\nOnly when these improvements began to al low for training models with 10 or more\\nlayers did deep learning start to shine.\\n Finally, in 2014, 2015, and 2016, even more advanced ways to help gradient propa-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 35, 'page_label': '36'}, page_content='gation were discovered, such as batch normalization, residual connections, and depth-\\nwise separable convolutions . Today we can train from scratch models that are\\nthousands of layers deep. \\n1.3.4 A new wave of investment\\nAs deep learning became the new state of  the art for computer vision in 2012–2013,\\nand eventually for all perceptual tasks, in dustry leaders took note. What followed was\\na gradual wave of industry investment far be yond anything previously seen in the his-\\ntory of AI.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 35, 'page_label': '36'}, page_content='tory of AI.\\n In 2011, right before deep learning took  the spotlight, the total venture capital\\ninvestment in AI was around $19 million, which went almost entirely to practical appli-\\ncations of shallow machine-le arning approaches. By 2014, it had risen to a staggering\\n$394 million. Dozens of startups launched in these three years, trying to capitalize on\\nthe deep-learning hype. Mean while, large tech companies such as Google, Facebook,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 35, 'page_label': '36'}, page_content='Baidu, and Microsoft have invested in inte rnal research departments in amounts that\\nwould most likely dwarf the flow of vent ure-capital money. Only a few numbers have\\nsurfaced: In 2013, Google acquired th e deep-learning startup DeepMind for a\\nreported $500 million—the largest acquisition of an AI company in history. In 2014,\\nBaidu started a deep-learning research center in Silicon Valley, investing $300 million'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 35, 'page_label': '36'}, page_content='in the project. The deep-learning hardware  startup Nervana Syst ems was acquired by\\nIntel in 2016 for over $400 million.\\n Machine learning—in particular, deep learning—has become central to the prod-\\nuct strategy of these tech gi ants. In late 2015, Google \\nCEO Sundar Pichai stated,\\n“Machine learning is a core, transformative  way by which we’re rethinking how we’re\\ndoing everything. We’re thoughtfully applying it across all our products, be it search,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 35, 'page_label': '36'}, page_content='ads, YouTube, or Play. And we’re in early days, but you’ll see us—in a systematic way—\\napply machine learning in all these areas.”7\\n7 Sundar Pichai, Alphabet earnings call, Oct. 22, 2015.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 36, 'page_label': '37'}, page_content='23Why deep learning? Why now?\\n A s  a  r e s u l t  o f  t h i s  w a v e  o f  i n v e s t m e n t ,  t h e  n u m b e r  o f  p e o p l e  w o r k i n g  o n  d e e p\\nlearning went in just five years from a few hundred to tens of thousands, and research\\nprogress has reached a frenetic pace. There ar e currently no signs that this trend will\\nslow any time soon. \\n1.3.5 The democratization of deep learning\\nOne of the key factors driving this inflow of  new faces in deep learning has been the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 36, 'page_label': '37'}, page_content='democratization of the toolsets used in the field. In the early days, doing deep learning\\nrequired significant C++ and CUDA expertise, which few peop le possessed. Nowadays,\\nbasic Python scripting skills suffice to do advanced deep-learning research. This has been\\ndriven most notably by the development of Theano and then TensorFlow—two symbolic\\ntensor-manipulation frameworks for Python that support autodifferentiation, greatly sim-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 36, 'page_label': '37'}, page_content='plifying the implementation of new models—a nd by the rise of user-friendly libraries\\nsuch as Keras, which makes deep learning as easy as manipulating LEGO bricks. After its\\nrelease in early 2015, Keras quickly became the go-to deep-learning solution for large\\nnumbers of new startups, graduate students, and researchers pivoting into the field. \\n1.3.6 Will it last?\\nIs there anything special abou t deep neural networks that makes them the “right”'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 36, 'page_label': '37'}, page_content='approach for companies to be investing in and for researchers to flock to? Or is deep\\nlearning just a fad that may not last? Will we still be using deep neural networks in\\n20 years?\\n Deep learning has several pr operties that justify its status as an AI revolution, and\\nit’s here to stay. We may not be using neural networks two decades from now, but what-\\never we use will directly inherit from mo dern deep learning and its core concepts.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 36, 'page_label': '37'}, page_content='These important properties can be broadly sorted into three categories:\\n\\uf0a1 Simplicity—Deep learning removes the need fo r feature engineering, replacing\\ncomplex, brittle, engineering-heavy pipe lines with simple, end-to-end trainable\\nmodels that are typically built using only five or six different tensor operations.\\n\\uf0a1 Scalability—Deep learning is highly amenable to parallelization on GPUs or\\nTPUs, so it can take full advantage of Moore’s law. In addition, deep-learning'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 36, 'page_label': '37'}, page_content='models are trained by iterating over small  batches of data, allowing them to be\\ntrained on datasets of arbitrary size. (The only bottleneck is the amount of\\nparallel computational power available, which, thanks to  Moore’s law, is a fast-\\nmoving barrier.)\\n\\uf0a1 Versatility and reusability —Unlike many prior machine-learning approaches,\\ndeep-learning models can be trained on additional data without restarting from\\nscratch, making them viable for cont inuous online learning—an important'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 36, 'page_label': '37'}, page_content='property for very large production models. Furthermore, trained deep-learning\\nmodels are repurposable and thus reusable : for instance, it’s possible to take a\\ndeep-learning model trained for image cla ssification and drop it into a video-\\nprocessing pipeline. This allows us to reinvest previous work into increasingly\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 37, 'page_label': '38'}, page_content='24 CHAPTER 1 What is deep learning?\\ncomplex and powerful mode ls. This also makes deep learning applicable to\\nfairly small datasets.\\nDeep learning has only been in the spotlight for a few years, and we haven’t yet estab-\\nlished the full scope of what it can do. Wi th every passing month, we learn about new\\nuse cases and engineering improvements that lift previous limitations. Following a sci-\\nentific revolution, progress generally follows a sigmoid curve: it starts with a period of'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 37, 'page_label': '38'}, page_content='fast progress, which graduall y stabilizes as researchers hi t hard limitations, and then\\nfurther improvements become incremental. Deep learning in 2017 seems to be in the\\nfirst half of that sigmoid, with much more progress to come in the next few years. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 38, 'page_label': '39'}, page_content='Before we begin: the\\nmathematical building\\nblocks of neural networks\\nUnderstanding deep learning requires familiarity with many simple mathematical\\nconcepts: tensors, tensor operations, differ entiation, gradient descent, and so on.\\nOur goal in this chapter will be to build your intuition about these notions without\\ngetting overly technical. In particular, we’ll steer away from mathematical notation,\\nwhich can be off-putting for those without any mathematics background and isn’t'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 38, 'page_label': '39'}, page_content='strictly necessary to explain things well.\\n To add some context for tensors and gr adient descent, we’ll begin the chapter\\nwith a practical example of a neural network. Then we’ll go over every new concept\\nThis chapter covers\\n\\uf0a1 A first example of a neural network\\n\\uf0a1 Tensors and tensor operations\\n\\uf0a1 How neural networks learn via backpropagation \\nand gradient descent'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 39, 'page_label': '40'}, page_content='26 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nthat’s been introduced, point by point. Keep in mind that these concepts will be essen-\\ntial for you to understand the practical ex amples that will come in the following\\nchapters!\\n After reading this chapter, you’ll have  an intuitive understanding of how neural\\nnetworks work, and you’ll be able to move  on to practical ap plications—which will\\nstart with chapter 3.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 40, 'page_label': '41'}, page_content='27A first look at a neural network\\n2.1 A first look at a neural network\\nLet’s look at a concrete example of a neural network that uses the Python library Keras\\nto learn to classify handwritten digits. Un less you already have ex perience with Keras\\nor similar libraries, you won’t understand everything about this first example right\\naway. You probably haven’t even installed Ke ras yet; that’s fine. In the next chapter,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 40, 'page_label': '41'}, page_content='we’ll review each element in the example and explain them in detail. So don’t worry if\\nsome steps seem arbitrary or look like magic to you! We’ve got to start somewhere.\\n The problem we’re trying to solve here is to classify grayscale images of handwrit-\\nten digits (28 × 28 pixels) into their 10 categories (0 through 9). We’ll use the MNIST\\ndataset, a classic in the machine-learning  community, which has been around almost'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 40, 'page_label': '41'}, page_content='as long as the field itself and has been inte nsively studied. It’s a set of 60,000 training\\nimages, plus 10,000 test images, assembled by  the National Institute of Standards and\\nTechnology (the NIST in MNIST) in the 1980s. You can think of “solving” MNIST as the\\n“Hello World” of deep learning—it’s what you do to verify that your algorithms are\\nworking as expected. As you become a ma chine-learning practitioner, you’ll see'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 40, 'page_label': '41'}, page_content='MNIST come up over and over again, in scientific papers, blog posts, and so on. You\\ncan see some MNIST samples in figure 2.1.\\nYou don’t need to try to reproduce this example on your machine just now. If you wish\\nto, you’ll first need to set up Keras, which is covered in section 3.3.\\n The MNIST dataset comes preloaded in Keras, in  the form of a set of four Numpy\\narrays.\\nfrom keras.datasets import mnist\\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 40, 'page_label': '41'}, page_content='train_images and train_labels form the training set , the data that the model will\\nlearn from. The model will then be tested on the test set, test_images and test_labels.\\nListing 2.1 Loading the MNIST dataset in Keras\\nNote on classes and labels\\nIn machine learning, a category in a classification problem is called a class. Data\\npoints are called samples. The class associated with a specific sample is called a\\nlabel.\\nFigure 2.1 MNIST sample digits\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 41, 'page_label': '42'}, page_content='28 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nThe images are encoded as Numpy arrays, and the labels are an array of digits, ranging\\nfrom 0 to 9. The images and labels have a one-to-one correspondence.\\n Let’s look at the training data:\\n>>> train_images.shape\\n(60000, 28, 28)\\n>>> len(train_labels)\\n60000\\n>>> train_labels\\narray([5, 0, 4, ..., 5, 6, 8], dtype=uint8)\\nAnd here’s the test data:\\n>>> test_images.shape\\n(10000, 28, 28)\\n>>> len(test_labels)\\n10000'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 41, 'page_label': '42'}, page_content='(10000, 28, 28)\\n>>> len(test_labels)\\n10000\\n>>> test_labels\\narray([7, 2, 1, ..., 4, 5, 6], dtype=uint8)\\nThe workflow will be as follow s: First, we’ll feed the neur al network the training data,\\ntrain_images and train_labels. The network will then learn to associate images and\\nlabels. Finally, we’ll ask the network to produce predictions for test_images, and we’ll\\nverify whether these predictions match the labels from test_labels.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 41, 'page_label': '42'}, page_content=\"Let’s build the network—again, remember that you aren’t expected to understand\\neverything about this example yet.\\nfrom keras import models\\nfrom keras import layers\\nnetwork = models.Sequential()\\nnetwork.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\\nnetwork.add(layers.Dense(10, activation='softmax'))\\nThe core building block of neural networks is the layer, a data-processing module that\\nyou can think of as a filter for data. Some data goes in, and it comes out in a more use-\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 41, 'page_label': '42'}, page_content='ful form. Specifically, layers extract representations out of the data fed into them—hope-\\nfully, representations that are more mean ingful for the problem at hand. Most of\\ndeep learning consists of chaining together  simple layers that will implement a form\\nof progressive data distillation. A deep-learning model is like a sieve for data process-\\ning, made of a succession of increasingly refined data filters—the layers.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 41, 'page_label': '42'}, page_content='Here, our network consists of a sequence of two Dense layers, which are densely\\nconnected (also called fully connected) neural layers. The second (and last) layer is a\\n10-way softmax layer, which means it will return an array of 10 probability scores (sum-\\nming to 1). Each score will be the probabi lity that the current digit image belongs to\\none of our 10 digit classes.\\nListing 2.2 The network architecture\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 42, 'page_label': '43'}, page_content='29A first look at a neural network\\n To make the network ready for training, we need to pick three more things, as part\\nof the compilation step:\\n\\uf0a1 A loss function —How the network will be able to measure its performance on\\nthe training data, and thus how it will be  able to steer itself in the right direc-\\ntion.\\n\\uf0a1 An optimizer —The mechanism through which the network will update itself\\nbased on the data it sees and its loss function.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 42, 'page_label': '43'}, page_content=\"based on the data it sees and its loss function.\\n\\uf0a1 Metrics to monitor during training and testing —Here, we’ll only care about accu-\\nracy (the fraction of the images that were correctly classified).\\nThe exact purpose of the loss function and the optimizer will be made clear through-\\nout the next two chapters.\\nnetwork.compile(optimizer='rmsprop',\\nloss='categorical_crossentropy',\\nmetrics=['accuracy'])\\nBefore training, we’ll preprocess the data by  reshaping it into the shape the network\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 42, 'page_label': '43'}, page_content=\"expects and scaling it so that all values are in the [0, 1] interval. Previously, our train-\\ning images, for instance, were stored in an array of shape (60000, 28, 28) of type\\nuint8 with values in the [0, 255] interval. We transform it into a float32 array of\\nshape (60000, 28 * 28) with values between 0 and 1.\\ntrain_images = train_images.reshape((60000, 28 * 28))\\ntrain_images = train_images.astype('float32') / 255\\ntest_images = test_images.reshape((10000, 28 * 28))\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 42, 'page_label': '43'}, page_content=\"test_images = test_images.astype('float32') / 255\\nWe also need to categorically encode the labels, a step that’s explained in chapter 3.\\nfrom keras.utils import to_categorical\\ntrain_labels = to_categorical(train_labels)\\ntest_labels = to_categorical(test_labels)\\nWe’re now ready to train the network, which in Keras is done via a call to the net-\\nwork’s fit method—we fit the model to its training data:\\n>>> network.fit(train_images, train_labels, epochs=5, batch_size=128)\\nEpoch 1/5\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 42, 'page_label': '43'}, page_content='Epoch 1/5\\n60000/60000 [==============================] - 9s - loss: 0.2524 - acc: 0.9273\\nEpoch 2/5\\n51328/60000 [========================>.....] - ETA: 1s - loss: 0.1035 - acc: 0.9692\\nListing 2.3 The compilation step\\nListing 2.4 Preparing the image data\\nListing 2.5 Preparing the labels\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 43, 'page_label': '44'}, page_content=\"30 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nTwo quantities are displayed during training: the loss of the network over the training\\ndata, and the accuracy of the network over the training data.\\n We quickly reach an accuracy of 0.989 (98.9%) on the training data. Now let’s\\ncheck that the model performs well on the test set, too:\\n>>> test_loss, test_acc = network.evaluate(test_images, test_labels)\\n>>> print('test_acc:', test_acc)\\ntest_acc: 0.9785\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 43, 'page_label': '44'}, page_content=\">>> print('test_acc:', test_acc)\\ntest_acc: 0.9785\\nThe test-set accuracy turns out to be 97.8% —that’s quite a bit lower than the training\\nset accuracy. This gap between  training accuracy and test accuracy is an example of\\noverfitting: the fact that machine-learning mode ls tend to perform worse on new data\\nthan on their training data. Overfitting is a central topic in chapter 3.\\n This concludes our first example—you just saw how you can build and train a neu-\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 43, 'page_label': '44'}, page_content='ral network to classify handwritten digits in  less than 20 lines of Python code. In the\\nnext chapter, I’ll go into detail about every moving piece we just previewed and clarify\\nwhat’s going on behind the scenes. You’ll learn about tensors, the data-storing objects\\ngoing into the network; tensor operations , which layers are made of; and gradient\\ndescent, which allows your network to learn from its training examples. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 44, 'page_label': '45'}, page_content='31Data representations for neural networks\\n2.2 Data representations for neural networks\\nIn the previous example, we started from  data stored in multidimensional Numpy\\narrays, also called tensors. In general, all current machin e-learning systems use tensors\\nas their basic data structur e. Tensors are fundamental to  the field—so fundamental\\nthat Google’s TensorFlow was named after them. So what’s a tensor?\\n At its core, a tensor is a container for data—almost always numerical data. So, it’s a'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 44, 'page_label': '45'}, page_content='container for numbers. You may be already familiar with matrices, which are 2D ten-\\nsors: tensors are a generalization of matric es to an arbitrary number of dimensions\\n(note that in the context of tensors, a dimension is often called an axis).\\n2.2.1 Scalars (0D tensors)\\nA tensor that contains only one number is called a scalar (or scalar tensor, or 0-dimensional\\ntensor, or 0D tensor). In Numpy, a float32 or float64 number is a scalar tensor (or scalar'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 44, 'page_label': '45'}, page_content='array). You can display the number of axes of a Numpy tensor via the ndim attribute; a sca-\\nlar tensor has 0 axes (ndim == 0). The number of axes of a tensor is also called its rank.\\nHere’s a Numpy scalar:\\n>>> import numpy as np\\n>>> x = np.array(12)\\n>>> x\\narray(12)\\n>>> x.ndim\\n0\\n2.2.2 Vectors (1D tensors)\\nAn array of numbers is called a vector, or 1D tensor. A 1D tensor is said to have exactly\\none axis. Following is a Numpy vector:\\n>>> x = np.array([12, 3, 6, 14])\\n>>> x\\narray([12, 3, 6, 14])'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 44, 'page_label': '45'}, page_content='>>> x\\narray([12, 3, 6, 14])\\n>>> x.ndim\\n1\\nThis vector has five entr ies and so is called a 5-dimensional vector. Don’t confuse a 5D\\nvector with a 5D tensor! A 5D vector has only one axis and has five dimensions along its\\naxis, whereas a 5D tensor has five axes (and ma y have any number of dimensions\\nalong each axis). Dimensionality can denote either the number of entries along a spe-\\ncific axis (as in the case of our 5D vector) or the number of axes in a tensor (such as a'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 44, 'page_label': '45'}, page_content='5D tensor), which can be confusing at time s. In the latter case, it’s technically more\\ncorrect to talk about a tensor of rank 5 (the rank of a tensor being the number of axes),\\nbut the ambiguous notation 5D tensor is common regardless. \\n2.2.3 Matrices (2D tensors)\\nAn array of vectors is a matrix, or 2D tensor. A matrix has two axes (often referred to\\nrows and columns). You can visually interpret a matrix as a rectangular grid of numbers.\\nThis is a Numpy matrix:\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 45, 'page_label': '46'}, page_content='32 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\n>>> x = np.array([[5, 78, 2, 34, 0],\\n[6, 79, 3, 35, 1],\\n[7, 80, 4, 36, 2]])\\n>>> x.ndim\\n2\\nThe entries from the first axis are called the rows, and the entries from the second axis\\nare called the columns. In the previous example, [5, 78, 2, 34, 0] is the first row of x,\\nand [5, 6, 7] is the first column. \\n2.2.4 3D tensors and higher-dimensional tensors'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 45, 'page_label': '46'}, page_content='2.2.4 3D tensors and higher-dimensional tensors\\nIf you pack such matrices in a new array, you obtain a 3D tensor, which you can visually\\ninterpret as a cube of numbers. Following is a Numpy 3D tensor:\\n>>> x = np.array([[[5, 78, 2, 34, 0],\\n[6, 79, 3, 35, 1],\\n[7, 80, 4, 36, 2]],\\n[[5, 78, 2, 34, 0],\\n[6, 79, 3, 35, 1],\\n[7, 80, 4, 36, 2]],\\n[[5, 78, 2, 34, 0],\\n[6, 79, 3, 35, 1],\\n[7, 80, 4, 36, 2]]])\\n>>> x.ndim\\n3\\nBy packing 3D tensors in an array, you can create a 4D tensor, and so on. In deep learn-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 45, 'page_label': '46'}, page_content='ing, you’ll generally manipulate tensors that are 0D to 4D, although you may go up to\\n5D if you process video data. \\n2.2.5 Key attributes\\nA tensor is defined by three key attributes:\\n\\uf0a1 Number of axes (rank)—For instance, a 3D tensor has three axes, and a matrix has\\ntwo axes. This is also called the tensor’s ndim in Python libraries such as Numpy.\\n\\uf0a1 Shape—This is a tuple of integers that de scribes how many dimensions the ten-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 45, 'page_label': '46'}, page_content='sor has along each axis. For instance, the previous matrix example has shape\\n(3, 5), and the 3D tensor example has shape (3, 3, 5). A vector has a shape\\nwith a single element, such as (5,), whereas a scalar has an empty shape, ().\\n\\uf0a1 Data type (usually called dtype in Python libraries)—This is the type of the data\\ncontained in the tensor; for inst ance, a tensor’s type could be float32, uint8,\\nfloat64, and so on. On rare o ccasions, you may see a char tensor. Note that'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 45, 'page_label': '46'}, page_content='string tensors don’t exist in Numpy (or in most other libraries), because tensors\\nlive in preallocated, contiguous memory  segments: and strings, being variable\\nlength, would preclude the use of this implementation.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 46, 'page_label': '47'}, page_content='33Data representations for neural networks\\nTo make this more concrete, let’s look back at the data we processed in the MNIST\\nexample. First, we load the MNIST dataset:\\nfrom keras.datasets import mnist\\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\\nNext, we display the number of axes of the tensor train_images, the ndim attribute:\\n>>> print(train_images.ndim)\\n3\\nHere’s its shape:\\n>>> print(train_images.shape)\\n(60000, 28, 28)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 46, 'page_label': '47'}, page_content='>>> print(train_images.shape)\\n(60000, 28, 28)\\nAnd this is its data type, the dtype attribute:\\n>>> print(train_images.dtype)\\nuint8\\nSo what we have here is a 3D tensor of 8-bit integers. More precisely, it’s an array of\\n60,000 matrices of 28 × 8 integers. Each such  matrix is a grayscale image, with coeffi-\\ncients between 0 and 255.\\n Let’s display the fourth digit in this 3D tensor, using the library Matplotlib (part of\\nthe standard scientific Python suite); see figure 2.2.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 46, 'page_label': '47'}, page_content='digit = train_images[4]\\nimport matplotlib.pyplot as plt\\nplt.imshow(digit, cmap=plt.cm.binary)\\nplt.show()\\nListing 2.6 Displaying the fourth digit\\nFigure 2.2 The fourth sample in our dataset\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 47, 'page_label': '48'}, page_content='34 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\n2.2.6 Manipulating tensors in Numpy\\nIn the previous example, we selected a specific digit alongside the first axis using the\\nsyntax train_images[i]. Selecting specific elements in a tensor is called tensor slicing.\\nLet’s look at the tensor-slicing operations you can do on Numpy arrays.\\n The following example selects digits #10 to #100 (#100 isn’t included) and puts\\nthem in an array of shape (90, 28, 28):'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 47, 'page_label': '48'}, page_content='them in an array of shape (90, 28, 28):\\n>>> my_slice = train_images[10:100]\\n>>> print(my_slice.shape)\\n(90, 28, 28)\\nIt’s equivalent to this more detailed nota tion, which specifies a start index and stop\\nindex for the slice along each tensor axis. Note that : is equivalent to selecting the\\nentire axis:\\n>>> my_slice = train_images[10:100, :, :]\\n>>> my_slice.shape\\n(90, 28, 28)\\n>>> my_slice = train_images[10:100, 0:28, 0:28]\\n>>> my_slice.shape\\n(90, 28, 28)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 47, 'page_label': '48'}, page_content='>>> my_slice.shape\\n(90, 28, 28)\\nIn general, you may select between any two indices along each tensor axis. For\\ninstance, in order to select 14 × 14 pixels in the bottom-right corner of all images, you\\ndo this:\\nmy_slice = train_images[:, 14:, 14:]\\nIt’s also possible to use negative indices. Much like negative indices in Python lists,\\nthey indicate a position relative to the end of the current axis. In order to crop the\\nimages to patches of 14 × 14 pixels centered in the middle, you do this:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 47, 'page_label': '48'}, page_content='my_slice = train_images[:, 7:-7, 7:-7]\\n2.2.7 The notion of data batches\\nIn general, the first axis (axis 0, because inde xing starts at 0) in all data tensors you’ll\\ncome across in deep learning will be the samples axis  (sometimes called the samples\\ndimension). In the MNIST example, samples are images of digits.\\n In addition, deep-learning models don’t pr ocess an entire dataset at once; rather,\\nthey break the data into small batches. Concretely, here’s one batch of our MNIST dig-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 47, 'page_label': '48'}, page_content='its, with batch size of 128:\\nbatch = train_images[:128]\\nAnd here’s the next batch:\\nbatch = train_images[128:256]\\nAnd the nth batch:\\nbatch = train_images[128 * n:128 * (n + 1)]\\nEquivalent to the \\nprevious example\\nAlso equivalent to the \\nprevious example\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 48, 'page_label': '49'}, page_content='35Data representations for neural networks\\nWhen considering such a batch tensor, the first axis (axis 0) is called the batch axis or\\nbatch dimension. This is a term you’ll frequently encounter when using Keras and other\\ndeep-learning libraries. \\n2.2.8 Real-world examples of data tensors\\nLet’s make data tensors more concrete wi th a few examples similar to what you’ll\\nencounter later. The data you’ll manipulate wi ll almost always fall into one of the fol-\\nlowing categories:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 48, 'page_label': '49'}, page_content='lowing categories:\\n\\uf0a1 Vector data—2D tensors of shape (samples, features)\\n\\uf0a1 Timeseries data or sequence data —3D tensors of shape (samples, timesteps,\\nfeatures)\\n\\uf0a1 Images—4D tensors of shape (samples, height, width, channels) or (samples,\\nchannels, height, width)\\n\\uf0a1 Video—5D tensors of shape (samples, frames, height, width, channels) or\\n(samples, frames, channels, height, width)\\n2.2.9 Vector data\\nThis is the most common case. In such a dataset, each single data point can be encoded'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 48, 'page_label': '49'}, page_content='as a vector, and thus a batch of data will be encoded as a 2D tensor (that is, an array of\\nvectors), where the first axis is the samples axis and the second axis is the features axis.\\n Let’s take a look at two examples:\\n\\uf0a1 An actuarial dataset of people, wher e we consider each person’s age, ZIP code,\\nand income. Each person can be characterized as a vector of 3 values, and thus\\nan entire dataset of 100,000 people can be stored in a \\n2D tensor of shape\\n(100000, 3).'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 48, 'page_label': '49'}, page_content='2D tensor of shape\\n(100000, 3).\\n\\uf0a1 A dataset of text documents, where we represent each document by the counts\\nof how many times each word appears in it (out of a dictionary of 20,000 com-\\nmon words). Each document can be encode d as a vector of 20,000 values (one\\ncount per word in the dictionary), and thus an entire dataset of 500 documents\\ncan be stored in a tensor of shape (500, 20000). \\n2.2.10 Timeseries data or sequence data'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 48, 'page_label': '49'}, page_content='2.2.10 Timeseries data or sequence data\\nWhenever time matters in your data (or the notion of sequence order), it makes sense\\nto store it in a 3D tensor with an explicit time axis. Each sample can be encoded as a\\nsequence of vectors (a 2D tensor), and thus a batch of data will be encoded as a 3D\\ntensor (see figure 2.3).\\nFeatures\\nTimesteps\\nSamples\\nFigure 2.3 A 3D timeseries data tensor\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 49, 'page_label': '50'}, page_content='36 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nThe time axis is always the second axis (axis of index 1), by convention. Let’s look at a\\nfew examples:\\n\\uf0a1 A dataset of stock prices. Every minute, we store the current price of the stock,\\nthe highest price in the past minute, an d the lowest price in the past minute.\\nThus every minute is encoded as a 3D vector, an entire day of trading is\\nencoded as a 2D tensor of shape (390, 3) (there are 390 minutes in a trading'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 49, 'page_label': '50'}, page_content='day), and 250 days’ worth of data can be stored in a 3D tensor of shape (250,\\n390, 3). Here, each sample would be one day’s worth of data.\\n\\uf0a1 A dataset of tweets, where we encode each tweet as a sequence of 280 characters\\nout of an alphabet of 128 unique characters. In this setting, each character can\\nbe encoded as a binary vector of size 128 (an all-zeros vector except for a 1 entry\\nat the index corresponding to the character). Then each tweet can be encoded'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 49, 'page_label': '50'}, page_content='as a 2D tensor of shape (280, 128) , and a dataset of 1 million tweets can be\\nstored in a tensor of shape (1000000, 280, 128). \\n2.2.11 Image data\\nImages typically have three dimensions: he ight, width, and color depth. Although\\ngrayscale images (like our MNIST digits) have only a single color channel and could\\nthus be stored in 2D tensors, by convention image tensors are always 3D, with a one-\\ndimensional color channel for grayscale im ages. A batch of 128 grayscale images of'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 49, 'page_label': '50'}, page_content='size 256 × 256 could thus be stored in a tensor of shape (128, 256, 256, 1), and a\\nbatch of 128 color images could be stored in a tensor of shape (128, 256, 256, 3)\\n(see figure 2.4).\\nThere are two conventions for shapes of images tensors: the channels-last convention\\n(used by TensorFlow) and the channels-first convention (used by Theano). The Tensor-\\nFlow machine-learning framework, from G oogle, places the color-depth axis at the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 49, 'page_label': '50'}, page_content='end: (samples, height, width, color_depth). Meanwhile, Theano places the color\\ndepth axis right after the batch axis: (samples, color_depth, height, width). With\\nColor channels\\nHeight\\nWidth\\nSamples\\nFigure 2.4 A 4D image data \\ntensor (channels-first convention)\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 50, 'page_label': '51'}, page_content='37Data representations for neural networks\\nthe Theano convention, the previous examples would become (128, 1, 256, 256)\\nand (128, 3, 256, 256). The Keras framework provides support for both formats. \\n2.2.12 Video data\\nVideo data is one of the few types of real-world data for which you’ll need 5D tensors.\\nA video can be understood as a sequence of frames, each frame being a color image.\\nBecause each frame can be stored in a 3D tensor (height, width, color_depth), a'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 50, 'page_label': '51'}, page_content='sequence of frames can be stored in a 4D tensor (frames, height, width, color_\\ndepth), and thus a batch of differen t videos can be stored in a 5D tensor of shape\\n(samples, frames, height, width, color_depth).\\n For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames per\\nsecond would have 240 frames. A batch of fo ur such video clips would be stored in a\\ntensor of shape (4, 240, 144, 256, 3). That’s a total of 106,168,320 values! If the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 50, 'page_label': '51'}, page_content='dtype of the tensor was float32, then each value would be stored in 32 bits, so the\\ntensor would represent 405 MB. Heavy! Vi deos you encounter in real life are much\\nlighter, because they aren’t stored in float32, and they’re typically compressed by a\\nlarge factor (such as in the MPEG format). \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 51, 'page_label': '52'}, page_content='38 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\n2.3 The gears of neural ne tworks: tensor operations\\nMuch as any computer program can be ulti mately reduced to a small set of binary\\noperations on binary inputs ( AND, OR, NOR, and so on), all transformations learned\\nby deep neural networks can be reduced to a handful of tensor operations applied to\\ntensors of numeric data. For instance, it’s possible to add tensor s, multiply tensors,\\nand so on.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 51, 'page_label': '52'}, page_content=\"and so on.\\n In our initial example, we were building our network by stacking Dense layers on\\ntop of each other. A Keras layer instance looks like this:\\nkeras.layers.Dense(512, activation='relu')\\nThis layer can be interpreted as a function, which takes as input a 2D tensor and\\nreturns another 2D tensor—a new representation for the input tensor. Specifically, the\\nfunction is as follows (where W is a 2D tensor and b is a vector, both attributes of the\\nlayer):\\noutput = relu(dot(W, input) + b)\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 51, 'page_label': '52'}, page_content='layer):\\noutput = relu(dot(W, input) + b)\\nLet’s unpack this. We have three tensor operations here: a dot product (dot) between\\nthe input tensor and a tensor named W; an addition (+) between the resulting 2D ten-\\nsor and a vector b; and, finally, a relu operation. relu(x) is max(x, 0).\\nNOTE Although this section deals entirely with linear algebra expressions,\\nyou won’t find any mathematical notation here. I’ve found that mathematical'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 51, 'page_label': '52'}, page_content='concepts can be more readily mastered  by programmers with no mathemati-\\ncal background if they’re expressed as short Python snippets instead of math-\\nematical equations. So we’ll use Numpy code throughout.\\n2.3.1 Element-wise operations\\nThe relu operation and addition are element-wise operations: operations that are\\napplied independently to each entry in the tensors being considered. This means\\nthese operations are highly amenable to massively parallel implementations (vectorized'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 51, 'page_label': '52'}, page_content='implementations, a term that comes from the vector processor  supercomputer archi-\\ntecture from the 1970–1990 period). If you want to write a naive Python imple-\\nmentation of an element-wise operation, you use a for loop, as in this naive\\nimplementation of an element-wise relu operation:\\ndef naive_relu(x):\\nassert len(x.shape) == 2\\nx = x.copy()\\nfor i in range(x.shape[0]):\\nfor j in range(x.shape[1]):\\nx[i, j] = max(x[i, j], 0)\\nreturn x\\nx is a 2D Numpy tensor.\\nAvoid overwriting the input tensor.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 51, 'page_label': '52'}, page_content='Avoid overwriting the input tensor.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 52, 'page_label': '53'}, page_content='39The gears of neural networks: tensor operations\\nYou do the same for addition:\\ndef naive_add(x, y):\\nassert len(x.shape) == 2\\nassert x.shape == y.shape\\nx = x.copy()\\nfor i in range(x.shape[0]):\\nfor j in range(x.shape[1]):\\nx[i, j] += y[i, j]\\nreturn x\\nOn the same principle, you can do element-wise multiplication, subtraction, and so on.\\n In practice, when dealing with Numpy arrays, these operations are available as well-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 52, 'page_label': '53'}, page_content='optimized built-in Numpy func tions, which themselves delegate the heavy lifting to a\\nBasic Linear Algebra Subprograms ( BLAS) implementation if you have one installed\\n(which you should). BLAS are low-level, highly parallel, efficient tensor-manipulation\\nroutines that are typically implemented in Fortran or C.\\n So, in Numpy, you can do the following element-wise operation, and it will be blaz-\\ning fast:\\nimport numpy as np\\nz=x+y\\nz = np.maximum(z, 0.)\\n2.3.2 Broadcasting'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 52, 'page_label': '53'}, page_content='z=x+y\\nz = np.maximum(z, 0.)\\n2.3.2 Broadcasting\\nOur earlier naive implementation of naive_add only supports the addition of 2D ten-\\nsors with identical shapes. But in the Dense layer introduced earlier, we added a 2D\\ntensor with a vector. What happens with ad dition when the shapes of the two tensors\\nbeing added differ?\\n When possible, and if there’s no ambiguity, the smaller tensor will be broadcasted to\\nmatch the shape of the larger tensor. Broadcasting consists of two steps:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 52, 'page_label': '53'}, page_content='1 Axes (called broadcast axes) are added to the smaller tensor to match the ndim of\\nthe larger tensor.\\n2 The smaller tensor is repeated alongside these new axes to match the full shape\\nof the larger tensor.\\nLet’s look at a concre te example. Consider X with shape (32, 10) and y with shape\\n(10,). First, we add an empty first axis to y, whose shape becomes (1, 10). Then, we\\nrepeat y 32 times alongside this new axis, so that we end up with a tensor Y with shape'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 52, 'page_label': '53'}, page_content='(32, 10), where Y[i, :] == y for i in range(0, 32). At this point, we can proceed to\\nadd X and Y, because they have the same shape.\\n In terms of implementation, no new 2D tensor is created, because that would be\\nterribly inefficient. The repetition operation is  entirely virtual: it happens at the algo-\\nrithmic level rather than at the memory level. But thinking of the vector being\\nx and y are 2D \\nNumpy tensors.\\nAvoid overwriting \\nthe input tensor.\\nElement-wise addition\\nElement-wise relu'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 52, 'page_label': '53'}, page_content='Element-wise addition\\nElement-wise relu \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 53, 'page_label': '54'}, page_content='40 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nrepeated 10 times alongside a new axis is a helpful mental model. Here’s what a naive\\nimplementation would look like:\\ndef naive_add_matrix_and_vector(x, y):\\nassert len(x.shape) == 2\\nassert len(y.shape) == 1\\nassert x.shape[1] == y.shape[0]\\nx = x.copy()\\nfor i in range(x.shape[0]):\\nfor j in range(x.shape[1]):\\nx[i, j] += y[j]\\nreturn x\\nWith broadcasting, you can generally apply tw o-tensor element-wise operations if one'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 53, 'page_label': '54'}, page_content='tensor has shape (a, b, … n, n + 1, … m) and the other has shape (n, n + 1, … m). The\\nbroadcasting will then automatically happen for axes a through n - 1.\\n The following example applies the element-wise maximum operation to two tensors\\nof different shapes via broadcasting:\\nimport numpy as np\\nx = np.random.random((64, 3, 32, 10))\\ny = np.random.random((32, 10))\\nz = np.maximum(x, y)\\n2.3.3 Tensor dot\\nThe dot operation,  also called a tensor product (not to be confused with an element-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 53, 'page_label': '54'}, page_content='wise product) is the most common, most  useful tensor operation. Contrary to\\nelement-wise operations, it combines entries in the input tensors.\\n An element-wise product is done with the * operator in Numpy, Keras, Theano,\\nand TensorFlow. dot uses a different syntax in Te nsorFlow, but in both Numpy and\\nKeras it’s done using the standard dot operator:\\nimport numpy as np\\nz = np.dot(x, y)\\nIn mathematical notation, you’d note the operation with a dot (.):\\nz=x.y'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 53, 'page_label': '54'}, page_content='z=x.y\\nMathematically, what does the dot operatio n do? Let’s start with the dot product of\\ntwo vectors x and y. It’s computed as follows:\\ndef naive_vector_dot(x, y):\\nassert len(x.shape) == 1\\nassert len(y.shape) == 1\\nassert x.shape[0] == y.shape[0]\\nx is a 2D Numpy tensor.\\ny is a Numpy vector.\\nAvoid overwriting \\nthe input tensor.\\nx is a random tensor with \\nshape (64, 3, 32, 10).\\ny is a random tensor \\nwith shape (32, 10).\\nThe output z has shape \\n(64, 3, 32, 10) like x.\\nx and y are Numpy vectors.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 53, 'page_label': '54'}, page_content='x and y are Numpy vectors.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 54, 'page_label': '55'}, page_content='41The gears of neural networks: tensor operations\\nz=0 .\\nfor i in range(x.shape[0]):\\nz += x[i] * y[i]\\nreturn z\\nYou’ll have noticed that the dot product betw een two vectors is a scalar and that only\\nvectors with the same number of elements are compatible for a dot product.\\n You can also take the dot product between a matrix x and a vector y, which returns\\na vector where the coefficients are the dot products between y and the rows of x. You\\nimplement it as follows:\\nimport numpy as np'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 54, 'page_label': '55'}, page_content='implement it as follows:\\nimport numpy as np\\ndef naive_matrix_vector_dot(x, y):\\nassert len(x.shape) == 2\\nassert len(y.shape) == 1\\nassert x.shape[1] == y.shape[0]\\nz = np.zeros(x.shape[0])\\nfor i in range(x.shape[0]):\\nfor j in range(x.shape[1]):\\nz[i] += x[i, j] * y[j]\\nreturn z\\nYou could also reuse the code we wrote pr eviously, which highlights the relationship\\nbetween a matrix-vector product and a vector product:\\ndef naive_matrix_vector_dot(x, y):\\nz = np.zeros(x.shape[0])'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 54, 'page_label': '55'}, page_content='z = np.zeros(x.shape[0])\\nfor i in range(x.shape[0]):\\nz[i] = naive_vector_dot(x[i, :], y)\\nreturn z\\nNote that as soon as one of the two tensors has an ndim greater than 1, dot is no lon-\\nger symmetric, which is to say that dot(x, y) isn’t the same as dot(y, x).\\n Of course, a dot product generalizes to te nsors with an arbitrary number of axes.\\nThe most common applications may be th e dot product between two matrices. You\\ncan take the dot product of two matrices x and y ( dot(x, y)) if and only if'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 54, 'page_label': '55'}, page_content='x.shape[1] == y.shape[0]. The result is a matrix with shape (x.shape[0],\\ny.shape[1]), where the coefficients are the vector products between the rows of x\\nand the columns of y. Here’s the naive implementation:\\ndef naive_matrix_dot(x, y):\\nassert len(x.shape) == 2\\nassert len(y.shape) == 2\\nassert x.shape[1] == y.shape[0]\\nz = np.zeros((x.shape[0], y.shape[1]))\\nfor i in range(x.shape[0]):\\nfor j in range(y.shape[1]):\\nrow_x = x[i, :]\\ncolumn_y = y[:, j]\\nz[i, j] = naive_vector_dot(row_x, column_y)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 54, 'page_label': '55'}, page_content='z[i, j] = naive_vector_dot(row_x, column_y)\\nreturn z\\nx is a Numpy matrix.\\ny is a Numpy vector.\\nThe first dimension of x must be the \\nsame as the 0th dimension of y!\\nThis operation returns a vector of \\n0s with the same shape as y.\\nx and y\\nare\\nNumpy\\nmatrices.\\nThe first dimension of x must be the \\nsame as the 0th dimension of y!\\nThis operation returns a matrix \\nof 0s with a specific shape.\\nIterates over the rows of x …\\n… and over the columns of y.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 55, 'page_label': '56'}, page_content='42 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nTo understand dot-product shape compatibility, it helps to visualize the input and out-\\nput tensors by aligning them as shown in figure 2.5.\\nx, y, and z are pictured as rectangl es (literal boxes of coefficients). Because the rows\\nand x and the columns of y must have the same size, it follows that the width of x must\\nmatch the height of y. If you go on to develop ne w machine-learning algorithms,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 55, 'page_label': '56'}, page_content='you’ll likely be drawing such diagrams often.\\n More generally, you can take the dot product between higher-dimensional tensors,\\nfollowing the same rules for shape compatibility as outlined earlier for the 2D case:\\n(a, b, c, d) . (d,) -> (a, b, c)\\n(a, b, c, d) . (d, e) -> (a, b, c, e)\\nAnd so on. \\n2.3.4 Tensor reshaping\\nA third type of tensor operation that’s essential to understand is tensor reshaping .\\nAlthough it wasn’t used in the Dense layers in our first neural network example, we'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 55, 'page_label': '56'}, page_content='used it when we preprocessed the digits data before feeding it into our network:\\ntrain_images = train_images.reshape((60000, 28 * 28))\\nReshaping a tensor means rearranging its ro ws and columns to match a target shape.\\nNaturally, the reshaped tensor has the same total number of coefficients as the initial\\ntensor. Reshaping is best understood via simple examples:\\n>>> x = np.array([[0., 1.],\\n[2., 3.],\\n[4., 5.]])\\n>>> print(x.shape)\\n(3, 2)\\na\\nb\\nx . y = z\\nb\\nx.shape:\\n(a, b)\\ny.shape:\\n(b, c)\\nz.shape:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 55, 'page_label': '56'}, page_content='b\\nx.shape:\\n(a, b)\\ny.shape:\\n(b, c)\\nz.shape:\\n(a, c)\\nRow of x\\nColumn of y\\nz [ i,  j ]\\nc\\nFigure 2.5 Matrix dot-product \\nbox diagram\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 56, 'page_label': '57'}, page_content='43The gears of neural networks: tensor operations\\n>>> x = x.reshape((6, 1))\\n>>> x\\narray([[ 0.],\\n[ 1.],\\n[ 2.],\\n[ 3.],\\n[ 4.],\\n[ 5.]])\\n>>> x = x.reshape((2, 3))\\n>>> x\\narray([[ 0., 1., 2.],\\n[ 3., 4., 5.]])\\nA special case of reshaping that’s commonly encountered is transposition. Transposing a\\nmatrix means exchanging its rows and its columns, so that x[i, :] becomes x[:, i]:\\n>>> x = np.zeros((300, 20))\\n>>> x = np.transpose(x)\\n>>> print(x.shape)\\n(20, 300)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 56, 'page_label': '57'}, page_content='>>> print(x.shape)\\n(20, 300)\\n2.3.5 Geometric interpretation of tensor operations\\nBecause the contents of the tensors manipu lated by tensor operations can be inter-\\npreted as coordinates of points in some ge ometric space, all tensor operations have a\\ngeometric interpretation. For instance, let’s consider addition. We’ll start with the fol-\\nlowing vector:\\nA = [0.5, 1]\\nIt’s a point in a 2D space (see figure 2.6). It’s commo n to picture a vector as an arrow'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 56, 'page_label': '57'}, page_content='linking the origin to the point, as shown in figure 2.7.\\nCreates an all-zeros matrix \\nof shape (300, 20) \\n1\\n1\\nA [0.5, 1]\\nFigure 2.6 A point in a 2D space\\n1\\n1\\nA [0.5, 1]\\nFigure 2.7 A point in a 2D space \\npictured as an arrow\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 57, 'page_label': '58'}, page_content='44 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nLet’s consider a new point, B = [1, 0.25], which we’ll add to the previous one. This is\\ndone geometrically by chaining together the vector arrows, with the resulting location\\nbeing the vector representing the sum of the previous two vectors (see figure 2.8).\\nIn general, elementary geometric operations such as affine transformations, rotations,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 57, 'page_label': '58'}, page_content='scaling, and so on can be ex pressed as tensor operations. For instance, a rotation of a\\n2D vector by an angle theta can be achiev ed via a dot product with a 2 × 2 matrix\\nR=[ u , v], where u and v are both vectors of the plane: u = [cos(theta),\\nsin(theta)] and v = [-sin(theta), cos(theta)]. \\n2.3.6 A geometric interpre tation of deep learning\\nYou just learned that neural networks consist entirely of chains of tensor operations and'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 57, 'page_label': '58'}, page_content='that all of these tensor operations are just geometric transformations of the input data.\\nIt follows that you can interpret a neural network as a very complex geometric transfor-\\nmation in a high-dimensional space, implemented via a long series of simple steps.\\n In 3D, the following mental image may prove useful. Imagine two sheets of colored\\npaper: one red and one blue. Put one on  top of the other. Now crumple them'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 57, 'page_label': '58'}, page_content='together into a small ball. That crumpled paper ball is your input data, and each sheet\\nof paper is a class of data in a classifica tion problem. What a neural network (or any\\nother machine-learning model) is meant to do is figure out a transformation of the\\npaper ball that would uncrumple it, so as to make the two classes cleanly separable\\nagain. With deep learning, this would be im plemented as a series of simple transfor-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 57, 'page_label': '58'}, page_content='mations of the 3D space, such as those you could apply on the paper ball with your fin-\\ngers, one movement at a time.\\n1\\n1\\nA\\nB\\nA + B\\nFigure 2.8 Geometric interpretation of \\nthe sum of two vectors\\nFigure 2.9 Uncrumpling a \\ncomplicated manifold of data\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 58, 'page_label': '59'}, page_content='45The gears of neural networks: tensor operations\\nUncrumpling paper balls is what machine lear ning is about: find ing neat representa-\\ntions for complex, highly folded data mani f o l d s .  A t  t h i s  p o i n t ,  y o u  s h o u l d  h a v e  a\\npretty good intuition as to why deep learni ng excels at this: it takes the approach of\\nincrementally decomposing a complicated geometric transformation into a long\\nchain of elementary ones, which is pretty much the strategy a human would follow to'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 58, 'page_label': '59'}, page_content='uncrumple a paper ball. Each layer in a d eep network applies a transformation that\\ndisentangles the data a little—and a deep stack of layers makes tractable an extremely\\ncomplicated disentanglement process. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 59, 'page_label': '60'}, page_content='46 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\n2.4 The engine of neural networks: \\ngradient-based optimization\\nAs you saw in the previous section, each neural layer from our first network example\\ntransforms its input data as follows:\\noutput = relu(dot(W, input) + b)\\nIn this expression, W and b are tensors that are attributes  of the layer. They’re called\\nthe weights or trainable parameters of the layer (the kernel and bias attributes, respec-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 59, 'page_label': '60'}, page_content='tively). These weights contain the informat ion learned by the network from exposure\\nto training data.\\n Initially, these weight matrices are filled with small random values (a step called ran-\\ndom initialization). Of course, there’s no reason to expect that relu(dot(W, input) + b),\\nwhen W and b are random, will yield any useful representations. The resulting represen-\\ntations are meaningless—but they’re a starting point. What comes next is to gradually'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 59, 'page_label': '60'}, page_content='adjust these weights, based on a feedback signal. This gradual adjustment, also called\\ntraining, is basically the learning that machine learning is all about.\\n This happens within what’s called a training loop, which works as follows. Repeat\\nthese steps in a loop, as long as necessary:\\n1 Draw a batch of training samples x and corresponding targets y.\\n2 Run the network on x (a step called the forward pass) to obtain predictions y_pred.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 59, 'page_label': '60'}, page_content='3 Compute the loss of the network on the batch, a measure of the mismatch\\nbetween y_pred and y.\\n4 Update all weights of the network in a wa y that slightly reduces the loss on this\\nbatch.\\nYou’ll eventually end up with a network that has a very low loss on its training data: a\\nlow mismatch between predictions y_pred and expected targets y. The network has\\n“learned” to map its inputs to correct target s. From afar, it may look like magic, but'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 59, 'page_label': '60'}, page_content='when you reduce it to elementary steps, it turns out to be simple.\\n Step 1 sounds easy enough—just I/O code. Steps 2 and 3 are merely the applica-\\ntion of a handful of tensor operations, so you could implement these steps purely\\nfrom what you learned in the previous section. The difficult part is step 4: updating\\nthe network’s weights. Given an individual weight coefficient in the network, how can\\nyou compute whether the coefficient should be increased or decreased, and by how\\nmuch?'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 59, 'page_label': '60'}, page_content='much?\\n One naive solution would be to freeze al l weights in the network except the one\\nscalar coefficient being consid ered, and try different values for this coefficient. Let’s\\nsay the initial value of the coefficient is 0. 3. After the forward pass on a batch of data,\\nthe loss of the network on the batch is 0.5. If you change the coefficient’s value to 0.35\\nand rerun the forward pass, the loss increases to 0.6. But if you lower the coefficient to'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 59, 'page_label': '60'}, page_content='0.25, the loss falls to 0.4. In this case, it seems that updating the coefficient by -0.05\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 60, 'page_label': '61'}, page_content='47The engine of neural networks: gradient-based optimization\\nwould contribute to minimizing the loss. This would have to be repeated for all coeffi-\\ncients in the network.\\n But such an approach would be horribly  inefficient, because you’d need to com-\\npute two forward passes (which are expensiv e) for every individual coefficient (of\\nwhich there are many, usually thousands an d sometimes up to millions). A much bet-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 60, 'page_label': '61'}, page_content='ter approach is to take advantage of the fa ct that all operations used in the network\\nare differentiable, and compute the gradient of the loss with regard to the network’s\\ncoefficients. You can then move the coeffici ents in the opposite direction from the\\ngradient, thus decreasing the loss.\\n If you already know what differentiable means and what a gradient is, you can skip to\\nsection 2.4.3. Otherwise, the following two sections will help you understand these\\nconcepts.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 60, 'page_label': '61'}, page_content='concepts.\\n2.4.1 What’s a derivative?\\nConsider a continuous, smooth function f(x) = y, mapping a real number x to a new\\nreal number y. Because the function is continuous, a small change in x can only result\\nin a small change in y—that’s the intuition behind continuity. Let’s say you increase x\\nby a small factor epsilon_x: this results in a small epsilon_y change to y:\\nf(x + epsilon_x) = y + epsilon_y\\nIn addition, because the function is smooth (its curve doesn’t have any abrupt angles),'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 60, 'page_label': '61'}, page_content='when epsilon_x is small enough, around a certain point p, it’s possible to approxi-\\nmate f as a linear function of slope a, so that epsilon_y becomes a * epsilon_x:\\nf(x + epsilon_x) = y + a * epsilon_x\\nObviously, this linear approximation is valid only when x is close enough to p.\\n The slope a is called the derivative of f in p. If a is negative, it means a small change\\nof x around p will result in a decrease of f(x) (as shown in figure 2.10); and if a is pos-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 60, 'page_label': '61'}, page_content=\"itive, a small change in x will result in an increase of f(x). Further, the absolute value\\nof a (the magnitude of the derivative) tells you how quickly this increase or decrease\\nwill happen.\\nFor every differentiable function f(x) (differentiable means “can be derived”: for exam-\\nple, smooth, continuous functions can be de rived), there exists a derivative function\\nf'(x) that maps values of x to the slope of the local linear approximation of f in those\\nLocal linear\\napproximation of f,\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 60, 'page_label': '61'}, page_content='Local linear\\napproximation of f,\\nwith slope a\\nf Figure 2.10 Derivative of f in p\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 61, 'page_label': '62'}, page_content=\"48 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\npoints. For instance, the derivative of cos(x) is -sin(x), the derivative of f(x) = a * x\\nis f'(x) = a, and so on.\\n If you’re trying to update x by a factor epsilon_x in order to minimize f(x), and\\nyou know the derivative of f, then your job is done: the derivative completely\\ndescribes how f(x) evolves as you change x. If you want to reduce the value of f(x),\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 61, 'page_label': '62'}, page_content='you just need to move x a little in the opposite direction from the derivative. \\n2.4.2 Derivative of a tensor  operation: the gradient\\nA gradient is the derivative of a tensor operation. It’s the generalization of the concept\\nof derivatives to functions of multidimension al inputs: that is, to functions that take\\ntensors as inputs.\\n Consider an input vector x, a matrix W, a target y, and a loss function loss. You can\\nuse W to compute a target candidate y_pred, and compute the loss, or mismatch,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 61, 'page_label': '62'}, page_content='between the target candidate y_pred and the target y:\\ny_pred = dot(W, x)\\nloss_value = loss(y_pred, y)\\nIf the data inputs x and y are frozen, then this can be interpreted as a function map-\\nping values of W to loss values:\\nloss_value = f(W)\\nLet’s say the current value of W is W0. Then the derivative of f in the point W0 is a tensor\\ngradient(f)(W0) with the same shape as W, where each coefficient gradient(f)\\n(W0)[i, j] indicates the direction and magnitude of the change in loss_value you'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 61, 'page_label': '62'}, page_content='observe when modifying W0[i, j]. That tensor gradient(f)(W0) is the gradient of\\nthe function f(W) = loss_value in W0.\\n You saw earlier that the derivative of a function f(x) of a single coefficient can be\\ninterpreted as the slope of the curve of f. Likewise, gradient(f)(W0) can be inter-\\npreted as the tensor describing the curvature of f(W) around W0.\\n For this reason, in much the same way that, for a function f(x), you can reduce'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 61, 'page_label': '62'}, page_content='the value of f(x) by moving x a little in the opposite direction from the derivative,\\nwith a function f(W) of a tensor, you can reduce f(W) by moving W in the opposite\\ndirection from the gradient: for example, W1 = W0 - step * gradient(f)(W0) (where\\nstep is a small scaling factor). That means going against the curvature, which intui-\\ntively should put you lower on the cu rve. Note that the scaling factor step is needed'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 61, 'page_label': '62'}, page_content='because gradient(f)(W0) only approximates the curv ature when you’re close to W0,\\nso you don’t want to get too far from W0. \\n2.4.3 Stochastic gradient descent\\nGiven a differentiable function, it’s theoreti cally possible to find its minimum analyti-\\ncally: it’s known that a function’s minimum is  a point where the derivative is 0, so all\\nyou have to do is find all the points where the derivative goes to 0 and check for which\\nof these points the function has the lowest value.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 61, 'page_label': '62'}, page_content='Licensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 62, 'page_label': '63'}, page_content='49The engine of neural networks: gradient-based optimization\\n Applied to a neural network, that mean s finding analytically the combination of\\nweight values that yields the smallest possibl e loss function. This can be done by solv-\\ning the equation gradient(f)(W) = 0 for W. This is a polynomial equation of N vari-\\nables, where N is the number of coefficients in the network. Although it would be\\npossible to solve such  an equation for N = 2 or N = 3, doing so is intractable for real'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 62, 'page_label': '63'}, page_content='neural networks, where the number of para meters is never less than a few thousand\\nand can often be several tens of millions.\\n Instead, you can use the four-step algorith m outlined at the beginning of this sec-\\ntion: modify the parameters little by littl e based on the current loss value on a ran-\\ndom batch of data. Because you’re dealing with a differentiable function, you can\\ncompute its gradient, which gives you an efficient way to implement step 4. If you'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 62, 'page_label': '63'}, page_content='update the weights in the opposite direction from the gradient, the loss will be a little\\nless every time:\\n1 Draw a batch of training samples x and corresponding targets y.\\n2 Run the network on x to obtain predictions y_pred.\\n3 Compute the loss of the network on the batch, a measure of the mismatch\\nbetween \\ny_pred and y.\\n4 Compute the gradient of the loss with regard to the network’s parameters (a\\nbackward pass).\\n5 Move the parameters a little in the o pposite direction from the gradient—for'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 62, 'page_label': '63'}, page_content='example W -= step * gradient—thus reducing the loss on the batch a bit.\\nEasy enough! What I just described is called mini-batch stochastic gradient descent  (mini-\\nbatch SGD). The term stochastic refers to the fact that each batch of data is drawn at\\nrandom (stochastic is a scientific synonym of random). Figure 2.11 illustrates what hap-\\npens in 1D, when the network has only one parameter and you have only one training\\nsample.\\nLoss\\nvalue Starting\\npoint (t=0)\\nStep, also called learning rate\\nt=1'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 62, 'page_label': '63'}, page_content='point (t=0)\\nStep, also called learning rate\\nt=1\\nt=2\\nt=3\\nParameter\\nvalue\\nFigure 2.11 SGD down a 1D loss \\ncurve (one learnable parameter)\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 63, 'page_label': '64'}, page_content='50 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\nAs you can see, intuitively it’s important to pick a reasonable value for the step factor.\\nIf it’s too small, the descent down the curv e will take many iterations, and it could get\\nstuck in a local minimum. If step is too large, your updates may end up taking you to\\ncompletely random locations on the curve.\\n Note that a variant of the mini-batch SGD algorithm would be to draw a single sam-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 63, 'page_label': '64'}, page_content='ple and target at each iteration, rather than drawing a batch of data. This would be\\ntrue SGD (as opposed to mini-batch SGD). Alternatively, going to the opposite extreme,\\nyou could run every step on all data available, which is called batch SGD. Each update\\nwould then be more accurate, but far mo re expensive. The efficient compromise\\nbetween these two extremes is to use mini-batches of reasonable size.\\n Although figure 2.11 illustrates gradient descent in a 1D parameter space, in prac-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 63, 'page_label': '64'}, page_content='tice you’ll use gradient descent in highly dimensional spaces: every weight coefficient\\nin a neural network is a free dimension in the space, and there may be tens of thou-\\nsands or even millions of them. To help you build intuition about loss surfaces, you\\ncan also visualize gradient descent along a 2D loss surface, as shown in figure 2.12. But\\nyou can’t possibly visualize what the actual process of training a neural network looks'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 63, 'page_label': '64'}, page_content='like—you can’t represent a 1,000,000-dimensional space in a way that makes sense to\\nhumans. As such, it’s good to keep in mi nd that the intuitions you develop through\\nthese low-dimensional representations may no t always be accurate in practice. This\\nhas historically been a source of issues in the world of deep-learning research.\\nAdditionally, there exist multiple variants of SGD tha t  d if fe r b y  t a ki ng i nto  a c co unt'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 63, 'page_label': '64'}, page_content='previous weight updates when computing the next weight update, rather than just\\nlooking at the current value of the gradients. There is, for instance, SGD with momen-\\ntum, as well as Adagrad, RMSProp, and several others. Such variants are known as opti-\\nmization methods or optimizers. In particular, the concept of momentum, which is used in\\nmany of these variants, deserves your atte ntion. Momentum addresses two issues with'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 63, 'page_label': '64'}, page_content='SGD: convergence speed and local minima. Co nsider figure 2.13, which shows the\\ncurve of a loss as a function of a network parameter.\\nStarting point\\nFinal point\\n45\\n40\\n35\\n30\\n25\\n20\\n15\\n10\\n5\\nFigure 2.12 Gradient descent \\ndown a 2D loss surface (two \\nlearnable parameters)\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 64, 'page_label': '65'}, page_content='51The engine of neural networks: gradient-based optimization\\nAs you can see, around a certai n parameter value, there is a local minimum : around\\nthat point, moving left would result in th e loss increasing, but so would moving right.\\nIf the parameter under consideration were being optimized via SGD with a small\\nlearning rate, then the optimization proc ess would get stuck at the local minimum\\ninstead of making its way to the global minimum.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 64, 'page_label': '65'}, page_content='instead of making its way to the global minimum.\\n You can avoid such issues by using momentum, which draws inspiration from phys-\\nics. A useful mental image he re is to think of the optimization process as a small ball\\nrolling down the loss curve. If it has enou gh momentum, the ball won’t get stuck in a\\nravine and will end up at the global minimum. Momentum is implemented by moving\\nthe ball at each step based not only on th e current slope value (current acceleration)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 64, 'page_label': '65'}, page_content='but also on the current velocity (resulting from past acceleration). In practice, this\\nmeans updating the parameter w based not only on the current gradient value but also\\non the previous parameter update, such as in this naive implementation:\\npast_velocity = 0.\\nmomentum = 0.1\\nwhile loss > 0.01:\\nw, loss, gradient = get_current_parameters()\\nvelocity = past_velocity * momentum + learning_rate * gradient\\nw=w+ momentum * velocity - learning_rate * gradient\\npast_velocity = velocity'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 64, 'page_label': '65'}, page_content='past_velocity = velocity\\nupdate_parameter(w)\\n2.4.4 Chaining derivatives: the Backpropagation algorithm\\nIn the previous algorithm, we casually assu med that because a function is differentia-\\nble, we can explicitly compute its derivative. In practice, a neural network function\\nconsists of many tensor operations chai ned together, each of which has a simple,\\nknown derivative. For instance, this is a network f composed of three tensor opera-\\ntions, a, b, and c, with weight matrices W1, W2, and W3:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 64, 'page_label': '65'}, page_content=\"f(W1, W2, W3) = a(W1, b(W2, c(W3)))\\nCalculus tells us that such a chain of functions can be derived using the following iden-\\ntity, called the chain rule: f(g(x)) = f'(g(x)) * g'(x). Applying the chain rule to the\\ncomputation of the gradient values of a ne ural network gives rise to an algorithm\\nLoss\\nvalue\\nParameter\\nvalue\\nLocal\\nminimum\\nGlobal\\nminimum\\nFigure 2.13 A local minimum \\nand a global minimum\\nConstant momentum factor\\nOptimization loop \\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 65, 'page_label': '66'}, page_content='52 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\ncalled Backpropagation (also sometimes called reverse-mode differentiation). Backpropaga-\\ntion starts with the final loss value and work s backward from the top layers to the bot-\\ntom layers, applying the chain rule to com pute the contribution that each parameter\\nhad in the loss value.\\n Nowadays, and for years to come, peop le will implement networks in modern'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 65, 'page_label': '66'}, page_content='frameworks that are capable of symbolic differentiation, such as TensorFlow. This means\\nthat, given a chain of operations with a known derivative, they can compute a gradient\\nfunction for the chain (by applying the chain rule) that maps network parameter values\\nto gradient values. When you have access to such a function, the backward pass is\\nreduced to a call to this gradient function . Thanks to symbolic differentiation, you’ll'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 65, 'page_label': '66'}, page_content='never have to implement the Backpropagation algorithm by hand. For this reason, we\\nwon’t waste your time and your focus on de riving the exact formulation of the Back-\\npropagation algorithm in these pages. All you need is a good understanding of how\\ngradient-based optimization works. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 66, 'page_label': '67'}, page_content='53Looking back at our first example\\n2.5 Looking back at our first example\\nYou’ve reached the end of this chapter, and you should now have a general under-\\nstanding of what’s going on behind the scenes in a neural network. Let’s go back to\\nthe first example and review each piece of it in the light of what you’ve learned in the\\nprevious three sections.\\n This was the input data:\\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 66, 'page_label': '67'}, page_content=\"train_images = train_images.reshape((60000, 28 * 28))\\ntrain_images = train_images.astype('float32') / 255\\ntest_images = test_images.reshape((10000, 28 * 28))\\ntest_images = test_images.astype('float32') / 255\\nNow you understand that the input images are stored in Numpy tensors, which are\\nhere formatted as float32 tensors of shape (60000, 784) (training data) and (10000,\\n784) (test data), respectively.\\n This was our network:\\nnetwork = models.Sequential()\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 66, 'page_label': '67'}, page_content=\"network = models.Sequential()\\nnetwork.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\\nnetwork.add(layers.Dense(10, activation='softmax'))\\nNow you understand that this network consists of a chain of two Dense layers, that\\neach layer applies a few simp le tensor operations to the input data, and that these\\noperations involve weight tensors. Weight te nsors, which are attributes of the layers,\\nare where the knowledge of the network persists.\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 66, 'page_label': '67'}, page_content=\"are where the knowledge of the network persists.\\n This was the network-compilation step:\\nnetwork.compile(optimizer='rmsprop',\\nloss='categorical_crossentropy',\\nmetrics=['accuracy'])\\nNow you understand that categorical_crossentropy is the loss function that’s used\\nas a feedback signal for learning the weight tensors, and which the training phase will\\nattempt to minimize. You also know that th is reduction of the loss happens via mini-\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 66, 'page_label': '67'}, page_content='batch stochastic gradient descent. The exact rules governing a specific use of gradient\\ndescent are defined by the rmsprop optimizer passed as the first argument.\\n Finally, this was the training loop:\\nnetwork.fit(train_images, train_labels, epochs=5, batch_size=128)\\nNow you understand what happens when you call fit: the network will start to iterate\\non the training data in mini-batches of 128  samples, 5 times over (each iteration over'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 66, 'page_label': '67'}, page_content='all the training data is called an epoch). At each iteration, the network will compute the\\ngradients of the weights with regard to th e loss on the batch, and update the weights\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 67, 'page_label': '68'}, page_content='54 CHAPTER 2 Before we begin: the mathematical building blocks of neural networks\\naccordingly. After these 5 epochs, the network will have performed 2,345 gradient\\nupdates (469 per epoch), and the loss of the network will be sufficiently low that the\\nnetwork will be capable of classifying handwritten digits with high accuracy.\\n At this point, you already know most of what there is to know about neural networks.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 68, 'page_label': '69'}, page_content='55Looking back at our first example\\nChapter summary\\n\\uf0a1 Learning means finding a combination of model parameters that mini-\\nmizes a loss function for a given set of training data samples and their cor-\\nresponding targets.\\n\\uf0a1 Learning happens by drawing random batches of data samples and their\\ntargets, and computing the gradient  of the network parameters with\\nrespect to the loss on the batch. The network parameters are then moved'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 68, 'page_label': '69'}, page_content='a bit (the magnitude of the move is defined by the learning rate) in the\\nopposite direction from the gradient.\\n\\uf0a1 The entire learning process is made possible by the fact that neural net-\\nworks are chains of differentiable tensor operations, and thus it’s possible\\nto apply the chain rule of derivation  to find the gradient function map-\\nping the current parameters and current batch of data to a gradient value.\\n\\uf0a1 Two key concepts you’ll see frequently in future chapters are loss and opti-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 68, 'page_label': '69'}, page_content='mizers. These are the two things you need to define before you begin feed-\\ning data into a network.\\n\\uf0a1 The loss is the quantity you’ll attempt to minimize during training, so it\\nshould represent a measure of success for the task you’re trying to solve.\\n\\uf0a1 The optimizer specifies the exact way in which the gradient of the loss will\\nbe used to update parameters: for instance, it could be the RMSProp opti-\\nmizer, SGD with momentum, and so on.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 69, 'page_label': '70'}, page_content='Getting started\\nwith neural networks\\nThis chapter is designed to get you starte d with using neural networks to solve real\\nproblems. You’ll consolidate the knowledg e you gained from our first practical\\nexample in chapter 2, and you’ll apply what you’ve learned to three new problems\\ncovering the three most common use cases of neural networks: binary classifica-\\ntion, multiclass classification, and scalar regression.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 69, 'page_label': '70'}, page_content='In this chapter, we’ll take a closer look at the core components of neural networks\\nthat we introduced in chapter 2: layers, networks, objective functions, and optimiz-\\ners. We’ll give you a quick introduction to Keras, the Python deep-learning library\\nthat we’ll use throughout the book. You’ll  set up a deep-learning workstation, with\\nThis chapter covers\\n\\uf0a1 Core components of neural networks\\n\\uf0a1 An introduction to Keras\\n\\uf0a1 Setting up a deep-learning workstation'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 69, 'page_label': '70'}, page_content='\\uf0a1 Setting up a deep-learning workstation\\n\\uf0a1 Using neural networks to solve basic \\nclassification and regression problems'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 70, 'page_label': '71'}, page_content='57\\nTensorFlow, Keras, and GPU support. We’ll dive into th ree introductory examples of\\nhow to use neural networks to address real problems: \\n\\uf0a1 Classifying movie reviews as positive or negative (binary classification) \\n\\uf0a1 Classifying news wires by topic (multiclass classification) \\n\\uf0a1 Estimating the price of a house, given real-estate data (regression)\\nBy the end of this chapter, you’ll be ab le to use neural networks to solve simple'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 70, 'page_label': '71'}, page_content='machine problems such as classification and regression over vector data. You’ll then\\nbe ready to start building a more principled, theory-driven understanding of machine\\nlearning in chapter 4.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 71, 'page_label': '72'}, page_content='58 CHAPTER 3 Getting started with neural networks\\n3.1 Anatomy of a neural network\\nAs you saw in the previous chapters, training a neural network revolves around the fol-\\nlowing objects:\\n\\uf0a1 Layers, which are combined into a network (or model)\\n\\uf0a1 The input data and corresponding targets\\n\\uf0a1 The loss function, which defines the feedback signal used for learning\\n\\uf0a1 The optimizer, which determines how learning proceeds'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 71, 'page_label': '72'}, page_content='You can visualize their interaction as illust rated in figure 3.1: the network, composed\\nof layers that are chained together, maps the input data to predictions. The loss func-\\ntion then compares these predictions to the targets, producing a loss value: a measure\\nof how well the network’s pr edictions match what was ex pected. The optimizer uses\\nthis loss value to update the network’s weights.\\nLet’s take a closer look at layers, networks, loss functions, and optimizers.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 71, 'page_label': '72'}, page_content='3.1.1 Layers: the building blocks of deep learning\\nThe fundamental data structure in neural networks is the layer, to which you were\\nintroduced in chapter 2. A layer is a data-processing module that takes as input one or\\nmore tensors and that output s one or more tensors. Some  layers are stateless, but\\nmore frequently layers have a state: the layer’s weights, one or several tensors learned\\nwith stochastic gradient descent, which together contain the network’s knowledge.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 71, 'page_label': '72'}, page_content='Different layers are appropriate for different tensor formats and different types of data\\nprocessing. For instance, simple vector data, stored in 2D tensors of shape (samples,\\nfeatures), is often processed by densely connected layers, also called fully connected or dense\\nlayers (the Dense class in Keras). Sequence data, stored in 3D tensors of shape (samples,\\ntimesteps, features), is typically processed by recurrent l a y e r s  s u c h  a s  a n  LSTM layer.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 71, 'page_label': '72'}, page_content=\"Image data, stored in 4D tensors, is usually processed by 2D convolution layers (Conv2D).\\nLayer\\n(data transformation)\\nInput X\\nWeights\\nLayer\\n(data transformation)\\nPredictions\\nY'\\nWeight\\nupdate\\nTrue targets\\nY\\nWeights\\nLoss functionOptimizer\\nLoss score Figure 3.1 Relationship between the \\nnetwork, layers, loss function, and optimizer\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 72, 'page_label': '73'}, page_content='59Anatomy of a neural network\\n You can think of layers as the LEGO bricks of deep learning, a metaphor that is\\nmade explicit by frameworks like Keras. Building deep-learning models in Keras is\\ndone by clipping together compatible layers  to form useful data-transformation pipe-\\nlines. The notion of layer compatibility here refers specifically to the fact that every layer\\nwill only accept input tensors of a certain shape and will return output tensors of a cer-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 72, 'page_label': '73'}, page_content='tain shape. Consider the following example:\\nfrom keras import layers\\nlayer = layers.Dense(32, input_shape=(784,))\\nWe’re creating a layer that will only accept as input 2D tensors where the first dimen-\\nsion is 784 (axis 0, the batch dimension, is unspecified, and thus any value would be\\naccepted). This layer will return a tensor  where the first dimension has been trans-\\nformed to be 32.\\n Thus this layer can only be connected to a downstream layer that expects 32-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 72, 'page_label': '73'}, page_content='dimensional vectors as its input. When us ing Keras, you don’t have to worry about\\ncompatibility, because the layers you add to your models are dynamically built to\\nmatch the shape of the incoming layer. For instance, suppose you write the following:\\nfrom keras import models\\nfrom keras import layers\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(32, input_shape=(784,)))\\nmodel.add(layers.Dense(32))\\nThe second layer didn’t receive an input shape argument—instead , it automatically'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 72, 'page_label': '73'}, page_content='inferred its input shape as being the output shape of the layer that came before. \\n3.1.2 Models: networks of layers\\nA deep-learning model is a directed, ac yclic graph of layers. The most common\\ninstance is a linear stack of layers, mapping a single input to a single output.\\n But as you move forward, you’ll be exposed to a much  broader variety of network\\ntopologies. Some common ones include the following:\\n\\uf0a1 Two-branch networks\\n\\uf0a1 Multihead networks\\n\\uf0a1 Inception blocks'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 72, 'page_label': '73'}, page_content='\\uf0a1 Multihead networks\\n\\uf0a1 Inception blocks\\nThe topology of a network defines a hypothesis space. You may remember that in chap-\\nter 1, we defined machine learning as “sea rching for useful representations of some\\ninput data, within a predefined space of po ssibilities, using guidance from a feedback\\nsignal.” By choosing a networ k topology, you constrain your space of possibilities\\n(hypothesis space) to a specific series of tensor operations, mapping input data to out-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 72, 'page_label': '73'}, page_content='put data. What you’ll then be searching for is  a good set of values for the weight ten-\\nsors involved in these tensor operations.\\nA dense layer with 32 \\noutput units\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 73, 'page_label': '74'}, page_content='60 CHAPTER 3 Getting started with neural networks\\n Picking the right network architecture is more an art than a science; and although\\nthere are some best practices and principles you can rely on, only practice can help\\nyou become a proper neural-network architec t. The next few chapters will both teach\\nyou explicit principles for building neural networks and help you develop intuition as\\nto what works or doesn’t work for specific problems. \\n3.1.3 Loss functions and optimizers:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 73, 'page_label': '74'}, page_content='3.1.3 Loss functions and optimizers: \\nkeys to configuring the learning process\\nOnce the network architecture is defined, you still have to choose two more things:\\n\\uf0a1 Loss function (objective function) —The quantity that will be minimized during\\ntraining. It represents a measure of success for the task at hand.\\n\\uf0a1 Optimizer—Determines how the network will be updated based on the loss func-\\ntion. It implements a specific variant of stochastic gradient descent (SGD).'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 73, 'page_label': '74'}, page_content='A neural network that has multiple outputs may have multiple loss functions (one per\\noutput). But the gradient-descent  process must be based on a single scalar loss value;\\nso, for multiloss networks, all losses are combin ed (via averaging) into a single scalar\\nquantity.\\n Choosing the right objective function fo r the right problem is extremely import-\\nant: your network will take any shortcut it can, to minimize the loss; so if the objective'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 73, 'page_label': '74'}, page_content='doesn’t fully correlate with success for the task at hand, your network will end up\\ndoing things you may not have want ed. Imagine a stupid, omnipotent AI trained via\\nSGD, with this poorly chosen objective fu nction: “maximizing the average well-being\\nof all humans alive.” To make its job easier, this AI might choose to kill all humans\\nexcept a few and focus on the well-being  of the remaining ones—because average\\nwell-being isn’t affected by how many humans are left. That might not be what you'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 73, 'page_label': '74'}, page_content='intended! Just remember that all neural networks you build will be just as ruthless in\\nlowering their loss function—so choose the objective wisely, or you’ll have to face\\nunintended side effects.\\n Fortunately, when it comes to common problems such as classification, regression,\\nand sequence prediction, ther e are simple guidelines you can follow to choose the\\ncorrect loss. For instance, you’ll use binary  crossentropy for a tw o-class classification'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 73, 'page_label': '74'}, page_content='problem, categorical crossentropy for a many-class classification problem, mean-\\nsquared error for a regressi on problem, connectionist temporal classification ( CTC)\\nfor a sequence-learning problem, and so on. Only when you’re working on truly new\\nresearch problems will you have to develop your ow n objective functions. In the next\\nfew chapters, we’ll detail explicitly which loss functions to choose for a wide range of\\ncommon tasks. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 74, 'page_label': '75'}, page_content='61Introduction to Keras\\n3.2 Introduction to Keras\\nThroughout this book, the code examples use Keras ( https:/ /keras.io). Keras is a\\ndeep-learning framework for Python that provides a convenient way to define and\\ntrain almost any kind of deep-learning model. Keras was initially developed for\\nresearchers, with the aim of enabling fast experimentation.\\n Keras has the following key features:\\n\\uf0a1 It allows the same code to run seamlessly on CPU or GPU.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 74, 'page_label': '75'}, page_content='\\uf0a1 It has a user-friendly API that makes it easy to qu ickly prototype deep-learning\\nmodels.\\n\\uf0a1 It has built-in support for convolutional networks (for computer vision), recur-\\nrent networks (for sequence processing), and any combination of both.\\n\\uf0a1 It supports arbitrary network architectures: multi-input or multi-output models,\\nlayer sharing, mo del sharing, and so on. This means Keras is appropriate for\\nbuilding essentially any deep-learning model, from a generative adversarial net-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 74, 'page_label': '75'}, page_content='work to a neural Turing machine.\\nKeras is distributed under the permissive MIT license, which means it can be freely\\nused in commercial projects. It’s compatible with any version of Python from 2.7 to 3.6\\n(as of mid-2017).\\n Keras has well over 200,000 users, rang ing from academic researchers and engi-\\nneers at both startups and large companies to graduate students and hobbyists. Keras\\nis used at Google, Netflix, Uber, CERN, Yelp, Square, and hundreds of startups work-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 74, 'page_label': '75'}, page_content='ing on a wide range of problems. Keras is  also a popular framework on Kaggle, the\\nmachine-learning competition website, where almost every recent deep-learning com-\\npetition has been won using Keras models.\\nFigure 3.2 Google web search interest for di fferent deep-learning frameworks over time\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 75, 'page_label': '76'}, page_content='62 CHAPTER 3 Getting started with neural networks\\n3.2.1 Keras, TensorFlow, Theano, and CNTK\\nKeras is a model-level library, providing high-level building blocks for developing\\ndeep-learning models. It doesn’t handle low-level operations such as tensor manipula-\\ntion and differentiation. Instead, it relie s on a specialized, well-optimized tensor\\nlibrary to do so, serving as the backend engine of Keras. Rather than choosing a single'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 75, 'page_label': '76'}, page_content='tensor library and tying the implementation of Keras to that library, Keras handles the\\nproblem in a modular way (see figure 3.3); thus several different backend engines can\\nbe plugged seamlessly into Keras. Currentl y, the three existing backend implementa-\\ntions are the TensorFlow backend, the Thea no backend, and the Microsoft Cognitive\\nToolkit (CNTK) backend. In the future, it’s likely that Keras will be extended to work\\nwith even more deep-learning execution engines.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 75, 'page_label': '76'}, page_content='with even more deep-learning execution engines.\\nTensorFlow, CNTK, and Theano are some of the pr imary platforms for deep learning\\ntoday. Theano (http:/ /deeplearning.net/software/theano) is developed by the MILA\\nlab at Université de Montréal, TensorFlow (www.tensorflow.org) is developed by Google,\\nand CNTK ( https:/ /github.com/Microsoft/CNTK) is developed by Microsoft. Any\\npiece of code that you write with Keras can be run with any of these backends without'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 75, 'page_label': '76'}, page_content='having to change anything in the code: you can seamlessly switch between the two\\nduring development, which often proves useful—for instance, if one of these backends\\nproves to be faster for a specific task. We recommend using the TensorFlow backend as\\nthe default for most of your deep-learning needs, because it’s the most widely adopted,\\nscalable, and production ready.\\n Via TensorFlow (or Theano, or CNTK), Keras is able to run seamlessly on both'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 75, 'page_label': '76'}, page_content='CPUs and GPUs. When running on CPU, TensorFlow is itself wrapping a low-level\\nlibrary for tensor operations called Eigen ( http:/ /eigen.tuxfamily.org). On GPU,\\nTensorFlow wraps a library of  well-optimized deep-learn ing operations called the\\nNVIDIA CUDA Deep Neural Network library (cuDNN). \\n3.2.2 Developing with Keras: a quick overview\\nYou’ve already seen one example of a Keras model: the MNIST example. The typical\\nKeras workflow looks just like that example:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 75, 'page_label': '76'}, page_content='Keras workflow looks just like that example:\\n1 Define your training data: input tensors and target tensors.\\n2 Define a network of layers (or model ) that maps your inputs to your targets.\\nFigure 3.3 The deep-learning \\nsoftware and hardware stack\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 76, 'page_label': '77'}, page_content='63Introduction to Keras\\n3 Configure the learning process by choos ing a loss function, an optimizer, and\\nsome metrics to monitor.\\n4 Iterate on your training data by calling the fit() method of your model.\\nThere are two ways to define a model: using the Sequential class (only for linear\\nstacks of layers, which is the most comm on network architectu re by far) or the func-\\ntional API (for directed acyclic grap hs of layers, which lets you build completely arbi-\\ntrary architectures).'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 76, 'page_label': '77'}, page_content=\"trary architectures).\\n As a refresher, here’s a two-layer model defined using the Sequential class (note\\nthat we’re passing the expected shape of the input data to the first layer):\\nfrom keras import models\\nfrom keras import layers\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(32, activation='relu', input_shape=(784,)))\\nmodel.add(layers.Dense(10, activation='softmax'))\\nAnd here’s the same model defined using the functional API:\\ninput_tensor = layers.Input(shape=(784,))\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 76, 'page_label': '77'}, page_content=\"input_tensor = layers.Input(shape=(784,))\\nx = layers.Dense(32, activation='relu')(input_tensor)\\noutput_tensor = layers.Dense(10, activation='softmax')(x)\\nmodel = models.Model(inputs=input_tensor, outputs=output_tensor)\\nWith the functional API, you’re manipulating the data tensors that the model pro-\\ncesses and applying layers to this tensor as if they were functions.\\nNOTE A detailed guide to what you can do with the functional API can be\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 76, 'page_label': '77'}, page_content='found in chapter 7. Until chapte r 7, we’ll only be using the Sequential class\\nin our code examples.\\nOnce your model architecture is define d, it doesn’t matter whether you used a\\nSequential model or the functional API. All of the following steps are the same.\\n The learning process is configured in the compilation step, where you specify the\\noptimizer and loss function(s) that the model should use, as well as the metrics you'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 76, 'page_label': '77'}, page_content=\"want to monitor during training. Here’s an example with a single loss function, which\\nis by far the most common case:\\nfrom keras import optimizers\\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\\nloss='mse',\\nmetrics=['accuracy'])\\nFinally, the learning process consists of pa ssing Numpy arrays of input data (and the\\ncorresponding target data) to the model via the fit() method, similar to what you\\nwould do in Scikit-Learn and several other machine-learning libraries:\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 76, 'page_label': '77'}, page_content='model.fit(input_tensor, target_tensor, batch_size=128, epochs=10)\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 77, 'page_label': '78'}, page_content='64 CHAPTER 3 Getting started with neural networks\\nOver the next few chapters, you’ll build a solid intuition about what type of network\\narchitectures work for different kinds of problems, how to pick the right learning con-\\nfiguration, and how to tweak a model until it  gives the results you want to see. We’ll\\nlook at three basic examples in sections 3.4, 3.5, and 3.6: a two-class classification\\nexample, a many-class classification example, and a regression example. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 78, 'page_label': '79'}, page_content='65Setting up a deep-learning workstation\\n3.3 Setting up a deep-learning workstation\\nBefore you can get started developing deep-learning applications, you need to set up\\nyour workstation. It’s highly  recommended, although not strictly necessary, that you\\nrun deep-learning code on a modern NVIDIA GPU. Some applications—in particular,\\nimage processing with convolutional networ ks and sequence processing with recur-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 78, 'page_label': '79'}, page_content='rent neural networks—will be  excruciatingly slow on CPU, even a fast multicore CPU.\\nAnd even for applications that can realistically be run on CPU, you’ll generally see\\nspeed increase by a factor or 5 or 10 by using a modern GPU. If you don’t want to\\ninstall a GPU on your machine, you can alternat ively consider running your experi-\\nments on an AWS EC2 GPU instance or on Google Cloud Platform. But note that cloud\\nGPU instances can become expensive over time.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 78, 'page_label': '79'}, page_content='GPU instances can become expensive over time.\\n Whether you’re running locally or in the cloud, it’s better to be using a Unix work-\\nstation. Although it’s technically possible to use Keras on Windows (all three Keras\\nbackends support Windows), We don’t recomme nd it. In the installation instructions\\nin appendix A, we’ll consider an Ubuntu machine. If you’re a Windows user, the sim-\\nplest solution to get everything running is to set up an Ubuntu dual boot on your'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 78, 'page_label': '79'}, page_content='machine. It may seem like a hassle, but us ing Ubuntu will save you a lot of time and\\ntrouble in the long run.\\n Note that in order to use Keras, you need to install TensorFlow or CNTK or Theano\\n(or all of them, if you want to be able to  switch back and forth among the three back-\\nends). In this book, we’ll focus on TensorFlow, with some light instructions relative to\\nTheano. We won’t cover CNTK.\\n3.3.1 Jupyter notebooks: the preferred way \\nto run deep-learning experiments'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 78, 'page_label': '79'}, page_content='to run deep-learning experiments\\nJupyter notebooks are a great way to run deep-learning experiments—in particular,\\nthe many code examples in this book. They ’re widely used in the data-science and\\nmachine-learning communities. A notebook is a file generated by the Jupyter Notebook\\napp (https:/ /jupyter.org), which you can edit in your browser. It mixes the ability to\\nexecute Python code with rich text-editing  capabilities for annotating what you’re'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 78, 'page_label': '79'}, page_content='doing. A notebook also allows you to br eak up long experiments into smaller pieces\\nthat can be executed independently, which makes development interactive and means\\nyou don’t have to rerun all of your previous  code if something goes wrong late in an\\nexperiment.\\n We recommend using Jupyter notebooks to get started with Keras, although that\\nisn’t a requirement: you can also run standalone Python scripts or run code from within'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 78, 'page_label': '79'}, page_content='an IDE such as PyCharm. All the code examples in this book are available as open source\\nnotebooks; you can download them from the book’s website at www.manning\\n.com/books/deep-learning-with-python. \\n \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 79, 'page_label': '80'}, page_content='66 CHAPTER 3 Getting started with neural networks\\n3.3.2 Getting Keras running: two options\\nTo get started in practice, we recommend one of the following two options:\\n\\uf0a1 Use the official EC2 Deep Learning AMI ( https:/ /aws.amazon.com/amazon-\\nai/amis), and run Keras experiment s as Jupyter notebooks on EC2. Do this if\\nyou don’t already have a GPU on your local machine. Appendix B provides a\\nstep-by-step guide.\\n\\uf0a1 Install everything from scratch on a lo cal Unix workstation. You can then run'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 79, 'page_label': '80'}, page_content='either local Jupyter notebooks or a regu lar Python codebase. Do this if you\\nalready have a high-end NVIDIA GPU. Appendix A provides an Ubuntu-specific,\\nstep-by-step guide.\\nLet’s take a closer look at some of the compromises involved in picking one option\\nover the other. \\n3.3.3 Running deep-learning jobs  in the cloud: pros and cons\\nIf you don’t already have a GPU that you can use for deep learning (a recent, high-end'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 79, 'page_label': '80'}, page_content='NVIDIA GPU), then running deep-learning experi ments in the cloud is a simple, low-\\ncost way for you to get started without having to buy any additional hardware. If you’re\\nusing Jupyter notebooks, the experience of running in the cloud is no different from\\nrunning locally. As of mid-2017, the cloud o ffering that makes it easiest to get started\\nwith deep learning is definitely \\nAWS EC2. Appendix B provides a step-by-step guide to\\nrunning Jupyter notebooks on a EC2 GPU instance.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 79, 'page_label': '80'}, page_content='running Jupyter notebooks on a EC2 GPU instance.\\n But if you’re a heavy user of deep learni ng, this setup isn’t sustainable in the long\\nterm—or even for more than a few weeks. EC2 instances are expensive: the instance\\ntype recommended in appendix B (the p2.xlarge instance, which won’t provide you\\nwith much power) costs $0.90 per hour as of mid-2017. Meanwhile, a solid consumer-\\nclass GPU will cost you somewher e between $1,000 and $1, 500—a price that has been'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 79, 'page_label': '80'}, page_content='fairly stable over time, even as the specs of these GPUs keep improving. If you’re serious\\nabout deep learning, you should set up a local workstation with one or more GPUs.\\n In short, EC2 is a great way to get started. Yo u could follow the code examples in\\nthis book entirely on an EC2 GPU instance. But if you’re going to be a power user of\\ndeep learning, get your own GPUs. \\n3.3.4 What is the best GPU for deep learning?'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 79, 'page_label': '80'}, page_content='3.3.4 What is the best GPU for deep learning?\\nIf you’re going to buy a GPU, which one should you choose? The first thing to note is\\nthat it must be an NVIDIA GPU. NVIDIA is the only graphics  computing company that\\nhas invested heavily in deep learning so  far, and modern deep-learning frameworks\\ncan only run on NVIDIA cards.\\n As of mid-2017, we recommend the NVIDIA TITAN Xp as the best card on the mar-\\nket for deep learning. For lower budg ets, you may want to consider the GTX 1060. If'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 79, 'page_label': '80'}, page_content='you’re reading these pages in 2018 or later,  take the time to look online for fresher\\nrecommendations, because new models come out every year.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 80, 'page_label': '81'}, page_content='67Setting up a deep-learning workstation\\n From this section onward , we’ll assume that you ha ve access to a machine with\\nKeras and its dependencies installed—preferably with GPU support. Make sure you\\nfinish this step before you proceed. Go th rough the step-by-step guides in the appen-\\ndixes, and look online if you need further help. There is no shortage of tutorials on\\nhow to install Keras and common deep-learning dependencies.\\n We can now dive into practical Keras examples.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 80, 'page_label': '81'}, page_content='We can now dive into practical Keras examples. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 81, 'page_label': '82'}, page_content='68 CHAPTER 3 Getting started with neural networks\\n3.4 Classifying movie reviews: \\na binary classification example\\nTwo-class classification, or binary classifi cation, may be the most  widely applied kind\\nof machine-learning problem. In this example, you’ll learn to classify movie reviews as\\npositive or negative, based on the text content of the reviews.\\n3.4.1 The IMDB dataset\\nYou’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 81, 'page_label': '82'}, page_content='Internet Movie Database. They’re split into  25,000 reviews for training and 25,000\\nreviews for testing, each set consisting of 50% negative and 50% positive reviews.\\n Why use separate training and test sets ? Because you should never test a machine-\\nlearning model on the same data that you used to train it! Just because a model per-\\nforms well on its training data doesn’t mean it will perform well on data it has never'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 81, 'page_label': '82'}, page_content='seen; and what you care about is your model’s performance on new data (because you\\nalready know the labels of your training data—obviously you don’t need your model\\nto predict those). For instance, it’s possible that your model could end up merely mem-\\norizing a mapping between your training samp les and their targets, which would be\\nuseless for the task of pred icting targets for data the model has never seen before.\\nWe’ll go over this point in much more detail in the next chapter.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 81, 'page_label': '82'}, page_content='Just like the \\nMNIST dataset, the IMDB dataset comes packaged with Keras. It has\\nalready been preprocessed: the reviews (seq uences of words) have been turned into\\nsequences of integers, where each integer stands for a specific word in a dictionary.\\n The following code will load the datase t (when you run it the first time, about\\n80 MB of data will be downloaded to your machine).\\nfrom keras.datasets import imdb\\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data('),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 81, 'page_label': '82'}, page_content='num_words=10000)\\nThe argument num_words=10000 means you’ll only keep the top 10,000 most fre-\\nquently occurring words in the training data. Rare words will be discarded. This allows\\nyou to work with vector data of manageable size.\\n The variables train_data and test_data are lists of reviews; each review is a list of\\nword indices (encoding a sequence of words). train_labels and test_labels are\\nlists of 0s and 1s, where 0 stands for negative and 1 stands for positive:\\n>>> train_data[0]'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 81, 'page_label': '82'}, page_content='>>> train_data[0]\\n[1, 14, 22, 16, ... 178, 32]\\n>>> train_labels[0]\\n1\\nListing 3.1 Loading the IMDB dataset\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 82, 'page_label': '83'}, page_content=\"69Classifying movie reviews: a binary classification example\\nBecause you’re restricting yourself to th e top 10,000 most freq uent words, no word\\nindex will exceed 10,000:\\n>>> max([max(sequence) for sequence in train_data])\\n9999\\nFor kicks, here’s how you can quickly deco de one of these reviews back to English\\nwords:\\nword_index = imdb.get_word_index()\\nreverse_word_index = dict(\\n[(value, key) for (key, value) in word_index.items()])\\ndecoded_review = ' '.join(\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 82, 'page_label': '83'}, page_content=\"decoded_review = ' '.join(\\n[reverse_word_index.get(i - 3, '?') for i in train_data[0]])\\n3.4.2 Preparing the data\\nYou can’t feed lists of integers into a neural network. You have to turn your lists into\\ntensors. There are two ways to do that:\\n\\uf0a1 Pad your lists so that they all have the same length, turn them into an integer\\ntensor of shape \\n(samples, word_indices), and then use as the first layer in\\nyour network a layer capable of ha ndling such integer tensors (the Embedding\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 82, 'page_label': '83'}, page_content='layer, which we’ll cover in detail later in the book).\\n\\uf0a1 One-hot encode your lists to turn them into vectors of 0s and 1s. This would\\nmean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vec-\\ntor that would be all 0s except for indices 3 and 5, which would be 1s. Then you\\ncould use as the first layer in your network a Dense layer, capable of handling\\nfloating-point vector data.\\nLet’s go with the latter solution to vector ize the data, which you’ll do manually for'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 82, 'page_label': '83'}, page_content='maximum clarity.\\nimport numpy as np\\ndef vectorize_sequences(sequences, dimension=10000):\\nresults = np.zeros((len(sequences), dimension))\\nfor i, sequence in enumerate(sequences):\\nresults[i, sequence] = 1.\\nreturn results\\nx_train = vectorize_sequences(train_data)\\nx_test = vectorize_sequences(test_data)\\n \\nListing 3.2 Encoding the integer sequences into a binary matrix\\nword_index is a dictionary mapping \\nwords to an integer index.\\nReverses it, mapping'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 82, 'page_label': '83'}, page_content='words to an integer index.\\nReverses it, mapping \\ninteger indices to words Decodes the review. Note that the indices\\nare offset by 3 because 0, 1, and 2 are\\nreserved indices for “padding,” “start of\\nsequence,” and “unknown.”\\nCreates an all-zero matrix \\nof shape (len(sequences), \\ndimension)\\nSets specific indices \\nof results[i] to 1s\\nVectorized training data\\nVectorized test data\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 83, 'page_label': '84'}, page_content=\"70 CHAPTER 3 Getting started with neural networks\\nHere’s what the samples look like now:\\n>>> x_train[0]\\narray([ 0., 1., 1., ..., 0., 0., 0.])\\nYou should also vectorize your labels, which is straightforward:\\ny_train = np.asarray(train_labels).astype('float32')\\ny_test = np.asarray(test_labels).astype('float32')\\nNow the data is ready to be fed into a neural network.\\n3.4.3 Building your network\\nThe input data is vectors, and the labels are scalars (1s and 0s): this is the easiest setup\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 83, 'page_label': '84'}, page_content=\"you’ll ever encounter. A type of networ k that performs well on such a problem is\\na simple stack of fully connected ( Dense) layers with relu activations: Dense(16,\\nactivation='relu').\\n The argument being passed to each Dense l a y e r  ( 1 6 )  i s  t h e  n u m b e r  o f  h i d d e n\\nunits of the layer. A hidden unit is a dimension in the representation space of the layer.\\nYou may remember from ch apter 2 that each such Dense layer with a relu activation\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 83, 'page_label': '84'}, page_content='implements the following chain of tensor operations:\\noutput = relu(dot(W, input) + b)\\nHaving 16 hidden units means the weight matrix W will have shape (input_dimension,\\n16): the dot product with W will project the input data onto a 16-dimensional represen-\\ntation space (and then you’ll add the bias vector b and apply the relu operation). You\\ncan intuitively understand the dimensionali ty of your representation space as “how'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 83, 'page_label': '84'}, page_content='much freedom you’re allowing the network to  have when learning internal represen-\\ntations.” Having more hidden units (a higher-dimensional representation space)\\nallows your network to learn more-complex representations, but it makes the network\\nmore computationally expensive and may lead  to learning unwanted patterns (pat-\\nterns that will improve performance on the training data but not on the test data).\\n There are two key architecture decisions to be made about such a stack of Dense layers:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 83, 'page_label': '84'}, page_content='\\uf0a1 How many layers to use\\n\\uf0a1 How many hidden units to choose for each layer\\nIn chapter 4, you’ll learn formal principles to guide you in making these choices. For\\nthe time being, you’ll have to trust me with the following architecture choice: \\n\\uf0a1 Two intermediate layers with 16 hidden units each \\n\\uf0a1 A third layer that will output the scalar  prediction regarding the sentiment of\\nthe current review\\nThe intermediate layers will use relu as their activation function, and the final layer'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 83, 'page_label': '84'}, page_content='will use a sigmoid activation so as to ou tput a probability (a score between 0 and 1,\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 84, 'page_label': '85'}, page_content='71Classifying movie reviews: a binary classification example\\nindicating how likely the sample is to have the target “1”: how likely the review is to be\\npositive). A relu (rectified linear unit) is a function meant to zero out negative values\\n(see figure 3.4), whereas a sigmoid “squashes” arbitrary values into the [0, 1] interval\\n(see figure 3.5), outputting something that can be interpreted as a probability.\\n \\nFigure 3.4 The rectified linear unit function\\nFigure 3.5 The sigmoid function'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 84, 'page_label': '85'}, page_content='Figure 3.5 The sigmoid function\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 85, 'page_label': '86'}, page_content=\"72 CHAPTER 3 Getting started with neural networks\\nFigure 3.6 shows what the network looks li ke. And here’s the Keras implementation,\\nsimilar to the MNIST example you saw previously.\\nfrom keras import models\\nfrom keras import layers\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(16, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 85, 'page_label': '86'}, page_content=\"model.add(layers.Dense(1, activation='sigmoid'))\\nFinally, you need to choose a loss function and an optimizer. Because you’re facing a\\nbinary classification problem and the output of your network is a probability (you end\\nyour network with a single-unit layer with a sigmoid activation), it’s best to use the\\nListing 3.3 The model definition\\nDense (units=1)\\nOutput\\n(probability)\\nInput\\n(vectorized text)\\nSequential\\nDense (units=16)\\nDense (units=16)\\nFigure 3.6 The three-layer network\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 85, 'page_label': '86'}, page_content='Figure 3.6 The three-layer network\\nWhat are activation functions, and why are they necessary?\\nWithout an activation function like relu (also called a non-linearity), the Dense layer\\nwould consist of two linear operations—a dot product and an addition:\\noutput = dot(W, input) + b\\nSo the layer could only learn linear transformations (affine transformations) of the\\ninput data: the hypothesis space of the layer would be the set of all possible linear'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 85, 'page_label': '86'}, page_content='transformations of the input data into a 16-dimensional space. Such a hypothesis\\nspace is too restricted and wouldn’t benefit from multiple layers of representations,\\nbecause a deep stack of linear layers would still implement a linear operation: adding\\nmore layers wouldn’t extend the hypothesis space.\\nIn order to get access to a much richer hypothesis space that would benefit from\\ndeep representations, you need a non-lin earity, or activation function. relu is the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 85, 'page_label': '86'}, page_content='most popular activation function in deep learning, but there are many other candi-\\ndates, which all come with similarly strange names: prelu, elu, and so on.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 86, 'page_label': '87'}, page_content='73Classifying movie reviews: a binary classification example\\nbinary_crossentropy loss. It isn’t the only viable ch oice: you could use, for instance,\\nmean_squared_error. But crossentropy is usually the best choice when you’re dealing\\nwith models that output probabilities. Crossentropy is a quantity from the field of Infor-\\nmation Theory that measures the distance between probability distributions or, in this\\ncase, between the ground-truth distribution and your predictions.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 86, 'page_label': '87'}, page_content=\"Here’s the step where you configure the model with the rmsprop optimizer and\\nthe binary_crossentropy loss function. Note that yo u’ll also monitor accuracy\\nduring training.\\nmodel.compile(optimizer='rmsprop',\\nloss='binary_crossentropy',\\nmetrics=['accuracy'])\\nYou’re passing your optimizer,  loss function, and metrics as  strings, which is possible\\nbecause rmsprop, binary_crossentropy, and accuracy are packaged as part of Keras.\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 86, 'page_label': '87'}, page_content=\"Sometimes you may want to configure the para meters of your optimizer or pass a cus-\\ntom loss function or metric function. The former can be done by passing an optimizer\\nclass instance as the optimizer argument, as shown in listing 3.5; the latter can be\\ndone by passing function objects as the loss and/or metrics arguments, as shown in\\nlisting 3.6.\\nfrom keras import optimizers\\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\\nloss='binary_crossentropy',\\nmetrics=['accuracy'])\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 86, 'page_label': '87'}, page_content=\"loss='binary_crossentropy',\\nmetrics=['accuracy'])\\nfrom keras import losses\\nfrom keras import metrics\\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\\nloss=losses.binary_crossentropy,\\nmetrics=[metrics.binary_accuracy])\\n3.4.4 Validating your approach\\nIn order to monitor during training the ac curacy of the model on data it has never\\nseen before, you’ll create a validation set by setting apart 10,0 00 samples from the\\noriginal training data.\\nx_val = x_train[:10000]\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 86, 'page_label': '87'}, page_content='original training data.\\nx_val = x_train[:10000]\\npartial_x_train = x_train[10000:]\\nListing 3.4 Compiling the model\\nListing 3.5 Configuring the optimizer\\nListing 3.6 Using custom losses and metrics\\nListing 3.7 Setting aside a validation set\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 87, 'page_label': '88'}, page_content=\"74 CHAPTER 3 Getting started with neural networks\\ny_val = y_train[:10000]\\npartial_y_train = y_train[10000:]\\nYou’ll now train the model for 20 epochs (20 iterations over all samples in the\\nx_train and y_train tensors), in mini-batches of 512 samples. At the same time,\\nyou’ll monitor loss and accuracy on the 10,000 samples that you set apart. You do so by\\npassing the validation data as the validation_data argument.\\nmodel.compile(optimizer='rmsprop',\\nloss='binary_crossentropy',\\nmetrics=['acc'])\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 87, 'page_label': '88'}, page_content=\"loss='binary_crossentropy',\\nmetrics=['acc'])\\nhistory = model.fit(partial_x_train,\\npartial_y_train,\\nepochs=20,\\nbatch_size=512,\\nvalidation_data=(x_val, y_val))\\nOn CPU, this will take less than 2 seconds per epoch—training is over in 20 seconds.\\nAt the end of every epoch, there is a slight pause as the model computes its loss and\\naccuracy on the 10,000 samples of the validation data.\\n Note that the call to \\nmodel.fit() returns a History object. This object has a mem-\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 87, 'page_label': '88'}, page_content=\"ber history, which is a dictionary containing data about everything that happened\\nduring training. Let’s look at it:\\n>>> history_dict = history.history\\n>>> history_dict.keys()\\n[u'acc', u'loss', u'val_acc', u'val_loss']\\nThe dictionary contains four entries: one per metric that was being monitored during\\ntraining and during validation. In the following two listing, let’s use Matplotlib to plot\\nthe training and validation loss side by side (see figure 3.7), as well as the training and\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 87, 'page_label': '88'}, page_content=\"validation accuracy (see figure 3.8). Note that your own results may vary slightly due to\\na different random initialization of your network.\\nimport matplotlib.pyplot as plt\\nhistory_dict = history.history\\nloss_values = history_dict['loss']\\nval_loss_values = history_dict['val_loss']\\nepochs = range(1, len(acc) + 1)\\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\\nplt.title('Training and validation loss')\\nplt.xlabel('Epochs')\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 87, 'page_label': '88'}, page_content=\"plt.xlabel('Epochs')\\nplt.ylabel('Loss')\\nplt.legend()\\nplt.show()\\nListing 3.8 Training your model\\nListing 3.9 Plotting the training and validation loss\\n“bo” is for \\n“blue dot.”\\n“b” is for “solid\\nblue line.”\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 88, 'page_label': '89'}, page_content=\"75Classifying movie reviews: a binary classification example\\nplt.clf()\\nacc_values = history_dict['acc']\\nval_acc_values = history_dict['val_acc']\\nplt.plot(epochs, acc, 'bo', label='Training acc')\\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\\nplt.title('Training and validation accuracy')\\nplt.xlabel('Epochs')\\nplt.ylabel('Loss')\\nplt.legend()\\nplt.show()\\nListing 3.10 Plotting the training and validation accuracy\\nFigure 3.7 Training and validation loss\\nClears the figure\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 88, 'page_label': '89'}, page_content='Clears the figure\\nFigure 3.8 Training and validation accuracy\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 89, 'page_label': '90'}, page_content='76 CHAPTER 3 Getting started with neural networks\\nAs you can see, the training loss decreases with every epoch, and the training accuracy\\nincreases with every epoch. That’s what you would expect when running gradient-\\ndescent optimization—the quantity you’re trying to minimize should be less with\\nevery iteration. But that isn’t the case for the validation loss and accuracy: they seem to\\npeak at the fourth epoch. This is an exam ple of what we warned against earlier: a'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 89, 'page_label': '90'}, page_content='model that performs better on the training data isn’t necessarily a model that will do\\nbetter on data it has never seen before. In precise terms, what you’re seeing is overfit-\\nting: after the second epoch, you’re overoptimizing on the training data, and you end\\nup learning representations that are specific to the training data and don’t generalize\\nto data outside of the training set.\\n In this case, to prevent overfitting, yo u could stop training after three epochs. In'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 89, 'page_label': '90'}, page_content=\"general, you can use a range of techniques to mitigate overfitting, which we’ll cover in\\nchapter 4.\\n Let’s train a new network from scratch for four epochs and then evaluate it on the\\ntest data.\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(16, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='rmsprop',\\nloss='binary_crossentropy',\\nmetrics=['accuracy'])\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 89, 'page_label': '90'}, page_content=\"loss='binary_crossentropy',\\nmetrics=['accuracy'])\\nmodel.fit(x_train, y_train, epochs=4, batch_size=512)\\nresults = model.evaluate(x_test, y_test)\\nThe final results are as follows:\\n>>> results\\n[0.2929924130630493, 0.88327999999999995]\\nThis fairly naive approach achieves an  accuracy of 88%. With state-of-the-art\\napproaches, you should be able to get close to 95%.\\n3.4.5 Using a trained network to generate predictions on new data\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 89, 'page_label': '90'}, page_content='After having trained a network, you’ll want to use it in a practical setting. You can gen-\\nerate the likelihood of reviews being positive by using the predict method:\\n>>> model.predict(x_test)\\narray([[ 0.98006207]\\n[ 0.99758697]\\n[ 0.99975556]\\n...,\\n[ 0.82167041]\\n[ 0.02885115]\\n[ 0.65371346]], dtype=float32)\\nListing 3.11 Retraining a model from scratch\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 90, 'page_label': '91'}, page_content='77Classifying movie reviews: a binary classification example\\nAs you can see, the network is confident for some samples (0.99 or more, or 0.01 or\\nless) but less confident for others (0.6, 0.4).\\n3.4.6 Further experiments\\nThe following experiments will help convince you that the architecture choices you’ve\\nmade are all fairly reasonable, although there’s still room for improvement:\\n\\uf0a1 You used two hidden layers. Try using one or three hidden layers, and see how'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 90, 'page_label': '91'}, page_content='doing so affects validation and test accuracy.\\n\\uf0a1 Try using layers with more hidden units or fewer hidden units: 32 units, 64 units,\\nand so on.\\n\\uf0a1 Try using the mse loss function instead of binary_crossentropy.\\n\\uf0a1 Try using the tanh activation (an activation that was popular in the early days of\\nneural networks) instead of relu.\\n3.4.7 Wrapping up\\nHere’s what you should take away from this example:\\n\\uf0a1 You usually need to do quite a bit of preprocessing on your raw data in order to'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 90, 'page_label': '91'}, page_content='be able to feed it—as tensors—into a neural network. Sequences of words can\\nbe encoded as binary vectors, but there are other encoding options, too.\\n\\uf0a1 Stacks of Dense layers with relu activations can solve a wide range of problems\\n(including sentiment classification), and you’ll likely use them frequently.\\n\\uf0a1 In a binary classification  problem (two output cla sses), your network should\\nend with a Dense layer with one unit and a sigmoid activation: the output of'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 90, 'page_label': '91'}, page_content='your network should be a scalar between 0 and 1, encoding a probability.\\n\\uf0a1 With such a scalar sigmoid output on a binary classi fication problem, the loss\\nfunction you should use is binary_crossentropy.\\n\\uf0a1 The rmsprop optimizer is generally a good enough choice, whatever your prob-\\nlem. That’s one less thing for you to worry about.\\n\\uf0a1 As they get better on their training data, neural networks eventually start over-\\nfitting and end up obtaining increasingly  worse results on data they’ve never'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 90, 'page_label': '91'}, page_content='seen before. Be sure to always monitor performance on data that is outside of\\nthe training set. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 91, 'page_label': '92'}, page_content='78 CHAPTER 3 Getting started with neural networks\\n3.5 Classifying newswires: \\na multiclass classification example\\nIn the previous section, you saw how to classify vector inputs into two mutually exclu-\\nsive classes using a densely connected neural ne twork. But what happens when you\\nhave more than two classes?\\n In this section, you’ll build a network to classify Reuters newswires into 46 mutually\\nexclusive topics. Because you have many cl asses, this problem is an instance of multi-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 91, 'page_label': '92'}, page_content='class classification; and because each data point should be classified into only one cate-\\ngory, the problem is more specifically an instance of single-label, multiclass classification.\\nIf each data point could belong to multiple  categories (in this case, topics), you’d be\\nfacing a multilabel, multiclass classification problem.\\n3.5.1 The Reuters dataset\\nYou’ll work with the Reuters dataset, a set of short newswires and their topics, published'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 91, 'page_label': '92'}, page_content='by Reuters in 1986. It’s a simple, widely us ed toy dataset for text classification. There\\nare 46 different topics; some topics are mo re represented than others, but each topic\\nhas at least 10 examples in the training set.\\n Like IMDB and MNIST, the Reuters dataset comes packag ed as part of Keras. Let’s\\ntake a look.\\nfrom keras.datasets import reuters\\n(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\\nnum_words=10000)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 91, 'page_label': '92'}, page_content='num_words=10000)\\nAs with the IMDB dataset, the argument num_words=10000 restricts the data to the\\n10,000 most frequently occurring words found in the data.\\n You have 8,982 training examples and 2,246 test examples:\\n>>> len(train_data)\\n8982\\n>>> len(test_data)\\n2246\\nAs with the IMDB reviews, each example is a list of integers (word indices):\\n>>> train_data[10]\\n[1, 245, 273, 207, 156, 53, 74, 160, 26, 14, 46, 296, 26, 39, 74, 2979,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 91, 'page_label': '92'}, page_content=\"3554, 14, 46, 4689, 4329, 86, 61, 3499, 4795, 14, 61, 451, 4329, 17, 12]\\nHere’s how you can decode it back to words, in case you’re curious.\\nword_index = reuters.get_word_index()\\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\\ndecoded_newswire='' . join([reverse_word_index.get(i - 3, '?') for i in\\ntrain_data[0]])\\nListing 3.12 Loading the Reuters dataset\\nListing 3.13 Decoding newswires back to text\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 91, 'page_label': '92'}, page_content='Listing 3.13 Decoding newswires back to text\\nNote that the indices are offset by 3 because 0, 1, and 2 are reserved \\nindices for “padding,” “start of sequence,” and “unknown.”\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 92, 'page_label': '93'}, page_content='79Classifying newswires: a multiclass classification example\\nThe label associated with an example is an integer between 0 and 45—a topic index:\\n>>> train_labels[10]\\n3\\n3.5.2 Preparing the data\\nYou can vectorize the data with the exact same code as in the previous example.\\nimport numpy as np\\ndef vectorize_sequences(sequences, dimension=10000):\\nresults = np.zeros((len(sequences), dimension))\\nfor i, sequence in enumerate(sequences):\\nresults[i, sequence] = 1.\\nreturn results'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 92, 'page_label': '93'}, page_content='results[i, sequence] = 1.\\nreturn results\\nx_train = vectorize_sequences(train_data)\\nx_test = vectorize_sequences(test_data)\\nTo vectorize the labels, there are two possibilities: you can cast the label list as an inte-\\nger tensor, or you can use one-hot encoding . One-hot encoding is a widely used for-\\nmat for categorical data, also called categorical encoding . For a more detailed\\nexplanation of one-hot encoding, see sectio n 6.1. In this case, one-hot encoding of'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 92, 'page_label': '93'}, page_content='the labels consists of embedding each label as an all-zero vector with a 1 in the place of\\nthe label index. Here’s an example:\\ndef to_one_hot(labels, dimension=46):\\nresults = np.zeros((len(labels), dimension))\\nfor i, label in enumerate(labels):\\nresults[i, label] = 1.\\nreturn results\\none_hot_train_labels = to_one_hot(train_labels)\\none_hot_test_labels = to_one_hot(test_labels)\\nNote that there is a built-in way to do this in Keras, which you’ve already seen in action\\nin the MNIST example:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 92, 'page_label': '93'}, page_content='in the MNIST example:\\nfrom keras.utils.np_utils import to_categorical\\none_hot_train_labels = to_categorical(train_labels)\\none_hot_test_labels = to_categorical(test_labels)\\n3.5.3 Building your network\\nThis topic-classification prob lem looks similar to the previous movie-review classifica-\\ntion problem: in both cases, you’re trying to classify short snippets of text. But there is\\na new constraint here: the number of outp ut classes has gone from 2 to 46. The'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 92, 'page_label': '93'}, page_content='dimensionality of the output space is much larger.\\n In a stack of Dense layers like that you’ve been using, each layer can only access infor-\\nmation present in the output of the previous layer. If one layer drops some information\\nListing 3.14 Encoding the data\\nVectorized training data\\nVectorized test data\\nVectorized training labels\\nVectorized test labels\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 93, 'page_label': '94'}, page_content='80 CHAPTER 3 Getting started with neural networks\\nrelevant to the classification problem, this information can never be recovered by later\\nlayers: each layer can potentially become an information bottleneck. In the previous\\nexample, you used 16-dimensional intermediate layers, but a 16-dimensional space may\\nbe too limited to learn to separate 46 different classes: such small layers may act as infor-\\nmation bottlenecks, permanently dropping relevant information.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 93, 'page_label': '94'}, page_content=\"For this reason you’ll use larger layers. Let’s go with 64 units.\\nfrom keras import models\\nfrom keras import layers\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(64, activation='relu'))\\nmodel.add(layers.Dense(46, activation='softmax'))\\nThere are two other things you should note about this architecture:\\n\\uf0a1 You end the network with a Dense layer of size 46. This means for each input\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 93, 'page_label': '94'}, page_content='sample, the network will output a 46-dimensional vector. Each entry in this vec-\\ntor (each dimension) will encode a different output class.\\n\\uf0a1 The last layer uses a softmax activation. You saw this pattern in the MNIST\\nexample. It means the network will output a probability distribution over the 46\\ndifferent output classes—for every input sample, the network will produce a 46-\\ndimensional output vector, where output[i] is the probability that the sample'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 93, 'page_label': '94'}, page_content=\"belongs to class i. The 46 scores will sum to 1.\\nThe best loss function to use in this case is categorical_crossentropy. It measures\\nthe distance between two probability distri butions: here, between the probability dis-\\ntribution output by the network and the true distribution of the labels. By minimizing\\nthe distance between these two distribution s, you train the network to output some-\\nthing as close as possible to the true labels.\\nmodel.compile(optimizer='rmsprop',\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 93, 'page_label': '94'}, page_content=\"model.compile(optimizer='rmsprop',\\nloss='categorical_crossentropy',\\nmetrics=['accuracy'])\\n3.5.4 Validating your approach\\nLet’s set apart 1,000 samples in the training data to use as a validation set.\\nx_val = x_train[:1000]\\npartial_x_train = x_train[1000:]\\ny_val = one_hot_train_labels[:1000]\\npartial_y_train = one_hot_train_labels[1000:]\\nListing 3.15 Model definition\\nListing 3.16 Compiling the model\\nListing 3.17 Setting aside a validation set\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 94, 'page_label': '95'}, page_content=\"81Classifying newswires: a multiclass classification example\\nNow, let’s train the network for 20 epochs.\\nhistory = model.fit(partial_x_train,\\npartial_y_train,\\nepochs=20,\\nbatch_size=512,\\nvalidation_data=(x_val, y_val))\\nAnd finally, let’s display its loss and accuracy curves (see figures 3.9 and 3.10).\\nimport matplotlib.pyplot as plt\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\nepochs = range(1, len(loss) + 1)\\nplt.plot(epochs, loss, 'bo', label='Training loss')\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 94, 'page_label': '95'}, page_content=\"plt.plot(epochs, val_loss, 'b', label='Validation loss')\\nplt.title('Training and validation loss')\\nplt.xlabel('Epochs')\\nplt.ylabel('Loss')\\nplt.legend()\\nplt.show()\\nplt.clf()\\nacc = history.history['acc']\\nval_acc = history.history['val_acc']\\nplt.plot(epochs, acc, 'bo', label='Training acc')\\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\\nplt.title('Training and validation accuracy')\\nplt.xlabel('Epochs')\\nplt.ylabel('Loss')\\nplt.legend()\\nplt.show()\\nListing 3.18 Training the model\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 94, 'page_label': '95'}, page_content='plt.show()\\nListing 3.18 Training the model\\nListing 3.19 Plotting the training and validation loss\\nListing 3.20 Plotting the training and validation accuracy\\nClears the figure\\nFigure 3.9 Training and validation loss\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 95, 'page_label': '96'}, page_content=\"82 CHAPTER 3 Getting started with neural networks\\nThe network begins to overfit after nine  epochs. Let’s train a new network from\\nscratch for nine epochs and then evaluate it on the test set.\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(64, activation='relu'))\\nmodel.add(layers.Dense(46, activation='softmax'))\\nmodel.compile(optimizer='rmsprop',\\nloss='categorical_crossentropy',\\nmetrics=['accuracy'])\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 95, 'page_label': '96'}, page_content=\"metrics=['accuracy'])\\nmodel.fit(partial_x_train,\\npartial_y_train,\\nepochs=9,\\nbatch_size=512,\\nvalidation_data=(x_val, y_val))\\nresults = model.evaluate(x_test, one_hot_test_labels)\\nHere are the final results:\\n>>> results\\n[0.9565213431445807, 0.79697239536954589]\\nThis approach reaches an accuracy of ~80 %. With a balanced binary classification\\nproblem, the accuracy reached by a purely random classifier would be 50%. But in\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 95, 'page_label': '96'}, page_content='this case it’s closer to 19%, so the results seem pretty good, at least when compared to\\na random baseline:\\n>>> import copy\\n>>> test_labels_copy = copy.copy(test_labels)\\n>>> np.random.shuffle(test_labels_copy)\\n>>> hits_array = np.array(test_labels) == np.array(test_labels_copy)\\n>>> float(np.sum(hits_array)) / len(test_labels)\\n0.18655387355298308\\nListing 3.21 Retraining a model from scratch\\nFigure 3.10 Training and \\nvalidation accuracy\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 96, 'page_label': '97'}, page_content='83Classifying newswires: a multiclass classification example\\n3.5.5 Generating predictions on new data\\nYou can verify that the predict method of the model instance returns a probability\\ndistribution over all 46 topics. Let’s generate topic predictions for all of the test data.\\npredictions = model.predict(x_test)\\nEach entry in predictions is a vector of length 46:\\n>>> predictions[0].shape\\n(46,)\\nThe coefficients in this vector sum to 1:\\n>>> np.sum(predictions[0])\\n1.0'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 96, 'page_label': '97'}, page_content='>>> np.sum(predictions[0])\\n1.0\\nThe largest entry is the predicted class—the class with the highest probability:\\n>>> np.argmax(predictions[0])\\n4\\n3.5.6 A different way to handle the labels and the loss\\nWe mentioned earlier that another way to en code the labels would be to cast them as\\nan integer tensor, like this:\\ny_train = np.array(train_labels)\\ny_test = np.array(test_labels)\\nThe only thing this approach would change is the choice of the loss function. The loss'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 96, 'page_label': '97'}, page_content=\"function used in listing 3.21, categorical_crossentropy, expects the labels to follow\\na categorical encoding. With integer labels, you should use sparse_categorical_\\ncrossentropy:\\nmodel.compile(optimizer='rmsprop',\\nloss='sparse_categorical_crossentropy',\\nmetrics=['acc'])\\nThis new loss function is still mathematically the same as categorical_crossentropy;\\nit just has a different interface.\\n3.5.7 The importance of having su fficiently large intermediate layers\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 96, 'page_label': '97'}, page_content='We mentioned earlier that because the final outputs are 46-dimensional, you should\\navoid intermediate layers with many fewer than 46 hidden units. Now let’s see what\\nhappens when you introduce an information bottleneck by having intermediate layers\\nthat are significantly less than 46-dimensional: for example, 4-dimensional.\\nListing 3.22 Generating predictions for new data\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 97, 'page_label': '98'}, page_content=\"84 CHAPTER 3 Getting started with neural networks\\n \\nmodel = models.Sequential()\\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(4, activation='relu'))\\nmodel.add(layers.Dense(46, activation='softmax'))\\nmodel.compile(optimizer='rmsprop',\\nloss='categorical_crossentropy',\\nmetrics=['accuracy'])\\nmodel.fit(partial_x_train,\\npartial_y_train,\\nepochs=20,\\nbatch_size=128,\\nvalidation_data=(x_val, y_val))\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 97, 'page_label': '98'}, page_content='batch_size=128,\\nvalidation_data=(x_val, y_val))\\nThe network now peaks at ~71% validation accuracy, an 8% absolute drop. This drop\\nis mostly due to the fact that you’re trying to compress a lot of information (enough\\ninformation to recover the separation hyperplanes of 46 classes) into an intermediate\\nspace that is too low-dimensional.  The network is able to cram most of the necessary\\ninformation into these eight-dimensional representations, but not all of it.\\n3.5.8 Further experiments'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 97, 'page_label': '98'}, page_content='3.5.8 Further experiments\\n\\uf0a1 Try using larger or smaller layers: 32 units, 128 units, and so on.\\n\\uf0a1 You used two hidden layers. Now try using a single hidden layer, or three hid-\\nden layers.\\n3.5.9 Wrapping up\\nHere’s what you should take away from this example:\\n\\uf0a1 If you’re trying to classify data points among N classes, your network should end\\nwith a Dense layer of size N.\\n\\uf0a1 In a single-label, multiclass classifica tion problem, your network should end'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 97, 'page_label': '98'}, page_content='with a softmax activation so that it will output a probability distribution over the\\nN output classes.\\n\\uf0a1 Categorical crossentropy is  almost always the loss fu nction you should use for\\nsuch problems. It minimizes the distance  between the probability distributions\\noutput by the network and the true distribution of the targets.\\n\\uf0a1 There are two ways to handle labels in multiclass classification:\\n– Encoding the labels via categorical encoding (also known as one-hot encod-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 97, 'page_label': '98'}, page_content='ing) and using categorical_crossentropy as a loss function\\n– Encoding the labels as integers and using the sparse_categorical_crossentropy\\nloss function\\n\\uf0a1 If you need to classify data into a larg e number of categories, you should avoid\\ncreating information bottlenecks in your network due to intermediate layers\\nthat are too small. \\nListing 3.23 A model with an information bottleneck\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 98, 'page_label': '99'}, page_content='85Predicting house prices: a regression example\\n3.6 Predicting house prices: a regression example\\nThe two previous examples we re considered classificati on problems, where the goal\\nwas to predict a single discrete label of an  input data point. Another common type of\\nmachine-learning problem is regression, which consists of predicting a continuous\\nvalue instead of a discrete label: for inst ance, predicting the temperature tomorrow,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 98, 'page_label': '99'}, page_content='given meteorological data; or predicting the time that a software project will take to\\ncomplete, given its specifications.\\nNOTE Don’t confuse regression and the algorithm logistic regression . Con-\\nfusingly, logistic regression isn’t a re gression algo rithm—it’s a classification\\nalgorithm.\\n3.6.1 The Boston Housing Price dataset\\nYou’ll attempt to predict the median price of  homes in a given Boston suburb in the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 98, 'page_label': '99'}, page_content='mid-1970s, given data points about the suburb  at the time, such as the crime rate, the\\nlocal property tax rate, and so on. The data set you’ll use has an interesting difference\\nfrom the two previous examples. It has relatively few data points: only 506, split\\nbetween 404 training samples and 102 test samples. And each feature in the input data\\n(for example, the crime rate) has a different  scale. For instance, some values are pro-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 98, 'page_label': '99'}, page_content='portions, which take values between 0 and 1; others take values between 1 and 12, oth-\\ners between 0 and 100, and so on.\\nfrom keras.datasets import boston_housing\\n(train_data, train_targets), (test_data, test_targets) =\\n➥ boston_housing.load_data()\\nLet’s look at the data:\\n>>> train_data.shape\\n(404, 13)\\n>>> test_data.shape\\n(102, 13)\\nAs you can see, you have 404 training sa mples and 102 test samples, each with 13'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 98, 'page_label': '99'}, page_content='numerical features, such as per capita crime rate, average number of rooms per dwell-\\ning, accessibility to highways, and so on.\\n The targets are the median values of owner-occupied homes, in thousands of\\ndollars:\\n>>> train_targets\\n[ 15.2, 42.3, 50. ... 19.4, 19.4, 29.1]\\nThe prices are typically between $10,000 an d $50,000. If that sounds cheap, remem-\\nber that this was the mid-1970s, and these prices aren’t adjusted for inflation.\\nListing 3.24 Loading the Boston housing dataset'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 98, 'page_label': '99'}, page_content='Listing 3.24 Loading the Boston housing dataset\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 99, 'page_label': '100'}, page_content='86 CHAPTER 3 Getting started with neural networks\\n3.6.2 Preparing the data\\nIt would be problematic to feed into a neural network values that all take wildly differ-\\nent ranges. The network might be able to automatically adapt to such heterogeneous\\ndata, but it would definitely make learning more difficult. A widespread best practice\\nto deal with such data is to do feature-wise normalization: for each feature in the input'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 99, 'page_label': '100'}, page_content='data (a column in the input data matrix),  you subtract the mean of the feature and\\ndivide by the standard deviation, so that the feature is centered around 0 and has a\\nunit standard deviation. This is easily done in Numpy.\\nmean = train_data.mean(axis=0)\\ntrain_data -= mean\\nstd = train_data.std(axis=0)\\ntrain_data /= std\\ntest_data -= mean\\ntest_data /= std\\nNote that the quantities us ed for normalizing the test data are computed using the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 99, 'page_label': '100'}, page_content='training data. You should never use in your  workflow any quantity computed on the\\ntest data, even for something as simple as data normalization.\\n3.6.3 Building your network\\nBecause so few samples are available, you’ll  use a very small network with two hidden\\nlayers, each with 64 units. In general, the less training data you have, the worse overfit-\\nting will be, and using a small network is one way to mitigate overfitting.\\nfrom keras import models\\nfrom keras import layers\\ndef build_model():'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 99, 'page_label': '100'}, page_content=\"from keras import layers\\ndef build_model():\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(64, activation='relu',\\ninput_shape=(train_data.shape[1],)))\\nmodel.add(layers.Dense(64, activation='relu'))\\nmodel.add(layers.Dense(1))\\nmodel.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\\nreturn model\\nThe network ends with a single unit and no activation (it will be a linear layer). This is\\na typical setup for scalar regression (a regression where you’re trying to predict a single\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 99, 'page_label': '100'}, page_content='continuous value). Applying an activation function would constrain the range the out-\\nput can take; for instance, if you applied a sigmoid activation function to the last layer,\\nthe network could only learn to predict values between 0 and 1. Here, because the last\\nlayer is purely linear, the network is free to learn to predict values in any range.\\nListing 3.25 Normalizing the data\\nListing 3.26 Model definition\\nBecause you’ll need to instantiate \\nthe same model multiple times, you'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 99, 'page_label': '100'}, page_content='the same model multiple times, you \\nuse a function to construct it.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 100, 'page_label': '101'}, page_content='87Predicting house prices: a regression example\\n Note that you compile the network with the mse loss function—mean squared error,\\nthe square of the difference between the predictions and the targets. This is a widely\\nused loss function for regression problems.\\n You’re also monitoring a new metric during training: mean absolute error (MAE). It’s\\nthe absolute value of the difference betw een the predictions and the targets. For'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 100, 'page_label': '101'}, page_content='instance, an MAE of 0.5 on this problem would mean your predictions are off by $500\\non average.\\n3.6.4 Validating your approach using K-fold validation\\nTo evaluate your network while you keep adjusting its parameters (such as the number\\nof epochs used for training), you could spli t the data into a training set and a valida-\\ntion set, as you did in the previous examples. But because you have so few data points,\\nthe validation set would end up being very small (for instance, about 100 examples).'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 100, 'page_label': '101'}, page_content='As a consequence, the validation scores mi ght change a lot depending on which data\\npoints you chose to use for validation and which you chose for training: the validation\\nscores might have a high variance with regard to the validation split. This would pre-\\nvent you from reliably evaluating your model.\\n The best practice in such situations is to use K-fold cross-validation (see figure 3.11).\\nIt consists of splitting the available data into K partitions (typically K = 4 or 5), instanti-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 100, 'page_label': '101'}, page_content='ating K identical models, and training each one on K – 1 partitions while evaluating on\\nthe remaining partition. The validation score for the model used is then the average of\\nthe K validation scores obtained. In terms of code, this is straightforward.\\nimport numpy as np\\nk=4\\nnum_val_samples = len(train_data) // k\\nnum_epochs = 100\\nall_scores = []\\nListing 3.27 K-fold validation\\nData split into 3 partitions\\nValidation Training Training Validation\\nscore #1Fold 1'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 100, 'page_label': '101'}, page_content='score #1Fold 1\\nValidation Validation Training Validation\\nscore #2\\nFinal score:\\naverageFold 2\\nValidation Training Validation Validation\\nscore #3Fold 3\\nFigure 3.11 3-fold cross-validation\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 101, 'page_label': '102'}, page_content=\"88 CHAPTER 3 Getting started with neural networks\\nfor i in range(k):\\nprint('processing fold #', i)\\nval_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\\nval_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\\npartial_train_data = np.concatenate(\\n[train_data[:i * num_val_samples],\\ntrain_data[(i + 1) * num_val_samples:]],\\naxis=0)\\npartial_train_targets = np.concatenate(\\n[train_targets[:i * num_val_samples],\\ntrain_targets[(i + 1) * num_val_samples:]],\\naxis=0)\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 101, 'page_label': '102'}, page_content='axis=0)\\nmodel = build_model()\\nmodel.fit(partial_train_data, partial_train_targets,\\nepochs=num_epochs, batch_size=1, verbose=0)\\nval_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\\nall_scores.append(val_mae)\\nRunning this with num_epochs = 100 yields the following results:\\n>>> all_scores\\n[2.588258957792037, 3.1289568449719116, 3.1856116051248984, 3.0763342615401386]\\n>>> np.mean(all_scores)\\n2.9947904173572462'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 101, 'page_label': '102'}, page_content='>>> np.mean(all_scores)\\n2.9947904173572462\\nThe different runs do indeed show rather different validation scores, from 2.6 to 3.2.\\nThe average (3.0) is a much more reliable metric than any sing le score—that’s the\\nentire point of K-fold cross-validation. In this case, you’re off by $3,000 on average,\\nwhich is significant considering that the prices range from $10,000 to $50,000.\\n Let’s try training the network a bit long er: 500 epochs. To keep a record of how'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 101, 'page_label': '102'}, page_content=\"well the model does at each epoch, you’ll modify the training loop to save the per-\\nepoch validation score log.\\nnum_epochs = 500\\nall_mae_histories = []\\nfor i in range(k):\\nprint('processing fold #', i)\\nval_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\\nval_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\\npartial_train_data = np.concatenate(\\n[train_data[:i * num_val_samples],\\ntrain_data[(i + 1) * num_val_samples:]],\\naxis=0)\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 101, 'page_label': '102'}, page_content='train_data[(i + 1) * num_val_samples:]],\\naxis=0)\\nListing 3.28 Saving the validation logs at each fold\\nPrepares the validation data: \\ndata from partition #k\\nPrepares the training data:\\ndata from all other partitions\\nBuilds the Keras model \\n(already compiled)\\nTrains the model \\n(in silent mode, \\nverbose = 0)\\nEvaluates the model\\non the validation data\\nPrepares the validation data:\\ndata from partition #k\\nPrepares the training \\ndata: data from all \\nother partitions\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 102, 'page_label': '103'}, page_content=\"89Predicting house prices: a regression example\\npartial_train_targets = np.concatenate(\\n[train_targets[:i * num_val_samples],\\ntrain_targets[(i + 1) * num_val_samples:]],\\naxis=0)\\nmodel = build_model()\\nhistory = model.fit(partial_train_data, partial_train_targets,\\nvalidation_data=(val_data, val_targets),\\nepochs=num_epochs, batch_size=1, verbose=0)\\nmae_history = history.history['val_mean_absolute_error']\\nall_mae_histories.append(mae_history)\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 102, 'page_label': '103'}, page_content=\"all_mae_histories.append(mae_history)\\nYou can then compute the average of the per-epoch MAE scores for all folds.\\naverage_mae_history = [\\nnp.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\\nLet’s plot this; see figure 3.12.\\nimport matplotlib.pyplot as plt\\nplt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\\nplt.xlabel('Epochs')\\nplt.ylabel('Validation MAE')\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 102, 'page_label': '103'}, page_content=\"plt.ylabel('Validation MAE')\\nplt.show()\\nIt may be a little difficult to see the plot, due  to scaling issues and relatively high vari-\\nance. Let’s do the following:\\n\\uf0a1 Omit the first 10 data points, which are on a different scale than the rest of the curve.\\n\\uf0a1 Replace each point with an exponential moving average of the previous points,\\nto obtain a smooth curve.\\nListing 3.29 Building the history of successive mean K-fold validation scores\\nListing 3.30 Plotting validation scores\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 102, 'page_label': '103'}, page_content='Listing 3.30 Plotting validation scores\\nBuilds the Keras model \\n(already compiled)\\nTrains the model\\n(in silent mode, verbose=0)\\nFigure 3.12 Validation \\nMAE by epoch\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 103, 'page_label': '104'}, page_content=\"90 CHAPTER 3 Getting started with neural networks\\nThe result is shown in figure 3.13.\\ndef smooth_curve(points, factor=0.9):\\nsmoothed_points = []\\nfor point in points:\\nif smoothed_points:\\nprevious = smoothed_points[-1]\\nsmoothed_points.append(previous * factor + point * (1 - factor))\\nelse:\\nsmoothed_points.append(point)\\nreturn smoothed_points\\nsmooth_mae_history = smooth_curve(average_mae_history[10:])\\nplt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\\nplt.xlabel('Epochs')\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 103, 'page_label': '104'}, page_content=\"plt.xlabel('Epochs')\\nplt.ylabel('Validation MAE')\\nplt.show()\\nAccording to this plot, validation MAE stops improving signific antly after 80 epochs.\\nPast that point, you start overfitting.\\n Once you’re finished tuning other para meters of the model (in addition to the\\nnumber of epochs, you could also adjust the size of the hidden layers), you can train a\\nfinal production model on all of the training data, with the best parameters, and then\\nlook at its performance on the test data.\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 103, 'page_label': '104'}, page_content='look at its performance on the test data.\\nmodel = build_model()\\nmodel.fit(train_data, train_targets,\\nepochs=80, batch_size=16, verbose=0)\\ntest_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\\nListing 3.31 Plotting validation scores, excluding the first 10 data points\\nListing 3.32 Training the final model\\nFigure 3.13 Validation MAE \\nby epoch, excluding the first \\n10 data points\\nGets a fresh, compiled model\\nTrains it on the entirety of the data\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 104, 'page_label': '105'}, page_content='91Predicting house prices: a regression example\\nHere’s the final result:\\n>>> test_mae_score\\n2.5532484335057877\\nYou’re still off by about $2,550.\\n3.6.5 Wrapping up\\nHere’s what you should take away from this example:\\n\\uf0a1 Regression is done using different loss fu nctions than what we used for classifi-\\ncation. Mean squared error ( MSE) is a loss function co mmonly used for regres-\\nsion.\\n\\uf0a1 Similarly, evaluation metrics to be used for regression differ from those used for'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 104, 'page_label': '105'}, page_content='classification; naturally, the concept of accuracy doesn’t apply for regression. A\\ncommon regression metric is mean absolute error (MAE).\\n\\uf0a1 When features in the input data have va lues in different ranges, each feature\\nshould be scaled independently as a preprocessing step.\\n\\uf0a1 When there is little data available, using K-fold validation is a great way to reli-\\nably evaluate a model.\\n\\uf0a1 When little training data is available, it’s preferable to use a small network with'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 104, 'page_label': '105'}, page_content='few hidden layers (typically only one or two), in order to avoid severe overfitting. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 105, 'page_label': '106'}, page_content='92 CHAPTER 3 Getting started with neural networks\\nChapter summary\\n\\uf0a1 You’re now able to handle the mo st common kinds of machine-learning\\ntasks on vector data: binary classification, multiclass classification, and sca-\\nlar regression. The “Wrapping up” sections earlier in the chapter summa-\\nrize the important points you’ve learned regarding these types of tasks.\\n\\uf0a1 You’ll usually need to preprocess raw data before feeding it into a neural\\nnetwork.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 105, 'page_label': '106'}, page_content='network.\\n\\uf0a1 W h e n  y o u r  d a t a  h a s  f e a t u r e s  w i t h  different ranges, scale each feature\\nindependently as part of preprocessing.\\n\\uf0a1 As training progresses, neural networ ks eventually begin to overfit and\\nobtain worse results on never-before-seen data.\\n\\uf0a1 If you don’t have much training data, use a small network with only one or\\ntwo hidden layers, to avoid severe overfitting.\\n\\uf0a1 If your data is divided into many categories, you may cause information'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 105, 'page_label': '106'}, page_content='bottlenecks if you make the intermediate layers too small.\\n\\uf0a1 Regression uses different loss functi ons and different evaluation metrics\\nthan classification.\\n\\uf0a1 When you’re working with little data, K-fold validation can help reliably\\nevaluate your model.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 106, 'page_label': '107'}, page_content='Fundamentals of\\nmachine learning\\nAfter the three practical examples in chapter 3, you should be starting to feel famil-\\niar with how to approach classification and regression proble ms using neural net-\\nworks, and you’ve witnesse d the central problem of ma chine learning: overfitting.\\nThis chapter will formalize some of yo ur new intuition into  a solid conceptual\\nframework for attacking and solving deep-l earning problems. We’ll consolidate all'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 106, 'page_label': '107'}, page_content='of these concepts—model evaluation, data preprocessing and feature engineering,\\nand tackling overfitting—into a detailed seven-step workflow for tackling any\\nmachine-learning task.\\nThis chapter covers\\n\\uf0a1 Forms of machine learning beyond classification \\nand regression\\n\\uf0a1 Formal evaluation procedures for machine-\\nlearning models\\n\\uf0a1 Preparing data for deep learning\\n\\uf0a1 Feature engineering\\n\\uf0a1 Tackling overfitting\\n\\uf0a1 The universal workflow for approaching machine-\\nlearning problems'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 107, 'page_label': '108'}, page_content='94 CHAPTER 4 Fundamentals of machine learning\\n4.1 Four branches of machine learning\\nIn our previous examples, you’ve become  familiar with three specific types of\\nmachine-learning problems: binary classifi cation, multiclass cla ssification, and scalar\\nregression. All three are instances of supervised learning, where the goal is to learn the\\nrelationship between training inputs and training targets.\\n Supervised learning is just the tip of th e iceberg—machine learning is a vast field'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 107, 'page_label': '108'}, page_content='with a complex subfield taxonomy. Machin e-learning algorithms generally fall into\\nfour broad categories, described in the following sections.\\n4.1.1 Supervised learning\\nThis is by far the most common case. It co nsists of learning to map input data to\\nknown targets (also called annotations), given a set of examples (often annotated by\\nhumans). All four examples you’ve encounte red in this book so far were canonical'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 107, 'page_label': '108'}, page_content='examples of supervised learning. Generally,  almost all applications of deep learning\\nthat are in the spotlight these days belong in this category, such as optical character\\nrecognition, speech recognition, image classification, and language translation.\\n Although supervised learning mostly consists of classification and regression, there\\nare more exotic variants as well, including the following (with examples):\\n\\uf0a1 Sequence generation —Given a picture, predict a ca ption describing it. Sequence'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 107, 'page_label': '108'}, page_content='generation can sometimes be reformulated as a series of classification problems\\n(such as repeatedly predicting a word or token in a sequence).\\n\\uf0a1 Syntax tree prediction —Given a sentence, predict its decomposition into a syntax\\ntree.\\n\\uf0a1 Object detection —Given a picture, draw a boundi ng box around certain objects\\ninside the picture. This can also be expressed as a classification problem (given\\nmany candidate bounding boxes, classify the contents of each one) or as a joint'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 107, 'page_label': '108'}, page_content='classification and regression problem, where the bounding-box coordinates are\\npredicted via vector regression.\\n\\uf0a1 Image segmentation—Given a picture, draw a pixel-level mask on a specific object. \\n4.1.2 Unsupervised learning\\nThis branch of machine learning consists of finding interesting transformations of the\\ninput data without the help of any targets, for the purposes of data visualization, data\\ncompression, or data denoising, or to be tter understand the correlations present in'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 107, 'page_label': '108'}, page_content='the data at hand. Unsupervised learning is the bread and butter of data analytics, and\\nit’s often a necessary step in better understanding a dataset before attempting to solve\\na supervised-learning problem. Dimensionality reduction  and clustering are well-known\\ncategories of unsupervised learning. \\n4.1.3 Self-supervised learning\\nThis is a specific instance of supervised learning, but it’s different enough that it'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 107, 'page_label': '108'}, page_content='deserves its own category. Self-supervised learning is supervised learning without\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 108, 'page_label': '109'}, page_content='95Four branches of machine learning\\nhuman-annotated labels—you can think of it as supervised learning without any\\nhumans in the loop. There are still labels involved (because the learning has to be\\nsupervised by something), but they’re generated from the input data, typically using a\\nheuristic algorithm.\\n For instance, autoencoders are a well-known instance of  self-supervised learning,\\nwhere the generated targets are the input, unmodified. In the same way, trying to pre-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 108, 'page_label': '109'}, page_content='dict the next frame in a video, given past frames, or the next word in a text, given previ-\\nous words, are instances of self-supervised learning (temporally supervised learning, in this\\ncase: supervision comes from future input data). Note that the distinction between\\nsupervised, self-supervised, and unsupervised learning can be blurry sometimes—these\\ncategories are more of a continuum without solid borders. Self-supervised learning can'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 108, 'page_label': '109'}, page_content='be reinterpreted as either supervised or unsupervised learning, depending on whether\\nyou pay attention to the learning mechanism or to the context of its application.\\nNOTE In this book, we’ll focus specifically  on supervised learning, because\\nit’s by far the dominant form of deep learning today, with a wide range of\\nindustry applications. We’ll also take a briefer look at self-supervised learning\\nin later chapters. \\n4.1.4 Reinforcement learning'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 108, 'page_label': '109'}, page_content='in later chapters. \\n4.1.4 Reinforcement learning\\nLong overlooked, this branch of machine learning recently started to get a lot of\\nattention after Google DeepMind successfully applied it  to learning to play Atari\\ngames (and, later, learning to play Go at the highest level). In reinforcement learning,\\nan agent receives information about its environment and learns to choose actions that\\nwill maximize some reward. For instance, a neur al network that “looks” at a video-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 108, 'page_label': '109'}, page_content='game screen and outputs game actions in or der to maximize its score can be trained\\nvia reinforcement learning.\\n Currently, reinforcement learning is mostly a research area and hasn’t yet had sig-\\nnificant practical succ esses beyond games. In time, ho wever, we expect to see rein-\\nforcement learning take over an increasing ly large range of real -world applications:\\nself-driving cars, robotics, resource management, educatio n, and so on. It’s an idea\\nwhose time has come, or will come soon.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 108, 'page_label': '109'}, page_content='whose time has come, or will come soon.  \\nClassification and regression glossary\\nClassification and regression involve many specialized terms. You’ve come across\\nsome of them in earlier examples, and you’ll see more of them in future chapters.\\nThey have precise, machine-learning-specific definitions, and you should be familiar\\nwith them:\\n\\uf0a1 Sample or input—One data point that goes into your model.\\n\\uf0a1 Prediction or output—What comes out of your model.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 108, 'page_label': '109'}, page_content='\\uf0a1 Target—The truth. What your model should ideally have predicted, according\\nto an external source of data.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 109, 'page_label': '110'}, page_content='96 CHAPTER 4 Fundamentals of machine learning\\n \\n(continued)\\n\\uf0a1 Prediction error  or loss value —A measure of the distance between your\\nmodel’s prediction and the target.\\n\\uf0a1 Classes—A set of possible labels to choose from in a classification problem.\\nFor example, when classifying cat and dog pictures, “dog” and “cat” are the\\ntwo classes.\\n\\uf0a1 Label—A specific instance of a class anno tation in a classification problem.\\nFor instance, if picture #1234 is annota ted as containing the class “dog,”'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 109, 'page_label': '110'}, page_content='then “dog” is a label of picture #1234.\\n\\uf0a1 Ground-truth or annotations—All targets for a dataset,  typically collected by\\nhumans.\\n\\uf0a1 Binary classification—A classification task where each input sample should\\nbe categorized into two exclusive categories.\\n\\uf0a1 Multiclass classification —A classification task where each input sample\\nshould be categorized into more than two categories: for instance, classifying\\nhandwritten digits.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 109, 'page_label': '110'}, page_content='handwritten digits.\\n\\uf0a1 Multilabel classification—A classification task where each input sample can\\nbe assigned multiple labels. For instance, a given image may contain both a\\ncat and a dog and should be annotated both with the “cat” label and the\\n“dog” label. The number of labels per image is usually variable.\\n\\uf0a1 Scalar regression—A task where the target is a continuous scalar value. Pre-\\ndicting house prices is a good example: the different target prices form a con-\\ntinuous space.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 109, 'page_label': '110'}, page_content='tinuous space.\\n\\uf0a1 Vector regression—A task where the target is a set of continuous values: for\\nexample, a continuous vector. If you’re doing regression against multiple val-\\nues (such as the coordinates of a bounding box in an image), then you’re\\ndoing vector regression.\\n\\uf0a1 Mini-batch or batch—A small set of samples (t ypically between 8 and 128)\\nthat are processed simultaneously by the model. The number of samples is\\noften a power of 2, to facilitate memory alloca tion on GPU. When training, a'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 109, 'page_label': '110'}, page_content='mini-batch is used to compute a si ngle gradient-descent update applied to\\nthe weights of the model. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 110, 'page_label': '111'}, page_content='97Evaluating machine-learning models\\n4.2 Evaluating machine-learning models\\nIn the three examples presented in chapter 3,  we split the data into a training set, a\\nvalidation set, and a test set. The reason not to evaluate the models on the same data\\nthey were trained on quickly became eviden t: after just a few epochs, all three models\\nbegan to overfit. That is, their performance on neve r-before-seen data started stalling'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 110, 'page_label': '111'}, page_content='(or worsening) compared to their perfor mance on the training data—which always\\nimproves as training progresses.\\n In machine learning, the goal is to achieve models that generalize—that perform\\nwell on never-before-seen data—and overfi tting is the central obstacle. You can only\\ncontrol that which you can observe, so it’s crucial to be able to reliably measure the\\ngeneralization power of your model. The following sections look at strategies for miti-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 110, 'page_label': '111'}, page_content='gating overfitting and maximizing generalizat ion. In this section, we’ll focus on how\\nto measure generalization: how to evaluate machine-learning models.\\n4.2.1 Training, validation, and test sets\\nEvaluating a model always boils down to sp litting the available data into three sets:\\ntraining, validation, and test. You train on the training data and evaluate your model\\non the validation data. Once your model is ready for prime time, you test it one final\\ntime on the test data.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 110, 'page_label': '111'}, page_content='time on the test data.\\n You may ask, why not have two sets: a training set and a test set? You’d train on the\\ntraining data and evaluate on the test data. Much simpler!\\n The reason is that developing a model alwa ys involves tuning its configuration: for\\ne x a m p l e ,  c h o o s i n g  t h e  n u m b e r  o f  l a y e r s  o r  t h e  s i z e  o f  t h e  l a y e r s  ( c a l l e d  t h e  hyper-\\nparameters of the model, to distinguish them from the parameters, which are the net-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 110, 'page_label': '111'}, page_content='work’s weights). You do this tuning by using as a feedback signal the performance of\\nthe model on the validation data. In essence, this tuning is a form of learning: a search\\nfor a good configuration in some parameter space. As a result, tuning the configura-\\ntion of the model based on its performance on the validation set can quickly result in\\noverfitting to the validation set, even though your model is never directly trained on it.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 110, 'page_label': '111'}, page_content='Central to this phenomenon is the notion of information leaks. Every time you tune\\na hyperparameter of your model based on the model’s performance on the validation\\nset, some information about the validation data leaks into the model. If you do this\\nonly once, for one parameter, then very few bits of information will leak, and your val-\\nidation set will remain reliable to evalua te the model. But if you repeat this many'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 110, 'page_label': '111'}, page_content='times—running one experiment, evaluating on the validation set, and modifying your\\nmodel as a result—then you’ll leak an incr easingly significant amount of information\\nabout the validation set into the model.\\n At the end of the day, you’ll end up with  a model that performs artificially well on\\nthe validation data, because that’s what you optimized it for. You care about perfor-\\nmance on completely new data, not the vali dation data, so you need to use a com-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 110, 'page_label': '111'}, page_content='pletely different, never-before-seen dataset to evaluate the model: the test dataset. Your\\nmodel shouldn’t have had access to any information about the test set, even indirectly.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 111, 'page_label': '112'}, page_content='98 CHAPTER 4 Fundamentals of machine learning\\nIf anything about the model has been tuned based on test set performance, then your\\nmeasure of generalization will be flawed.\\n Splitting your data into training, validation, and test sets may seem straightforward,\\nbut there are a few advanced ways to do it that can come in handy when little data is\\navailable. Let’s review three classic evaluati on recipes: simple ho ld-out validation, K-\\nfold validation, and iterated K-fold validation with shuffling.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 111, 'page_label': '112'}, page_content='SIMPLE HOLD-OUT VALIDATION\\nSet apart some fraction of your data as your test set. Train on the remaining data, and\\nevaluate on the test set. As you saw in the previous sections, in order to prevent infor-\\nmation leaks, you shouldn’t tune your model based on the test set, and therefore you\\nshould also reserve a validation set.\\n Schematically, hold-out validation looks like figure 4.1. The following listing shows\\na simple implementation.\\nnum_validation_samples = 10000\\nnp.random.shuffle(data)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 111, 'page_label': '112'}, page_content='np.random.shuffle(data)\\nvalidation_data = data[:num_validation_samples]\\ndata = data[num_validation_samples:]\\ntraining_data = data[:]\\nmodel = get_model()\\nmodel.train(training_data)\\nvalidation_score = model.evaluate(validation_data)\\n# At this point you can tune your model,\\n# retrain it, evaluate it, tune it again...\\nmodel = get_model()\\nmodel.train(np.concatenate([training_data,\\nvalidation_data]))\\ntest_score = model.evaluate(test_data)\\nListing 4.1 Hold-out validation\\nTraining set'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 111, 'page_label': '112'}, page_content='Listing 4.1 Hold-out validation\\nTraining set\\nTotal available labeled data\\nTrain on this Evaluate\\non this\\nHeld-out\\nvalidation\\nset\\nFigure 4.1 Simple hold-\\nout validation split\\nShuffling the data is \\nusually appropriate. Defines the \\nvalidation set\\nDefines the training set\\nTrains a model on the training \\ndata, and evaluates it on the \\nvalidation data\\nOnce you’ve tuned your \\nhyperparameters, it’s common to \\ntrain your final model from scratch \\non all non-test data available.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 112, 'page_label': '113'}, page_content='99Evaluating machine-learning models\\nThis is the simplest evaluation protocol, an d it suffers from one flaw: if little data is\\navailable, then your validation and test sets  may contain too few samples to be statisti-\\ncally representative of the data at hand. This  is easy to recognize: if different random\\nshuffling rounds of the data before splitti ng end up yielding very different measures\\nof model performance, then you’re having this issue. K-fold validation and iterated'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 112, 'page_label': '113'}, page_content='K-fold validation are two ways to address this, as discussed next. \\nK-FOLD VALIDATION\\nWith this approach, you split your data into K partitions of equal size. For each parti-\\ntion i, train a model on the remaining K – 1 partitions, and evaluate it on partition i.\\nYour final score is then  the averages of the K scores obtained. This method is helpful\\nwhen the performance of your model shows significant variance based on your train-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 112, 'page_label': '113'}, page_content='test split. Like hold-out validation, this method doesn’t exempt you from using a dis-\\ntinct validation set for model calibration.\\n Schematically, K-fold cross-validation looks like figure 4.2. Listing 4.2 shows a simple\\nimplementation.\\nk=4\\nnum_validation_samples = len(data) // k\\nnp.random.shuffle(data)\\nvalidation_scores = []\\nfor fold in range(k):\\nvalidation_data = data[num_validation_samples * fold:\\nnum_validation_samples * (fold + 1)]\\ntraining_data = data[:num_validation_samples * fold] +'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 112, 'page_label': '113'}, page_content='data[num_validation_samples * (fold + 1):]\\nmodel = get_model()\\nmodel.train(training_data)\\nvalidation_score = model.evaluate(validation_data)\\nvalidation_scores.append(validation_score)\\nListing 4.2 K-fold cross-validation\\nData split into 3 partitions\\nValidation Training Training Validation\\nscore #1Fold 1\\nValidation Validation Training Validation\\nscore #2\\nFinal score:\\naverageFold 2\\nValidation Training Validation Validation\\nscore #3Fold 3\\nFigure 4.2 Three-fold validation\\nSelects the validation-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 112, 'page_label': '113'}, page_content='Selects the validation-\\ndata partition\\nUses the remainder of the data \\nas training data. Note that the \\n+ operator is list concatenation, \\nnot summation.\\nCreates a brand-new instance \\nof the model (untrained)\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 113, 'page_label': '114'}, page_content='100 CHAPTER 4 Fundamentals of machine learning\\nvalidation_score = np.average(validation_scores)\\nmodel = get_model()\\nmodel.train(data)\\ntest_score = model.evaluate(test_data)\\nITERATED K-FOLD VALIDATION WITH SHUFFLING\\nThis one is for situations in which you have relatively little data available and you need\\nto evaluate your model as precisely as possible. I’ve found it to be extremely helpful in\\nKaggle competitions. It consists of applying K-fold validation multiple times, shuffling'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 113, 'page_label': '114'}, page_content='the data every time before splitting it K ways. The final score is the average of the\\nscores obtained at each run of K-fold validation. Note that you end up training and\\nevaluating P × K models (where P is the number of iterations you use), which can very\\nexpensive. \\n4.2.2 Things to keep in mind\\nKeep an eye out for the following when you’re choosing an evaluation protocol:\\n\\uf0a1 Data representativeness —You want both your training set and test set to be repre-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 113, 'page_label': '114'}, page_content='sentative of the data at hand. For instance , if you’re trying to classify images of\\ndigits, and you’re starting from an array of samples where the samples are\\nordered by their class, taking the first 80%  of the array as your training set and\\nthe remaining 20% as your test set will result in your training set containing\\nonly classes 0–7, whereas your test set co ntains only classes 8–9. This seems like\\na ridiculous mistake, but it’s surprising ly common. For this reason, you usually'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 113, 'page_label': '114'}, page_content='should randomly shuffle your data before splitting it into training and test sets.\\n\\uf0a1 The arrow of time—If you’re trying to predict the future given the past (for exam-\\nple, tomorrow’s weathe r, stock movements, and so on), you should not ran-\\ndomly shuffle your data before splitting  it, because doing so will create a\\ntemporal leak: your model will effectively be trained on data from the future. In\\nsuch situations, you should always make  sure all data in your test set is posterior'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 113, 'page_label': '114'}, page_content='to the data in the training set.\\n\\uf0a1 Redundancy in your data —If some data points in your data appear twice (fairly\\ncommon with real-world data ), then shuffling the data and splitting it into a\\ntraining set and a validation set will result in redundancy between the training\\nand validation sets. In effect, you’ll be testing on part of your training data,\\nwhich is the worst thing you can do! Make  sure your training set and validation\\nset are disjoint. \\nValidation score: \\naverage of the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 113, 'page_label': '114'}, page_content='Validation score: \\naverage of the \\nvalidation scores \\nof the k folds\\nTrains the final \\nmodel on all non-\\ntest data available\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 114, 'page_label': '115'}, page_content='101Data preprocessing, feature engineering, and feature learning\\n4.3 Data preprocessing, feature engineering, \\nand feature learning\\nIn addition to model evaluation, an important question we must tackle before we dive\\ndeeper into model development is the foll owing: how do you prepare the input data\\nand targets before feeding them into a neur al network? Many data-preprocessing and\\nfeature-engineering techniques are domain specific (for example, specific to text data'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 114, 'page_label': '115'}, page_content='or image data); we’ll cover those in the fo llowing chapters as we encounter them in\\npractical examples. For now, we’ll review  the basics that are common to all data\\ndomains.\\n4.3.1 Data preprocessing for neural networks\\nData preprocessing aims at making the raw data at hand more amenable to neural\\nnetworks. This includes vectorization, no rmalization, handling missing values, and\\nfeature extraction.\\nVECTORIZATION'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 114, 'page_label': '115'}, page_content='feature extraction.\\nVECTORIZATION\\nAll inputs and targets in a neural network must be tensors of floating-point data (or, in\\nspecific cases, tensors of integers). Whatever data you need to process—sound,\\nimages, text—you must first turn  into tensors, a step called data vectorization . For\\ninstance, in the two previous text-classifica tion examples, we started from text repre-\\nsented as lists of integers (standing for sequences of words), and we used one-hot'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 114, 'page_label': '115'}, page_content='encoding to turn them into a tensor of float32 data. In the examples of classifying\\ndigits and predicting house prices, the data  already came in vectorized form, so you\\nwere able to skip this step. \\nVALUE NORMALIZATION\\nIn the digit-classification example, you started from image data encoded as integers in\\nthe 0–255 range, encoding grayscale values. Before you fed this data into your net-\\nwork, you had to cast it to float32 and divide by 255 so you’d end up with floating-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 114, 'page_label': '115'}, page_content='point values in the 0–1 rang e. Similarly, when predicti ng house prices, you started\\nfrom features that took a variety of ranges—some features had small floating-point val-\\nues, others had fairly large integer values. Before you fed this data into your network,\\nyou had to normalize each feature independen tly so that it had a standard deviation\\nof 1 and a mean of 0.\\n In general, it isn’t safe to feed into a neural network data that takes relatively large val-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 114, 'page_label': '115'}, page_content='ues (for example, multidigit integers, which are much larger than the initial values taken\\nby the weights of a network) or data that is heterogeneous (for example, data where one\\nfeature is in the range 0–1 and another is in the range 100–200). Doing so can trigger\\nlarge gradient updates that will prevent the network from converging. To make learning\\neasier for your network, your data should have the following characteristics:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 114, 'page_label': '115'}, page_content='\\uf0a1 Take small values—Typically, most values should be in the 0–1 range.\\n\\uf0a1 Be homogenous —That is, all features should take values in roughly the same\\nrange.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 115, 'page_label': '116'}, page_content='102 CHAPTER 4 Fundamentals of machine learning\\nAdditionally, the following stricter normal ization practice is common and can help,\\nalthough it isn’t always necessary (for example, you didn’t do this in the digit-classification\\nexample):\\n\\uf0a1 Normalize each feature independently to have a mean of 0.\\n\\uf0a1 Normalize each feature independently to have a standard deviation of 1.\\nThis is easy to do with Numpy arrays:\\nx -= x.mean(axis=0)\\nx /= x.std(axis=0)\\nHANDLING MISSING VALUES'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 115, 'page_label': '116'}, page_content='x /= x.std(axis=0)\\nHANDLING MISSING VALUES\\nYou may sometimes have missing values in yo ur data. For instance, in the house-price\\nexample, the first feature (the column of index 0 in the data) was the per capita crime\\nrate. What if this feature wasn’t available for all samples? You’d then have missing val-\\nues in the training or test data.\\n In general, with neural networks, it’s safe to input missing values as 0, with the con-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 115, 'page_label': '116'}, page_content='dition that 0 isn’t already a meaningful value. The network will learn from exposure to\\nthe data that the value 0 means missing data and will start ignoring the value.\\n Note that if you’re expecting missing values in the test data, but the network was\\ntrained on data without any missing values, the network won’t have learned to ignore\\nmissing values! In this situation, you should  artificially generate training samples with'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 115, 'page_label': '116'}, page_content='missing entries: copy some training samples several ti mes, and drop some of the fea-\\ntures that you expect are likely to be missing in the test data. \\n4.3.2 Feature engineering\\nFeature engineering is the process of using your own knowledge about the data and about\\nthe machine-learning algorithm at hand (in this case, a neural network) to make the\\nalgorithm work better by applying\\nhardcoded (nonlearned) transfor-\\nmations to the data before it goes\\ninto the model. In many cases, it isn’t'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 115, 'page_label': '116'}, page_content='into the model. In many cases, it isn’t\\nreasonable to expect a machine-\\nlearning model to be able to learn\\nfrom completely arbitrary data. The\\ndata needs to be presented to the\\nmodel in a way that will make the\\nmodel’s job easier.\\n Let’s look at an intuitive example.\\nSuppose you’re trying to develop a\\nmodel that can take as input an\\nimage of a clock and can output the\\ntime of day (see figure 4.3).\\nAssuming x is a 2D data matrix \\nof shape (samples, features) \\nRaw data:\\npixel grid\\nBetter'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 115, 'page_label': '116'}, page_content='Raw data:\\npixel grid\\nBetter\\nfeatures:\\nclock hands’\\ncoordinates\\n{x1: 0.7,\\ny1: 0.7}\\n{x2: 0.5,\\ny2: 0.0}\\n{x1: 0.0,\\ny2: 1.0}\\n{x2: -0.38,\\n2: 0.32}\\nEven better\\nfeatures:\\nangles of\\nclock hands\\ntheta1: 45\\ntheta2: 0\\ntheta1: 90\\ntheta2: 140\\nFigure 4.3 Feature engineering for reading the time on \\na clock\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 116, 'page_label': '117'}, page_content='103Data preprocessing, feature engineering, and feature learning\\nIf you choose to use the raw pixels of the image as input data, then you have a difficult\\nmachine-learning problem on your hands. You’ll need a convolutional neural net-\\nwork to solve it, and you’ll have to expend  quite a bit of com putational resources to\\ntrain the network.\\n But if you already understa nd the problem at a high level (you understand how'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 116, 'page_label': '117'}, page_content='humans read time on a clock face), then you can come up with much better input fea-\\ntures for a machine-learning algorithm: for instance, it’s easy to write a five-line\\nPython script to follow the black pixels of  the clock hands and ou tput the (x, y) coor-\\ndinates of the tip of each hand. Then a si mple machine-learning algorithm can learn\\nto associate these coordinates with the appropriate time of day.\\n You can go even further: do a coordina te change, and express the (x, y) coordi-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 116, 'page_label': '117'}, page_content='nates as polar coordinates with regard to the center of the image. Your input will\\nbecome the angle theta of each clock hand. At this point, your features are making\\nthe problem so easy that no machine learni ng is required; a simple rounding opera-\\ntion and dictionary lookup are enough to recover the approximate time of day.\\n That’s the essence of feature engineerin g: making a problem easier by expressing\\nit in a simpler way. It usually requires understanding the problem in depth.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 116, 'page_label': '117'}, page_content='Before deep learning, feature engineering used to be critical, because classical\\nshallow algorithms didn’t have hypothesis sp aces rich enough to learn useful features\\nby themselves. The way you presented the data to the algorithm was essential to its suc-\\ncess. For instance, before convolutional ne ural networks became successful on the\\nMNIST digit-classification probl em, solutions were typically  based on hardcoded fea-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 116, 'page_label': '117'}, page_content='tures such as the number of loops in a digit image, the height of each digit in an\\nimage, a histogram of pixel values, and so on.\\n Fortunately, modern deep learning remo ves the need for most feature engineer-\\ning, because neural networks are capable of  automatically extrac ting useful features\\nfrom raw data. Does this mean you don’t have to worry about feature engineering as\\nlong as you’re using deep neural networks? No, for two reasons:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 116, 'page_label': '117'}, page_content='\\uf0a1 Good features still allow you to solve problems more elegantly while using fewer\\nresources. For instance, it would be ridiculous to solve the problem of reading a\\nclock face using a convolutional neural network.\\n\\uf0a1 Good features let you solve a problem wi th far less data. The ability of deep-\\nlearning models to learn fe atures on their own relies on having lots of training\\ndata available; if you have only a few samples, then the information value in\\ntheir features becomes critical.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 116, 'page_label': '117'}, page_content='their features becomes critical. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 117, 'page_label': '118'}, page_content='104 CHAPTER 4 Fundamentals of machine learning\\n4.4 Overfitting and underfitting\\nIn all three examples in the previous chapter—predicting movie reviews, topic classifi-\\ncation, and house-price regression—the performance of the model on the held-out\\nvalidation data always peaked after a fe w epochs and then began to degrade: the\\nmodel quickly started to overfit to the training data. Ov erfitting happens in every\\nmachine-learning problem. Learning how to de al with overfitting is essential to mas-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 117, 'page_label': '118'}, page_content='tering machine learning.\\n The fundamental issue in machine learni ng is the tension between optimization\\nand generalization. Optimization refers to the process of adjusting a model to get the\\nbest performance possible on the training data (the learning in machine learning ),\\nwhereas generalization refers to how well the trained model performs on data it has\\nnever seen before. The goal of the game is to get good generalization, of course, but'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 117, 'page_label': '118'}, page_content='you don’t control generalization; you can on ly adjust the model based on its training\\ndata.\\n At the beginning of training, optimizati on and generalization are correlated: the\\nlower the loss on training data, the lower the loss on test data. While this is happening,\\nyour model is said to be underfit: there is still progress to be made; the network hasn’t\\nyet modeled all relevant patterns in the tr aining data. But after a certain number of'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 117, 'page_label': '118'}, page_content='iterations on the training data, generalization stops improving, and validation metrics\\nstall and then begin to degrade: the model is starting to overfit. That is, it’s beginning\\nto learn patterns that are specific to the training data but that are misleading or irrele-\\nvant when it comes to new data.\\n To prevent a model from le arning misleading or irrelevant patterns found in the\\ntraining data, the best solution is to get more training data . A model trained on more data'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 117, 'page_label': '118'}, page_content='will naturally generalize better. When that isn’t possible, the next-best solution is to\\nmodulate the quantity of information that yo ur model is allowed to store or to add\\nconstraints on what information it’s allowed to store. If a network can only afford to\\nmemorize a small number of patterns, the opti mization process will force it to focus\\non the most prominent patterns, which have a better chance of generalizing well.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 117, 'page_label': '118'}, page_content='The processing of fighting ov erfitting this way is called regularization. Let’s review\\nsome of the most common regularization techniques and apply them in practice to\\nimprove the movie-classification model from section 3.4.\\n4.4.1 Reducing the network’s size\\nThe simplest way to prevent overfitting is to reduce the size of the model: the number\\nof learnable parameters in the model (which  is determined by the number of layers'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 117, 'page_label': '118'}, page_content='and the number of units per layer). In deep learning, the number of learnable param-\\neters in a model is often referred to as the model’s capacity. Intuitively, a model with\\nmore parameters has more memorization capacity and therefore can easily learn a per-\\nfect dictionary-like mapping between trai ning samples and their targets—a mapping\\nwithout any generalization power. For instan ce, a model with 500,000 binary parame-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 117, 'page_label': '118'}, page_content='ters could easily be made to lear n the class of every digit in the MNIST training set:\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 118, 'page_label': '119'}, page_content='105Overfitting and underfitting\\nwe’d need only 10 binary parameters for each of the 50,000 digits. But such a model\\nwould be useless for cl assifying new digit sa mples. Always keep this in mind: deep-\\nlearning models tend to be good at fitting to the training data, but the real challenge\\nis generalization, not fitting.\\n On the other hand, if the network has limited memorization resources, it won’t be\\nable to learn this mapping as easily; thus, in  order to minimize it s loss, it will have to'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 118, 'page_label': '119'}, page_content='resort to learning compressed representati ons that have predictive power regarding\\nthe targets—precisely the type of represen tations we’re interested in. At the same\\ntime, keep in mind that you should use models that have enough parameters that they\\ndon’t underfit: your model shouldn’t be starved for memorization resources. There is\\na compromise to be found between too much capacity and not enough capacity.\\n Unfortunately, there is no magical form ula to determine the right number of lay-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 118, 'page_label': '119'}, page_content='ers or the right size for each layer. You mu st evaluate an array of different architec-\\ntures (on your validation set, not on your test set, of course) in order to find the\\ncorrect model size for your data. The genera l workflow to find an appropriate model\\nsize is to start with relatively few layers and parameters, and increase the size of the lay-\\ners or add new layers until you see diminishing returns with regard to validation loss.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 118, 'page_label': '119'}, page_content=\"Let’s try this on the movie-review classi fication network. The original network is\\nshown next.\\nfrom keras import models\\nfrom keras import layers\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(16, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nNow let’s try to replace it with this smaller network.\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 118, 'page_label': '119'}, page_content=\"model.add(layers.Dense(4, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nFigure 4.4 shows a comparison of the validation losses of the original network and the\\nsmaller network. The dots are the validation  loss values of the smaller network, and\\nthe crosses are the initial ne twork (remember, a lower valid ation loss signals a better\\nmodel).\\nListing 4.3 Original model\\nListing 4.4 Version of the model with lower capacity\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 119, 'page_label': '120'}, page_content=\"106 CHAPTER 4 Fundamentals of machine learning\\n \\nAs you can see, the smaller network starts overfitting later than the reference network\\n(after six epochs rather than four), and its performance degrades more slowly once it\\nstarts overfitting.\\n Now, for kicks, let’s add to this benc hmark a network that has much more capac-\\nity—far more than the problem warrants.\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(512, activation='relu', input_shape=(10000,)))\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 119, 'page_label': '120'}, page_content=\"model.add(layers.Dense(512, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nFigure 4.5 shows how the bigger network fa res compared to the reference network.\\nThe dots are the validation loss values of the bigger network, and the crosses are the\\ninitial network.\\nListing 4.5 Version of the model with higher capacity\\nFigure 4.4 Effect of model \\ncapacity on validation loss: trying \\na smaller model\\nFigure 4.5 Effect of model \\ncapacity on validation loss: \\ntrying a bigger model\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 119, 'page_label': '120'}, page_content='trying a bigger model\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 120, 'page_label': '121'}, page_content='107Overfitting and underfitting\\nThe bigger network starts over fitting almost immediately, after just one epoch, and it\\noverfits much more severely. Its validation loss is also noisier.\\n Meanwhile, figure 4.6 shows the training  losses for the two networks. As you can\\nsee, the bigger network gets its training loss near zero very quickly. The more capacity\\nthe network has, the more quickly it can model the training data (resulting in a low'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 120, 'page_label': '121'}, page_content='training loss), but the more su sceptible it is to overfittin g (resulting in a large differ-\\nence between the training and validation loss).  \\n4.4.2 Adding weight regularization\\nYou may be familiar wi th the principle of Occam’s razor : given two explanations for\\nsomething, the explanation most likely to be correct is the simplest one—the one that\\nmakes fewer assumptions. This idea also ap plies to the models le arned by neural net-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 120, 'page_label': '121'}, page_content='works: given some training data and a netw ork architecture, multiple sets of weight\\nvalues (multiple models) could explain the data. Simpler models are less likely to over-\\nfit than complex ones.\\n A simple model in this context is a model where the distribution of parameter values\\nhas less entropy (or a model with fewer para meters, as you saw in the previous sec-\\ntion). Thus a common way to mitigate overfitting is to put constraints on the complex-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 120, 'page_label': '121'}, page_content='ity of a network by forcing its weights to take only small values, which makes the\\ndistribution of weight values more regular. This is called weight regularization, and it’s\\ndone by adding to the loss function of the network a cost associated with having large\\nweights. This cost comes in two flavors:\\n\\uf0a1 L1 regularization —The cost added is proportional to the absolute value of the\\nweight coefficients (the L1 norm of the weights).'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 120, 'page_label': '121'}, page_content='weight coefficients (the L1 norm of the weights).\\n\\uf0a1 L2 regularization —The cost added is proportional to the square of the value of the\\nweight coefficients  (the L2 norm  of the weights). L2 regularization is also called\\nweight decay in the context of neural networks. Don’t let the different name con-\\nfuse you: weight decay is mathematically the same as L2 regularization.\\nFigure 4.6 Effect of model \\ncapacity on training loss: \\ntrying a bigger model\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 121, 'page_label': '122'}, page_content=\"108 CHAPTER 4 Fundamentals of machine learning\\nIn Keras, weight regulariza tion is added by passing weight regularizer instances to layers\\nas keyword arguments. Let’s add L2 weight regularization to the movie-review classifi-\\ncation network.\\nfrom keras import regularizers\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\\nactivation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 121, 'page_label': '122'}, page_content=\"activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nl2(0.001) means every coefficient in the weight matrix of the layer will add 0.001 *\\nweight_coefficient_value to the total loss of the network. Note that because this\\npenalty is only added at training time , the loss for this network will be much higher at\\ntraining than at test time.\\n Figure 4.7 shows the impact of the L2 regularization penalty. As you can see, the\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 121, 'page_label': '122'}, page_content='model with L2 regularization (dots) has become much more resistant to overfitting\\nthan the reference model (crosses), even though both models have the same number\\nof parameters.\\nAs an alternative to L2 regularization, you can use one of the following Keras weight\\nregularizers.\\nfrom keras import regularizers\\nregularizers.l1(0.001)\\nregularizers.l1_l2(l1=0.001, l2=0.001)\\nListing 4.6 Adding L2 weight regularization to the model\\nListing 4.7 Different weight regularizers available in Keras'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 121, 'page_label': '122'}, page_content='Figure 4.7 Effect of L2 weight \\nregularization on validation loss\\nL1 regularization Simultaneous L1 and \\nL2 regularization \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 122, 'page_label': '123'}, page_content='109Overfitting and underfitting\\n4.4.3 Adding dropout\\nDropout is one of the most effective and mo st commonly used regularization tech-\\nniques for neural networks, developed by Geoff Hinton and his students at the Uni-\\nversity of Toronto. Dropout, applied to a layer, consists of randomly dropping out\\n(setting to zero) a number of output featur es of the layer during training. Let’s say a\\ngiven layer would normally return a vector [0 .2, 0.5, 1.3, 0.8, 1.1] for a given input'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 122, 'page_label': '123'}, page_content='sample during training. After applying dropout, this vector will have a few zero entries\\ndistributed at random: for example, [0, 0.5, 1.3, 0, 1.1]. The dropout rate is the fraction\\nof the features that are zeroed out; it’s usually set between 0.2 and 0.5. At test time, no\\nunits are dropped out; instead, the layer’s output values are scaled down by a factor\\nequal to the dropout rate, to balance for th e fact that more units are active than at\\ntraining time.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 122, 'page_label': '123'}, page_content='training time.\\n Consider a Numpy matrix contai ning the output of a layer, \\nlayer_output, of\\nshape (batch_size, features). At training time, we zero out at random a fraction of\\nthe values in the matrix:\\nlayer_output *= np.random.randint(0, high=2, size=layer_output.shape)\\nAt test time, we scale down the output by the dropout rate. Here, we scale by 0.5\\n(because we previously dropped half the units):\\nlayer_output *= 0.5'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 122, 'page_label': '123'}, page_content='layer_output *= 0.5\\nNote that this process can be implemented by doing both operations at training time\\nand leaving the output unchanged at test time, which is often the way it’s imple-\\nmented in practice (see figure 4.8):\\nlayer_output *= np.random.randint(0, high=2, size=layer_output.shape)\\nlayer_output /= 0.5\\nThis technique may seem strange and arbitr ary. Why would this help reduce overfit-\\nting? Hinton says he was inspired by, am ong other things, a fr aud-prevention mecha-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 122, 'page_label': '123'}, page_content='nism used by banks. In his own words, “I went to my bank. The tellers kept changing\\nand I asked one of them why. He said he didn’t know but they got moved around a lot.\\nAt training time, drops out 50%\\nof the units in the output\\nAt test time\\nAt training timeNote that we’re scaling up rather \\nscaling down in this case.\\n0.3\\n* 2\\n0.6\\n0.2\\n0.7\\n0.2\\n0.1\\n1.9\\n0.5\\n1.5\\n0.0\\n0.3\\n1.0\\n0.0\\n0.3\\n1.2\\n0.0\\n0.0\\n50%\\ndropout 0.6\\n0.0\\n0.7\\n0.2\\n0.1\\n1.9\\n0.0\\n1.5\\n0.0\\n0.3\\n0.0\\n0.0\\n0.3\\n0.0\\n0.0\\nFigure 4.8 Dropout applied to an'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 122, 'page_label': '123'}, page_content='0.0\\n0.3\\n0.0\\n0.0\\nFigure 4.8 Dropout applied to an \\nactivation matrix at training time, \\nwith rescaling happening during \\ntraining. At test time, the activation \\nmatrix is unchanged.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 123, 'page_label': '124'}, page_content='110 CHAPTER 4 Fundamentals of machine learning\\nI figured it must be becaus e it would require cooperation between employees to suc-\\ncessfully defraud the bank. This made me re alize that randomly removing a different\\nsubset of neurons on each example would prevent conspiracies and thus reduce over-\\nfitting.”\\n1 The core idea is that introducing nois e in the output values of a layer can\\nbreak up happenstance patterns that aren’t  significant (what Hinton refers to as con-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 123, 'page_label': '124'}, page_content=\"spiracies), which the network will start memorizing if no noise is present.\\n In Keras, you can introduce dropout in a network via the Dropout layer, which is\\napplied to the output of the layer right before it:\\nmodel.add(layers.Dropout(0.5))\\nLet’s add two Dropout layers in the IMDB network to see how well they do at reducing\\noverfitting.\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\\nmodel.add(layers.Dropout(0.5))\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 123, 'page_label': '124'}, page_content=\"model.add(layers.Dropout(0.5))\\nmodel.add(layers.Dense(16, activation='relu'))\\nmodel.add(layers.Dropout(0.5))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nFigure 4.9 shows a plot of the results. Again, this is a clear improvement over the refer-\\nence network.\\nTo recap, these are the most common ways to prevent overfitting in neural networks:\\n\\uf0a1 Get more training data.\\n\\uf0a1 Reduce the capacity of the network.\\n\\uf0a1 Add weight regularization.\\n\\uf0a1 Add dropout.\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 123, 'page_label': '124'}, page_content='\\uf0a1 Add weight regularization.\\n\\uf0a1 Add dropout. \\n1 See the Reddit thread “AMA: We are the Google Brain team. We’d love to answer your questions about\\nmachine learning,” http:/ /mng.bz/XrsS.\\nListing 4.8 Adding dropout to the IMDB network\\nFigure 4.9 Effect of dropout \\non validation loss\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 124, 'page_label': '125'}, page_content='111The universal workflow of machine learning\\n4.5 The universal workfl ow of machine learning\\nIn this section, we’ll present a universal bl ueprint that you can use to attack and solve\\nany machine-learning problem. The bluepr int ties together the concepts you’ve\\nlearned about in this chapter: problem de finition, evaluation, feature engineering,\\nand fighting overfitting.\\n4.5.1 Defining the problem and assembling a dataset\\nFirst, you must define the problem at hand:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 124, 'page_label': '125'}, page_content='First, you must define the problem at hand:\\n\\uf0a1 What will your input data be? What are you trying to predict? You can only learn\\nto predict something if you have availa ble training data: for example, you can\\nonly learn to classify the sentiment of  movie reviews if you have both movie\\nreviews and sentiment annotations available.  As such, data availability is usually\\nthe limiting factor at this stage (unless you have the means to pay people to col-\\nlect data for you).'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 124, 'page_label': '125'}, page_content='lect data for you).\\n\\uf0a1 What type of problem are you facing? Is it binary classification? Multiclass classi-\\nfication? Scalar regression? Vector regression? Multicla ss, multilabel classifica-\\ntion? Something else, like clustering, ge neration, or reinforcement learning?\\nIdentifying the problem type will guide yo ur choice of model architecture, loss\\nfunction, and so on.\\nYou can’t move to the next stage until you know what your inputs and outputs are, and'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 124, 'page_label': '125'}, page_content='what data you’ll use. Be aware of the hypotheses you make at this stage:\\n\\uf0a1 You hypothesize that your outputs can be predicted given your inputs.\\n\\uf0a1 You hypothesize that your available data is sufficiently informative to learn the\\nrelationship between inputs and outputs.\\nUntil you have a working model, these are merely hypotheses, waiting to be validated\\nor invalidated. Not all problems can be solv ed; just because you’ve assembled exam-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 124, 'page_label': '125'}, page_content='ples of inputs X and targets Y doesn’t mean X contains enough information to predict\\nY. For instance, if you’re trying to predic t the movements of a stock on the stock mar-\\nket given its recent price hi story, you’re unlikely to succeed, because price history\\ndoesn’t contain much predictive information.\\n One class of unsolvable problems you should be aware of is nonstationary problems.\\nSuppose you’re trying to build a recommendation engine for clothing, you’re training'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 124, 'page_label': '125'}, page_content='it on one month of data (August), and you want to start generating recommendations\\nin the winter. One big issue is that the kinds of clothes people buy change from season\\nto season: clothes buying is a nonstation ary phenomenon over the scale of a few\\nmonths. What you’re trying to model changes over time. In this case, the right move is\\nt o  c o n s t a n t l y  r e t r a i n  y o u r  m o d e l  o n  d a t a  f r o m  t h e  r e c e n t  p a s t ,  o r  g a t h e r  d a t a  a t  a'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 124, 'page_label': '125'}, page_content='timescale where the problem is stationary. For a cyclical problem like clothes buying, a\\nfew years’ worth of data will suffice to capture seasonal variation—but remember to\\nmake the time of the year an input of your model!\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 125, 'page_label': '126'}, page_content='112 CHAPTER 4 Fundamentals of machine learning\\n Keep in mind that machine learning can only be used to memorize patterns that\\nare present in your training data. You can only recognize what you’ve seen before.\\nUsing machine learning trained on past da ta to predict the future is making the\\nassumption that the future will behave like the past. That often isn’t the case. \\n4.5.2 Choosing a measure of success\\nTo control something, you need to be able to observe it. To achieve success, you must'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 125, 'page_label': '126'}, page_content='define what you mean by success—accuracy? Precision and recall? Customer-retention\\nrate? Your metric for success will guide the choice of a loss function: what your model\\nwill optimize. It should direct ly align with your higher-level  goals, such as the success\\nof your business.\\n For balanced-classification problems, where every class is equally likely, accuracy and\\narea under the receiver operating characteristic curve  (ROC AUC) are common metrics. For'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 125, 'page_label': '126'}, page_content='class-imbalanced problems, you can use precision and recall. For ranking problems or\\nmultilabel classification, you can use mean  average precision. And it isn’t uncommon\\nto have to define your own custom metric by which to measure success. To get a sense\\nof the diversity of machine-learning success metrics and how they relate to different\\nproblem domains, it’s helpfu l to browse the data scie nce competitions on Kaggle'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 125, 'page_label': '126'}, page_content='(https:/ /kaggle.com); they showcase a wide range of problems and evaluation metrics. \\n4.5.3 Deciding on an evaluation protocol\\nOnce you know what you’re aiming for, you must establish ho w you’ll measure your\\ncurrent progress. We’ve previously reviewed three common evaluation protocols:\\n\\uf0a1 Maintaining a hold-out validation set —The way to go when you have plenty of\\ndata\\n\\uf0a1 Doing K-fold cross-validation —The right choice when you have too few samples\\nfor hold-out validation to be reliable'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 125, 'page_label': '126'}, page_content='for hold-out validation to be reliable\\n\\uf0a1 Doing iterated K-fold validation —For performing highly accurate model evalua-\\ntion when little data is available\\nJust pick one of these. In most cases, the first will work well enough. \\n4.5.4 Preparing your data\\nOnce you know what you’re training on, what you’re optimizing for, and how to evalu-\\nate your approach, you’re almost ready to begin training models. But first, you should'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 125, 'page_label': '126'}, page_content='format your data in a way that can be fed into a machine-learning model—here, we’ll\\nassume a deep neural network:\\n\\uf0a1 As you saw previously, your data should be formatted as tensors.\\n\\uf0a1 The values taken by these tensors should usually be scaled to small values: for\\nexample, in the [-1, 1] range or [0, 1] range.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 126, 'page_label': '127'}, page_content='113The universal workflow of machine learning\\n\\uf0a1 If different features take values in di fferent ranges (heterogeneous data), then\\nthe data should be normalized.\\n\\uf0a1 You may want to do some feature engineering, especially for small-data problems.\\nOnce your tensors of input data and target data are ready, you can begin to train models. \\n4.5.5 Developing a model that does better than a baseline\\nYour goal at this stage is to achieve statistical power : that is, to develop a small model'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 126, 'page_label': '127'}, page_content='that is capable of beating a dumb baseline. In the MNIST digit-classification example,\\nanything that achieves an a ccuracy greater than 0.1 can be said to have statistical\\npower; in the IMDB example, it’s anything with an accuracy greater than 0.5.\\n Note that it’s not always possible to achieve statistical power. If you can’t beat a ran-\\ndom baseline after trying multiple reasonable architectures, it may be that the answer'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 126, 'page_label': '127'}, page_content='to the question you’re asking isn’t present in the input data. Remember that you make\\ntwo hypotheses:\\n\\uf0a1 You hypothesize that your outputs can be predicted given your inputs.\\n\\uf0a1 You hypothesize that the available data is sufficiently informative to learn the\\nrelationship between inputs and outputs.\\nIt may well be that these hypotheses are false, in which case you must go back to the\\ndrawing board.\\n Assuming that things go well, you need  to make three key choices to build your'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 126, 'page_label': '127'}, page_content='first working model:\\n\\uf0a1 Last-layer activation —This establishes useful cons traints on the network’s out-\\nput. For instance, the IMDB classification example used sigmoid in the last\\nlayer; the regression example didn’t use any last-layer activation; and so on.\\n\\uf0a1 Loss function—This should match the type of problem you’re trying to solve. For\\ninstance, the IMDB example used binary_crossentropy, the regression exam-\\nple used mse, and so on.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 126, 'page_label': '127'}, page_content='ple used mse, and so on.\\n\\uf0a1 Optimization configuration —What optimizer will you use? What will its learning\\nrate be? In most cases, it’s safe to go with rmsprop and its default learning rate.\\nRegarding the choice of a loss function, note  that it isn’t always possible to directly\\noptimize for the metric that measures su ccess on a problem. Sometimes there is no\\neasy way to turn a metric into a loss function ; loss functions, after all, need to be com-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 126, 'page_label': '127'}, page_content='putable given only a mini-batch of data (ideally, a loss function should be computable\\nfor as little as a single data point) and must be differentiable (otherwise, you can’t use\\nbackpropagation to train your  network). For instance, the widely used classification\\nmetric ROC AUC can’t be directly optimi zed. Hence, in classifi cation tasks, it’s com-\\nmon to optimize for a proxy metric of ROC AUC, such as crossentropy. In general, you'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 126, 'page_label': '127'}, page_content='can hope that the lower the crossentropy gets, the higher the ROC AUC will be.\\n Table 4.1 can help you choose a last-layer  activation and a loss function for a few\\ncommon problem types.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 127, 'page_label': '128'}, page_content='114 CHAPTER 4 Fundamentals of machine learning\\n \\n4.5.6 Scaling up: developing a model that overfits\\nOnce you’ve obtained a model that has statistical power, the question becomes, is your\\nmodel sufficiently powerful? Does it have enough layers and parameters to properly\\nmodel the problem at hand? For instance, a network with a single hidden layer with\\ntwo units would have statistical power on MNIST but wouldn’t be sufficient to solve the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 127, 'page_label': '128'}, page_content='problem well. Remember that the universa l tension in machine learning is between\\noptimization and generalization; the ideal model is one that stands right at the border\\nbetween underfitting and overfitting; between undercapacity and overcapacity. To fig-\\nure out where this border lies, first you must cross it.\\n To figure out how big a model you’ll need, you must develop a model that overfits.\\nThis is fairly easy:\\n1 Add layers.\\n2 Make the layers bigger.\\n3 Train for more epochs.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 127, 'page_label': '128'}, page_content='3 Train for more epochs.\\nAlways monitor the training loss and validati on loss, as well as the training and valida-\\ntion values for any metrics you care abou t. When you see that the model’s perfor-\\nmance on the validation data begins to degrade, you’ve achieved overfitting.\\n The next stage is to start regularizing and tuning the model, to get as close as pos-\\nsible to the ideal model that neither underfits nor overfits. \\n4.5.7 Regularizing your model and tuning your hyperparameters'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 127, 'page_label': '128'}, page_content='This step will take the most time: you’ll re peatedly modify your model, train it, evalu-\\nate on your validation data (not the test da ta, at this point), modify it again, and\\nrepeat, until the model is as good as it can get. These are some things you should try:\\n\\uf0a1 Add dropout.\\n\\uf0a1 Try different architectures: add or remove layers.\\n\\uf0a1 Add L1 and/or L2 regularization.\\nTable 4.1 Choosing the right last-layer activation and loss function for your model\\nProblem type Last-layer activation Loss function'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 127, 'page_label': '128'}, page_content='Problem type Last-layer activation Loss function\\nBinary classification sigmoid binary_crossentropy\\nMulticlass, single-label classification softmax categorical_crossentropy\\nMulticlass, multilabel classification sigmoid binary_crossentropy\\nRegression to arbitrary values None mse\\nRegression to values between 0 and 1 sigmoid mse  or binary_crossentropy\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 128, 'page_label': '129'}, page_content='115The universal workflow of machine learning\\n\\uf0a1 Try different hyperparameter s (such as the number of units per layer or the\\nlearning rate of the optimizer) to find the optimal configuration.\\n\\uf0a1 Optionally, iterate on feature engineer ing: add new features, or remove fea-\\ntures that don’t seem to be informative.\\nBe mindful of the following: every time you use feedback from your validation process\\nto tune your model, you leak information about the validation process into the model.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 128, 'page_label': '129'}, page_content='Repeated just a few times, th is is innocuous; but done sy stematically over many itera-\\ntions, it will eventually cause your mode l to overfit to the validation process (even\\nthough no model is directly trained on an y of the validation data). This makes the\\nevaluation process less reliable.\\n Once you’ve developed a satisfactory model configuration, you can train your final\\nproduction model on all the available data  (training and validation) and evaluate it'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 128, 'page_label': '129'}, page_content='one last time on the test set.  If it turns out that performance on the test set is signifi-\\ncantly worse than the performance measured on the validation data, this may mean\\neither that your validation pr ocedure wasn’t reliable after all, or that you began over-\\nfitting to the validation data while tuning the parameters of the model. In this case,\\nyou may want to switch to a more reliabl e evaluation protocol (such as iterated K-fold\\nvalidation). \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 129, 'page_label': '130'}, page_content='116 CHAPTER 4 Fundamentals of machine learning\\nChapter summary\\n\\uf0a1 Define the problem at hand and the data on which you’ll train. Collect\\nthis data, or annotate it with labels if need be.\\n\\uf0a1 Choose how you’ll meas ure success on your probl em. Which metrics will\\nyou monitor on your validation data?\\n\\uf0a1 Determine your evaluation protocol: hold-out validation? K-fold valida-\\ntion? Which portion of the data should you use for validation?'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 129, 'page_label': '130'}, page_content='\\uf0a1 Develop a first model that does better  than a basic baseline: a model with\\nstatistical power.\\n\\uf0a1 Develop a model that overfits.\\n\\uf0a1 Regularize your model and tune its hyperparameters, based on perfor-\\nmance on the validation data. A lot of machine-learning research tends to\\nfocus only on this step—but keep the big picture in mind.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 130, 'page_label': '131'}, page_content='Part 2\\nDeep learning in practice\\nChapters 5–9 will help you gain practical intuition about how to solve real-\\nworld problems using deep learning, and will familiarize you with essential deep-\\nlearning best practices. Most of the code examples in the book are concentrated\\nin this second half.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 131, 'page_label': '132'}, page_content='Deep learning\\nfor computer vision\\nThis chapter introduces convolutiona l neural networks, also known as convnets, a\\ntype of deep-learning model almost univer sally used in computer vision applica-\\ntions. You’ll learn to apply convnets to image-classification problems—in particular\\nthose involving small training datasets, which are the most common use case if you\\naren’t a large tech company.\\nThis chapter covers\\n\\uf0a1 Understanding convolutional neural networks \\n(convnets)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 131, 'page_label': '132'}, page_content='(convnets)\\n\\uf0a1 Using data augmentation to mitigate overfitting\\n\\uf0a1 Using a pretrained convnet to do feature \\nextraction\\n\\uf0a1 Fine-tuning a pretrained convnet\\n\\uf0a1 Visualizing what convnets learn and how they \\nmake classification decisions'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 132, 'page_label': '133'}, page_content='120 CHAPTER 5 Deep learning for computer vision\\n5.1 Introduction to convnets\\nWe’re about to dive into the theory of what convnets are and why they have been so\\nsuccessful at computer vision tasks. But first, let’s take a practical look at a simple conv-\\nnet example. It uses a convnet to classify MNIST digits, a task we performed in chapter\\n2 using a densely connected network (our test accuracy then was 97.8%). Even though'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 132, 'page_label': '133'}, page_content=\"the convnet will be basic, its accuracy will  blow out of the water that of the densely\\nconnected model from chapter 2.\\n The following lines of code show you what a basic convnet looks like. It’s a stack of\\nConv2D and MaxPooling2D layers. You’ll see in a minute exactly what they do.\\nfrom keras import layers\\nfrom keras import models\\nmodel = models.Sequential()\\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\\nmodel.add(layers.MaxPooling2D((2, 2)))\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 132, 'page_label': '133'}, page_content=\"model.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\\nImportantly, a convnet takes as input tensors of shape (image_height, image_width,\\nimage_channels) (not including the batch dimension). In this case, we’ll configure\\nthe convnet to process inputs of size (28, 28, 1) , which is the format of MNIST\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 132, 'page_label': '133'}, page_content='images. We’ll do this by passing the argument input_shape=(28, 28, 1) to the first\\nlayer.\\n Let’s display the architecture of the convnet so far:\\n>>> model.summary()\\n________________________________________________________________\\nLayer (type) Output Shape Param #\\n================================================================\\nconv2d_1 (Conv2D) (None, 26, 26, 32) 320\\n________________________________________________________________\\nmaxpooling2d_1 (MaxPooling2D) (None, 13, 13, 32) 0'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 132, 'page_label': '133'}, page_content='________________________________________________________________\\nconv2d_2 (Conv2D) (None, 11, 11, 64) 18496\\n________________________________________________________________\\nmaxpooling2d_2 (MaxPooling2D) (None, 5, 5, 64) 0\\n________________________________________________________________\\nconv2d_3 (Conv2D) (None, 3, 3, 64) 36928\\n================================================================\\nTotal params: 55,744\\nTrainable params: 55,744\\nNon-trainable params: 0'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 132, 'page_label': '133'}, page_content='Trainable params: 55,744\\nNon-trainable params: 0\\nYou can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of\\nshape (height, width, channels). The width and height dimensions tend to shrink\\nListing 5.1 Instantiating a small convnet\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 133, 'page_label': '134'}, page_content='121Introduction to convnets\\nas you go deeper in the netw ork. The number of channels is controlled by the first\\nargument passed to the Conv2D layers (32 or 64).\\n The next step is to feed the last output tensor (of shape (3, 3, 64)) into a densely\\nconnected classifier network like those yo u’re already familiar with: a stack of Dense\\nlayers. These classifiers process vectors, which are 1D, whereas the current output is a'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 133, 'page_label': '134'}, page_content=\"3D tensor. First we have to flatten the 3D outputs to 1D, and then add a few Dense lay-\\ners on top.\\nmodel.add(layers.Flatten())\\nmodel.add(layers.Dense(64, activation='relu'))\\nmodel.add(layers.Dense(10, activation='softmax'))\\nWe’ll do 10-way classification, using a final layer with 10 outputs and a softmax activa-\\ntion. Here’s what the network looks like now:\\n>>> model.summary()\\nLayer (type) Output Shape Param #\\n================================================================\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 133, 'page_label': '134'}, page_content='conv2d_1 (Conv2D) (None, 26, 26, 32) 320\\n________________________________________________________________\\nmaxpooling2d_1 (MaxPooling2D) (None, 13, 13, 32) 0\\n________________________________________________________________\\nconv2d_2 (Conv2D) (None, 11, 11, 64) 18496\\n________________________________________________________________\\nmaxpooling2d_2 (MaxPooling2D) (None, 5, 5, 64) 0\\n________________________________________________________________\\nconv2d_3 (Conv2D) (None, 3, 3, 64) 36928'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 133, 'page_label': '134'}, page_content='conv2d_3 (Conv2D) (None, 3, 3, 64) 36928\\n________________________________________________________________\\nflatten_1 (Flatten) (None, 576) 0\\n________________________________________________________________\\ndense_1 (Dense) (None, 64) 36928\\n________________________________________________________________\\ndense_2 (Dense) (None, 10) 650\\n================================================================\\nTotal params: 93,322\\nTrainable params: 93,322\\nNon-trainable params: 0'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 133, 'page_label': '134'}, page_content='Trainable params: 93,322\\nNon-trainable params: 0\\nAs you can see, the (3, 3, 64) outputs are flattened into vectors of shape (576,)\\nbefore going through two Dense layers.\\n Now, let’s train the convnet on the MNIST digits. We’ll reuse a lot of the code from\\nthe MNIST example in chapter 2.\\nfrom keras.datasets import mnist\\nfrom keras.utils import to_categorical\\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\\nListing 5.2 Adding a classifier on top of the convnet'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 133, 'page_label': '134'}, page_content='Listing 5.3 Training the convnet on MNIST images\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 134, 'page_label': '135'}, page_content=\"122 CHAPTER 5 Deep learning for computer vision\\ntrain_images = train_images.reshape((60000, 28, 28, 1))\\ntrain_images = train_images.astype('float32') / 255\\ntest_images = test_images.reshape((10000, 28, 28, 1))\\ntest_images = test_images.astype('float32') / 255\\ntrain_labels = to_categorical(train_labels)\\ntest_labels = to_categorical(test_labels)\\nmodel.compile(optimizer='rmsprop',\\nloss='categorical_crossentropy',\\nmetrics=['accuracy'])\\nmodel.fit(train_images, train_labels, epochs=5, batch_size=64)\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 134, 'page_label': '135'}, page_content='Let’s evaluate the model on the test data:\\n>>> test_loss, test_acc = model.evaluate(test_images, test_labels)\\n>>> test_acc\\n0.99080000000000001\\nWhereas the densely connected network from chapter 2 had a test accuracy of 97.8%,\\nthe basic convnet has a test accuracy of 99.3%: we decreased the error rate by 68%\\n(relative). Not bad!\\n But why does this simple convnet work so well, compared to a densely connected\\nmodel? To answer this, let’s dive into what the \\nConv2D and MaxPooling2D layers do.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 134, 'page_label': '135'}, page_content='Conv2D and MaxPooling2D layers do.\\n5.1.1 The convolution operation\\nThe fundamental difference between a dens ely connected layer and a convolution\\nlayer is this: Dense layers learn global patterns in their input feature space (for exam-\\nple, for a MNIST digit, patterns involving all pixels), whereas convolution layers learn\\nlocal patterns (see figure 5.1): in the case of images, patterns found in small 2D win-\\ndows of the inputs. In the previous example, these windows were all 3 × 3.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 134, 'page_label': '135'}, page_content='Figure 5.1 Images can be broken \\ninto local patterns such as edges, \\ntextures, and so on.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 135, 'page_label': '136'}, page_content='123Introduction to convnets\\nThis key characteristic gives convnets two interesting properties:\\n\\uf0a1 The patterns they learn are translation invariant.  After learning a certain pattern in\\nthe lower-right corner of a picture, a convnet can recognize it anywhere: for\\nexample, in the upper-left corner. A densely connected network would have to\\nlearn the pattern anew if it appeared at a new location. This makes convnets\\ndata efficient when processing images (because the visual world is fundamentally'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 135, 'page_label': '136'}, page_content='translation invariant): they need fewer training samples to learn representations\\nthat have generalization power.\\n\\uf0a1 They can learn spatial hierarchies of patterns (see figure 5.2). A first convolution layer\\nwill learn small local patterns such as  edges, a second convolution layer will\\nlearn larger patterns made of the features of the first layers, and so on. This\\nallows convnets to efficiently learn increasingly complex and abstract visual con-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 135, 'page_label': '136'}, page_content='cepts (because the visual world is fundamentally spatially hierarchical).\\nConvolutions operate over \\n3D tensors, called feature maps, with two spatial axes ( height\\nand width) as well as a depth axis (also called the channels axis). For an RGB image, the\\ndimension of the depth axis is 3, becaus e the image has three color channels: red,\\ngreen, and blue. For a black- and-white picture, like the MNIST digits, the depth is 1'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 135, 'page_label': '136'}, page_content='(levels of gray). The convolution operatio n extracts patches from its input feature\\nmap and applies the same transformation to all of these patches, producing an output\\nfeature map. This output feature map is still a 3D tensor: it has a width and a height. Its\\ndepth can be arbitrary, because the output depth is a parameter of the layer, and the\\n“cat”\\nFigure 5.2 The visual world forms a spatial hierarchy of visual \\nmodules: hyperlocal edges combine into local objects such as eyes'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 135, 'page_label': '136'}, page_content='or ears, which combine into high-level concepts such as “cat.”\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 136, 'page_label': '137'}, page_content='124 CHAPTER 5 Deep learning for computer vision\\ndifferent channels in that depth axis no longer stand for specific colors as in RGB\\ninput; rather, they stand for filters. Filters encode specific aspects of the input data: at a\\nhigh level, a single filter could encode th e concept “presence of a face in the input,”\\nfor instance.\\n In the MNIST example, the first convolution layer takes a feature map of size (28,\\n28, 1) and outputs a feature map of size (26, 26, 32): it computes 32 filters over its'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 136, 'page_label': '137'}, page_content='input. Each of these 32 output channels cont ains a 26 × 26 grid of values, which is a\\nresponse map of the filter over the input, indicating the response of that filter pattern at\\ndifferent locations in the input (see figure 5.3). That is what the term feature map\\nmeans: every dimension in the depth axis is a feature (or filter), and the 2D tensor\\noutput[:, :, n] is the 2D spatial map of the response of this filter over the input.\\nConvolutions are defined by two key parameters:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 136, 'page_label': '137'}, page_content='Convolutions are defined by two key parameters:\\n\\uf0a1 Size of the patches extracted from the inputs—These are typically 3 × 3 or 5 × 5. In the\\nexample, they were 3 × 3, which is a common choice.\\n\\uf0a1 Depth of the output feature map —The number of filters computed by the convolu-\\ntion. The example started with a depth of 32 and ended with a depth of 64.\\nIn Keras Conv2D layers, these parameters are the first arguments passed to the layer:\\nConv2D(output_depth, (window_height, window_width)).'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 136, 'page_label': '137'}, page_content='A convolution works by sliding these windows of size 3 × 3 or 5 × 5 over the 3D input\\nfeature map, stopping at every po ssible location, and extracting the 3D patch of sur-\\nrounding features (shape (window_height, window_width, input_depth)). Each\\nsuch 3D patch is then transformed (via a tensor product with the same learned weight\\nmatrix, called the convolution kernel) into a 1D vector of shape (output_depth,). All of'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 136, 'page_label': '137'}, page_content='these vectors are then spatially reassembled into a 3D output map of shape (height,\\nwidth, output_depth). Every spatial location in the output feature map corresponds\\nto the same location in the input feature map (for example, the lower-right corner of\\nthe output contains information about th e lower-right corner of the input). For\\ninstance, with 3 × 3 windows, the vector output[i, j, :] comes from the 3D patch\\ninput[i-1:i+1, j-1:j+1, :]. The full process is detailed in figure 5.4.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 136, 'page_label': '137'}, page_content='Response map,\\nquantifying the presence\\nof the filter’s pattern at \\ndifferent locationsOriginal input\\nSingle filter\\nFigure 5.3 The concept of a \\nresponse map: a 2D map of the \\npresence of a pattern at different \\nlocations in an input\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 137, 'page_label': '138'}, page_content='125Introduction to convnets\\n \\nNote that the output width and height ma y differ from the input width and height.\\nThey may differ for two reasons:\\n\\uf0a1 Border effects, which can be countered by padding the input feature map\\n\\uf0a1 The use of strides, which I’ll define in a second\\nLet’s take a deeper look at these notions.\\nUNDERSTANDING BORDER EFFECTS AND PADDING\\nConsider a 5 × 5 feature map (25 tiles total) . There are only 9 tiles around which you'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 137, 'page_label': '138'}, page_content='can center a 3 × 3 window, forming a 3 × 3 grid (see figure 5.5). Hence, the output fea-\\nture map will be 3 × 3. It shrinks a little: by exactly two tiles alongside each dimension,\\nin this case. You can see this border effect in action in the earlier example: you start\\nwith 28 × 28 inputs, which become 26 × 26 after the first convolution layer.\\nHeight\\nInput feature map\\nOutput feature map\\n3 × 3 input patches\\nTransformed patches\\nWidth\\nInput\\ndepth\\nDot product\\nwith kernel\\nOutput\\ndepth\\nOutput\\ndepth'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 137, 'page_label': '138'}, page_content='Dot product\\nwith kernel\\nOutput\\ndepth\\nOutput\\ndepth\\nFigure 5.4 How convolution works\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 138, 'page_label': '139'}, page_content='126 CHAPTER 5 Deep learning for computer vision\\n \\nIf you want to get an output feature map with the same spatial dimensions as the\\ninput, you can use padding. Padding consists of adding an appropriate number of rows\\nand columns on each side of the input feature map so as to make it possible to fit cen-\\nter convolution windows around every input tile. For a 3 × 3 window, you add one col-\\numn on the right, one column on the left,  one row at the top, and one row at the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 138, 'page_label': '139'}, page_content='bottom. For a 5 × 5 window, you add two rows (see figure 5.6).\\nIn Conv2D layers, padding is configurable via the padding argument, which takes two\\nvalues: \"valid\", which means no padding (only valid window locations will be used);\\nand \"same\", which means “pad in such a way as to have an output with the same width\\nand height as the input.” The padding argument defaults to \"valid\". \\n \\nFigure 5.5 Valid locations of 3 × 3 patches in a 5 × 5 input feature map\\netc.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 138, 'page_label': '139'}, page_content='etc.\\nFigure 5.6 Padding a 5 × 5 input in order to be able to extract 25 3 × 3 patches\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 139, 'page_label': '140'}, page_content='127Introduction to convnets\\nUNDERSTANDING CONVOLUTION STRIDES\\nThe other factor that can influence output size is the notion of strides. The description\\nof convolution so far has assumed that the center tiles of the convolution windows are\\nall contiguous. But the distance between two successive windows is a parameter of the\\nconvolution, called its stride, which defaults to 1. It’s possible to have strided convolu-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 139, 'page_label': '140'}, page_content='tions: convolutions with a stride higher than 1. In figure 5.7, you can see the patches\\nextracted by a 3 × 3 convolution with stride 2 over a 5 × 5 input (without padding).\\nUsing stride 2 means the width and height of the feature map are downsampled by a\\nfactor of 2 (in addition to any changes in duced by border effects). Strided convolu-\\ntions are rarely used in practice, although  they can come in handy for some types of\\nmodels; it’s good to be familiar with the concept.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 139, 'page_label': '140'}, page_content='To downsample feature maps, instea d of strides, we tend to use the max-pooling\\noperation, which you saw in action in the first convnet example. Let’s look at it in\\nmore depth. \\n5.1.2 The max-pooling operation\\nIn the convnet example, you may have noticed that the size of the feature maps is\\nhalved after every \\nMaxPooling2D layer. For instance, before the first MaxPooling2D lay-\\ners, the feature map is 26 × 26, but the max-pooling operation halves it to 13 × 13.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 139, 'page_label': '140'}, page_content='That’s the role of max pooling: to aggre ssively downsample fe ature maps, much like\\nstrided convolutions.\\n Max pooling consists of extracting wind ows from the input feature maps and out-\\nputting the max value of each channel. It’s conceptually similar to convolution, except\\nthat instead of transforming local patches via a learned linear transformation (the con-\\nvolution kernel), they’re transformed via a hardcoded max tensor operation. A big dif-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 139, 'page_label': '140'}, page_content='ference from convolution is that max poolin g is usually done with 2 × 2 windows and\\n1\\n1 2\\n3 4\\n2\\n34\\nFigure 5.7 3 × 3 convolution patches with 2 × 2 strides\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 140, 'page_label': '141'}, page_content='128 CHAPTER 5 Deep learning for computer vision\\nstride 2, in order to downsample the feature maps by a factor of 2. On the other hand,\\nconvolution is typically done with 3 × 3 windows and no stride (stride 1).\\n Why downsample feature maps this way?  Why not remove the max-pooling layers\\nand keep fairly large feature maps all the way up? Let’s look at this option. The convo-\\nlutional base of the model would then look like this:\\nmodel_no_max_pool = models.Sequential()'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 140, 'page_label': '141'}, page_content=\"model_no_max_pool = models.Sequential()\\nmodel_no_max_pool.add(layers.Conv2D(32, (3, 3), activation='relu',\\ninput_shape=(28, 28, 1)))\\nmodel_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu'))\\nmodel_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu'))\\nHere’s a summary of the model:\\n>>> model_no_max_pool.summary()\\nLayer (type) Output Shape Param #\\n================================================================\\nconv2d_4 (Conv2D) (None, 26, 26, 32) 320\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 140, 'page_label': '141'}, page_content='conv2d_4 (Conv2D) (None, 26, 26, 32) 320\\n________________________________________________________________\\nconv2d_5 (Conv2D) (None, 24, 24, 64) 18496\\n________________________________________________________________\\nconv2d_6 (Conv2D) (None, 22, 22, 64) 36928\\n================================================================\\nTotal params: 55,744\\nTrainable params: 55,744\\nNon-trainable params: 0\\nWhat’s wrong with this setup? Two things:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 140, 'page_label': '141'}, page_content='What’s wrong with this setup? Two things:\\n\\uf0a1 It isn’t conducive to learning a spatial hierarchy of features. The 3 × 3 windows\\nin the third layer will only contain information coming from 7 × 7 windows in\\nthe initial input. The high-level patterns learned by the convnet will still be very\\nsmall with regard to the initial input, which may not be enough to learn to clas-\\nsify digits (try recognizing a digit by only looking at it through windows that are'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 140, 'page_label': '141'}, page_content='7 × 7 pixels!). We need the features from the last convolution layer to contain\\ninformation about the totality of the input.\\n\\uf0a1 The final feature map has 22 × 22 × 64 = 30,976 total coefficients per sample.\\nThis is huge. If you were to flatten it to stick a \\nDense layer of size 512 on top,\\nthat layer would have 15.8 million parameters. This is far too large for such a\\nsmall model and would result in intense overfitting.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 140, 'page_label': '141'}, page_content='In short, the reason to use downsampling is to reduce the number of feature-map\\ncoefficients to process, as well as to induce  spatial-filter hierarchies by making succes-\\nsive convolution layers look at increasingly  large windows (in terms of the fraction of\\nthe original input they cover).\\n Note that max pooling isn’t the only way you can achieve such downsampling. As\\nyou already know, you can also use strides in the prior convolution layer. And you can\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 141, 'page_label': '142'}, page_content='129Introduction to convnets\\nuse average pooling instead of max pooling, where each local input patch is trans-\\nformed by taking the average value of each  channel over the patch, rather than the\\nmax. But max pooling tends to work better than these alternative solutions. In a nut-\\nshell, the reason is that features tend to  encode the spatial presence of some pattern\\nor concept over the different tiles of the feature map (hence, the term feature map),'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 141, 'page_label': '142'}, page_content='and it’s more informative to look at the maximal presence of different features than at\\ntheir average presence. So the most reasonable subsampling strategy is to first produce\\ndense maps of features (via unstrided co nvolutions) and then look at the maximal\\nactivation of the features over small patches, rather than looking at sparser windows of\\nthe inputs (via strided convolutions) or av eraging input patches, which could cause\\nyou to miss or dilute feature-presence information.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 141, 'page_label': '142'}, page_content='At this point, you should understand the basics of convnets—feature maps, convo-\\nlution, and max pooling—and you know how to build a small convnet to solve a toy\\nproblem such as \\nMNIST digits classification. Now let’s move on to more useful, practi-\\ncal applications. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 142, 'page_label': '143'}, page_content='130 CHAPTER 5 Deep learning for computer vision\\n5.2 Training a convnet from scratch on a small dataset\\nHaving to train an image-classification mode l using very little data is a common situ-\\nation, which you’ll likely encounter in prac tice if you ever do computer vision in a\\nprofessional context. A “few” samples can mean anywhere from a few hundred to a\\nfew tens of thousands of images. As a prac tical example, we’ll focus on classifying'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 142, 'page_label': '143'}, page_content='images as dogs or cats, in a dataset contai ning 4,000 pictures of cats and dogs (2,000\\ncats, 2,000 dogs). We’ll use 2,000 pictures for training—1,000 for validation, and\\n1,000 for testing.\\n In this section, we’ll review one basic strategy to tackle this problem: training a new\\nmodel from scratch using what little data yo u have. You’ll start by naively training a\\nsmall convnet on the 2,000 training samples, without any regularization, to set a base-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 142, 'page_label': '143'}, page_content='line for what can be achieved. This will ge t you to a classification accuracy of 71%. At\\nthat point, the main issue will be overfitting. Then we’ll introduce data augmentation, a\\npowerful technique for mitigating overfitting in computer vision. By using data aug-\\nmentation, you’ll improve the network to reach an accuracy of 82%.\\n In the next section, we’ ll review two more essential techniques for applying deep'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 142, 'page_label': '143'}, page_content='learning to small datasets: feature extraction with a pretrained network  (which will get you\\nto an accuracy of 90% to 96%) and fine-tuning a pretrained network (this will get you to a\\nfinal accuracy of 97%). Together, these three strategies—training a small model from\\nscratch, doing feature extraction using a pretrained model, and fine-tuning a pre-\\ntrained model—will constitute your future  toolbox for tackling the problem of per-\\nforming image classification with small datasets.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 142, 'page_label': '143'}, page_content='forming image classification with small datasets.\\n5.2.1 The relevance of deep le arning for small-data problems\\nYou’ll sometimes hear th at deep learning only works when lots of data is available.\\nThis is valid in part: one fundamental characteristic of deep learning is that it can find\\ninteresting features in the training data on  its own, without any need for manual fea-\\nture engineering, and this can only be achi eved when lots of training examples are'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 142, 'page_label': '143'}, page_content='available. This is especially true for pr oblems where the input samples are very high-\\ndimensional, like images.\\n But what constitutes lots of samples is relative—relative to the size and depth of the\\nnetwork you’re trying to train, for starters. It isn’t possible to train a convnet to solve a\\ncomplex problem with just a few tens of samples, bu t a few hundred can potentially\\nsuffice if the model is small and well regularized and the task is simple. Because conv-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 142, 'page_label': '143'}, page_content='nets learn local, translation-invariant features, they’re highly data efficient on percep-\\ntual problems. Training a convnet from scratc h on a very small image dataset will still\\nyield reasonable results despite a relative lack of data, without the need for any custom\\nfeature engineering. You’ll see this in action in this section.\\n What’s more, deep-learning models are by nature highly repurposable: you can'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 142, 'page_label': '143'}, page_content='take, say, an image-classification or speech-to-text model trained on a large-scale dataset\\nand reuse it on a significantly different problem with only minor changes. Specifically,\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 143, 'page_label': '144'}, page_content='131Training a convnet from scratch on a small dataset\\nin the case of computer vision, many pretrained models (usually trained on the Image-\\nNet dataset) are now publicly available for download and can be used to bootstrap pow-\\nerful vision models out of very little data. That’s what you’ll do in the next section. Let’s\\nstart by getting your hands on the data. \\n5.2.2 Downloading the data\\nThe Dogs vs. Cats dataset that you’ll use isn’t packaged with Keras. It was made avail-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 143, 'page_label': '144'}, page_content='a b l e  b y  K a g g l e  a s  p a r t  o f  a  c o m p u t e r - vision competition in late 2013, back when\\nconvnets weren’t mainstream. You can download the original dataset from www.kaggle\\n.com/c/dogs-vs-cats/data (you’ll need to create a Kaggle account if you don’t already\\nhave one—don’t worry, the process is painless).\\n The pictures are medium-resolution color \\nJPEGs. Figure 5.8 shows some examples.\\nUnsurprisingly, the dogs-versus-cats Kaggle  competition in 2013 was won by entrants'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 143, 'page_label': '144'}, page_content='who used convnets. The best entries achiev ed up to 95% accuracy. In this example,\\nyou’ll get fairly close to this accuracy (i n the next section), even though you’ll train\\nyour models on less than 10% of the data that was available to the competitors.\\n This dataset contains 25,000 images of dogs and cats (12,500 from each class) and\\nis 543 MB (compressed). After downloading and uncompressing it, you’ll create a new'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 143, 'page_label': '144'}, page_content='dataset containing three subsets: a training se t with 1,000 samples of each class, a vali-\\ndation set with 500 samples of each class, and a test set with 500 samples of each class.\\nFigure 5.8 Samples from the Dogs vs. Cats dat aset. Sizes weren’t modified: the samples are \\nheterogeneous in size, appearance, and so on.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 144, 'page_label': '145'}, page_content=\"132 CHAPTER 5 Deep learning for computer vision\\n Following is the code to do this.\\nimport os, shutil\\noriginal_dataset_dir = '/Users/fchollet/Downloads/kaggle_original_data'\\nbase_dir = '/Users/fchollet/Downloads/cats_and_dogs_small'\\nos.mkdir(base_dir)\\ntrain_dir = os.path.join(base_dir, 'train')\\nos.mkdir(train_dir)\\nvalidation_dir = os.path.join(base_dir, 'validation')\\nos.mkdir(validation_dir)\\ntest_dir = os.path.join(base_dir, 'test')\\nos.mkdir(test_dir)\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 144, 'page_label': '145'}, page_content=\"os.mkdir(test_dir)\\ntrain_cats_dir = os.path.join(train_dir, 'cats')\\nos.mkdir(train_cats_dir)\\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\\nos.mkdir(train_dogs_dir)\\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')\\nos.mkdir(validation_cats_dir)\\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')\\nos.mkdir(validation_dogs_dir)\\ntest_cats_dir = os.path.join(test_dir, 'cats')\\nos.mkdir(test_cats_dir)\\ntest_dogs_dir = os.path.join(test_dir, 'dogs')\\nos.mkdir(test_dogs_dir)\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 144, 'page_label': '145'}, page_content=\"os.mkdir(test_dogs_dir)\\nfnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\\nfor fname in fnames:\\nsrc = os.path.join(original_dataset_dir, fname)\\ndst = os.path.join(train_cats_dir, fname)\\nshutil.copyfile(src, dst)\\nfnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\\nfor fname in fnames:\\nsrc = os.path.join(original_dataset_dir, fname)\\ndst = os.path.join(validation_cats_dir, fname)\\nshutil.copyfile(src, dst)\\nfnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 144, 'page_label': '145'}, page_content='for fname in fnames:\\nsrc = os.path.join(original_dataset_dir, fname)\\ndst = os.path.join(test_cats_dir, fname)\\nshutil.copyfile(src, dst)\\nListing 5.4 Copying images to training, validation, and test directories\\nPath to the directory where the \\noriginal dataset was uncompressed\\nDirectory where you’ll store\\nyour smaller dataset\\nDirectories for \\nthe training, \\nvalidation, and \\ntest splits\\nDirectory with \\ntraining cat pictures\\nDirectory with \\ntraining dog pictures\\nDirectory with'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 144, 'page_label': '145'}, page_content='training dog pictures\\nDirectory with \\nvalidation cat pictures\\nDirectory with \\nvalidation dog pictures\\nDirectory with test cat pictures\\nDirectory with test dog pictures\\nCopies the first \\n1,000 cat images \\nto train_cats_dir\\nCopies the next 500 \\ncat images to \\nvalidation_cats_dir\\nCopies the next 500 \\ncat images to \\ntest_cats_dir\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 145, 'page_label': '146'}, page_content=\"133Training a convnet from scratch on a small dataset\\nfnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\\nfor fname in fnames:\\nsrc = os.path.join(original_dataset_dir, fname)\\ndst = os.path.join(train_dogs_dir, fname)\\nshutil.copyfile(src, dst)\\nfnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\\nfor fname in fnames:\\nsrc = os.path.join(original_dataset_dir, fname)\\ndst = os.path.join(validation_dogs_dir, fname)\\nshutil.copyfile(src, dst)\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 145, 'page_label': '146'}, page_content=\"shutil.copyfile(src, dst)\\nfnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\\nfor fname in fnames:\\nsrc = os.path.join(original_dataset_dir, fname)\\ndst = os.path.join(test_dogs_dir, fname)\\nshutil.copyfile(src, dst)\\nAs a sanity check, let’s count how many pict ures are in each training split (train/vali-\\ndation/test):\\n>>> print('total training cat images:', len(os.listdir(train_cats_dir)))\\ntotal training cat images: 1000\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 145, 'page_label': '146'}, page_content=\"total training cat images: 1000\\n>>> print('total training dog images:', len(os.listdir(train_dogs_dir)))\\ntotal training dog images: 1000\\n>>> print('total validation cat images:', len(os.listdir(validation_cats_dir)))\\ntotal validation cat images: 500\\n>>> print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\\ntotal validation dog images: 500\\n>>> print('total test cat images:', len(os.listdir(test_cats_dir)))\\ntotal test cat images: 500\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 145, 'page_label': '146'}, page_content=\"total test cat images: 500\\n>>> print('total test dog images:', len(os.listdir(test_dogs_dir)))\\ntotal test dog images: 500\\nSo you do indeed have 2,000 training imag es, 1,000 validation images, and 1,000 test\\nimages. Each split contains the same number of samples from each class: this is a bal-\\nanced binary-classification problem, which me ans classification accuracy will be an\\nappropriate measure of success. \\n5.2.3 Building your network\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 145, 'page_label': '146'}, page_content='5.2.3 Building your network\\nYou built a small convnet for MNIST in the previous example, so you should be famil-\\niar with such convnets. You’ll reuse the same  general structure: th e convnet will be a\\nstack of alternated Conv2D (with relu activation) and MaxPooling2D layers.\\n But because you’re dealing with bigger images and a more complex problem, you’ll\\nmake your network larger, accord ingly: it will have one more Conv2D + MaxPooling2D'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 145, 'page_label': '146'}, page_content='stage. This serves both to augment the capa city of the network and to further reduce\\nthe size of the feature maps so they aren’t overly large when you reach the Flatten\\nlayer. Here, because you start from inputs of size 150 × 150 (a somewhat arbitrary\\nchoice), you end up with feature maps of size 7 × 7 just before the Flatten layer.\\nCopies the first \\n1,000 dog images \\nto train_dogs_dir\\nCopies the next 500 \\ndog images to \\nvalidation_dogs_dir\\nCopies the next 500 \\ndog images to \\ntest_dogs_dir'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 145, 'page_label': '146'}, page_content='Copies the next 500 \\ndog images to \\ntest_dogs_dir\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 146, 'page_label': '147'}, page_content='134 CHAPTER 5 Deep learning for computer vision\\nNOTE The depth of the feature maps prog ressively increases in the network\\n(from 32 to 128), whereas the size of the feature maps decreases (from 148 ×\\n148 to 7 × 7). This is a pattern you’ll see in almost all convnets.\\nBecause you’re attacking a binary-classification problem, you’ll end the network with a\\nsingle unit (a Dense layer of size 1) and a sigmoid activation. This unit will encode the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 146, 'page_label': '147'}, page_content=\"probability that the network is looking at one class or the other.\\nfrom keras import layers\\nfrom keras import models\\nmodel = models.Sequential()\\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu',\\ninput_shape=(150, 150, 3)))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 146, 'page_label': '147'}, page_content=\"model.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Flatten())\\nmodel.add(layers.Dense(512, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nLet’s look at how the dimensions of the feature maps change with every successive\\nlayer:\\n>>> model.summary()\\nLayer (type) Output Shape Param #\\n================================================================\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 146, 'page_label': '147'}, page_content='conv2d_1 (Conv2D) (None, 148, 148, 32) 896\\n________________________________________________________________\\nmaxpooling2d_1 (MaxPooling2D) (None, 74, 74, 32) 0\\n________________________________________________________________\\nconv2d_2 (Conv2D) (None, 72, 72, 64) 18496\\n________________________________________________________________\\nmaxpooling2d_2 (MaxPooling2D) (None, 36, 36, 64) 0\\n________________________________________________________________\\nconv2d_3 (Conv2D) (None, 34, 34, 128) 73856'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 146, 'page_label': '147'}, page_content='conv2d_3 (Conv2D) (None, 34, 34, 128) 73856\\n________________________________________________________________\\nmaxpooling2d_3 (MaxPooling2D) (None, 17, 17, 128) 0\\n________________________________________________________________\\nconv2d_4 (Conv2D) (None, 15, 15, 128) 147584\\n________________________________________________________________\\nmaxpooling2d_4 (MaxPooling2D) (None, 7, 7, 128) 0\\n________________________________________________________________\\nflatten_1 (Flatten) (None, 6272) 0'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 146, 'page_label': '147'}, page_content='flatten_1 (Flatten) (None, 6272) 0\\n________________________________________________________________\\ndense_1 (Dense) (None, 512) 3211776\\n________________________________________________________________\\nListing 5.5 Instantiating a small convnet for dogs vs. cats classification\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 147, 'page_label': '148'}, page_content='135Training a convnet from scratch on a small dataset\\ndense_2 (Dense) (None, 1) 513\\n================================================================\\nTotal params: 3,453,121\\nTrainable params: 3,453,121\\nNon-trainable params: 0\\nFor the compilation step, you’ll go with the RMSprop optimizer, as usual. Because you\\nended the network with a single sigmoid unit, you’ll use binary crossentropy as the\\nloss (as a reminder, check out table 4.1 for a cheatsheet on what loss function to use in'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 147, 'page_label': '148'}, page_content=\"various situations).   \\nfrom keras import optimizers\\nmodel.compile(loss='binary_crossentropy',\\noptimizer=optimizers.RMSprop(lr=1e-4),\\nmetrics=['acc'])\\n5.2.4 Data preprocessing\\nAs you know by now, data should be formatted into appropriately preprocessed floating-\\npoint tensors before being fed into the network. Currently, the data sits on a drive as\\nJPEG files, so the steps for getting it into the network are roughly as follows:\\n1 Read the picture files.\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 147, 'page_label': '148'}, page_content='1 Read the picture files.\\n2 Decode the JPEG content to RGB grids of pixels.\\n3 Convert these into floating-point tensors.\\n4 Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know,\\nneural networks prefer to deal with small input values).\\nIt may seem a bit daunting, bu t fortunately Keras has utilities to take care of these\\nsteps automatically. Keras has a module with image-processing helper tools, located at'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 147, 'page_label': '148'}, page_content='keras.preprocessing.image. In particular, it contains the class ImageDataGenerator,\\nwhich lets you quickly set up Python generators that can automatically turn image files\\non disk into batches of preprocessed tensors. This is what you’ll use here.\\nfrom keras.preprocessing.image import ImageDataGenerator\\ntrain_datagen = ImageDataGenerator(rescale=1./255)\\ntest_datagen = ImageDataGenerator(rescale=1./255)\\ntrain_generator = train_datagen.flow_from_directory(\\ntrain_dir,\\ntarget_size=(150, 150)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 147, 'page_label': '148'}, page_content=\"train_dir,\\ntarget_size=(150, 150)\\nbatch_size=20,\\nclass_mode='binary')\\nvalidation_generator = test_datagen.flow_from_directory(\\nvalidation_dir,\\nListing 5.6 Configuring the model for training\\nListing 5.7 Using ImageDataGenerator to read images from directories\\nRescales all images by 1/255\\nTarget\\ndirectory\\nResizes all images to 150 × 150\\nBecause you use \\nbinary_crossentropy \\nloss, you need binary \\nlabels.\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 148, 'page_label': '149'}, page_content=\"136 CHAPTER 5 Deep learning for computer vision\\ntarget_size=(150, 150),\\nbatch_size=20,\\nclass_mode='binary')\\nLet’s look at the output of one of these generators: it yields batches of 150 × 150 RGB\\nimages (shape (20, 150, 150, 3)) and binary labels (shape (20,)). There are 20 sam-\\nples in each batch (the batch size). Note that the generator yields these batches indef-\\ninitely: it loops endlessly over the images in the target folder. For this reason, you need\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 148, 'page_label': '149'}, page_content=\"to break the iteration loop at some point:\\n>>> for data_batch, labels_batch in train_generator:\\n>>> print('data batch shape:', data_batch.shape)\\n>>> print('labels batch shape:', labels_batch.shape)\\n>>> break\\ndata batch shape: (20, 150, 150, 3)\\nlabels batch shape: (20,)\\nLet’s fit the model to the data using the generator. You do so using the fit_generator\\nmethod, the equivalent of fit for data generators like this one. It expects as its first\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 148, 'page_label': '149'}, page_content='argument a Python generator that will yield batches of inputs and targets indefinitely,\\nlike this one does. Because the data is being generated endlessly, the Keras model\\nneeds to know how many samples to draw  from the generator before declaring an\\nepoch over. This is the role of the steps_per_epoch argument: after having drawn\\nsteps_per_epoch batches from the generator—that is, after having run for\\nUnderstanding Python generators'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 148, 'page_label': '149'}, page_content='Understanding Python generators\\nA Python generator is an object that acts as an iterator: it’s an object you can use\\nwith the for … in operator. Generators are built using the yield operator.\\nHere is an example of a generator that yields integers:\\ndef generator():\\ni=0\\nwhile True:\\ni+ =1\\nyield i\\nfor item in generator():\\nprint(item)\\nif item > 4:\\nbreak\\nIt prints this:\\n1\\n2\\n3\\n4\\n5\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 149, 'page_label': '150'}, page_content='137Training a convnet from scratch on a small dataset\\nsteps_per_epoch gradient descent steps—the fitt ing process will go to the next\\nepoch. In this case, batches are 20 samples,  so it will take 100 batches until you see\\nyour target of 2,000 samples.\\n When using fit_generator, you can pass a validation_data argument, much as\\nwith the fit method. It’s important to note that this argument is allowed to be a data\\ngenerator, but it could also be a tuple of Numpy arrays. If you pass a generator as'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 149, 'page_label': '150'}, page_content=\"validation_data, then this generator is expected to yield batches of validation data\\nendlessly; thus you sh ould also specify the validation_steps argument, which tells\\nthe process how many batches to draw from the validation generator for evaluation.\\nhistory = model.fit_generator(\\ntrain_generator,\\nsteps_per_epoch=100,\\nepochs=30,\\nvalidation_data=validation_generator,\\nvalidation_steps=50)\\nIt’s good practice to always save your models after training.\\nmodel.save('cats_and_dogs_small_1.h5')\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 149, 'page_label': '150'}, page_content=\"model.save('cats_and_dogs_small_1.h5')\\nLet’s plot the loss and accuracy of the model over the training and validation data\\nduring training (see figures 5.9 and 5.10).\\nimport matplotlib.pyplot as plt\\nacc = history.history['acc']\\nval_acc = history.history['val_acc']\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\nepochs = range(1, len(acc) + 1)\\nplt.plot(epochs, acc, 'bo', label='Training acc')\\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 149, 'page_label': '150'}, page_content=\"plt.title('Training and validation accuracy')\\nplt.legend()\\nplt.figure()\\nplt.plot(epochs, loss, 'bo', label='Training loss')\\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\\nplt.title('Training and validation loss')\\nplt.legend()\\nplt.show()\\nListing 5.8 Fitting the model using a batch generator\\nListing 5.9 Saving the model\\nListing 5.10 Displaying curves of loss and accuracy during training\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 150, 'page_label': '151'}, page_content='138 CHAPTER 5 Deep learning for computer vision\\nThese plots are characteristic of overfitting. The training accuracy increases linearly\\nover time, until it reaches nearly 100%, whereas the validation accuracy stalls at 70–72%.\\nThe validation loss reaches its minimum after only five epochs and then stalls, whereas\\nthe training loss keeps decreasing linearly until it reaches nearly 0.\\n Because you have relatively few training  samples (2,000), overfitting will be your'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 150, 'page_label': '151'}, page_content='number-one concern. You alre ady know about a number of  techniques that can help\\nmitigate overfitting, such as dropout and weight decay (L2 regularization). We’re now\\ngoing to work with a new one, specific to computer vision and used almost universally\\nwhen processing images with deep-learning models: data augmentation. \\n5.2.5 Using data augmentation\\nOverfitting is caused by having too few samples to learn from, rendering you unable'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 150, 'page_label': '151'}, page_content='to train a model that can generalize to new data. Given infinite data, your model\\nFigure 5.9 Training and \\nvalidation accuracy\\nFigure 5.10 Training and \\nvalidation loss\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 151, 'page_label': '152'}, page_content='139Training a convnet from scratch on a small dataset\\nwould be exposed to every possible aspect of the data distribution at hand: you would\\nnever overfit. Data augmentation takes the approach of generating more training data\\nfrom existing training samples, by augmenting the samples via a number of random\\ntransformations that yield believable-looking images. The goal is that at training time,\\nyour model will never see the exact same pi cture twice. This helps expose the model'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 151, 'page_label': '152'}, page_content=\"to more aspects of the data and generalize better.\\n In Keras, this can be done by configur ing a number of random transformations to\\nbe performed on the images read by the ImageDataGenerator instance. Let’s get\\nstarted with an example.\\ndatagen = ImageDataGenerator(\\nrotation_range=40,\\nwidth_shift_range=0.2,\\nheight_shift_range=0.2,\\nshear_range=0.2,\\nzoom_range=0.2,\\nhorizontal_flip=True,\\nfill_mode='nearest')\\nThese are just a few of the options available (for more, see the Keras documentation).\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 151, 'page_label': '152'}, page_content='Let’s quickly go over this code:\\n\\uf0a1 rotation_range is a value in degrees (0–180),  a range within which to ran-\\ndomly rotate pictures.\\n\\uf0a1 width_shift and height_shift are ranges (as a fraction of total width or\\nheight) within which to randomly translate pictures vertically or horizontally.\\n\\uf0a1 shear_range is for randomly applying shearing transformations.\\n\\uf0a1 zoom_range is for randomly zooming inside pictures.\\n\\uf0a1 horizontal_flip is for randomly flipping half the images horizontally—rele-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 151, 'page_label': '152'}, page_content='vant when there are no assumptions of  horizontal asymmetry (for example,\\nreal-world pictures).\\n\\uf0a1 fill_mode is the strategy used for filling in newly created pixels, which can\\nappear after a rotation or a width/height shift.\\nLet’s look at the augmented images (see figure 5.11).\\nfrom keras.preprocessing import image\\nfnames = [os.path.join(train_cats_dir, fname) for\\nfname in os.listdir(train_cats_dir)]\\nimg_path = fnames[3]\\nimg = image.load_img(img_path, target_size=(150, 150))'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 151, 'page_label': '152'}, page_content='Listing 5.11 Setting up a data augmentation configuration via ImageDataGenerator\\nListing 5.12 Displaying some randomly augmented training images\\nModule with image-\\npreprocessing utilities\\nChooses one image to augment\\nReads the image \\nand resizes it\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 152, 'page_label': '153'}, page_content='140 CHAPTER 5 Deep learning for computer vision\\nx = image.img_to_array(img)\\nx = x.reshape((1,) + x.shape)\\ni=0\\nfor batch in datagen.flow(x, batch_size=1):\\nplt.figure(i)\\nimgplot = plt.imshow(image.array_to_img(batch[0]))\\ni+ =1\\ni fi%4= =0 :\\nbreak\\nplt.show()\\nIf you train a new network using this data -augmentation configuration, the network\\nwill never see the same input twice. But the inputs it sees  are still heavily intercor-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 152, 'page_label': '153'}, page_content='related, because they come from a small nu mber of original images—you can’t pro-\\nduce new information, you can only remix existing information. As such, this may not\\nbe enough to completely get rid of overfitt ing. To further fight overfitting, you’ll also\\nadd a Dropout layer to your model, right before the densely connected classifier.\\nConverts it to a Numpy array with shape (150, 150, 3)\\nReshapes it to (1, 150, 150, 3)\\nGenerates batches of \\nrandomly transformed \\nimages. Loops indefinitely,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 152, 'page_label': '153'}, page_content='images. Loops indefinitely, \\nso you need to break the \\nloop at some point!\\nFigure 5.11 Generation of cat pictures via random data augmentation\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 153, 'page_label': '154'}, page_content=\"141Training a convnet from scratch on a small dataset\\n \\nmodel = models.Sequential()\\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu',\\ninput_shape=(150, 150, 3)))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\\nmodel.add(layers.MaxPooling2D((2, 2)))\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 153, 'page_label': '154'}, page_content=\"model.add(layers.MaxPooling2D((2, 2)))\\nmodel.add(layers.Flatten())\\nmodel.add(layers.Dropout(0.5))\\nmodel.add(layers.Dense(512, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nmodel.compile(loss='binary_crossentropy',\\noptimizer=optimizers.RMSprop(lr=1e-4),\\nmetrics=['acc'])\\nLet’s train the network using data augmentation and dropout.\\ntrain_datagen = ImageDataGenerator(\\nrescale=1./255,\\nrotation_range=40,\\nwidth_shift_range=0.2,\\nheight_shift_range=0.2,\\nshear_range=0.2,\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 153, 'page_label': '154'}, page_content=\"height_shift_range=0.2,\\nshear_range=0.2,\\nzoom_range=0.2,\\nhorizontal_flip=True,)\\ntest_datagen = ImageDataGenerator(rescale=1./255)\\ntrain_generator = train_datagen.flow_from_directory(\\ntrain_dir,\\ntarget_size=(150, 150),\\nbatch_size=32,\\nclass_mode='binary')\\nvalidation_generator = test_datagen.flow_from_directory(\\nvalidation_dir,\\ntarget_size=(150, 150),\\nbatch_size=32,\\nclass_mode='binary')\\nhistory = model.fit_generator(\\ntrain_generator,\\nsteps_per_epoch=100,\\nepochs=100,\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 153, 'page_label': '154'}, page_content='train_generator,\\nsteps_per_epoch=100,\\nepochs=100,\\nvalidation_data=validation_generator,\\nvalidation_steps=50)\\nListing 5.13 Defining a new convnet that includes dropout\\nListing 5.14 Training the convnet using data-augmentation generators\\nNote that the \\nvalidation data \\nshouldn’t be \\naugmented!\\nTarget\\ndirectory Resizes all images to 150 × 150\\nBecause you use \\nbinary_crossentropy \\nloss, you need binary \\nlabels.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 154, 'page_label': '155'}, page_content=\"142 CHAPTER 5 Deep learning for computer vision\\nLet’s save the model—you’ll use it in section 5.4.\\nmodel.save('cats_and_dogs_small_2.h5')\\nAnd let’s plot the results again: see figures 5.12 and 5.13. Thanks to data augmenta-\\ntion and dropout, you’re no longer overfitting: the training curves are closely tracking\\nthe validation curves. You now reach an accuracy of 82%, a 15% relative improvement\\nover the non-regularized model.\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 154, 'page_label': '155'}, page_content='over the non-regularized model.\\n \\nBy using regularization techniques even further, and by tuning the network’s parame-\\nters (such as the number of filters per convolution layer, or the number of layers in\\nthe network), you may be able to get an even better accuracy, likely up to 86% or 87%.\\nBut it would prove difficult to go any high er just by training your own convnet from\\nscratch, because you have so little data to  work with. As a next step to improve your'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 154, 'page_label': '155'}, page_content='accuracy on this problem, you’ll have to us e a pretrained model, which is the focus of\\nthe next two sections. \\nListing 5.15 Saving the model\\nFigure 5.12 Training and validation \\naccuracy with data augmentation\\nFigure 5.13 Training and validation \\nloss with data augmentation\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 155, 'page_label': '156'}, page_content='143Using a pretrained convnet\\n5.3 Using a pretrained convnet\\nA common and highly effective approach to deep learning on small image datasets is\\nto use a pretrained network. A pretrained network is a saved network that was previously\\ntrained on a large dataset, typically on a la rge-scale image-classifi cation task. If this\\noriginal dataset is large enough and general enough, then the spatial hierarchy of fea-\\ntures learned by the pretrained network can effectively act as a generic model of the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 155, 'page_label': '156'}, page_content='visual world, and hence its features can prove useful for many different computer-\\nvision problems, even though these new problems may involve completely different\\nclasses than those of the original task. For instance, you might train a network on\\nImageNet (where classes are mostly animal s and everyday objects) and then repur-\\npose this trained network for something as  remote as identifying furniture items in'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 155, 'page_label': '156'}, page_content='images. Such portability of learned features across differ ent problems is a key advan-\\ntage of deep learning compared to many older, shallow-learning approaches, and it\\nmakes deep learning very effective for small-data problems.\\n In this case, let’s consider a large convnet trained on the ImageNet dataset\\n(1.4 million labeled images and 1,000 different classes). ImageNet contains many ani-\\nmal classes, including different species of cats and dogs, and you can thus expect to'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 155, 'page_label': '156'}, page_content='perform well on the dogs-versus-cats classification problem.\\n You’ll use the VGG16 architecture, developed by Karen Simonyan and Andrew\\nZisserman in 2014; it’s a si mple and widely used convne t architecture for ImageNet. 1\\nAlthough it’s an older model, far from the current state of the art and somewhat\\nheavier than many other recent models, I chose it because its architecture is similar to\\nwhat you’re already familiar with and is easy to understand without introducing any'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 155, 'page_label': '156'}, page_content='new concepts. This may be your first encounter with one of these cutesy model\\nnames—\\nVGG, ResNet, Inception, Ince ption-ResNet, Xception, and so on; you’ll get\\nused to them, because they will come up fr equently if you keep doing deep learning\\nfor computer vision.\\n There are two ways to us e a pretrained network: feature extraction and fine-tuning.\\nWe’ll cover both of them. Let’s start with feature extraction.\\n5.3.1 Feature extraction'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 155, 'page_label': '156'}, page_content='5.3.1 Feature extraction\\nFeature extraction consists of using the representations learned by a previous network\\nto extract interesting features from new samples. These features are then run through\\na new classifier, which is trained from scratch.\\n As you saw previously, convnets used fo r image classification  comprise two parts:\\nthey start with a series of pooling and conv olution layers, and they end with a densely'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 155, 'page_label': '156'}, page_content='connected classifier. The fi rst part is called the convolutional base of the model. In the\\ncase of convnets, feature extraction consists of taking the convolutional base of a\\n1 Karen Simonyan and Andrew Zisserman, “Very Deep Co nvolutional Networks for Large-Scale Image Recog-\\nnition,” arXiv (2014), https:/ /arxiv.org/abs/1409.1556.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 156, 'page_label': '157'}, page_content='144 CHAPTER 5 Deep learning for computer vision\\npreviously trained network, running the new data through it, and training a new clas-\\nsifier on top of the output (see figure 5.14).\\nWhy only reuse the convolutional base? Could you reuse the densely connected classi-\\nfier as well? In general, doing so should be avoided. The reason is that the representa-\\ntions learned by the convolutional base are likely to be more generic and therefore'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 156, 'page_label': '157'}, page_content='more reusable: the feature maps of a convnet are presence maps of generic concepts\\nover a picture, which is likely to be useful regardless of the computer-vision problem at\\nhand. But the representations learned by the classifier will necessarily be specific to the\\nset of classes on which the model was trained—they will only contain information about\\nthe presence probability of this or that cla ss in the entire picture. Additionally, repre-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 156, 'page_label': '157'}, page_content='sentations found in densely connected layers no longer contain any information about\\nwhere objects are located in the input image: these layers get rid of the notion of space,\\nwhereas the object location is still described by convolutional feature maps. For prob-\\nlems where object location matters, densely connected features are largely useless.\\n Note that the level of generality (and th erefore reusability) of the representations'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 156, 'page_label': '157'}, page_content='extracted by specific convolution layers depends on the depth of the layer in the\\nmodel. Layers that come earlier in the mo del extract local, highly generic feature\\nmaps (such as visual edges, colors, and te xtures), whereas layers that are higher up\\nextract more-abstract concepts (such as “cat ear” or “dog eye”). So if your new dataset\\ndiffers a lot from the dataset on which the original model was trained, you may be bet-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 156, 'page_label': '157'}, page_content='ter off using only the first few layers of the model to do feature extraction, rather than\\nusing the entire convolutional base.\\nPrediction\\nInput\\nTrained\\nclassifier\\nTrained\\nconvolutional\\nbase\\nPrediction\\nInput\\nTrained\\nclassifier\\nTrained\\nconvolutional\\nbase\\nPrediction\\nInput\\nNew classifier\\n(randomly initialized)\\nTrained\\nconvolutional\\nbase\\n(frozen)\\nFigure 5.14 Swapping classifiers while keeping the same convolutional base\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 157, 'page_label': '158'}, page_content='145Using a pretrained convnet\\n In this case, because the ImageNet class set contains multiple dog and cat classes,\\nit’s likely to be beneficial to reuse the information contained in the densely connected\\nlayers of the original model. But we’ll choo se not to, in order to cover the more gen-\\neral case where the class set of the new pr oblem doesn’t overlap the class set of the\\noriginal model. Let’s put this in practice by using the convolutional base of the VGG16'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 157, 'page_label': '158'}, page_content='network, trained on ImageNet, to extrac t interesting features from cat and dog\\nimages, and then train a dogs-versus-cats classifier on top of these features.\\n The VGG16 model, among others, comes prepac kaged with Keras. You can import\\nit from the keras.applications module. Here’s the list of image-classification\\nmodels (all pretrained on the ImageNet da taset) that are available as part of keras\\n.applications:\\n\\uf0a1 Xception\\n\\uf0a1 Inception V3\\n\\uf0a1 ResNet50\\n\\uf0a1 VGG16\\n\\uf0a1 VGG19\\n\\uf0a1 MobileNet'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 157, 'page_label': '158'}, page_content=\"\\uf0a1 ResNet50\\n\\uf0a1 VGG16\\n\\uf0a1 VGG19\\n\\uf0a1 MobileNet\\nLet’s instantiate the VGG16 model.\\nfrom keras.applications import VGG16\\nconv_base = VGG16(weights='imagenet',\\ninclude_top=False,\\ninput_shape=(150, 150, 3))\\nYou pass three arguments to the constructor:\\n\\uf0a1 weights specifies the weight checkpoint from which to initialize the model.\\n\\uf0a1 include_top refers to including (or not) the densely connected classifier on\\ntop of the network. By default, this de nsely connected classifier corresponds to\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 157, 'page_label': '158'}, page_content='the 1,000 classes from ImageNet. Becaus e you intend to use your own densely\\nconnected classifier (w ith only two classes: cat and dog), you don’t need to\\ninclude it.\\n\\uf0a1 input_shape is the shape of the image tensors that you’ll feed to the network.\\nThis argument is purely optional: if you don’t pass it, the network will be able to\\nprocess inputs of any size.\\nHere’s the detail of the architecture of the VGG16 convolutional base. It’s similar to\\nthe simple convnets you’re already familiar with:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 157, 'page_label': '158'}, page_content='the simple convnets you’re already familiar with:\\n>>> conv_base.summary()\\nLayer (type) Output Shape Param #\\n================================================================\\ninput_1 (InputLayer) (None, 150, 150, 3) 0\\nListing 5.16 Instantiating the VGG16 convolutional base\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 158, 'page_label': '159'}, page_content='146 CHAPTER 5 Deep learning for computer vision\\n________________________________________________________________\\nblock1_conv1 (Convolution2D) (None, 150, 150, 64) 1792\\n________________________________________________________________\\nblock1_conv2 (Convolution2D) (None, 150, 150, 64) 36928\\n________________________________________________________________\\nblock1_pool (MaxPooling2D) (None, 75, 75, 64) 0\\n________________________________________________________________'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 158, 'page_label': '159'}, page_content='block2_conv1 (Convolution2D) (None, 75, 75, 128) 73856\\n________________________________________________________________\\nblock2_conv2 (Convolution2D) (None, 75, 75, 128) 147584\\n________________________________________________________________\\nblock2_pool (MaxPooling2D) (None, 37, 37, 128) 0\\n________________________________________________________________\\nblock3_conv1 (Convolution2D) (None, 37, 37, 256) 295168\\n________________________________________________________________'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 158, 'page_label': '159'}, page_content='block3_conv2 (Convolution2D) (None, 37, 37, 256) 590080\\n________________________________________________________________\\nblock3_conv3 (Convolution2D) (None, 37, 37, 256) 590080\\n________________________________________________________________\\nblock3_pool (MaxPooling2D) (None, 18, 18, 256) 0\\n________________________________________________________________\\nblock4_conv1 (Convolution2D) (None, 18, 18, 512) 1180160\\n________________________________________________________________'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 158, 'page_label': '159'}, page_content='block4_conv2 (Convolution2D) (None, 18, 18, 512) 2359808\\n________________________________________________________________\\nblock4_conv3 (Convolution2D) (None, 18, 18, 512) 2359808\\n________________________________________________________________\\nblock4_pool (MaxPooling2D) (None, 9, 9, 512) 0\\n________________________________________________________________\\nblock5_conv1 (Convolution2D) (None, 9, 9, 512) 2359808\\n________________________________________________________________'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 158, 'page_label': '159'}, page_content='block5_conv2 (Convolution2D) (None, 9, 9, 512) 2359808\\n________________________________________________________________\\nblock5_conv3 (Convolution2D) (None, 9, 9, 512) 2359808\\n________________________________________________________________\\nblock5_pool (MaxPooling2D) (None, 4, 4, 512) 0\\n================================================================\\nTotal params: 14,714,688\\nTrainable params: 14,714,688\\nNon-trainable params: 0'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 158, 'page_label': '159'}, page_content='Non-trainable params: 0\\nThe final feature map has shape (4, 4, 512). That’s the feature on top of which you’ll\\nstick a densely connected classifier.\\n At this point, there are two ways you could proceed:\\n\\uf0a1 Running the convolutional base over your  dataset, recording its output to a\\nNumpy array on disk, and then using this data as input to a standalone, densely\\nconnected classifier similar to those you saw in part 1 of this book. This solution'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 158, 'page_label': '159'}, page_content='is fast and cheap to run, because it only requires running the convolutional\\nbase once for every input image, and the convolutional base is by far the most\\nexpensive part of the pipeline. But for the same reason, this technique won’t\\nallow you to use data augmentation.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 159, 'page_label': '160'}, page_content='147Using a pretrained convnet\\n\\uf0a1 Extending the model you have (conv_base) by adding Dense layers on top, and\\nrunning the whole thing end to end on the input data. This will allow you to use\\ndata augmentation, because every input image goes through the convolutional\\nbase every time it’s seen by the model. But for the same reason, this technique is\\nfar more expensive than the first.\\nWe’ll cover both techniques. Let’s walk throug h the code required to set up the first'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 159, 'page_label': '160'}, page_content='one: recording the output of conv_base on your data and using these outputs as\\ninputs to a new model.\\nFAST FEATURE EXTRACTION WITHOUT DATA AUGMENTATION\\nYou’ll start by running instances of the previously introduced ImageDataGenerator to\\nextract images as Numpy arrays as well as their labels. You’ll extract features from\\nthese images by calling the predict method of the conv_base model.\\nimport os\\nimport numpy as np\\nfrom keras.preprocessing.image import ImageDataGenerator'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 159, 'page_label': '160'}, page_content=\"base_dir = '/Users/fchollet/Downloads/cats_and_dogs_small'\\ntrain_dir = os.path.join(base_dir, 'train')\\nvalidation_dir = os.path.join(base_dir, 'validation')\\ntest_dir = os.path.join(base_dir, 'test')\\ndatagen = ImageDataGenerator(rescale=1./255)\\nbatch_size = 20\\ndef extract_features(directory, sample_count):\\nfeatures = np.zeros(shape=(sample_count, 4, 4, 512))\\nlabels = np.zeros(shape=(sample_count))\\ngenerator = datagen.flow_from_directory(\\ndirectory,\\ntarget_size=(150, 150),\\nbatch_size=batch_size,\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 159, 'page_label': '160'}, page_content=\"target_size=(150, 150),\\nbatch_size=batch_size,\\nclass_mode='binary')\\ni=0\\nfor inputs_batch, labels_batch in generator:\\nfeatures_batch = conv_base.predict(inputs_batch)\\nfeatures[i * batch_size : (i + 1) * batch_size] = features_batch\\nlabels[i * batch_size : (i + 1) * batch_size] = labels_batch\\ni+ =1\\nif i * batch_size >= sample_count:\\nbreak\\nreturn features, labels\\ntrain_features, train_labels = extract_features(train_dir, 2000)\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 159, 'page_label': '160'}, page_content='validation_features, validation_labels = extract_features(validation_dir, 1000)\\ntest_features, test_labels = extract_features(test_dir, 1000)\\nThe extracted features are currently of shape (samples, 4, 4, 512). You’ll feed them\\nto a densely connected classifier, so first you must flatten them to (samples, 8192):\\nListing 5.17 Extracting features using the pretrained convolutional base\\nNote that because generators\\nyield data indefinitely in a loop,\\nyou must break after every'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 159, 'page_label': '160'}, page_content='you must break after every\\nimage has been seen once.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 160, 'page_label': '161'}, page_content='148 CHAPTER 5 Deep learning for computer vision\\ntrain_features = np.reshape(train_features, (2000, 4*4* 512))\\nvalidation_features = np.reshape(validation_features, (1000, 4*4* 512))\\ntest_features = np.reshape(test_features, (1000, 4*4* 512))\\nAt this point, you can define your densely connected classifier (note the use of drop-\\nout for regularization) and train it on the data and labels that you just recorded.\\nfrom keras import models\\nfrom keras import layers\\nfrom keras import optimizers'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 160, 'page_label': '161'}, page_content=\"from keras import optimizers\\nmodel = models.Sequential()\\nmodel.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\\nmodel.add(layers.Dropout(0.5))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer=optimizers.RMSprop(lr=2e-5),\\nloss='binary_crossentropy',\\nmetrics=['acc'])\\nhistory = model.fit(train_features, train_labels,\\nepochs=30,\\nbatch_size=20,\\nvalidation_data=(validation_features, validation_labels))\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 160, 'page_label': '161'}, page_content=\"Training is very fast, because you only have to deal with two Dense layers—an epoch\\ntakes less than one second even on CPU.\\n Let’s look at the loss and accuracy curves  during training (see figures 5.15 and\\n5.16).\\nimport matplotlib.pyplot as plt\\nacc = history.history['acc']\\nval_acc = history.history['val_acc']\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\nepochs = range(1, len(acc) + 1)\\nplt.plot(epochs, acc, 'bo', label='Training acc')\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 160, 'page_label': '161'}, page_content=\"plt.plot(epochs, acc, 'bo', label='Training acc')\\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\\nplt.title('Training and validation accuracy')\\nplt.legend()\\nplt.figure()\\nplt.plot(epochs, loss, 'bo', label='Training loss')\\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\\nplt.title('Training and validation loss')\\nplt.legend()\\nplt.show()\\nListing 5.18 Defining and training the densely connected classifier\\nListing 5.19 Plotting the results\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 161, 'page_label': '162'}, page_content='149Using a pretrained convnet\\nYou reach a validation accuracy of about 90 %—much better than you achieved in the\\nprevious section with the small model trained from scratch. But the plots also indicate\\nthat you’re overfitting almost from the start—despite using dropout with a fairly large\\nrate. That’s because this technique doesn’t use data augmentation, which is essential\\nfor preventing overfitting with small image datasets. \\nFEATURE EXTRACTION WITH DATA AUGMENTATION'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 161, 'page_label': '162'}, page_content='FEATURE EXTRACTION WITH DATA AUGMENTATION\\nNow, let’s review the second technique I mentioned for doing feature extraction,\\nwhich is much slower and more expensive, but which allows you to use data augmenta-\\ntion during traini ng: extending the conv_base model and running it end to end on\\nthe inputs.\\nNOTE This technique is so expensive that you should only attempt it if you\\nhave access to a GPU—it’s absolutely intractable on CPU. If you can’t run your'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 161, 'page_label': '162'}, page_content='code on GPU, then the previous technique is the way to go.\\nFigure 5.15 Training and validation \\naccuracy for simple feature extraction \\nFigure 5.16 Training and validation \\nloss for simple feature extraction \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 162, 'page_label': '163'}, page_content=\"150 CHAPTER 5 Deep learning for computer vision\\nBecause models behave just like layers, you can add a model (like conv_base) to a\\nSequential model just like you would add a layer.\\nfrom keras import models\\nfrom keras import layers\\nmodel = models.Sequential()\\nmodel.add(conv_base)\\nmodel.add(layers.Flatten())\\nmodel.add(layers.Dense(256, activation='relu'))\\nmodel.add(layers.Dense(1, activation='sigmoid'))\\nThis is what the model looks like now:\\n>>> model.summary()\\nLayer (type) Output Shape Param #\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 162, 'page_label': '163'}, page_content='Layer (type) Output Shape Param #\\n================================================================\\nvgg16 (Model) (None, 4, 4, 512) 14714688\\n________________________________________________________________\\nflatten_1 (Flatten) (None, 8192) 0\\n________________________________________________________________\\ndense_1 (Dense) (None, 256) 2097408\\n________________________________________________________________\\ndense_2 (Dense) (None, 1) 257'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 162, 'page_label': '163'}, page_content='dense_2 (Dense) (None, 1) 257\\n================================================================\\nTotal params: 16,812,353\\nTrainable params: 16,812,353\\nNon-trainable params: 0\\nAs you can see, the convolutional base of VGG16 has 14,714,688 parameters, which is\\nvery large. The classifier you’re adding on top has 2 million parameters.\\n Before you compile and train the model, it’s very important to freeze the convolu-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 162, 'page_label': '163'}, page_content='tional base. Freezing a layer or set of layers means pr eventing their weights from being\\nupdated during training. If you don’t do th is, then the representations that were pre-\\nviously learned by the convolutional base wi ll be modified during training. Because\\nthe Dense layers on top are randomly initialize d, very large weight updates would be\\npropagated through the network, effectively destroying the representations previously\\nlearned.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 162, 'page_label': '163'}, page_content=\"learned.\\n In Keras, you freeze a network by setting its trainable attribute to False:\\n>>> print('This is the number of trainable weights '\\n'before freezing the conv base:', len(model.trainable_weights))\\nThis is the number of trainable weights before freezing the conv base: 30\\n>>> conv_base.trainable = False\\n>>> print('This is the number of trainable weights '\\n'after freezing the conv base:', len(model.trainable_weights))\\nThis is the number of trainable weights after freezing the conv base: 4\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 162, 'page_label': '163'}, page_content='Listing 5.20 Adding a densely connected classifier on top of the convolutional base\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 163, 'page_label': '164'}, page_content='151Using a pretrained convnet\\nWith this setup, only the weights from the two Dense layers that you added will be\\ntrained. That’s a total of four weight tensors: two per layer (the main weight matrix\\nand the bias vector). Note that in order for these changes to take effect, you must first\\ncompile the model. If you ever modify weight trainability after compilation, you\\nshould then recompile the model, or these changes will be ignored.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 163, 'page_label': '164'}, page_content=\"Now you can start training your model, with the same data-augmentation configu-\\nration that you used in the previous example.\\nfrom keras.preprocessing.image import ImageDataGenerator\\nfrom keras import optimizers\\ntrain_datagen = ImageDataGenerator(\\nrescale=1./255,\\nrotation_range=40,\\nwidth_shift_range=0.2,\\nheight_shift_range=0.2,\\nshear_range=0.2,\\nzoom_range=0.2,\\nhorizontal_flip=True,\\nfill_mode='nearest')\\ntest_datagen = ImageDataGenerator(rescale=1./255)\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 163, 'page_label': '164'}, page_content=\"test_datagen = ImageDataGenerator(rescale=1./255)\\ntrain_generator = train_datagen.flow_from_directory(\\ntrain_dir,\\ntarget_size=(150, 150),\\nbatch_size=20,\\nclass_mode='binary')\\nvalidation_generator = test_datagen.flow_from_directory(\\nvalidation_dir,\\ntarget_size=(150, 150),\\nbatch_size=20,\\nclass_mode='binary')\\nmodel.compile(loss='binary_crossentropy',\\noptimizer=optimizers.RMSprop(lr=2e-5),\\nmetrics=['acc'])\\nhistory = model.fit_generator(\\ntrain_generator,\\nsteps_per_epoch=100,\\nepochs=30,\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 163, 'page_label': '164'}, page_content='train_generator,\\nsteps_per_epoch=100,\\nepochs=30,\\nvalidation_data=validation_generator,\\nvalidation_steps=50)\\nLet’s plot the results again (see figures 5.17 and 5.18). As you can see, you reach a val-\\nidation accuracy of about 96%. This is mu ch better than you achieved with the small\\nconvnet trained from scratch.\\nListing 5.21 Training the model end to end with a frozen convolutional base\\nNote that the\\nvalidation data\\nshouldn’t be\\naugmented!\\nTarget\\ndirectory Resizes all images to 150 × 150'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 163, 'page_label': '164'}, page_content='Target\\ndirectory Resizes all images to 150 × 150\\nBecause you use \\nbinary_crossentropy \\nloss, you need binary \\nlabels.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 164, 'page_label': '165'}, page_content='152 CHAPTER 5 Deep learning for computer vision\\n     \\n5.3.2 Fine-tuning\\nAnother widely used technique for mo del reuse, complementary to feature\\nextraction, is fine-tuning (see figure 5.19). Fine-tuning consists of unfreezing a few of\\nthe top layers of a frozen model base used for feature extraction, and jointly training\\nboth the newly added part of the model (in this case, the fully connected classifier)\\nand these top layers. This is called fine-tuning because it slight ly adjusts the more'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 164, 'page_label': '165'}, page_content='abstract representations of the model being reused, in order to make them more rele-\\nvant for the problem at hand.\\nFigure 5.17 Training and validation \\naccuracy for feature extraction with \\ndata augmentation\\nFigure 5.18 Training and validation \\nloss for feature extraction with data \\naugmentation\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 165, 'page_label': '166'}, page_content='153Using a pretrained convnet\\nDense\\nDense\\nFlatten\\nMaxPooling2D\\nConvolution2D\\nConvolution2D\\nConvolution2D\\nMaxPooling2D\\nConvolution2D\\nConvolution2D\\nConvolution2D\\nMaxPooling2D\\nConvolution2D\\nConvolution2D\\nConvolution2D\\nMaxPooling2D\\nConvolution2D\\nConvolution2D\\nMaxPooling2D\\nConvolution2D\\nConvolution2D\\nConv block 1: \\nfrozen\\nConv block 2: \\nfrozen\\nConv block 3: \\nfrozen\\nConv block 4: \\nfrozen\\nWe fine-tune\\nConv block 5.\\nWe fine-tune\\nour own fully \\nconnected \\nclassifier. Figure 5.19 Fine-tuning the last'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 165, 'page_label': '166'}, page_content='classifier. Figure 5.19 Fine-tuning the last \\nconvolutional block of the VGG16 network\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 166, 'page_label': '167'}, page_content='154 CHAPTER 5 Deep learning for computer vision\\nI stated earlier that it’s necessary to freeze the convolution base of VGG16 in order to\\nbe able to train a randomly initialized classi fier on top. For the same reason, it’s only\\npossible to fine-tune the top layers of the convolutional base once the classifier on top\\nhas already been trained. If the classifier isn’t already tr ained, then the error signal\\npropagating through the network during traini ng will be too large, and the represen-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 166, 'page_label': '167'}, page_content='tations previously learned by the layers bein g fine-tuned will be destroyed. Thus the\\nsteps for fine-tuning a network are as follow:\\n1 Add your custom network on top of an already-trained base network.\\n2 Freeze the base network.\\n3 Train the part you added.\\n4 Unfreeze some layers in the base network.\\n5 Jointly train both these layers and the part you added.\\nYou already completed the first three steps when doing feature extraction. Let’s pro-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 166, 'page_label': '167'}, page_content='ceed with step 4: yo u’ll unfreeze your conv_base and then freeze individual layers\\ninside it.\\n As a reminder, this is what your convolutional base looks like:\\n>>> conv_base.summary()\\nLayer (type) Output Shape Param #\\n================================================================\\ninput_1 (InputLayer) (None, 150, 150, 3) 0\\n________________________________________________________________\\nblock1_conv1 (Convolution2D) (None, 150, 150, 64) 1792'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 166, 'page_label': '167'}, page_content='________________________________________________________________\\nblock1_conv2 (Convolution2D) (None, 150, 150, 64) 36928\\n________________________________________________________________\\nblock1_pool (MaxPooling2D) (None, 75, 75, 64) 0\\n________________________________________________________________\\nblock2_conv1 (Convolution2D) (None, 75, 75, 128) 73856\\n________________________________________________________________\\nblock2_conv2 (Convolution2D) (None, 75, 75, 128) 147584'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 166, 'page_label': '167'}, page_content='________________________________________________________________\\nblock2_pool (MaxPooling2D) (None, 37, 37, 128) 0\\n________________________________________________________________\\nblock3_conv1 (Convolution2D) (None, 37, 37, 256) 295168\\n________________________________________________________________\\nblock3_conv2 (Convolution2D) (None, 37, 37, 256) 590080\\n________________________________________________________________\\nblock3_conv3 (Convolution2D) (None, 37, 37, 256) 590080'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 166, 'page_label': '167'}, page_content='________________________________________________________________\\nblock3_pool (MaxPooling2D) (None, 18, 18, 256) 0\\n________________________________________________________________\\nblock4_conv1 (Convolution2D) (None, 18, 18, 512) 1180160\\n________________________________________________________________\\nblock4_conv2 (Convolution2D) (None, 18, 18, 512) 2359808\\n________________________________________________________________\\nblock4_conv3 (Convolution2D) (None, 18, 18, 512) 2359808'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 166, 'page_label': '167'}, page_content='________________________________________________________________\\nblock4_pool (MaxPooling2D) (None, 9, 9, 512) 0\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 167, 'page_label': '168'}, page_content='155Using a pretrained convnet\\n________________________________________________________________\\nblock5_conv1 (Convolution2D) (None, 9, 9, 512) 2359808\\n________________________________________________________________\\nblock5_conv2 (Convolution2D) (None, 9, 9, 512) 2359808\\n________________________________________________________________\\nblock5_conv3 (Convolution2D) (None, 9, 9, 512) 2359808\\n________________________________________________________________'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 167, 'page_label': '168'}, page_content='block5_pool (MaxPooling2D) (None, 4, 4, 512) 0\\n================================================================\\nTotal params: 14714688\\nYou’ll fine-tune the last three convolutio nal layers, which means all layers up to\\nblock4_pool should be frozen, and the layers block5_conv1, block5_conv2, and\\nblock5_conv3 should be trainable.\\n Why not fine-tune more layers? Why not fine-tune the entire convolutional base?\\nYou could. But you need to consider the following:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 167, 'page_label': '168'}, page_content='\\uf0a1 Earlier layers in the convolutional base encode more-generic, reusable features,\\nwhereas layers higher up encode more-specialized features. It’s more useful to\\nfine-tune the more specialized features, because these are the ones that need to\\nbe repurposed on your new problem. There would be fast-decreasing returns in\\nfine-tuning lower layers.\\n\\uf0a1 The more parameters you’re training, the more you’re at risk of overfitting.\\nThe convolutional base has 15 million parameters, so it would be risky to'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 167, 'page_label': '168'}, page_content=\"attempt to train it on your small dataset.\\nThus, in this situation, it’s a good strategy to fine-tune only the top two or three layers\\nin the convolutional base. Let’s set this up, starting from where you left off in the pre-\\nvious example.\\nconv_base.trainable = True\\nset_trainable = False\\nfor layer in conv_base.layers:\\nif layer.name == 'block5_conv1':\\nset_trainable = True\\nif set_trainable:\\nlayer.trainable = True\\nelse:\\nlayer.trainable = False\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 167, 'page_label': '168'}, page_content='else:\\nlayer.trainable = False\\nNow you can begin fine-tuning the ne twork. You’ll do this with the RMSProp opti-\\nmizer, using a very low learning rate. The re ason for using a low learning rate is that\\nyou want to limit the magnitude of the modifications you make to the representations\\nof the three layers you’re fine-tuning. Up dates that are too large may harm these rep-\\nresentations.\\n \\nListing 5.22 Freezing all layers up to a specific one\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 168, 'page_label': '169'}, page_content=\"156 CHAPTER 5 Deep learning for computer vision\\n \\nmodel.compile(loss='binary_crossentropy',\\noptimizer=optimizers.RMSprop(lr=1e-5),\\nmetrics=['acc'])\\nhistory = model.fit_generator(\\ntrain_generator,\\nsteps_per_epoch=100,\\nepochs=100,\\nvalidation_data=validation_generator,\\nvalidation_steps=50)\\nLet’s plot the results using the same plotting code as before (see figures 5.20 and 5.21).\\nThese curves look noisy. To make them more readable, you can smooth them by\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 168, 'page_label': '169'}, page_content='replacing every loss and accura cy with exponential moving averages of these quanti-\\nties. Here’s a trivial utility function to do this (see figures 5.22 and 5.23).\\nListing 5.23 Fine-tuning the model\\nFigure 5.20 Training and \\nvalidation accuracy for fine-tuning \\nFigure 5.21 Training and \\nvalidation loss for fine-tuning \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 169, 'page_label': '170'}, page_content=\"157Using a pretrained convnet\\n \\ndef smooth_curve(points, factor=0.8):\\nsmoothed_points = []\\nfor point in points:\\nif smoothed_points:\\nprevious = smoothed_points[-1]\\nsmoothed_points.append(previous * factor + point * (1 - factor))\\nelse:\\nsmoothed_points.append(point)\\nreturn smoothed_points\\nplt.plot(epochs,\\nsmooth_curve(acc), 'bo', label='Smoothed training acc')\\nplt.plot(epochs,\\nsmooth_curve(val_acc), 'b', label='Smoothed validation acc')\\nplt.title('Training and validation accuracy')\\nplt.legend()\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 169, 'page_label': '170'}, page_content=\"plt.legend()\\nplt.figure()\\nplt.plot(epochs,\\nsmooth_curve(loss), 'bo', label='Smoothed training loss')\\nplt.plot(epochs,\\nsmooth_curve(val_loss), 'b', label='Smoothed validation loss')\\nplt.title('Training and validation loss')\\nplt.legend()\\nplt.show()\\nListing 5.24 Smoothing the plots\\nFigure 5.22 Smoothed curves for training and validation accuracy \\nfor fine-tuning\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 170, 'page_label': '171'}, page_content='158 CHAPTER 5 Deep learning for computer vision\\nThe validation accuracy curve look much cl eaner. You’re seeing a nice 1% absolute\\nimprovement in accuracy, from about 96% to above 97%.\\n Note that the loss curve doesn’t show any real improvement (in fact, it’s deteriorat-\\ning). You may wonder, how could accuracy st ay stable or improve if the loss isn’t\\ndecreasing? The answer is simple: what you display is an average of pointwise loss val-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 170, 'page_label': '171'}, page_content=\"ues; but what matters for accuracy is the di stribution of the loss values, not their aver-\\nage, because accuracy is the result of a bi nary thresholding of the class probability\\npredicted by the model. The model may still be improving even if this isn’t reflected\\nin the average loss.\\n You can now finally evaluate this model on the test data:\\ntest_generator = test_datagen.flow_from_directory(\\ntest_dir,\\ntarget_size=(150, 150),\\nbatch_size=20,\\nclass_mode='binary')\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 170, 'page_label': '171'}, page_content=\"batch_size=20,\\nclass_mode='binary')\\ntest_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\\nprint('test acc:', test_acc)\\nHere you get a test accuracy of 97%. In th e original Kaggle competition around this\\ndataset, this would have been one of the top results. But using modern deep-learning\\ntechniques, you managed to reach this result using only  a small fraction of the train-\\ning data available (about 10%). There is a huge difference between being able to train\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 170, 'page_label': '171'}, page_content='on 20,000 samples compared to 2,000 samples!\\n \\nFigure 5.23 Smoothed curves for training and validation loss for fine-tuning\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 171, 'page_label': '172'}, page_content='159Using a pretrained convnet\\n5.3.3 Wrapping up\\nHere’s what you should take away from the exercises in the past two sections:\\n\\uf0a1 Convnets are the best type of machin e-learning models for computer-vision\\ntasks. It’s possible to train one from sc ratch even on a very small dataset, with\\ndecent results.\\n\\uf0a1 On a small dataset, overfitting will be the main issue. Data augmentation is a\\npowerful way to fight overfitting when you’re working with image data.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 171, 'page_label': '172'}, page_content='\\uf0a1 It’s easy to reuse an existing convnet on a new dataset via feature extraction.\\nThis is a valuable technique for working with small image datasets.\\n\\uf0a1 As a complement to feature extraction, you can use fine-tuning, which adapts to\\na new problem some of the representations previously learned by an existing\\nmodel. This pushes performance a bit further.\\nNow you have a solid set of tools for deal ing with image-classi fication problems—in\\nparticular with small datasets. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 172, 'page_label': '173'}, page_content='160 CHAPTER 5 Deep learning for computer vision\\n5.4 Visualizing what convnets learn\\nIt’s often said that deep-learning models are “black boxes”: learning representations\\nthat are difficult to extract and present in  a human-readable form. Although this is\\npartially true for certain types of deep-lea rning models, it’s defi nitely not true for\\nconvnets. The representations learned by co nvnets are highly amenable to visualiza-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 172, 'page_label': '173'}, page_content='tion, in large part because they’re representations of visual concepts . Since 2013, a wide\\narray of techniques have been developed for visualizing and interpreting these repre-\\nsentations. We won’t survey all of them, bu t we’ll cover three of the most accessible\\nand useful ones:\\n\\uf0a1 Visualizing intermediate convnet outputs (intermediate activations) —Useful for\\nunderstanding how successive convnet layers transform their input, and for get-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 172, 'page_label': '173'}, page_content='ting a first idea of the meaning of individual convnet filters.\\n\\uf0a1 Visualizing convnets filters —Useful for understanding precisely what visual pat-\\ntern or concept each filter in a convnet is receptive to.\\n\\uf0a1 Visualizing heatmaps of class activation in an image —Useful for understanding\\nwhich parts of an image were identified as belonging to a given class, thus allow-\\ning you to localize objects in images.\\nFor the first method—activation visualizat ion—you’ll use the small convnet that you'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 172, 'page_label': '173'}, page_content='trained from scratch on the dogs-versus-cats classification problem in section 5.2. For\\nthe next two methods, you’ll use the VGG16 model introduced in section 5.3.\\n5.4.1 Visualizing intermediate activations\\nVisualizing intermediate activations consists  of displaying the feature maps that are\\noutput by various convolution and pooling la yers in a network, given a certain input\\n(the output of a layer is often called its activation, the output of the activation func-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 172, 'page_label': '173'}, page_content='tion). This gives a view into how an inpu t is decomposed into the different filters\\nlearned by the network. You want to visualize feature maps with three dimensions:\\nwidth, height, and depth (channels). Each  channel encodes relatively independent\\nfeatures, so the proper way to visualize these feature maps is by independently plot-\\nting the contents of every channel as a \\n2D image. Let’s start by loading the model that\\nyou saved in section 5.2:\\n>>> from keras.models import load_model'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 172, 'page_label': '173'}, page_content=\">>> from keras.models import load_model\\n>>> model = load_model('cats_and_dogs_small_2.h5')\\n>>> model.summary() <1> As a reminder.\\n________________________________________________________________\\nLayer (type) Output Shape Param #\\n================================================================\\nconv2d_5 (Conv2D) (None, 148, 148, 32) 896\\n________________________________________________________________\\nmaxpooling2d_5 (MaxPooling2D) (None, 74, 74, 32) 0\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 172, 'page_label': '173'}, page_content='________________________________________________________________\\nconv2d_6 (Conv2D) (None, 72, 72, 64) 18496\\n________________________________________________________________\\nmaxpooling2d_6 (MaxPooling2D) (None, 36, 36, 64) 0\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 173, 'page_label': '174'}, page_content='161Visualizing what convnets learn\\n________________________________________________________________\\nconv2d_7 (Conv2D) (None, 34, 34, 128) 73856\\n________________________________________________________________\\nmaxpooling2d_7 (MaxPooling2D) (None, 17, 17, 128) 0\\n________________________________________________________________\\nconv2d_8 (Conv2D) (None, 15, 15, 128) 147584\\n________________________________________________________________\\nmaxpooling2d_8 (MaxPooling2D) (None, 7, 7, 128) 0'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 173, 'page_label': '174'}, page_content='maxpooling2d_8 (MaxPooling2D) (None, 7, 7, 128) 0\\n________________________________________________________________\\nflatten_2 (Flatten) (None, 6272) 0\\n________________________________________________________________\\ndropout_1 (Dropout) (None, 6272) 0\\n________________________________________________________________\\ndense_3 (Dense) (None, 512) 3211776\\n________________________________________________________________\\ndense_4 (Dense) (None, 1) 513'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 173, 'page_label': '174'}, page_content=\"dense_4 (Dense) (None, 1) 513\\n================================================================\\nTotal params: 3,453,121\\nTrainable params: 3,453,121\\nNon-trainable params: 0\\nNext, you’ll get an input image—a picture of a cat, not part of the images the network\\nwas trained on.\\nimg_path = '/Users/fchollet/Downloads/cats_and_dogs_small/test/cats/cat.1700.jpg'\\nfrom keras.preprocessing import image\\nimport numpy as np\\nimg = image.load_img(img_path, target_size=(150, 150))\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 173, 'page_label': '174'}, page_content='img_tensor = image.img_to_array(img)\\nimg_tensor = np.expand_dims(img_tensor, axis=0)\\nimg_tensor /= 255.\\n<1> Its shape is (1, 150, 150, 3)\\nprint(img_tensor.shape)\\nLet’s display the picture (see figure 5.24).\\nimport matplotlib.pyplot as plt\\nplt.imshow(img_tensor[0])\\nplt.show()\\nListing 5.25 Preprocessing a single image\\nListing 5.26 Displaying the test picture\\nPreprocesses the image \\ninto a 4D tensor\\nRemember that the model \\nwas trained on inputs that \\nwere preprocessed this way.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 173, 'page_label': '174'}, page_content='were preprocessed this way.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 174, 'page_label': '175'}, page_content='162 CHAPTER 5 Deep learning for computer vision\\nIn order to extract the feature maps you want  to look at, you’ll create a Keras model\\nthat takes batches of images as input, and outputs the activations of all convolution and\\npooling layers. To do this, you’ll use the Keras class Model. A model is instantiated\\nusing two arguments: an input tensor (or li st of input tensors) and an output tensor\\n(or list of output tensors). The resulting class is a Keras model, just like the Sequential'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 174, 'page_label': '175'}, page_content='models you’re familiar with, mapping the specified inputs to the specified outputs.\\nWhat sets the Model class apart is that it allows for models with multiple outputs, unlike\\nSequential. For more information about the Model class, see section 7.1.\\nfrom keras import models\\nlayer_outputs = [layer.output for layer in model.layers[:8]]\\nactivation_model = models.Model(inputs=model.input, outputs=layer_outputs)\\nWhen fed an image input, this model returns the values of the layer activations in the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 174, 'page_label': '175'}, page_content='original model. This is the first time you’ve encountered a multi-output model in this\\nbook: until now, the models you’ve seen have had exactly one input and one output.\\nIn the general case, a model can have any number of inputs and outputs. This one has\\none input and eight outputs: one output per layer activation.\\n \\n \\n \\n \\nListing 5.27 Instantiating a model from an input tensor and a list of output tensors\\nFigure 5.24 The test cat picture\\nExtracts the outputs of \\nthe top eight layers'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 174, 'page_label': '175'}, page_content='Extracts the outputs of \\nthe top eight layers\\nCreates a model that will return these\\noutputs, given the model input\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 175, 'page_label': '176'}, page_content='163Visualizing what convnets learn\\n \\nactivations = activation_model.predict(img_tensor)\\nFor instance, this is the activation of the first convolution layer for the cat image input:\\n>>> first_layer_activation = activations[0]\\n>>> print(first_layer_activation.shape)\\n(1, 148, 148, 32)\\nIt’s a 148 × 148 feature map with 32 channels. Let’s try plotting the fourth channel of\\nthe activation of the first layer of the original model (see figure 5.25).\\nimport matplotlib.pyplot as plt'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 175, 'page_label': '176'}, page_content=\"import matplotlib.pyplot as plt\\nplt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')\\nThis channel appears to encode a diagonal edge detector. Let’s try the seventh chan-\\nnel (see figure 5.26)—but note that your own channels may vary, because the specific\\nfilters learned by convolution layers aren’t deterministic.\\nplt.matshow(first_layer_activation[0, :, :, 7], cmap='viridis')\\nListing 5.28 Running the model in predict mode\\nListing 5.29 Visualizing the fourth channel\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 175, 'page_label': '176'}, page_content='Listing 5.29 Visualizing the fourth channel\\nListing 5.30 Visualizing the seventh channel\\nReturns a list of five \\nNumpy arrays: one array \\nper layer activation\\nFigure 5.25 Fourth channel of the activation \\nof the first layer on the test cat picture\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 176, 'page_label': '177'}, page_content='164 CHAPTER 5 Deep learning for computer vision\\nThis one looks like a “bright green dot” dete ctor, useful to encode cat eyes. At this\\npoint, let’s plot a complete visualization of  all the activations in the network (see fig-\\nure 5.27). You’ll extract and plot every cha nnel in each of the eight activation maps,\\nand you’ll stack the results in one big image tensor, with channels stacked side by side.\\nlayer_names = []\\nfor layer in model.layers[:8]:\\nlayer_names.append(layer.name)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 176, 'page_label': '177'}, page_content='layer_names.append(layer.name)\\nimages_per_row = 16\\nfor layer_name, layer_activation in zip(layer_names, activations):\\nn_features = layer_activation.shape[-1]\\nsize = layer_activation.shape[1]\\nn_cols = n_features // images_per_row\\ndisplay_grid = np.zeros((size * n_cols, images_per_row * size))\\nfor col in range(n_cols):\\nfor row in range(images_per_row):\\nchannel_image = layer_activation[0,\\n:, :,\\ncol * images_per_row + row]\\nchannel_image -= channel_image.mean()\\nchannel_image /= channel_image.std()'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 176, 'page_label': '177'}, page_content=\"channel_image /= channel_image.std()\\nchannel_image *= 64\\nchannel_image += 128\\nchannel_image = np.clip(channel_image, 0, 255).astype('uint8')\\ndisplay_grid[col * size : (col + 1) * size,\\nrow * size : (row + 1) * size] = channel_image\\nscale = 1. / size\\nplt.figure(figsize=(scale * display_grid.shape[1],\\nscale * display_grid.shape[0]))\\nplt.title(layer_name)\\nplt.grid(False)\\nplt.imshow(display_grid, aspect='auto', cmap='viridis')\\nListing 5.31 Visualizing every channel in every intermediate activation\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 176, 'page_label': '177'}, page_content='Figure 5.26 Seventh channel of the activation \\nof the first layer on the test cat picture\\nNames of the layers, so you can \\nhave them as part of your plot\\nDisplays the feature maps\\nNumber of\\nfeatures in the\\nfeature map\\nThe feature map has shape \\n(1, size, size, n_features).\\nTiles the\\nactivation\\nchannels in\\nthis matrix\\nTiles each filter into \\na big horizontal grid\\nPost-processes\\nthe feature to\\nmake it visually\\npalatable\\nDisplays the grid\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 177, 'page_label': '178'}, page_content='165Visualizing what convnets learn\\nFigure 5.27 Every channel of every layer activation on the test cat picture\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 178, 'page_label': '179'}, page_content='166 CHAPTER 5 Deep learning for computer vision\\nThere are a few things to note here:\\n\\uf0a1 The first layer acts as a coll ection of various edge detectors. At that stage, the\\nactivations retain almost all of the information present in the initial picture.\\n\\uf0a1 As you go higher, the activations become  increasingly abstract and less visually\\ninterpretable. They begin to encode higher-level concepts such as “cat ear” and\\n“cat eye.” Higher presentations carry increasingly less information about the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 178, 'page_label': '179'}, page_content='visual contents of the image, and increa singly more information related to the\\nclass of the image.\\n\\uf0a1 The sparsity of the activations increases wi th the depth of the layer: in the first\\nlayer, all filters are activated by the in put image; but in the following layers,\\nmore and more filters are blank. This means the pattern encoded by the filter\\nisn’t found in the input image.\\nWe have just evidenced an important univer sal characteristic of the representations'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 178, 'page_label': '179'}, page_content='learned by deep neural networks: the feat ures extracted by a layer become increas-\\ningly abstract with the depth of the layer. The activations of higher layers carry less\\nand less information about the specific in put being seen, and more and more infor-\\nmation about the target (in this case, the cl ass of the image: cat or dog). A deep neu-\\nral network effectively acts as an information distillation pipeline, with raw data going in'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 178, 'page_label': '179'}, page_content='(in this case, RGB pictures) and being repeatedly transformed so that irrelevant infor-\\nmation is filtered out (for example, the sp ecific visual appearance of the image), and\\nuseful information is magnified and refined (for example, the class of the image).\\n This is analogous to the way humans an d animals perceive the world: after observ-\\ning a scene for a few seconds, a human ca n remember which abstract objects were'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 178, 'page_label': '179'}, page_content='present in it (bicycle, tree) but can’t re member the specific appearance of these\\nobjects. In fact, if you tried to draw a generic bicycle from memory, chances are you\\ncouldn’t get it even remotely right, even though you’ve seen th ousands of bicycles in\\nyour lifetime (see, for example, figure 5.28). Try it right now: this effect is absolutely\\nreal. You brain has learned to completely abstract its visual input—to transform it into'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 178, 'page_label': '179'}, page_content='high-level visual concepts while filtering out irrelevant visual details—making it tre-\\nmendously difficult to remember how things around you look.   \\nFigure 5.28 Left: attempts \\nto draw a bicycle from \\nmemory. Right: what a \\nschematic bicycle should \\nlook like.Licensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 179, 'page_label': '180'}, page_content='167Visualizing what convnets learn\\n5.4.2 Visualizing convnet filters\\nAnother easy way to inspect the filters learne d by convnets is to display the visual pat-\\ntern that each filter is meant to respond to. This can be done with gradient ascent in\\ninput space : applying gradient descent to the value of the input image of a convnet so as\\nto maximize the response of a specif ic filter, starting from a blank input image. The'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 179, 'page_label': '180'}, page_content='resulting input image will be one that the chosen filter is maximally responsive to.\\n The process is simple: you’ ll build a loss function that  maximizes the value of a\\ngiven filter in a given convolution layer,  and then you’ll use stochastic gradient\\ndescent to adjust the values of the input image so as to maximize this activation value.\\nFor instance, here’s a loss for the activation of filter 0 in the layer block3_conv1 of the\\nVGG16 network, pretrained on ImageNet.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 179, 'page_label': '180'}, page_content=\"VGG16 network, pretrained on ImageNet.\\nfrom keras.applications import VGG16\\nfrom keras import backend as K\\nmodel = VGG16(weights='imagenet',\\ninclude_top=False)\\nlayer_name = 'block3_conv1'\\nfilter_index = 0\\nlayer_output = model.get_layer(layer_name).output\\nloss = K.mean(layer_output[:, :, :, filter_index])\\nTo implement gradient descent, you’ll need  the gradient of this loss with respect to\\nthe model’s input. To do this, you’ll use the gradients function packaged with the\\nbackend module of Keras.\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 179, 'page_label': '180'}, page_content='backend module of Keras.\\ngrads = K.gradients(loss, model.input)[0]\\nA non-obvious trick to use to help the grad ient-descent process go smoothly is to nor-\\nmalize the gradient tensor by dividing it by its L2 norm (the square root of the average\\nof the square of the values in the tensor ). This ensures that the magnitude of the\\nupdates done to the input image is always within the same range.\\ngrads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 179, 'page_label': '180'}, page_content='grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\\nNow you need a way to compute the value of the loss tensor and the gradient tensor,\\ngiven an input image. You can define a Keras backend function to do this: iterate is\\nListing 5.32 Defining the loss tensor for filter visualization\\nListing 5.33 Obtaining the gradient of the loss with regard to the input\\nListing 5.34 Gradient-normalization trick\\nThe call to gradients returns a list of \\ntensors (of size 1 in this case). Hence,'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 179, 'page_label': '180'}, page_content='tensors (of size 1 in this case). Hence, \\nyou keep only the first element—\\nwhich is a tensor.\\nAdd 1e–5 before dividing \\nto avoid accidentally \\ndividing by 0.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 180, 'page_label': '181'}, page_content='168 CHAPTER 5 Deep learning for computer vision\\na function that takes a Numpy tensor (as a list of tensors of size 1) and returns a list of\\ntwo Numpy tensors: the loss value and the gradient value.\\niterate = K.function([model.input], [loss, grads])\\nimport numpy as np\\nloss_value, grads_value = iterate([np.zeros((1, 150, 150, 3))])\\nAt this point, you can define a Python loop to do stochastic gradient descent.\\ninput_img_data = np.random.random((1, 150, 150, 3)) * 20 + 128.\\nstep = 1.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 180, 'page_label': '181'}, page_content='step = 1.\\nfor i in range(40):\\nloss_value, grads_value = iterate([input_img_data])\\ninput_img_data += grads_value * step\\nThe resulting image tensor is a floating-point tensor of shape (1, 150, 150, 3), with\\nvalues that may not be integers within [0 , 255]. Hence, you need to postprocess this\\ntensor to turn it into a displayable image. You do so with the following straightforward\\nutility function.\\ndef deprocess_image(x):\\nx -= x.mean()\\nx /= (x.std() + 1e-5)\\nx* =0 . 1\\nx+ =0 . 5\\nx = np.clip(x, 0, 1)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 180, 'page_label': '181'}, page_content=\"x* =0 . 1\\nx+ =0 . 5\\nx = np.clip(x, 0, 1)\\nx* =2 5 5\\nx = np.clip(x, 0, 255).astype('uint8')\\nreturn x\\nNow you have all the pieces. Let’s put them together into a Python function that takes\\nas input a layer name and a filter index, and returns a valid image tensor representing\\nthe pattern that maximizes the activation of the specified filter.\\nListing 5.35 Fetching Numpy output values given Numpy input values\\nListing 5.36 Loss maximization via stochastic gradient descent\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 180, 'page_label': '181'}, page_content='Listing 5.37 Utility function to convert a tensor into a valid image\\nStarts from a gray image \\nwith some noise\\nRuns gradient \\nascent for 40 \\nsteps\\nComputes the loss value \\nand gradient value\\nAdjusts the input image in the \\ndirection that maximizes the loss\\nMagnitude of each gradient update\\nNormalizes the tensor: \\ncenters on 0, ensures \\nthat std is 0.1\\nClips to [0, 1]\\nConverts to an RGB array\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 181, 'page_label': '182'}, page_content='169Visualizing what convnets learn\\n \\ndef generate_pattern(layer_name, filter_index, size=150):\\nlayer_output = model.get_layer(layer_name).output\\nloss = K.mean(layer_output[:, :, :, filter_index])\\ngrads = K.gradients(loss, model.input)[0]\\ngrads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\\niterate = K.function([model.input], [loss, grads])\\ninput_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\\nstep = 1.\\nfor i in range(40):\\nloss_value, grads_value = iterate([input_img_data])'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 181, 'page_label': '182'}, page_content=\"input_img_data += grads_value * step\\nimg = input_img_data[0]\\nreturn deprocess_image(img)\\nLet’s try it (see figure 5.29):\\n>>> plt.imshow(generate_pattern('block3_conv1', 0))\\nIt seems that filter 0 in layer block3_conv1 is responsive to a polka-dot pattern. Now\\nthe fun part: you can start visualizing every f ilter in every layer. For simplicity, you’ll\\nonly look at the first 64 filters in each laye r, and you’ll only look at the first layer of\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 181, 'page_label': '182'}, page_content='each convolution block ( block1_conv1, block2_conv1, block3_conv1, block4_\\nconv1, block5_conv1). You’ll arrange the outputs on an 8 × 8 grid of 64 × 64 filter pat-\\nterns, with some black margins between each filter pattern (see figures 5.30–5.33).\\nListing 5.38 Function to generate filter visualizations\\nRuns\\ngradient\\nascent for\\n40 steps\\nBuilds a loss function that maximizes \\nthe activation of the nth filter of the \\nlayer under consideration\\nComputes the \\ngradient of the \\ninput picture with'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 181, 'page_label': '182'}, page_content='gradient of the \\ninput picture with \\nregard to this loss\\nNormalization \\ntrick: normalizes \\nthe gradient\\nReturns the loss \\nand grads given \\nthe input picture\\nStarts from a\\ngray image with\\nsome noise\\nFigure 5.29 Pattern that the zeroth \\nchannel in layer block3_conv1 \\nresponds to maximally\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 182, 'page_label': '183'}, page_content=\"170 CHAPTER 5 Deep learning for computer vision\\n \\nlayer_name = 'block1_conv1'\\nsize = 64\\nmargin = 5\\nresults = np.zeros((8 * size+7* margin, 8 * size+7* margin, 3))\\nfor i in range(8):\\nfor j in range(8):\\nfilter_img = generate_pattern(layer_name, i + (j * 8), size=size)\\nhorizontal_start =i*s i z e+i* margin\\nhorizontal_end = horizontal_start + size\\nvertical_start =j*s i z e+j* margin\\nvertical_end = vertical_start + size\\nresults[horizontal_start: horizontal_end,\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 182, 'page_label': '183'}, page_content='results[horizontal_start: horizontal_end,\\nvertical_start: vertical_end, :] = filter_img\\nplt.figure(figsize=(20, 20))\\nplt.imshow(results)\\nListing 5.39 Generating a grid of all filter response patterns in a layer\\nEmpty (black) image\\nto store results\\nIterates over the rows of the results grid\\nIterates over the columns of the results grid\\nGenerates the\\npattern for\\nfilter i + (j * 8)\\nin layer_name\\nPuts the result \\nin the square \\n(i, j) of the \\nresults grid\\nDisplays the results grid'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 182, 'page_label': '183'}, page_content='results grid\\nDisplays the results grid\\nFigure 5.30 Filter patterns for layer block1_conv1\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 183, 'page_label': '184'}, page_content='171Visualizing what convnets learn\\nFigure 5.31 Filter patterns for layer block2_conv1\\nFigure 5.32 Filter patterns for layer block3_conv1\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 184, 'page_label': '185'}, page_content='172 CHAPTER 5 Deep learning for computer vision\\nThese filter visualizations tell you a lot about how convnet layers see the world: each\\nlayer in a convnet learns a collection of filt ers such that their inputs can be expressed\\nas a combination of the filters. This is similar to how the Fourier transform decom-\\nposes signals onto a bank of cosine function s. The filters in these convnet filter banks\\nget increasingly complex and refined as you go higher in the model:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 184, 'page_label': '185'}, page_content='\\uf0a1 The filters from the first layer in the model ( block1_conv1) encode simple\\ndirectional edges and colors (or colored edges, in some cases).\\n\\uf0a1 The filters from block2_conv1 encode simple textures made from combina-\\ntions of edges and colors.\\n\\uf0a1 The filters in higher layers begin to resemble textures found in natural images:\\nfeathers, eyes, leaves, and so on. \\n5.4.3 Visualizing heatmaps of class activation\\nI’ll introduce one more visualization techni que: one that is useful for understanding'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 184, 'page_label': '185'}, page_content='which parts of a given image led a convnet to  its final classification decision. This is\\nhelpful for debugging th e decision process of a convnet, particularly in the case of a\\nclassification mistake. It also allows you to locate specific objects in an image.\\n This general category of techniques is called class activation map (CAM) visualization,\\nand it consists of producing heatmaps of class activation over input images. A class acti-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 184, 'page_label': '185'}, page_content='vation heatmap is a 2D grid of scores associated with a specific output class, computed\\nfor every location in any input image, indi cating how important each location is with\\nFigure 5.33 Filter patterns for layer block4_conv1\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 185, 'page_label': '186'}, page_content='173Visualizing what convnets learn\\nrespect to the class under consideration. For instance, given an image fed into a dogs-\\nversus-cats convnet, CAM visualization allows you to ge nerate a heatmap for the class\\n“cat,” indicating how cat-like different parts of the image are, and also a heatmap for the\\nclass “dog,” indicating how dog-like parts of the image are.\\n The specific implementation you’ll  use is the one described in “Grad- CAM: Visual'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 185, 'page_label': '186'}, page_content='Explanations from Deep Networks via Gradient-based Localization.” 2 It’s very simple:\\nit consists of taking the output feature map of a convolution layer, given an input\\nimage, and weighing every channel in that feature map by the gradient of the class\\nwith respect to the channel. Intuitively, one way to understand this trick is that you’re\\nweighting a spatial map of “how intensely the input image activates different chan-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 185, 'page_label': '186'}, page_content=\"nels” by “how important each channel is with regard to the class,” resulting in a spatial\\nmap of “how intensely the input image activates the class.”\\n We’ll demonstrate this technique using the pretrained VGG16 network again.\\nfrom keras.applications.vgg16 import VGG16\\nmodel = VGG16(weights='imagenet')\\nConsider the image of two African elephant s shown in figure 5.34 (under a Creative\\nCommons license), possibly a mother and her calf, strolling on the savanna. Let’s con-\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 185, 'page_label': '186'}, page_content='vert this image into something the VGG 16 model can read: the model was trained on\\nimages of size 224 × 244, preprocessed acco rding to a few rules that are packaged in\\nthe utility function keras.applications.vgg16.preprocess_input. So you need to\\nload the image, resize it to 224 × 224, convert it to a Numpy float32 tensor, and apply\\nthese preprocessing rules.\\n2 Ramprasaath R. Selvaraju et al., arXiv (2017), https:/ /arxiv.org/abs/ 1610.02391.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 185, 'page_label': '186'}, page_content='Listing 5.40 Loading the VGG16 network with pretrained weights\\nNote that you include the densely \\nconnected classifier on top; in all \\nprevious cases, you discarded it.\\nFigure 5.34 Test picture of African elephants\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 186, 'page_label': '187'}, page_content=\"174 CHAPTER 5 Deep learning for computer vision\\nfrom keras.preprocessing import image\\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\\nimport numpy as np\\nimg_path = '/Users/fchollet/Downloads/creative_commons_elephant.jpg'\\nimg = image.load_img(img_path, target_size=(224, 224))\\nx = image.img_to_array(img)\\nx = np.expand_dims(x, axis=0)\\nx = preprocess_input(x)\\nYou can now run the pretrained network on the image and decode its prediction vec-\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 186, 'page_label': '187'}, page_content=\"tor back to a human-readable format:\\n>>> preds = model.predict(x)\\n>>> print('Predicted:', decode_predictions(preds, top=3)[0])\\nPredicted:', [(u'n02504458', u'African_elephant', 0.92546833),\\n(u'n01871265', u'tusker', 0.070257246),\\n(u'n02504013', u'Indian_elephant', 0.0042589349)]\\nThe top three classes predicted for this image are as follows:\\n\\uf0a1 African elephant (with 92.5% probability)\\n\\uf0a1 Tusker (with 7% probability)\\n\\uf0a1 Indian elephant (with 0.4% probability)\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 186, 'page_label': '187'}, page_content='\\uf0a1 Indian elephant (with 0.4% probability)\\nThe network has recognized the image as containing an undetermined quantity of\\nAfrican elephants. The entry in the predicti on vector that was maximally activated is\\nthe one corresponding to the “African elephant” class, at index 386:\\n>>> np.argmax(preds[0])\\n386\\nTo visualize which parts of the image are th e most African elephant–like, let’s set up\\nthe Grad-CAM process.\\nafrican_e66lephant_output = model.output[:, 386]'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 186, 'page_label': '187'}, page_content=\"african_e66lephant_output = model.output[:, 386]\\nlast_conv_layer = model.get_layer('block5_conv3')\\nListing 5.41 Preprocessing an input image for VGG16\\nListing 5.42 Setting up the Grad-CAM algorithm\\nPython Imaging Library (PIL) image \\nof size 224 × 224\\nLocal path to the target image\\nfloat32 Numpy array of shape \\n(224, 224, 3)\\nAdds a dimension to transform the array \\ninto a batch of size (1, 224, 224, 3)\\nPreprocesses the batch (this does \\nchannel-wise color normalization)\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 186, 'page_label': '187'}, page_content='channel-wise color normalization)\\n“African elephant” entry in the \\nprediction vector Output feature map of \\nthe block5_conv3 layer, \\nthe last convolutional \\nlayer in VGG16\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 187, 'page_label': '188'}, page_content='175Visualizing what convnets learn\\ngrads = K.gradients(african_elephant_output, last_conv_layer.output)[0]\\npooled_grads = K.mean(grads, axis=(0, 1, 2))\\niterate = K.function([model.input],\\n[pooled_grads, last_conv_layer.output[0]])\\npooled_grads_value, conv_layer_output_value = iterate([x])\\nfor i in range(512):\\nconv_layer_output_value[:, :, i] *= pooled_grads_value[i]\\nheatmap = np.mean(conv_layer_output_value, axis=-1)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 187, 'page_label': '188'}, page_content='For visualization purposes, yo u’ll also normalize the he atmap between 0 and 1. The\\nresult is shown in figure 5.35.\\nheatmap = np.maximum(heatmap, 0)\\nheatmap /= np.max(heatmap)\\nplt.matshow(heatmap)\\nListing 5.43 Heatmap post-processing\\nGradient of the “African \\nelephant” class with regard to \\nthe output feature map of \\nblock5_conv3\\nVector of shape (512,), where each entry\\nis the mean intensity of the gradient\\nover a specific feature-map channel\\nValues of these two quantities, as'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 187, 'page_label': '188'}, page_content='Values of these two quantities, as \\nNumpy arrays, given the sample image \\nof two elephants\\nLets you access the values of the quantities \\nyou just defined: pooled_grads and the \\noutput feature map of block5_conv3, given \\na sample image\\nThe channel-wise mean of\\nthe resulting feature map\\nis the heatmap of the\\nclass activation.\\nMultiplies each\\nchannel in the\\nfeature-map array\\nby “how\\nimportant this\\nchannel is” with\\nregard to the\\n“elephant” class\\n0\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n2 4 6 8 10 12'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 187, 'page_label': '188'}, page_content='“elephant” class\\n0\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n2 4 6 8 10 12\\nFigure 5.35 African elephant class  \\nactivation heatmap over the test picture\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 188, 'page_label': '189'}, page_content=\"176 CHAPTER 5 Deep learning for computer vision\\nFinally, you’ll use OpenCV to generate an image that superimposes the original image\\non the heatmap you just obtained (see figure 5.36).\\nimport cv2\\nimg = cv2.imread(img_path)\\nheatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\\nheatmap = np.uint8(255 * heatmap)\\nheatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\\nsuperimposed_img = heatmap * 0.4 + img\\ncv2.imwrite('/Users/fchollet/Downloads/elephant_cam.jpg', superimposed_img)\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 188, 'page_label': '189'}, page_content='This visualization technique answers two important questions:\\n\\uf0a1 Why did the network think this image contained an African elephant?\\n\\uf0a1 Where is the African elephant located in the picture?\\nIn particular, it’s interesting to note that the ears of the elephant calf are strongly acti-\\nvated: this is probably how the network can tell the difference between African and\\nIndian elephants. \\nListing 5.44 Superimposing the heatmap with the original picture\\nUses cv2 to load the \\noriginal image'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 188, 'page_label': '189'}, page_content='Uses cv2 to load the \\noriginal image\\nResizes the heatmap to\\nbe the same size as the\\noriginal image\\nApplies the heatmap to the \\noriginal image\\nConverts the \\nheatmap to RGB\\n0.4 here is a heatmap \\nintensity factor.\\nSaves the image to disk\\nFigure 5.36 Superimposing the class activation heatmap on the original picture\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 189, 'page_label': '190'}, page_content='177Visualizing what convnets learn\\nChapter summary\\n\\uf0a1 Convnets are the best tool for attacking visual-classification problems.\\n\\uf0a1 Convnets work by learning a hierar chy of modular patterns and concepts\\nto represent the visual world.\\n\\uf0a1 The representations they learn are easy to inspect—convnets are the\\nopposite of black boxes!\\n\\uf0a1 You’re now capable of training your own convnet from scratch to solve an\\nimage-classification problem.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 189, 'page_label': '190'}, page_content='image-classification problem.\\n\\uf0a1 You understand how to use visual data augmentation to fight overfitting.\\n\\uf0a1 You know how to use a pretrained co nvnet to do feature extraction and\\nfine-tuning.\\n\\uf0a1 You can generate visualizations of th e filters learned by your convnets, as\\nwell as heatmaps of class activity.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 190, 'page_label': '191'}, page_content='Deep learning for\\ntext and sequences\\nThis chapter explores deep-learning mode ls that can process text (understood as\\nsequences of word or sequences of charac ters), timeseries, and sequence data in\\ngeneral. The two fundamental deep-learning algorithms for sequence processing\\nare recurrent neural networks  and 1D convnets, the one-dimensional version of the 2D\\nconvnets that we covered in the previous chapters. We’ll discuss both of these\\napproaches in this chapter.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 190, 'page_label': '191'}, page_content='approaches in this chapter.\\n Applications of these algorithms include the following:\\n\\uf0a1 Document classification and timeseries classification, such as identifying the\\ntopic of an article or the author of a book\\n\\uf0a1 Timeseries comparisons, such as esti mating how closely related two docu-\\nments or two stock tickers are\\nThis chapter covers\\n\\uf0a1 Preprocessing text data into useful \\nrepresentations\\n\\uf0a1 Working with recurrent neural networks\\n\\uf0a1 Using 1D convnets for sequence processing'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 191, 'page_label': '192'}, page_content='179\\n\\uf0a1 Sequence-to-sequence learning, such as  decoding an English sentence into\\nFrench\\n\\uf0a1 Sentiment analysis, such as classifying th e sentiment of tweets or movie reviews\\nas positive or negative\\n\\uf0a1 Timeseries forecasting, such  as predicting the future weather at a certain loca-\\ntion, given recent weather data\\nThis chapter’s examples focus on two na rrow tasks: sentiment analysis on the IMDB\\ndataset, a task we approached earlier in the book, and temperature forecasting. But'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 191, 'page_label': '192'}, page_content='the techniques demonstrated for these two tasks are relevant to all the applications\\njust listed, and many more.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 192, 'page_label': '193'}, page_content='180 CHAPTER 6 Deep learning for text and sequences\\n6.1 Working with text data\\nText is one of the most widespread forms of sequence data. It can be understood as\\neither a sequence of characters or a sequence of words, but it’s most common to work\\nat the level of words. The deep-learning sequence-processing mo dels introduced in\\nthe following sections can use text to produce a basic form of natural-language under-\\nstanding, sufficient for applications incl uding document classi fication, sentiment'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 192, 'page_label': '193'}, page_content='analysis, author identification, and even question-answering ( QA) (in a constrained\\ncontext). Of course, keep in mind throughout this chapter that none of these deep-\\nlearning models truly understand text in  a human sense; rather, these models can\\nmap the statistical structure of written lang uage, which is sufficient to solve many sim-\\nple textual tasks. Deep learning for natural-language processing is pattern recognition'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 192, 'page_label': '193'}, page_content='applied to words, sentences, and paragrap hs, in much the same way that computer\\nvision is pattern recognition applied to pixels.\\n Like all other neural netw orks, deep-learning models don’t take as input raw text:\\nthey only work with numeric tensors. Vectorizing text is the process of transforming text\\ninto numeric tensors. This can be done in multiple ways:\\n\\uf0a1 Segment text into words, and transform each word into a vector.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 192, 'page_label': '193'}, page_content='\\uf0a1 Segment text into characters, and transform each character into a vector.\\n\\uf0a1 Extract n-grams of words or characters, and transform each n-gram into a vector.\\nN-grams are overlapping groups of multiple consecutive words or characters.\\nCollectively, the different units into which you can break down text (words, charac-\\nters, or n-grams) are called tokens, and breaking text into such tokens is called tokeniza-\\ntion. All text-vectorization processes consist of applying some tokenization scheme and'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 192, 'page_label': '193'}, page_content='then associating numeric vectors with the generated tokens. These vectors, packed\\ninto sequence tensors, are fed into deep neural networks. There are multiple ways to\\nassociate a vector with a to ken. In this section, I’ ll present two major ones: one-hot\\nencoding of tokens, and token embedding (typically used exclusively for words, and called\\nword embedding). The remainder of this section ex plains these techniques and shows'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 192, 'page_label': '193'}, page_content='how to use them to go from raw text to a Numpy tensor that you can send to a Keras\\nnetwork.\\nText\\n“The cat sat on the mat.”\\nTokens\\n“the”, “cat”, “sat”, “on”, “the”, “mat”, “.”\\nVector encoding of the tokens\\n0.0 0.0 0.4 0.0 0.0 1.0 0.0\\n0.5 1.0 0.5 0.2 0.5 0.5 0.0\\n1.0 0.2 1.0 1.0 1.0 0.0 0.0\\nthe cat sat on the mat .\\nFigure 6.1 From text \\nto tokens to vectors\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 193, 'page_label': '194'}, page_content='181Working with text data\\n \\n6.1.1 One-hot encoding of words and characters\\nOne-hot encoding is the most common, most basic way to turn a token into a vector.\\nYou saw it in action in the initial IMDB and Reuters examples in chapter 3 (done with\\nwords, in that case). It consists of asso ciating a unique integer index with every word\\nand then turning this integer index i into a binary vector of size N (the size of the\\nvocabulary); the vector is all zeros except for the i th entry, which is 1.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 193, 'page_label': '194'}, page_content='Of course, one-hot encoding can be done at the character level, as well. To unam-\\nbiguously drive home what one-hot encoding  is and how to implement it, listings 6.1\\nand 6.2 show two toy examples: one for words, the other for characters.\\nUnderstanding n-grams and bag-of-words\\nWord n-grams are groups of N (or fewer) consecutive words that you can extract from\\na sentence. The same concept may also be applied to characters instead of words.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 193, 'page_label': '194'}, page_content='Here’s a simple example. Consider the sentence “The cat sat on the mat.” It may be\\ndecomposed into the following set of 2-grams:\\n{\"The\", \"The cat\", \"cat\", \"cat sat\", \"sat\",\\n\"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}\\nIt may also be decomposed into the following set of 3-grams:\\n{\"The\", \"The cat\", \"cat\", \"cat sat\", \"The cat sat\",\\n\"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"the\",\\n\"sat on the\", \"the mat\", \"mat\", \"on the mat\"}'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 193, 'page_label': '194'}, page_content='\"sat on the\", \"the mat\", \"mat\", \"on the mat\"}\\nSuch a set is called a bag-of-2-grams or bag-of-3-grams, respectively. The term bag\\nhere refers to the fact that you’re dealing with a set of tokens rather than a list or\\nsequence: the tokens have no specific order. This family of tokenization methods is\\ncalled bag-of-words.\\nBecause bag-of-words isn’t an order-preserving tokenization method (the tokens gen-\\nerated are understood as a set, not a sequence, and the general structure of the sen-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 193, 'page_label': '194'}, page_content='tences is lost), it tends to be used in shallow language-processing models rather than\\nin deep-learning models. Extracting n-grams is a form of feature engineering, and\\ndeep learning does away with this kind of rigid, brittle approach, replacing it with hier-\\narchical feature learning. One-dimensional convnets and recurrent neural networks,\\nintroduced later in this chapter, are capable of learning representations for groups of'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 193, 'page_label': '194'}, page_content='words and characters without being explicitly told about the existence of such groups,\\nby looking at continuous word or character sequences. For this reason, we won’t\\ncover n-grams any further in this book. But do keep in mind that they’re a powerful,\\nunavoidable feature-engineering tool when using lightweight, shallow text-processing\\nmodels such as logistic regression and random forests.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 194, 'page_label': '195'}, page_content=\"182 CHAPTER 6 Deep learning for text and sequences\\n \\nimport numpy as np\\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\\ntoken_index = {}\\nfor sample in samples:\\nfor word in sample.split():\\nif word not in token_index:\\ntoken_index[word] = len(token_index) + 1\\nmax_length = 10\\nresults = np.zeros(shape=(len(samples),\\nmax_length,\\nmax(token_index.values()) + 1))\\nfor i, sample in enumerate(samples):\\nfor j, word in list(enumerate(sample.split()))[:max_length]:\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 194, 'page_label': '195'}, page_content=\"index = token_index.get(word)\\nresults[i, j, index] = 1.\\nimport string\\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\\ncharacters = string.printable\\ntoken_index = dict(zip(range(1, len(characters) + 1), characters))\\nmax_length = 50\\nresults = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\\nfor i, sample in enumerate(samples):\\nfor j, character in enumerate(sample):\\nindex = token_index.get(character)\\nresults[i, j, index] = 1.\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 194, 'page_label': '195'}, page_content='results[i, j, index] = 1.\\nNote that Keras has built-in utilities for doing one-hot encoding of text at the word level\\nor character level, starting from raw text data. You should use these utilities, because\\nthey take care of a number of important fe atures such as strippi ng special characters\\nfrom strings and only taking into account the N most common words in your dataset (a\\ncommon restriction, to avoid dealing with very large input vector spaces).'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 194, 'page_label': '195'}, page_content='Listing 6.1 Word-level one-hot encoding (toy example)\\nListing 6.2 Character-level one-hot encoding (toy example)\\nInitial data: one entry per sample (in \\nthis example, a sample is a sentence, \\nbut it could be an entire document)\\nBuilds an index of all tokens in the data\\nTokenizes the samples via the split\\nmethod. In real life, you’d also strip\\npunctuation and special characters\\nfrom the samples.\\nAssigns a unique index to each \\nunique word. Note that you don’t \\nattribute index 0 to anything.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 194, 'page_label': '195'}, page_content='attribute index 0 to anything.\\nThis is where you\\nstore the results.\\nVectorizes the samples. You’ll only\\nconsider the first max_length\\nwords in each sample.\\nAll printable ASCII\\ncharacters\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 195, 'page_label': '196'}, page_content=\"183Working with text data\\n \\nfrom keras.preprocessing.text import Tokenizer\\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\\ntokenizer = Tokenizer(num_words=1000)\\ntokenizer.fit_on_texts(samples)\\nsequences = tokenizer.texts_to_sequences(samples)\\none_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\\nword_index = tokenizer.word_index\\nprint('Found %s unique tokens.' % len(word_index))\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 195, 'page_label': '196'}, page_content='A variant of one-hot encoding is the so-called one-hot hashing trick , which you can use\\nwhen the number of unique tokens in your vocabulary is too large to handle explicitly.\\nInstead of explicitly assigning an index to each word and keeping a reference of these\\nindices in a dictionary, you can hash words in to vectors of fixed size. This is typically\\ndone with a very lightweight hashing functi on. The main advantage of this method is'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 195, 'page_label': '196'}, page_content='that it does away with maintaining an ex plicit word index, wh ich saves memory and\\nallows online encoding of the data (you can generate token vectors right away, before\\nyou’ve seen all of the available data). The one drawback of this approach is that it’s\\nsusceptible to hash collisions : two different words may end up with the same hash, and\\nsubsequently any machine-learning model looking at these hashes won’t be able to tell'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 195, 'page_label': '196'}, page_content=\"the difference between these words. The li kelihood of hash collisions decreases when\\nthe dimensionality of the hashing space is  much larger than the total number of\\nunique tokens being hashed.\\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\\ndimensionality = 1000\\nmax_length = 10\\nresults = np.zeros((len(samples), max_length, dimensionality))\\nfor i, sample in enumerate(samples):\\nfor j, word in list(enumerate(sample.split()))[:max_length]:\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 195, 'page_label': '196'}, page_content='index = abs(hash(word)) % dimensionality\\nresults[i, j, index] = 1.\\nListing 6.3 Using Keras for word-level one-hot encoding\\nListing 6.4 Word-level one-hot encoding with hashing trick (toy example)\\nCreates a tokenizer, configured\\nto only take into account the\\n1,000 most common words\\nTurns strings into lists \\nof integer indices\\nHow you can recover \\nthe word index that \\nwas computedYou could also directly get the one-hot \\nbinary representations. Vectorization \\nmodes other than one-hot encoding'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 195, 'page_label': '196'}, page_content='modes other than one-hot encoding \\nare supported by this tokenizer.\\nBuilds\\nthe\\nword\\nindex\\nStores the words as vectors of size 1,000. If you have close \\nto 1,000 words (or more), you’ll see many hash collisions, \\nwhich will decrease the accuracy of this encoding method.\\nHashes the word into a \\nrandom integer index \\nbetween 0 and 1,000\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 196, 'page_label': '197'}, page_content='184 CHAPTER 6 Deep learning for text and sequences\\n6.1.2 Using word embeddings\\nAnother popular and powerful way to associate a vector with a word is the use of dense\\nword vectors, also called word embeddings. Whereas the vectors obtained through one-hot\\nencoding are binary, sparse (mostly made of zeros), and very high-dimensional (same\\ndimensionality as the number of words in the vocabulary), word embeddings are low-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 196, 'page_label': '197'}, page_content='dimensional floating-point ve ctors (that is, dense vectors,  as opposed to sparse vec-\\ntors); see figure 6.2. Unlike the word vectors obtained via one-hot encoding, word\\nembeddings are learned from data. It’s common to see word embeddings that are\\n256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large\\nvocabularies. On the other hand, one-hot en coding words generall y leads to vectors'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 196, 'page_label': '197'}, page_content='that are 20,000-dimensional or greater (capturing a vocabulary of 20,000 tokens, in\\nthis case). So, word embeddings pack more information into far fewer dimensions.\\nThere are two ways to obtain word embeddings:\\n\\uf0a1 Learn word embeddings jointly with the main task you care about (such as doc-\\nument classification or sent iment prediction). In this setup, you start with ran-\\ndom word vectors and then learn word vectors in the same way you learn the\\nweights of a neural network.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 196, 'page_label': '197'}, page_content='weights of a neural network.\\n\\uf0a1 Load into your model word embeddings  that were precomputed using a differ-\\nent machine-learning task than the one yo u’re trying to solve. These are called\\npretrained word embeddings.\\nLet’s look at both.\\nOne-hot word vectors:\\n - Sparse\\n - High-dimensional\\n - Hardcoded\\nWord embeddings:\\n - Dense\\n - Lower-dimensional\\n - Learned from data\\nFigure 6.2 Whereas word representations \\nobtained from one-hot encoding or hashing are'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 196, 'page_label': '197'}, page_content='obtained from one-hot encoding or hashing are \\nsparse, high-dimensional, and hardcoded, word \\nembeddings are dense, relatively low-\\ndimensional, and learned from data.\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 197, 'page_label': '198'}, page_content='185Working with text data\\nLEARNING WORD EMBEDDINGS WITH THE EMBEDDING LAYER\\nThe simplest way to associate a dense vector  with a word is to choose the vector at\\nrandom. The problem with this approach is  that the resulting embedding space has\\nno structure: for instance, the words accurate and exact may end up with completely\\ndifferent embeddings, even though they’re interchangeable in mo st sentences. It’s\\ndifficult for a deep neural network to ma ke sense of such a noisy, unstructured'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 197, 'page_label': '198'}, page_content='embedding space.\\n To get a bit more abstract, the geomet ric relationships between word vectors\\nshould reflect the semantic relationships between these words. Word embeddings are\\nmeant to map human language into a geomet ric space. For instan ce, in a reasonable\\nembedding space, you would expect synonyms to be embedded into similar word vec-\\ntors; and in general, you would expect the geometric distance (such as L2 distance)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 197, 'page_label': '198'}, page_content='between any two word vectors to relate to the semantic distance between the associ-\\nated words (words meaning different things  are embedded at points far away from\\neach other, whereas related words are closer ). In addition to distance, you may want\\nspecific directions in the embedding space to be meaningful. To make this clearer, let’s\\nlook at a concrete example.\\n In figure 6.3, four words are embedded on a 2D plane:\\ncat, dog, wolf, and tiger. With the vector representations we'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 197, 'page_label': '198'}, page_content='chose here, some semantic relationships between these\\nwords can be encoded as geometric transformations. For\\ninstance, the same vector a llo ws  us  t o go  fr om  cat to tiger\\nand from dog to wolf: this vector could be interpreted as the\\n“from pet to wild animal” vector. Similarly, another vector\\nlets us go from dog to cat and from wolf to tiger, which could\\nbe interpreted as a “from canine to feline” vector.\\n In real-world word-embed ding spaces, common exam-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 197, 'page_label': '198'}, page_content='ples of meaningful geometric transformations are “gender”\\nvectors and “plural” vectors. For instance, by adding a “female” vector to the vector\\n“king,” we obtain the vector “queen.” By ad ding a “plural” vector, we obtain “kings.”\\nWord-embedding spaces typically feature th ousands of such inte rpretable and poten-\\ntially useful vectors.\\n Is there some ideal word-embedding sp ace that would perf ectly map human lan-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 197, 'page_label': '198'}, page_content='guage and could be used for any natural-language-processing task? Possibly, but we\\nhave yet to compute anything of the so rt. Also, there is no such a thing as human lan-\\nguage—there are many different languages, and they aren’t isomorphic, because a lan-\\nguage is the reflection of a specific culture and a specific  context. But more\\npragmatically, what makes a good word-embedding space depends heavily on your task:'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 197, 'page_label': '198'}, page_content='the perfect word-embedding space for an English-language movie-review sentiment-\\nanalysis model may look different from th e perfect embedding space for an English-\\nlanguage legal-document-classification model, because the importance of certain\\nsemantic relationships varies from task to task.\\n1\\n0\\n10\\nWolf Tiger\\nCat\\nDog\\nX\\nFigure 6.3 A toy example \\nof a word-embedding space\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 198, 'page_label': '199'}, page_content='186 CHAPTER 6 Deep learning for text and sequences\\n It’s thus reasonable to learn a new embedding space wi th every new task. Fortu-\\nnately, backpropagation makes this easy, an d Keras makes it even easier. It’s about\\nlearning the weights of a layer: the Embedding layer.\\nfrom keras.layers import Embedding\\nembedding_layer = Embedding(1000, 64)\\nThe Embedding l a y e r  i s  b e s t  u n d e r s t o o d  a s  a  d i c t i o n a r y  t h a t  m a p s  i n t e g e r  i n d i c e s'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 198, 'page_label': '199'}, page_content='(which stand for specific words) to dense vectors. It takes integers as input, it looks up\\nthese integers in an internal dictionary, and it returns the associated vectors. It’s effec-\\ntively a dictionary lookup (see figure 6.4).\\nThe Embedding layer takes as input a 2D tensor of integers, of shape (samples,\\nsequence_length), where each entry is a sequence of integers. It can embed\\nsequences of variable lengths: for instance, you could feed into the Embedding layer in'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 198, 'page_label': '199'}, page_content='the previous example batches with shapes (32, 10) (batch of 32 sequences of length\\n10) or (64, 15) (batch of 64 sequences of length  15). All sequences in a batch must\\nhave the same length, though (because you need to pack them into a single tensor),\\nso sequences that are shorter than others should be padded with zeros, and sequences\\nthat are longer should be truncated.\\n This layer returns a 3D floating-point tensor of shape (samples, sequence_'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 198, 'page_label': '199'}, page_content='length, embedding_dimensionality). Such a 3D tensor can then be processed by\\nan RNN layer or a 1D convolution layer (both will be  introduced in the following\\nsections).\\n When you instantiate an Embedding layer, its weights (its internal dictionary of\\ntoken vectors) are initially random, just as with any other layer. During training, these\\nword vectors are grad ually adjusted via backpropagat ion, structuring the space into'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 198, 'page_label': '199'}, page_content='something the downstream model can exploi t. Once fully trained, the embedding\\nspace will show a lot of structure—a kind of structure specialized for the specific prob-\\nlem for which you’re training your model.\\n Let’s apply this idea to the IMDB movie-review sentimen t-prediction task that\\nyou’re already familiar with. First, you’ll qu ickly prepare the data. You’ll restrict the\\nmovie reviews to the top 10 ,000 most common words (as you did the first time you'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 198, 'page_label': '199'}, page_content='worked with this dataset) and cut off the reviews after only 20 words. The network will\\nlearn 8-dimensional embeddings for each of the 10,000 words, turn the input integer\\nListing 6.5 Instantiating an Embedding layer\\nThe Embedding layer takes at least two \\narguments: the number of possible tokens \\n(here, 1,000: 1 + maximum word index) \\nand the dimensionality of the embeddings \\n(here, 64).\\nWord index Embedding layer Corresponding word vecto r\\nFigure 6.4 The Embedding layer\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 199, 'page_label': '200'}, page_content='187Working with text data\\nsequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the\\ntensor to 2D, and train a single Dense layer on top for classification.\\nfrom keras.datasets import imdb\\nfrom keras import preprocessing\\nmax_features = 10000\\nmaxlen = 20\\n(x_train, y_train), (x_test, y_test) = imdb.load_data(\\nnum_words=max_features)\\nx_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen\\nx_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 199, 'page_label': '200'}, page_content=\"from keras.models import Sequential\\nfrom keras.layers import Flatten, Dense\\nmodel = Sequential()\\nmodel.add(Embedding(10000, 8, input_length=maxlen))\\nmodel.add(Flatten())\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\\nmodel.summary()\\nhistory = model.fit(x_train, y_train,\\nepochs=10,\\nbatch_size=32,\\nvalidation_split=0.2)\\nYou get to a validation accuracy of ~76%, which is pretty good considering that you’re\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 199, 'page_label': '200'}, page_content='only looking at the first 20 words in every review. But note that merely flattening the\\nembedded sequences and training a single Dense layer on top leads to a model that\\ntreats each word in the input sequence separately, without considering inter-word\\nrelationships and sentence structure (for ex ample, this model would likely treat both\\n“this movie is a bomb” and “this movie is the bomb” as being ne gative reviews). It’s'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 199, 'page_label': '200'}, page_content='much better to add recurrent layers or 1D convolutional layers on top of the embed-\\nded sequences to learn features that take into account each sequence as a whole.\\nThat’s what we’ll focus on in the next few sections. \\nListing 6.6 Loading the IMDB data for use with an Embedding layer\\nListing 6.7 Using an Embedding layer and classifier on the IMDB data\\nNumber of words to \\nconsider as features\\nCuts off the text after this \\nnumber of words (among \\nthe max_features most \\ncommon words)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 199, 'page_label': '200'}, page_content='the max_features most \\ncommon words)\\nLoads the data as lists of integers\\nTurns the lists of integers into\\na 2D integer tensor of shape\\n(samples, maxlen)\\nSpecifies the maximum input length to the \\nEmbedding layer so you can later flatten the \\nembedded inputs. After the Embedding layer, \\nthe activations have shape (samples, maxlen, 8).\\nFlattens the 3D tensor of \\nembeddings into a 2D \\ntensor of shape (samples, \\nmaxlen * 8)\\nAdds the \\nclassifier on top\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 200, 'page_label': '201'}, page_content='188 CHAPTER 6 Deep learning for text and sequences\\nUSING PRETRAINED WORD EMBEDDINGS\\nSometimes, you have so litt le training data available that you can’t use your data\\nalone to learn an appropriat e task-specific embedding of your vocabulary. What do\\nyou do then?\\n Instead of learning word embeddings jo intly with the problem you want to solve,\\nyou can load embedding vectors from a precomputed embedding space that you\\nknow is highly structured and exhibits useful properties—that captures generic'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 200, 'page_label': '201'}, page_content='aspects of language structure. The ration ale behind using pretrained word embed-\\ndings in natural-language processing is mu ch the same as for using pretrained conv-\\nnets in image classification: you don’t ha ve enough data available to learn truly\\npowerful features on your own, but you expe ct the features that you need to be fairly\\ngeneric—that is, common visual features or semantic features. In this case, it makes\\nsense to reuse features learned on a different problem.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 200, 'page_label': '201'}, page_content='Such word embeddings are generally co mputed using word-occurrence statistics\\n(observations about what words co-occur in sentences or documents), using a variety of\\ntechniques, some involving ne ural networks, others not. The idea of a dense, low-\\ndimensional embedding space fo r words, computed in an unsupervised way, was ini-\\ntially explored by Bengio et al. in the early 2000s,\\n1 but it only started to take off in'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 200, 'page_label': '201'}, page_content='1 but it only started to take off in\\nresearch and industry applications after the release of one of the most famous and suc-\\ncessful word-embedding schemes: the Word2vec algorithm (https:/ /code.google.com/\\narchive/p/word2vec), developed by Tomas Mikolov at Google in 2013. Word2vec\\ndimensions capture specific semantic properties, such as gender.\\n There are various precomputed databases of word embeddings that you can down-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 200, 'page_label': '201'}, page_content='load and use in a Keras Embedding layer. Word2vec is one of them. Another popular\\none is called Global Vectors for Word Representation (GloVe, https:/ /nlp.stanford\\n.edu/projects/glove), which was developed by Stanford researchers in 2014. This\\nembedding technique is based on factorizin g a matrix of word co-occurrence statis-\\ntics. Its developers have made availabl e precomputed embeddings for millions of\\nEnglish tokens, obtained from Wikipedia data and Common Crawl data.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 200, 'page_label': '201'}, page_content='Let’s look at how you can get started using GloVe embeddings in a Keras model.\\nThe same method is valid for Word2vec embeddings or any other word-embedding\\ndatabase. You’ll also use this example to  refresh the text-tokenization techniques\\nintroduced a few paragraphs ago: you’ll start from raw text and work your way up. \\n6.1.3 Putting it all together: fr om raw text to word embeddings\\nYou’ll use a model similar to the one we just went over: embedding sentences in'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 200, 'page_label': '201'}, page_content='sequences of vectors, flattening them, and training a Dense layer on top. But you’ll do\\nso using pretrained word embeddings; and instead of using the pretokenized IMDB\\ndata packaged in Keras, you’ll start from scratch by downloading the original text data.\\n1 Yoshua Bengio et al., Neural Probabilistic Language Models (Springer, 2003).\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 201, 'page_label': '202'}, page_content=\"189Working with text data\\nDOWNLOADING THE IMDB DATA AS RAW TEXT\\nFirst, head to http:/ /mng.bz/0tIo and download the raw IMDB dataset. Uncompress it.\\n Now, let’s collect the individual training reviews into a list of strings, one string per\\nreview. You’ll also collect the review labels (positive/negative) into a labels list.  \\nimport os\\nimdb_dir = '/Users/fchollet/Downloads/aclImdb'\\ntrain_dir = os.path.join(imdb_dir, 'train')\\nlabels = []\\ntexts = []\\nfor label_type in ['neg', 'pos']:\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 201, 'page_label': '202'}, page_content=\"texts = []\\nfor label_type in ['neg', 'pos']:\\ndir_name = os.path.join(train_dir, label_type)\\nfor fname in os.listdir(dir_name):\\nif fname[-4:] == '.txt':\\nf = open(os.path.join(dir_name, fname))\\ntexts.append(f.read())\\nf.close()\\nif label_type == 'neg':\\nlabels.append(0)\\nelse:\\nlabels.append(1)\\nTOKENIZING THE DATA\\nLet’s vectorize the text and prepare a training and validation split, using the concepts\\nintroduced earlier in this section. Because pretrained word embeddings are meant to\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 201, 'page_label': '202'}, page_content='be particularly useful on problems where li ttle training data is available (otherwise,\\ntask-specific embeddings are likely to outperform them), we’ll add the following twist:\\nrestricting the training data to the first 200 samples. So you’ll le arn to classify movie\\nreviews after looking at just 200 examples.\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nimport numpy as np\\nmaxlen = 100\\ntraining_samples = 200\\nvalidation_samples = 10000'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 201, 'page_label': '202'}, page_content='training_samples = 200\\nvalidation_samples = 10000\\nmax_words = 10000\\ntokenizer = Tokenizer(num_words=max_words)\\ntokenizer.fit_on_texts(texts)\\nsequences = tokenizer.texts_to_sequences(texts)\\nListing 6.8 Processing the labels of the raw IMDB data\\nListing 6.9 Tokenizing the text of the raw IMDB data\\nCuts off reviews after 100 words\\nTrains on 200 samples\\nValidates on 10,000 samples\\nConsiders only the top \\n10,000 words in the dataset\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 202, 'page_label': '203'}, page_content=\"190 CHAPTER 6 Deep learning for text and sequences\\nword_index = tokenizer.word_index\\nprint('Found %s unique tokens.' % len(word_index))\\ndata = pad_sequences(sequences, maxlen=maxlen)\\nlabels = np.asarray(labels)\\nprint('Shape of data tensor:', data.shape)\\nprint('Shape of label tensor:', labels.shape)\\nindices = np.arange(data.shape[0])\\nnp.random.shuffle(indices)\\ndata = data[indices]\\nlabels = labels[indices]\\nx_train = data[:training_samples]\\ny_train = labels[:training_samples]\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 202, 'page_label': '203'}, page_content='y_train = labels[:training_samples]\\nx_val = data[training_samples: training_samples + validation_samples]\\ny_val = labels[training_samples: training_samples + validation_samples]\\nDOWNLOADING THE GLOVE WORD EMBEDDINGS\\nGo to https:/ /nlp.stanford.edu/projects/glove, and download the precomputed\\nembeddings from 2014 English Wikipedia. It’s an 822 MB zip file called glove.6B.zip,\\ncontaining 100-dimensional embedding vectors for 400,000 words (or nonword\\ntokens). Unzip it.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 202, 'page_label': '203'}, page_content=\"tokens). Unzip it. \\nPREPROCESSING THE EMBEDDINGS\\nLet’s parse the unzipped file (a .txt file) to build an index that maps words (as strings)\\nto their vector representation (as number vectors).\\nglove_dir = '/Users/fchollet/Downloads/glove.6B'\\nembeddings_index = {}\\nf = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\\nfor line in f:\\nvalues = line.split()\\nword = values[0]\\ncoefs = np.asarray(values[1:], dtype='float32')\\nembeddings_index[word] = coefs\\nf.close()\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 202, 'page_label': '203'}, page_content=\"embeddings_index[word] = coefs\\nf.close()\\nprint('Found %s word vectors.' % len(embeddings_index))\\nNext, you’ll build an embedding ma trix that you can load into an Embedding layer. It\\nmust be a matrix of shape (max_words, embedding_dim), where each entry i contains\\nthe embedding_dim-dimensional vector for the word of index i in the reference word\\nindex (built during tokenization). Note that  index 0 isn’t supposed to stand for any\\nword or token—it’s a placeholder.\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 202, 'page_label': '203'}, page_content='word or token—it’s a placeholder.\\n \\nListing 6.10 Parsing the GloVe word-embeddings file\\nSplits the data into a training set and a \\nvalidation set, but first shuffles the data, \\nbecause you’re starting with data in which \\nsamples are ordered (all negative first, then \\nall positive) \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 203, 'page_label': '204'}, page_content='191Working with text data\\n \\nembedding_dim = 100\\nembedding_matrix = np.zeros((max_words, embedding_dim))\\nfor word, i in word_index.items():\\nif i < max_words:\\nembedding_vector = embeddings_index.get(word)\\nif embedding_vector is not None:\\nembedding_matrix[i] = embedding_vector\\nDEFINING A MODEL\\nYou’ll use the same model architecture as before.  \\nfrom keras.models import Sequential\\nfrom keras.layers import Embedding, Flatten, Dense\\nmodel = Sequential()'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 203, 'page_label': '204'}, page_content=\"model = Sequential()\\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\\nmodel.add(Flatten())\\nmodel.add(Dense(32, activation='relu'))\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.summary()\\nLOADING THE GLOVE EMBEDDINGS IN THE MODEL\\nThe Embedding layer has a single weight matrix: a 2D float matrix where each entry i is\\nthe word vector meant to be associated with index i. Simple enough. Load the GloVe\\nmatrix you prepared into the Embedding layer, the first layer in the model.\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 203, 'page_label': '204'}, page_content='model.layers[0].set_weights([embedding_matrix])\\nmodel.layers[0].trainable = False\\nAdditionally, you’ll freeze the Embedding layer (set its trainable attribute to False),\\nfollowing the same rationale you’re already familiar with in the context of pretrained\\nconvnet features: when parts of a model are pretrained (like your Embedding layer)\\nand parts are randomly initialized (like your classifier), the pretrained parts shouldn’t'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 203, 'page_label': '204'}, page_content='be updated during training, to avoid forgetting what they already know. The large gra-\\ndient updates triggered by the randomly init ialized layers would be disruptive to the\\nalready-learned features. \\n \\n \\n \\nListing 6.11 Preparing the GloVe word-embeddings matrix\\nListing 6.12 Model definition\\nListing 6.13 Loading pretrained word embeddings into the Embedding layer\\nWords not found in the \\nembedding index will \\nbe all zeros. \\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 204, 'page_label': '205'}, page_content=\"192 CHAPTER 6 Deep learning for text and sequences\\nTRAINING AND EVALUATING THE MODEL\\nCompile and train the model.\\nmodel.compile(optimizer='rmsprop',\\nloss='binary_crossentropy',\\nmetrics=['acc'])\\nhistory = model.fit(x_train, y_train,\\nepochs=10,\\nbatch_size=32,\\nvalidation_data=(x_val, y_val))\\nmodel.save_weights('pre_trained_glove_model.h5')\\nNow, plot the model’s performance over time (see figures 6.5 and 6.6).\\nimport matplotlib.pyplot as plt\\nacc = history.history['acc']\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 204, 'page_label': '205'}, page_content=\"acc = history.history['acc']\\nval_acc = history.history['val_acc']\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\nepochs = range(1, len(acc) + 1)\\nplt.plot(epochs, acc, 'bo', label='Training acc')\\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\\nplt.title('Training and validation accuracy')\\nplt.legend()\\nplt.figure()\\nplt.plot(epochs, loss, 'bo', label='Training loss')\\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\\nplt.title('Training and validation loss')\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 204, 'page_label': '205'}, page_content=\"plt.title('Training and validation loss')\\nplt.legend()\\nplt.show()\\nListing 6.14 Training and evaluation\\nListing 6.15 Plotting the results\\nFigure 6.5 Training and validation loss \\nwhen using pretrained word embeddings\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 205, 'page_label': '206'}, page_content='193Working with text data\\nThe model quickly starts overfitting, which is unsurprising given the small number of\\ntraining samples. Validation accuracy has high variance for the same reason, but it\\nseems to reach the high 50s.\\n Note that your mileage may vary: because you have so few training samples, perfor-\\nmance is heavily dependent on exactly which 200 samples you choose—and you’re\\nchoosing them at random. If this works po orly for you, try choosing a different ran-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 205, 'page_label': '206'}, page_content='dom set of 200 samples, for the sake of the exercise (in real life, you don’t get to\\nchoose your training data).\\n You can also train the same model with out loading the pretrained word embed-\\ndings and without freezing the embedding laye r. In that case, you’ll learn a task-\\nspecific embedding of the input tokens, which is generally more powerful than\\npretrained word embeddings when lots of da ta is available. But in this case, you have'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 205, 'page_label': '206'}, page_content=\"only 200 training samples. Let’s try it (see figures 6.7 and 6.8).\\nfrom keras.models import Sequential\\nfrom keras.layers import Embedding, Flatten, Dense\\nmodel = Sequential()\\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\\nmodel.add(Flatten())\\nmodel.add(Dense(32, activation='relu'))\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.summary()\\nmodel.compile(optimizer='rmsprop',\\nloss='binary_crossentropy',\\nmetrics=['acc'])\\nhistory = model.fit(x_train, y_train,\\nepochs=10,\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 205, 'page_label': '206'}, page_content='history = model.fit(x_train, y_train,\\nepochs=10,\\nbatch_size=32,\\nvalidation_data=(x_val, y_val))\\nListing 6.16 Training the same model without pretrained word embeddings\\nFigure 6.6 Training and \\nvalidation accuracy when using \\npretrained word embeddings\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 206, 'page_label': '207'}, page_content=\"194 CHAPTER 6 Deep learning for text and sequences\\nValidation accuracy stalls in the low 50s. So  in this case, pretrained word embeddings\\noutperform jointly learned embeddings. If you increase the number of training sam-\\nples, this will quickly stop being the case—try it as an exercise.\\n Finally, let’s evaluate the model on the test data. First, you need to tokenize the test\\ndata.\\ntest_dir = os.path.join(imdb_dir, 'test')\\nlabels = []\\ntexts = []\\nfor label_type in ['neg', 'pos']:\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 206, 'page_label': '207'}, page_content=\"texts = []\\nfor label_type in ['neg', 'pos']:\\ndir_name = os.path.join(test_dir, label_type)\\nfor fname in sorted(os.listdir(dir_name)):\\nif fname[-4:] == '.txt':\\nf = open(os.path.join(dir_name, fname))\\ntexts.append(f.read())\\nListing 6.17 Tokenizing the data of the test set\\nFigure 6.7 Training and \\nvalidation loss without using \\npretrained word embeddings\\nFigure 6.8 Training and validation \\naccuracy without using pretrained \\nword embeddings\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 207, 'page_label': '208'}, page_content=\"195Working with text data\\nf.close()\\nif label_type == 'neg':\\nlabels.append(0)\\nelse:\\nlabels.append(1)\\nsequences = tokenizer.texts_to_sequences(texts)\\nx_test = pad_sequences(sequences, maxlen=maxlen)\\ny_test = np.asarray(labels)\\nNext, load and evaluate the first model.\\nmodel.load_weights('pre_trained_glove_model.h5')\\nmodel.evaluate(x_test, y_test)\\nYou get an appalling test accuracy of 56%.  Working with just a handful of training\\nsamples is difficult!\\n6.1.4 Wrapping up\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 207, 'page_label': '208'}, page_content='samples is difficult!\\n6.1.4 Wrapping up\\nNow you’re able to do the following:\\n\\uf0a1 Turn raw text into something a neural network can process\\n\\uf0a1 Use the Embedding layer in a Keras model to learn task-specific token embed-\\ndings\\n\\uf0a1 Use pretrained word embeddings to get an extra boost on small natural-\\nlanguage-processing problems \\nListing 6.18 Evaluating the model on the test set\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 208, 'page_label': '209'}, page_content='196 CHAPTER 6 Deep learning for text and sequences\\n6.2 Understanding recurrent neural networks\\nA major characteristic of all neural networks you’ve seen so far, such as densely con-\\nnected networks and convnets, is that th ey have no memory. Each input shown to\\nthem is processed independently, with no state kept in between inputs. With such net-\\nworks, in order to pr ocess a sequence or a temporal se ries of data points, you have to'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 208, 'page_label': '209'}, page_content='show the entire sequence to the network at on ce: turn it into a single data point. For\\ninstance, this is what you did in the IMDB example: an entire movie review was trans-\\nformed into a single large vector and processed in one go. Such networks are called\\nfeedforward networks.\\n In contrast, as you’re reading the presen t sentence, you’re pr ocessing it word by\\nword—or rather, eye saccade by eye saccade—while keeping memories of what came'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 208, 'page_label': '209'}, page_content='before; this gives you a fluid representation of the meaning conveyed by this sentence.\\nBiological intelligence processes information incrementally while maintaining an\\ninternal model of what it’s processing, built from past information and constantly\\nupdated as new information comes in.\\n A recurrent neural network  (RNN) adopts the same principle, albeit in an extremely\\nsimplified version: it processes sequences by iterating through the sequence elements'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 208, 'page_label': '209'}, page_content='and maintaining a state containing information relative\\nto what it has seen so far. In effect, an RNN is a type of\\nneural network that has an internal loop (see figure 6.9).\\nThe state of the RNN is reset between processing two dif-\\nferent, independent sequences (such as two different\\nIMDB reviews), so you still co nsider one sequence a sin-\\ngle data point: a single input to the network. What\\nchanges is that this data point is no longer processed in a'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 208, 'page_label': '209'}, page_content='single step; rather, the ne twork internally loops over\\nsequence elements.\\n To make these notions of loop and state clear, let’s implement the forward pass of a\\ntoy \\nRNN in Numpy. This RNN takes as input a sequence of vectors, which you’ll encode\\nas a 2D tensor of size (timesteps, input_features). It loops over timesteps, and at\\neach timestep, it consid ers its current state at t and the input at t (of shape (input_'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 208, 'page_label': '209'}, page_content='features,), and combines them to obtain the output at t. You’ll then set the state for\\nthe next step to be this previous output. For the first timestep, the previous output\\nisn’t defined; hence, there is no current stat e. So, you’ll initialize the state as an all-\\nzero vector called the initial state of the network.\\n In pseudocode, this is the RNN.\\nstate_t = 0\\nfor input_t in input_sequence:\\noutput_t = f(input_t, state_t)\\nstate_t = output_t\\nListing 6.19 Pseudocode RNN\\nThe state at t'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 208, 'page_label': '209'}, page_content='Listing 6.19 Pseudocode RNN\\nThe state at t\\nIterates over sequence elements\\nThe previous output becomes the state for the next iteration.\\nRNN\\nInput\\nOutput\\nRecurrent\\nconnection\\nFigure 6.9 A recurrent \\nnetwork: a network with a loop\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 209, 'page_label': '210'}, page_content='197Understanding recurrent neural networks\\nYou can even flesh out the function f: the transformation of the input and state into an\\noutput will be parameterized by two matrices, W and U, and a bias vector. It’s similar to\\nthe transformation operated by a densely connected layer in a feedforward network.\\nstate_t = 0\\nfor input_t in input_sequence:\\noutput_t = activation(dot(W, input_t) + dot(U, state_t) + b)\\nstate_t = output_t'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 209, 'page_label': '210'}, page_content='state_t = output_t\\nTo make these notions absolutely unambiguous, let’s write a naive Numpy implemen-\\ntation of the forward pass of the simple RNN.\\nimport numpy as np\\ntimesteps = 100\\ninput_features = 32\\noutput_features = 64\\ninputs = np.random.random((timesteps, input_features))\\nstate_t = np.zeros((output_features,))\\nW = np.random.random((output_features, input_features))\\nU = np.random.random((output_features, output_features))\\nb = np.random.random((output_features,))\\nsuccessive_outputs = []'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 209, 'page_label': '210'}, page_content='successive_outputs = []\\nfor input_t in inputs:\\noutput_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\\nsuccessive_outputs.append(output_t)\\nstate_t = output_t\\nfinal_output_sequence = np.concatenate(successive_outputs, axis=0)\\nEasy enough: in summary, an RNN is a for loop that reuses quantities computed\\nduring the previous iteration of the loop , nothing more. Of course, there are many\\ndifferent RNNs fitting this definition that you co uld build—this example is one of the'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 209, 'page_label': '210'}, page_content='simplest RNN formulations. RNNs are characterized by their step function, such as the\\nfollowing function in this case (see figure 6.10):\\noutput_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\\nListing 6.20 More detailed pseudocode for the RNN\\nListing 6.21 Numpy implementation of a simple RNN\\nNumber of timesteps in \\nthe input sequence Dimensionality of the \\ninput feature space\\nDimensionality of the \\noutput feature space\\nInput data: random \\nnoise for the sake of \\nthe example'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 209, 'page_label': '210'}, page_content='noise for the sake of \\nthe example\\nInitial state: an \\nall-zero vector\\nCreates random \\nweight matrices\\ninput_t is a vector of \\nshape (input_features,).\\nCombines the input with the current \\nstate (the previous output) to obtain \\nthe current output\\nStores this output in a list\\nUpdates the state of the\\nnetwork for the next timestep\\nThe final output is a 2D tensor of\\nshape (timesteps, output_features).\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 210, 'page_label': '211'}, page_content='198 CHAPTER 6 Deep learning for text and sequences\\nNOTE In this example, the final outp ut is a 2D tensor of shape (timesteps,\\noutput_features), where each timestep is the output of the loop at time t.\\nEach timestep t in the output tensor contai ns information about timesteps 0\\nto t in the input sequence—about the entire past. For this reason, in many\\ncases, you don’t need this full sequence of outputs; you just need the last out-'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 210, 'page_label': '211'}, page_content='put (output_t at the end of the loop), because it already contains informa-\\ntion about the entire sequence.\\n6.2.1 A recurrent layer in Keras\\nThe process you just naively implemented in  Numpy corresponds to an actual Keras\\nlayer—the SimpleRNN layer:\\nfrom keras.layers import SimpleRNN\\nThere is one minor difference: SimpleRNN processes batches of sequences, like all other\\nKeras layers, not a single sequence as in the Numpy example. This means it takes inputs'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 210, 'page_label': '211'}, page_content='of shape (batch_size, timesteps, input_features), rather than (timesteps,\\ninput_features).\\n Like all recurrent layers in Keras, SimpleRNN can be run in two different modes: it\\ncan return either the full sequences of successive outputs for each timestep (a 3D ten-\\nsor of shape (batch_size, timesteps, output_features)) or only the last output for\\neach input sequence (a 2D tensor of shape (batch_size, output_features)). These'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 210, 'page_label': '211'}, page_content='two modes are controlled by the return_sequences constructor argument. Let’s look\\nat an example that uses SimpleRNN and returns only the output at the last timestep:\\n>>> from keras.models import Sequential\\n>>> from keras.layers import Embedding, SimpleRNN\\n>>> model = Sequential()\\n>>> model.add(Embedding(10000, 32))\\n>>> model.add(SimpleRNN(32))\\n>>> model.summary()\\n...\\noutput t-1 output t output t+1\\ninput t-1 input t input t+1\\n...\\nState t State t+1\\noutput_t =\\n activation(\\n  W•input_t +'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 210, 'page_label': '211'}, page_content='output_t =\\n activation(\\n  W•input_t +\\n  U•state_t +\\n  bo)\\nFigure 6.10 A simple RNN, unrolled over time\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 211, 'page_label': '212'}, page_content='199Understanding recurrent neural networks\\n________________________________________________________________\\nLayer (type) Output Shape Param #\\n================================================================\\nembedding_22 (Embedding) (None, None, 32) 320000\\n________________________________________________________________\\nsimplernn_10 (SimpleRNN) (None, 32) 2080\\n================================================================\\nTotal params: 322,080\\nTrainable params: 322,080\\nNon-trainable params: 0'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 211, 'page_label': '212'}, page_content='Trainable params: 322,080\\nNon-trainable params: 0\\nThe following example returns the full state sequence:\\n>>> model = Sequential()\\n>>> model.add(Embedding(10000, 32))\\n>>> model.add(SimpleRNN(32, return_sequences=True))\\n>>> model.summary()\\n________________________________________________________________\\nLayer (type) Output Shape Param #\\n================================================================\\nembedding_23 (Embedding) (None, None, 32) 320000'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 211, 'page_label': '212'}, page_content='embedding_23 (Embedding) (None, None, 32) 320000\\n________________________________________________________________\\nsimplernn_11 (SimpleRNN) (None, None, 32) 2080\\n================================================================\\nTotal params: 322,080\\nTrainable params: 322,080\\nNon-trainable params: 0\\nIt’s sometimes useful to stack several recu rrent layers one after the other in order to\\nincrease the representational power of a netw ork. In such a setup, you have to get all'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 211, 'page_label': '212'}, page_content='of the intermediate layers to return full sequence of outputs:\\n>>> model = Sequential()\\n>>> model.add(Embedding(10000, 32))\\n>>> model.add(SimpleRNN(32, return_sequences=True))\\n>>> model.add(SimpleRNN(32, return_sequences=True))\\n>>> model.add(SimpleRNN(32, return_sequences=True))\\n>>> model.add(SimpleRNN(32))\\n>>> model.summary()\\n________________________________________________________________\\nLayer (type) Output Shape Param #\\n================================================================'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 211, 'page_label': '212'}, page_content='embedding_24 (Embedding) (None, None, 32) 320000\\n________________________________________________________________\\nsimplernn_12 (SimpleRNN) (None, None, 32) 2080\\n________________________________________________________________\\nsimplernn_13 (SimpleRNN) (None, None, 32) 2080\\n________________________________________________________________\\nsimplernn_14 (SimpleRNN) (None, None, 32) 2080\\n________________________________________________________________\\nsimplernn_15 (SimpleRNN) (None, 32) 2080'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 211, 'page_label': '212'}, page_content='simplernn_15 (SimpleRNN) (None, 32) 2080\\n================================================================\\nTotal params: 328,320\\nTrainable params: 328,320\\nNon-trainable params: 0\\nLast layer only returns \\nthe last output\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 212, 'page_label': '213'}, page_content=\"200 CHAPTER 6 Deep learning for text and sequences\\nNow, let’s use such  a model on the IMDB movie-review-classifica tion problem. First,\\npreprocess the data.\\nfrom keras.datasets import imdb\\nfrom keras.preprocessing import sequence\\nmax_features = 10000\\nmaxlen = 500\\nbatch_size = 32\\nprint('Loading data...')\\n(input_train, y_train), (input_test, y_test) = imdb.load_data(\\nnum_words=max_features)\\nprint(len(input_train), 'train sequences')\\nprint(len(input_test), 'test sequences')\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 212, 'page_label': '213'}, page_content=\"print(len(input_test), 'test sequences')\\nprint('Pad sequences (samples x time)')\\ninput_train = sequence.pad_sequences(input_train, maxlen=maxlen)\\ninput_test = sequence.pad_sequences(input_test, maxlen=maxlen)\\nprint('input_train shape:', input_train.shape)\\nprint('input_test shape:', input_test.shape)\\nLet’s train a simple recu rrent network using an Embedding layer and a SimpleRNN\\nlayer.\\nfrom keras.layers import Dense\\nmodel = Sequential()\\nmodel.add(Embedding(max_features, 32))\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 212, 'page_label': '213'}, page_content=\"model.add(Embedding(max_features, 32))\\nmodel.add(SimpleRNN(32))\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\\nhistory = model.fit(input_train, y_train,\\nepochs=10,\\nbatch_size=128,\\nvalidation_split=0.2)\\nNow, let’s display the training and validation loss and accuracy (see figures 6.11 and 6.12).\\nimport matplotlib.pyplot as plt\\nacc = history.history['acc']\\nval_acc = history.history['val_acc']\\nloss = history.history['loss']\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 212, 'page_label': '213'}, page_content=\"loss = history.history['loss']\\nval_loss = history.history['val_loss']\\nepochs = range(1, len(acc) + 1)\\nplt.plot(epochs, acc, 'bo', label='Training acc')\\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\\nListing 6.22 Preparing the IMDB data\\nListing 6.23 Training the model with Embedding and SimpleRNN layers\\nListing 6.24 Plotting results\\nNumber of words to \\nconsider as features\\nCuts off texts after this many words (among \\nthe max_features most common words)\\nLicensed to   <null>\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 213, 'page_label': '214'}, page_content=\"201Understanding recurrent neural networks\\nplt.title('Training and validation accuracy')\\nplt.legend()\\nplt.figure()\\nplt.plot(epochs, loss, 'bo', label='Training loss')\\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\\nplt.title('Training and validation loss')\\nplt.legend()\\nplt.show()\\nAs a reminder, in chapter 3, the first naive approach to this dataset got you to a test\\naccuracy of 88%. Unfortunately, this sma ll recurrent network doesn’t perform well\"),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 213, 'page_label': '214'}, page_content='compared to this baseline (only 85% validat ion accuracy). Part of the problem is that\\nyour inputs only consider the first 500 word s, rather than full sequences—hence, the\\nRNN has access to less information than the earlier baseline model. The remainder of\\nthe problem is that SimpleRNN isn’t good at processing lo ng sequences, such as text.\\nFigure 6.11 Training and validation \\nloss on IMDB with SimpleRNN\\nFigure 6.12 Training and validation \\naccuracy on IMDB with SimpleRNN\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 214, 'page_label': '215'}, page_content='202 CHAPTER 6 Deep learning for text and sequences\\nOther types of recurrent layers perform much better. Let’s l ook at some more-\\nadvanced layers. \\n6.2.2 Understanding the LSTM and GRU layers\\nSimpleRNN isn’t the only recurrent layer available in Keras. There are two others: LSTM\\nand GRU. In practice, you’ll always use one of these, because SimpleRNN is generally too\\nsimplistic to be of real use. SimpleRNN has a major issue: although it should theoretically'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 214, 'page_label': '215'}, page_content='be able to retain at time t information about inputs s een many timesteps before, in\\npractice, such long-term dependencies are impossible to learn. This is due to the van-\\nishing gradient problem , an effect that is similar to what is observed with non-recurrent\\nnetworks (feedforward networks) that are many layers deep: as you keep adding layers\\nto a network, the network eventually becomes untrainable. The theoretical reasons for'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 214, 'page_label': '215'}, page_content='this effect were studied by Hochreiter, Sc hmidhuber, and Bengio in the early 1990s. 2\\nThe LSTM and GRU layers are designed to solve this problem.\\n Let’s consider the LSTM layer. The underlying Lo ng Short-Term Memory ( LSTM)\\nalgorithm was developed by Hoch reiter and Schmidhuber in 1997; 3 it was the culmi-\\nnation of their research on the vanishing gradient problem.\\n This layer is a variant of the SimpleRNN layer you already know about; it adds a way'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 214, 'page_label': '215'}, page_content='to carry information across many timesteps.  Imagine a conveyor belt running parallel\\nto the sequence you’re processing. Information from the sequence can jump onto the\\nconveyor belt at any point, be transported to a later time step, and jump off, intact,\\nwhen you need it. This is essentially what LSTM does: it saves information for later,\\nthus preventing older signals from gradually vanishing during processing.'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 214, 'page_label': '215'}, page_content='To understand this in detail, let’s start from the SimpleRNN cell (see figure 6.13).\\nBecause you’ll have a lot of weight matrices, index the W and U matrices in the cell with\\nthe letter o (Wo and Uo) for output.\\n2 See, for example, Yoshua Bengio, Patrice Simard, and Paolo Frasconi, “Learning Long-Term Dependencies\\nwith Gradient Descent Is Difficult,” IEEE Transactions on Neural Networks 5, no. 2 (1994).'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 214, 'page_label': '215'}, page_content='3 Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” Neural Computation 9, no. 8 (1997).\\n...\\noutput t-1 output t output t+1\\ninput t-1 input t input t+1\\n...\\nState t State t+1\\noutput_t =\\n activation(\\n  Wo•input_t +\\n  Uo•state_t +\\n  bo)\\nFigure 6.13 The starting point of an LSTM layer: a SimpleRNN\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 215, 'page_label': '216'}, page_content='203Understanding recurrent neural networks\\nLet’s add to this picture an additional data  flow that carries information across time-\\nsteps. Call its values at different timesteps Ct, where C stands for carry. This informa-\\nt i o n  w i l l  h a v e  t h e  f o l l o w i n g  i m p a c t  o n  t h e  c e l l :  i t  w i l l  b e  c o m b i n e d  w i t h  t h e  i n p u t\\nconnection and the recurrent connection (via  a dense transformation: a dot product'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 215, 'page_label': '216'}, page_content='with a weight matrix followed  by a bias add and the applic ation of an activation func-\\ntion), and it will affect the state being sent  to the next timestep (via an activation\\nfunction an a multiplication  operation). Conceptually, th e carry dataflow is a way to\\nmodulate the next output and the next state (see figure 6.14). Simple so far.\\nNow the subtlety: the way the next value of the carry dataflow is computed. It involves\\nthree distinct transformations. All three have the form of a'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 215, 'page_label': '216'}, page_content='SimpleRNN cell:\\ny = activation(dot(state_t, U) + dot(input_t, W) + b)\\nBut all three transformations have their ow n weight matrices, which you’ll index with\\nthe letters i, f, and k. Here’s what you have so far (it may seem a bit arbitrary, but bear\\nwith me).\\noutput_t = activation(dot(state_t, Uo) + dot(input_t, Wo) + dot(C_t, Vo) + bo)\\ni_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)\\nf_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 215, 'page_label': '216'}, page_content='k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)\\nYou obtain the new carry state (the next c_t) by combining i_t, f_t, and k_t.\\nc_t+1 = i_t * k_t + c_t * f_t\\nAdd this as shown in figure 6.15. And that’s it. Not so complicated—merely a tad\\ncomplex.\\nListing 6.25 Pseudocode details of the LSTM architecture (1/2)\\nListing 6.26 Pseudocode details of the LSTM architecture (2/2)\\n...\\noutput t-1 output t output t+1\\ninput t-1 input t input t+1\\n...\\nState t State t+1\\nCarry trackc t+1c t\\nc t c t'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 215, 'page_label': '216'}, page_content='...\\nState t State t+1\\nCarry trackc t+1c t\\nc t c t\\nc t-1\\noutput_t =\\n activation(\\n  Wo•input_t +\\n  Uo•state_t +\\n  Vo•c_t + \\n  bo)\\nFigure 6.14 Going from a SimpleRNN to an LSTM: adding a carry track\\nLicensed to   <null>'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 216, 'page_label': '217'}, page_content='204 CHAPTER 6 Deep learning for text and sequences\\n \\nIf you want to get philosophical, you can interpret what each of these operations is\\nmeant to do. For instance, you can say that multiplying c_t and f_t is a way to deliber-\\nately forget irrelevant information in the carry dataflow. Meanwhile, i_t and k_t pro-\\nvide information about the present, updat ing the carry track with new information.\\nBut at the end of the day, these interpretations don’t mean much, because what these'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 216, 'page_label': '217'}, page_content='operations actually d o  i s  d e t e r m i n e d  b y  t h e  c o n t e n t s  o f  t h e  w e i g h t s  p a r a m e t e r i z i n g\\nthem; and the weights are learned in an end- to-end fashion, starting over with each\\ntraining round, making it impossible to credit this or that operation with a specific\\npurpose. The specification of an RNN cell (as just described) determines your hypoth-\\nesis space—the space in which you’ll sear ch for a good model configuration during'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 216, 'page_label': '217'}, page_content='training—but it doesn’t determine what the cell does; that is up to the cell weights.\\nThe same cell with different weights can be doing very different things. So the combi-\\nnation of operations making up an RNN cell is better interpreted as a set of constraints\\non your search, not as a design in an engineering sense.\\n To a researcher, it seems that the choice of such constraints—the question of how to\\nimplement RNN cells—is better left to optimization algorithms (like genetic algorithms'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 216, 'page_label': '217'}, page_content='or reinforcement learning processes) than to human engineers. And in the future,\\nthat’s how we’ll build networks. In summary : you don’t need to understand anything\\nabout the specific architecture of an LSTM cell; as a human, it shouldn’t be your job to\\nunderstand it. Just keep in mind what the LSTM cell is meant to do: allow past informa-\\ntion to be reinjected at a later time, thus fighting the vanishing-gradient problem. \\n6.2.3 A concrete LSTM example in Keras'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 216, 'page_label': '217'}, page_content='6.2.3 A concrete LSTM example in Keras\\nNow let’s switch to more practical co ncerns: you’ll set up a model using an LSTM layer\\nand train it on the IMDB data (see figures 6.16 and 6.17). The network is similar to the\\none with SimpleRNN that was just presented. You on ly specify the output dimensional-\\nity of the LSTM layer; leave every other argument (there are many) at the Keras\\n...\\noutput t-1 output t output t+1\\ninput t-1 input t input t+1\\n...\\nState t State t+1\\nCarry trackc t+1c t\\nc t c t'),\n",
       " Document(metadata={'producer': 'PDF-XChange (PDFTools4.exe v4.0.0201.0000) (Windows XP)', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-12-26T08:46:48+05:30', 'pxcviewerinfo': \"PDF-XChange Viewer;2.5.201.0;Jan 23 2012;21:08:47;D:20171227113728+01'00'\", 'source': 'Deep Learning with Python - François Chollet - Manning (2018).pdf', 'total_pages': 373, 'page': 216, 'page_label': '217'}, page_content='...\\nState t State t+1\\nCarry trackc t+1c t\\nc t c t\\nc t-1\\noutput_t =\\n activation(\\n  Wo•input_t +\\n  Uo•state_t +\\n  Vo•c_t + \\n  bo)\\nCompute \\nnew \\ncarry\\nCompute \\nnew \\ncarry\\nFigure 6.15 Anatomy of an LSTM\\nLicensed to   <null>'),\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_transformed_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
